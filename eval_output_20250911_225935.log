=========================================
Starting evaluations at Thu Sep 11 22:59:35 PDT 2025
=========================================

Running evaluation with sequential_eval and calibrate_prefix_rms...
Command 1 started at Thu Sep 11 22:59:35 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:495: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:805: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.699817300397296, 'neutral_chat': 11.828901409770166, 'llama_chat': 11.748646440727766} | picked=raw
Saved Z[llama_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/Z_llama_raw.pt
[calib:llama] prefix_rms=0.29690 -> embed_rms=0.01491
[debug:llama] adapter.scale=1.0015 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0149 prefix.mean||=0.6745 | embed.RMS=0.0149
Saved Llama results to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:906: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': nan, 'neutral_chat': nan, 'qwen_chat': nan} | picked=raw
Saved Z[qwen_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/Z_qwen_raw.pt
[calib:qwen] prefix_rms=0.29945 -> embed_rms=0.01523
[debug:qwen] adapter.scale=1.0006 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0152 prefix.mean||=0.4556 | embed.RMS=0.0152
Saved Qwen results to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): nan
Wall clock: 8.17s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.001  |  NLL/token (gold): 9.782721981669582
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): nan
Wall clock: 9.17s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 6.99s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.001
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.001

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 8.166573762893677,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": NaN
    }
  },
  "latent": {
    "wall_clock_sec": 9.168784856796265,
    "llama": {
      "em": 0.0,
      "f1": 0.00125,
      "nll_token": 9.782721981669582
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": NaN
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 6.98961615562439,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.00125,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0014615058898926,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.014908921904861927,
      "prefix_mean_norm": 0.6745172142982483,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "raw"
    },
    "qwen": {
      "adapter_scale": 1.0005558729171753,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.015229102224111557,
      "prefix_mean_norm": 0.4556165933609009,
      "embed_rms": 0.01522822491824627,
      "encoder_text_mode": "raw"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 0.5,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.00125
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250911_211615/squad_eval_se_nc/predictions.jsonl

=========================================
Starting prefix_gain sweep...
=========================================

Running evaluation with prefix_gain=0.5...
Started at Thu Sep 11 23:00:43 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:495: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:805: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.699817300397296, 'neutral_chat': 11.828901409770166, 'llama_chat': 11.748646440727766} | picked=raw
Saved Z[llama_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/Z_llama_raw.pt
[calib:llama] prefix_rms=0.29690 -> embed_rms=0.01491
[debug:llama] adapter.scale=1.0015 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0149 prefix.mean||=0.6745 | embed.RMS=0.0149
Saved Llama results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:906: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': nan, 'neutral_chat': nan, 'qwen_chat': nan} | picked=raw
Saved Z[qwen_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/Z_qwen_raw.pt
[calib:qwen] prefix_rms=0.29945 -> embed_rms=0.01524
[debug:qwen] adapter.scale=1.0006 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0152 prefix.mean||=0.4559 | embed.RMS=0.0152
Saved Qwen results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): nan
Wall clock: 7.76s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.005  |  NLL/token (gold): 9.782721981669582
Qwen   EM: 0.000   F1: 0.002   |  NLL/token (gold): nan
Wall clock: 9.30s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 6.90s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.007

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 7.764987945556641,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": NaN
    }
  },
  "latent": {
    "wall_clock_sec": 9.297197580337524,
    "llama": {
      "em": 0.0,
      "f1": 0.004519230769230769,
      "nll_token": 9.782721981669582
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002,
      "nll_token": NaN
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 6.898752212524414,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004519230769230769,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0014615058898926,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.014908921904861927,
      "prefix_mean_norm": 0.6745172142982483,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "raw"
    },
    "qwen": {
      "adapter_scale": 1.0005558729171753,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.015238205902278423,
      "prefix_mean_norm": 0.45588889718055725,
      "embed_rms": 0.015223497524857521,
      "encoder_text_mode": "raw"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 0.5,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.006519230769230769
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain0.5/predictions.jsonl

Completed prefix_gain=0.5 at Thu Sep 11 23:01:51 PDT 2025
----------------------------------------

Running evaluation with prefix_gain=1...
Started at Thu Sep 11 23:01:51 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:495: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:805: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.699817300397296, 'neutral_chat': 11.828901409770166, 'llama_chat': 11.748646440727766} | picked=raw
Saved Z[llama_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/Z_llama_raw.pt
[calib:llama] prefix_rms=0.59380 -> embed_rms=0.01491
[debug:llama] adapter.scale=1.0015 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0149 prefix.mean||=0.6745 | embed.RMS=0.0149
Saved Llama results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:906: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': nan, 'neutral_chat': nan, 'qwen_chat': nan} | picked=raw
Saved Z[qwen_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/Z_qwen_raw.pt
[calib:qwen] prefix_rms=0.59891 -> embed_rms=0.01523
[debug:qwen] adapter.scale=1.0006 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0152 prefix.mean||=0.4557 | embed.RMS=0.0152
Saved Qwen results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): nan
Wall clock: 8.00s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.003  |  NLL/token (gold): 9.782721981669582
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): nan
Wall clock: 9.69s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 6.78s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.003
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.003

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 8.004377126693726,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": NaN
    }
  },
  "latent": {
    "wall_clock_sec": 9.68968653678894,
    "llama": {
      "em": 0.0,
      "f1": 0.0025,
      "nll_token": 9.782721981669582
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": NaN
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 6.779505252838135,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0025,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0014615058898926,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.014908921904861927,
      "prefix_mean_norm": 0.6745172142982483,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "raw"
    },
    "qwen": {
      "adapter_scale": 1.0005558729171753,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.015232441946864128,
      "prefix_mean_norm": 0.45571646094322205,
      "embed_rms": 0.015231349505484104,
      "encoder_text_mode": "raw"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0025
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain1/predictions.jsonl

Completed prefix_gain=1 at Thu Sep 11 23:03:01 PDT 2025
----------------------------------------

Running evaluation with prefix_gain=2...
Started at Thu Sep 11 23:03:01 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:495: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:805: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.699817300397296, 'neutral_chat': 11.828901409770166, 'llama_chat': 11.748646440727766} | picked=raw
Saved Z[llama_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/Z_llama_raw.pt
[calib:llama] prefix_rms=1.18761 -> embed_rms=0.01491
[debug:llama] adapter.scale=1.0015 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0149 prefix.mean||=0.6745 | embed.RMS=0.0149
Saved Llama results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:906: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': nan, 'neutral_chat': nan, 'qwen_chat': nan} | picked=raw
Saved Z[qwen_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/Z_qwen_raw.pt
[calib:qwen] prefix_rms=1.19782 -> embed_rms=0.01523
[debug:qwen] adapter.scale=1.0006 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0152 prefix.mean||=0.4557 | embed.RMS=0.0152
Saved Qwen results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): nan
Wall clock: 8.00s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.004  |  NLL/token (gold): 9.782721981669582
Qwen   EM: 0.000   F1: 0.002   |  NLL/token (gold): nan
Wall clock: 9.18s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 7.17s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.006

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 7.996275424957275,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": NaN
    }
  },
  "latent": {
    "wall_clock_sec": 9.178614854812622,
    "llama": {
      "em": 0.0,
      "f1": 0.00426923076923077,
      "nll_token": 9.782721981669582
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002,
      "nll_token": NaN
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 7.167890310287476,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.00426923076923077,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0014615058898926,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.014908921904861927,
      "prefix_mean_norm": 0.6745172142982483,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "raw"
    },
    "qwen": {
      "adapter_scale": 1.0005558729171753,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.015230449847877026,
      "prefix_mean_norm": 0.45565688610076904,
      "embed_rms": 0.015231391414999962,
      "encoder_text_mode": "raw"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 2.0,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.006269230769230769
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain2/predictions.jsonl

Completed prefix_gain=2 at Thu Sep 11 23:04:09 PDT 2025
----------------------------------------

Running evaluation with prefix_gain=4...
Started at Thu Sep 11 23:04:09 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:495: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:805: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:822: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.699817300397296, 'neutral_chat': 11.828901409770166, 'llama_chat': 11.748646440727766} | picked=raw
Saved Z[llama_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/Z_llama_raw.pt
[calib:llama] prefix_rms=2.37522 -> embed_rms=0.01491
[debug:llama] adapter.scale=1.0015 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0149 prefix.mean||=0.6745 | embed.RMS=0.0149
Saved Llama results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:906: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': nan, 'neutral_chat': nan, 'qwen_chat': nan} | picked=raw
Saved Z[qwen_raw] to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/Z_qwen_raw.pt
[calib:qwen] prefix_rms=2.39564 -> embed_rms=0.01523
[debug:qwen] adapter.scale=1.0006 | Z.std=0.9996 Z.mean||=15.9930 | prefix.std=0.0152 prefix.mean||=0.4556 | embed.RMS=0.0152
Saved Qwen results to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): nan
Wall clock: 7.97s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.005  |  NLL/token (gold): 9.782721981669582
Qwen   EM: 0.000   F1: 0.002   |  NLL/token (gold): nan
Wall clock: 9.21s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 6.90s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.006
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.006

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 7.969381809234619,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": NaN
    }
  },
  "latent": {
    "wall_clock_sec": 9.212482452392578,
    "llama": {
      "em": 0.0,
      "f1": 0.005166666666666666,
      "nll_token": 9.782721981669582
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002,
      "nll_token": NaN
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 6.902597665786743,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0055000000000000005,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0014615058898926,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.014908921904861927,
      "prefix_mean_norm": 0.6745172142982483,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "raw"
    },
    "qwen": {
      "adapter_scale": 1.0005558729171753,
      "Z_std": 0.9995617270469666,
      "Z_mean_norm": 15.992977142333984,
      "prefix_std": 0.015227808617055416,
      "prefix_mean_norm": 0.4555778503417969,
      "embed_rms": 0.015239818952977657,
      "encoder_text_mode": "raw"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 4.0,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0055000000000000005
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250911_211615/squad_eval_gain4/predictions.jsonl

Completed prefix_gain=4 at Thu Sep 11 23:05:18 PDT 2025
----------------------------------------

=========================================
All evaluations completed at Thu Sep 11 23:05:18 PDT 2025
=========================================
