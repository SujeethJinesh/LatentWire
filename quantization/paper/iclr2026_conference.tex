\documentclass{article} % For LaTeX2e
\usepackage{iclr2026_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}

\title{Quantized Cache-to-Cache: Communication-Budgeted KV Transfer for Heterogeneous LLMs}

% Anonymous for submission.
\author{Anonymous Authors}

% \iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
We study communication-efficient cache transfer between heterogeneous LLMs using Cache-to-Cache (C2C) under explicit byte budgets. We quantify precision and length trade-offs (INT8/INT4 PTQ and cache pruning) and introduce two budgeted transfer mechanisms: receiver-aware delta token selection and rate--distortion (RD) token$\times$precision scheduling. On OpenBookQA and ARC-C, INT8/INT4 are nearly lossless, back-pruning dominates front-pruning, delta selection improves accuracy at fixed token budgets (e.g., +2.8/+5.2 points at $p=0.25$), and RD scheduling matches or slightly improves fixed-precision baselines at 1/16--1/8 budgets with moderate overhead. We release a reproducible pipeline for evaluating cross-model cache communication.
\end{abstract}

\section{Introduction}
Large language models (LLMs) often communicate via text, which is slow and lossy for distributed agents. Cache-to-Cache (C2C) communicates by projecting and fusing KV caches, but practical deployments face strict bandwidth and latency constraints. We ask: \emph{How low can KV precision go before accuracy collapses, and under a fixed byte budget what should be transmitted?}

\paragraph{Contributions.}
\begin{itemize}[leftmargin=*]
  \item We build a precision-aware C2C evaluation pipeline and quantify INT8/INT4 PTQ plus cache-length reduction under equal-byte budgets.
  \item We introduce receiver-aware delta token selection and RD token$\times$precision scheduling, with a formal budgeted formulation.
  \item We report accuracy--bytes curves, system-level timing measurements, and stability shards to characterize accuracy and overhead.
  \item We release a reproducible benchmark setup and analysis scripts for extending C2C under communication constraints.
\end{itemize}

\section{Background and Motivation}
C2C projects sharer KV caches into receiver space and fuses them with learned gates, preserving rich semantics compared to text relay. However, KV caches are large: they scale with sequence length, KV heads, and head dimension. Quantization and cache-length reduction can shrink the communication footprint while retaining accuracy. This work reframes C2C through a communication-budget lens.

\section{Related Work}
\textbf{C2C.} Cache-to-Cache (C2C) enables direct semantic communication by projecting and fusing a sharer model's KV cache into a receiver's KV cache with learnable gates, avoiding intermediate text generation \citep{c2c}.

\textbf{KV communication across agents.} KVComm aligns KV caches across diverging prefixes using training-free offset correction with online anchors \citep{kvcomm}. Q-KVComm adds adaptive layer-wise quantization, hybrid information extraction, and heterogeneous calibration for compressed KV transfer \citep{qkvcomm}. These works focus on multi-agent cache reuse/compression; our work studies quantization and cache-length pruning within the C2C projector+fuser pipeline.

\textbf{Latent collaboration and cache alignment.} KV cache alignment learns a shared latent space with adapters to align KV caches across models \citep{kvalign}. LatentMAS enables latent-space collaboration with shared working memory without extra training \citep{latentmas}. Our approach stays within C2C's KV fusion but emphasizes communication budgets and precision/length tradeoffs.

\textbf{Token selection and KV compression.} Token-level KV selection and value-norm importance improve long-context inference for a single model (ZipCache, TokenSelect, VATP) \citep{zipcache,tokenselect,vatp}. We adopt the budget perspective for C2C rather than single-model KV compression.

\section{Method}
\subsection{C2C Recap}
Let the sharer model produce KV caches $(K^S_\ell,V^S_\ell)$ and the receiver produce $(K^R_\ell,V^R_\ell)$ at layer $\ell$. C2C projects sharer KV into receiver space via $\Pi^K_\ell,\Pi^V_\ell$ and fuses them through a learnable gate:
\[
(K^{R\prime}_\ell,V^{R\prime}_\ell)=\mathcal{F}_\ell\left(K^R_\ell,V^R_\ell,\Pi^K_\ell(K^S_\ell),\Pi^V_\ell(V^S_\ell)\right).
\]
This avoids intermediate text and transfers richer internal semantics.

\subsection{Post-Training Quantization (PTQ)}
We quantize the KV caches using INT8 or INT4/NF4 with per-head scaling. We evaluate accuracy and latency under fixed precision budgets. Our current implementation uses fake-quant (quantize then dequantize) to model quantization noise without bit-packing.

\subsection{Cache-Length Reduction}
We prune KV tokens using a fixed ratio (e.g., 50\%, 25\%, 10\%), reducing transmitted bytes further. We evaluate front-pruning and back-pruning to diagnose which instruction tokens are most valuable for cross-model transfer.

\subsection{Selective and Compressed Cache Transfer (SparseC2C)}
As a main-conference extension, we select a sparse subset of token positions to transfer and fuse. Let $I \subset \{1,\dots,T\}$ be selected tokens and $S_I$ the gather operator. We fuse only selected tokens and scatter updates back:
\[
(\tilde{K}^R_\ell, \tilde{V}^R_\ell) = S_I^\top (K^R_\ell, V^R_\ell),\quad
(\tilde{K}^S_\ell, \tilde{V}^S_\ell) = S_I^\top (K^S_\ell, V^S_\ell)
\]
\[
(\tilde{K}^{R\prime}_\ell, \tilde{V}^{R\prime}_\ell) =
\mathcal{F}_\ell\big(\tilde{K}^R_\ell, \tilde{V}^R_\ell, \Pi^K_\ell(\tilde{K}^S_\ell), \Pi^V_\ell(\tilde{V}^S_\ell)\big).
\]
We then scatter the update to the full cache. We use projector-aware token scoring by computing value norms in receiver space (\texttt{proj\_vnorm\_topk}), tying selection to the cross-model mapping.

\subsubsection{Receiver-Aware Delta Selection (M9)}
For sparse transfer, the receiver already has a cache baseline, so sending large-but-redundant tokens wastes bandwidth. We score each token by its \emph{marginal update} in receiver space:
\[
\Delta V^\ell_t = \widehat V^\ell_{:,:,t,:} - V^{R,\ell}_{:,:,t,:},\quad
u^\ell(t)=\mathbb{E}_{b,h}\left[\lVert \Delta V^\ell_{b,h,t}\rVert_2\right].
\]
We select the top-$k$ tokens by $u^\ell(t)$ under a token budget $|I_\ell|\le \lfloor pT\rfloor$:
\[
I_\ell=\operatorname{TopK}\big(u^\ell(t); \lfloor pT\rfloor\big).
\]
This \texttt{delta\_proj\_vnorm\_topk} score is projector-aware and redundancy-aware by construction.

\subsubsection{Rate--Distortion Token$\times$Precision Scheduling (M10)}
Under a fixed communication budget, we jointly select tokens and precisions. Each token $t$ chooses an action $a_t \in \{\text{drop}, \text{int4}, \text{int8}\}$ with rate $r(a_t)$ (bits/element). We minimize distortion under a byte budget:
\[
\min_{a_t} \sum_t D_t(a_t)\quad \text{s.t.}\quad \sum_t r(a_t)\le R_{\text{budget}},
\]
with $D_t(\text{drop})=\lVert \widehat V_t - V^R_t\rVert_2^2$ and $D_t(\text{int}b)=\lVert \widehat V_t - \widehat V^{(\text{int}b)}_t\rVert_2^2$. We implement a deterministic greedy allocator (RD-Greedy) that assigns INT8 to highest-utility tokens, then INT4, then drop, until the byte budget is met.

\subsection{Communication-Budget Curves}
We report accuracy as a function of transmitted bytes, enabling fair comparison under equal communication constraints. For a sequence of length $T$, the approximate bytes are
\[
{\rm bytes} \approx T \cdot p \cdot 2 \cdot L \cdot H_{kv} \cdot d_h \cdot b/8,
\]
where $p$ is the retained cache proportion, $L$ the number of layers, $H_{kv}$ KV heads, $d_h$ head dim, and $b$ bits per element. We use this accounting for consistent budget curves.

\section{Experiments}
\subsection{Setup}
We evaluate on OpenBookQA (500) and ARC-C (1150) with a Qwen3-0.6B receiver and Qwen2.5-0.5B-Instruct sharer. We follow the C2C eval protocol: greedy decoding, max\_new\_tokens 64, unified chat template, and no CoT (except for the GSM8K CoT ablation). All models are frozen; only the projector is trained when QAT is enabled. We report a hetero spot check with Qwen3$\rightarrow$Llama3.2-1B-Instruct using alignment-on.

\subsection{Main Results}
All results below are full runs. PTQ is effectively lossless relative to FP16, and cache pruning shows a strong front/back asymmetry.

\begin{table}[ht]
\centering
\caption{Baseline vs. PTQ (full-cache, \%).}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA & ARC-C \\
\midrule
FP16 baseline & 52.8 & 55.1 \\
INT8 PTQ & 52.8 & 55.0 \\
INT4 PTQ & 52.6 & 55.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{OpenBookQA accuracy (\%, 500 samples) for cache-length pruning (INT8).}
\begin{tabular}{lcccc}
\toprule
Order mode & 75\% & 50\% & 25\% & 10\% \\
\midrule
Front & 44.6 & 43.0 & 38.8 & 38.6 \\
Back  & 52.2 & 52.0 & 50.8 & 49.2 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{ARC-C accuracy (\%, 1150 samples) for cache-length pruning (INT8).}
\begin{tabular}{lcccc}
\toprule
Order mode & 75\% & 50\% & 25\% & 10\% \\
\midrule
Front & 40.2 & 46.3 & 38.3 & 40.7 \\
Back  & 55.7 & 57.2 & 56.2 & 53.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Communication-Budget Curve}
Figure~\ref{fig:budget-openbookqa} and Figure~\ref{fig:budget-arc} report accuracy versus effective transmitted bytes. Each point is annotated with the retained cache proportion. These curves provide a single, comparable view across precision (FP16/INT8/INT4) and cache-length reduction.
\begin{figure}[ht]
\centering
\includegraphics[width=0.98\linewidth]{../analysis/m4_budget_curve/budget_curve_openbookqa.png}
\caption{Accuracy vs. communication budget (OpenBookQA).}
\label{fig:budget-openbookqa}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.98\linewidth]{../analysis/m4_budget_curve/budget_curve_arc_c.png}
\caption{Accuracy vs. communication budget (ARC-C).}
\label{fig:budget-arc}
\end{figure}


\subsection{Order-Mode Ablation}
Across all cache lengths, \textbf{back-pruning} (keeping later instruction tokens) consistently outperforms \textbf{front-pruning}. At 50\% cache length, for example, back-pruning retains near-baseline accuracy while front-pruning degrades sharply. This suggests late instruction tokens carry higher utility for cross-model KV fusion, a useful design signal for future selective transfer methods.

\subsection{Receiver-Aware Selection and RD Scheduling}
We compare M9 delta selection against value-norm baselines at a fixed token budget ($p=0.25$, INT8, prompt-only). Delta selection improves accuracy over both vnorm and proj\_vnorm heuristics on both datasets (Table~\ref{tab:m9-select}).

\begin{table}[ht]
\centering
\caption{M9 selection at $p=0.25$ (INT8, base pair).}
\label{tab:m9-select}
\begin{tabular}{lcc}
\toprule
Token selector & OpenBookQA & ARC-C \\
\midrule
vnorm\_topk & 47.0 & 49.6 \\
proj\_vnorm\_topk & 46.2 & 52.6 \\
delta\_proj\_vnorm\_topk & \textbf{49.8} & \textbf{54.8} \\
\bottomrule
\end{tabular}
\end{table}

RD scheduling provides an additional budget axis. At 1/16 and 1/8 byte budgets, mixed-precision RD (drop+int4+int8) matches or slightly improves over drop+int8 (Table~\ref{tab:rd-ablate}), indicating that token-level precision allocation is competitive under tight budgets.

\begin{table}[ht]
\centering
\caption{M10 RD ablation (base pair).}
\label{tab:rd-ablate}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA & ARC-C \\
\midrule
RD 1/16 (drop+int8) & 52.6 & 57.2 \\
RD 1/16 (drop+int4+int8) & \textbf{53.4} & 57.0 \\
RD 1/8 (drop+int8) & 52.4 & 54.9 \\
RD 1/8 (drop+int4+int8) & 52.4 & 54.9 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Heterogeneous Pair Spot Check}
On a heterogeneous pair (Qwen3$\rightarrow$Llama3.2, alignment on), M9 delta selection and M10 RD scheduling remain viable (Table~\ref{tab:hetero}).

\begin{table}[ht]
\centering
\caption{Hetero spot check (alignment on).}
\label{tab:hetero}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA & ARC-C \\
\midrule
M9 delta (p=0.25) & 40.0 & 44.6 \\
M10 RD (1/8) & 40.4 & 46.0 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{System Metrics}
We report end-to-end evaluation time on a single H100 with timing synchronization enabled. M10 incurs additional overhead from token$\times$precision scheduling relative to M9 (Table~\ref{tab:sysmetrics}).
\begin{table}[ht]
\centering
\caption{End-to-end timing (seconds; per-sample in parentheses).}
\label{tab:sysmetrics}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA (500) & ARC-C (1150) \\
\midrule
M9 delta (p=0.25) & 279.5 (0.56) & 675.1 (0.59) \\
M10 RD (1/8) & 412.2 (0.82) & 1056.9 (0.92) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Additional Extensions}
Mixed precision (INT8 with FP16 in the last layers) remains near baseline across last-2/last-4/last-8 schedules. Projector-only QAT (INT8) currently degrades accuracy (39.6/40.2), indicating that longer training or recipe tuning is needed. An alignment-only ablation (same model pair, alignment enabled) reduces accuracy, suggesting alignment should be reserved for heterogeneous pairs.
\begin{table}[ht]
\centering
\caption{Additional extension results (\% accuracy).}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA & ARC-C \\
\midrule
Mixed precision (INT8 + last-2 FP16) & 53.0 & 55.0 \\
Mixed precision (INT8 + last-4 FP16) & 52.8 & 55.3 \\
Mixed precision (INT8 + last-8 FP16) & 52.4 & 55.2 \\
QAT (projector-only, INT8) & 39.6 & 40.2 \\
Alignment ablation (same pair) & 46.8 & 49.6 \\
Hetero pair (Qwen3$\rightarrow$Llama3.2, align on) & 44.2 & 47.8 \\
\bottomrule
\end{tabular}
\end{table}
\paragraph{GSM8K with CoT.}
On GSM8K, CoT prompting remains challenging at this model scale. M10 RD (1/8 budget) improves accuracy over M9 delta (p=0.25) by +1.44 points (4.25\% vs 2.81\%), but both remain far below competitive levels.

\section{Discussion}
Quantized C2C provides large bandwidth reductions with limited accuracy drop. Receiver-aware delta selection consistently improves low-budget accuracy, and RD-C2C achieves comparable performance at fixed byte budgets while increasing evaluation time by roughly 1.5$\times$ relative to delta selection. Shard-based repeats show low variance (std $\leq$ 0.02), supporting robustness at tight budgets. These results highlight a clear compute--communication tradeoff. A main-conference path includes QAT recovery, broader heterogeneity, and system-level profiling beyond single-GPU eval time.

\section{Limitations}
Our results focus on a single base pair and two primary datasets; heterogeneity is evaluated via a single spot check. Timing-sync evals capture end-to-end runtime on a single GPU but do not measure distributed communication overhead or kernel-level profiling. GSM8K remains challenging at this model scale (2.8--4.3\% with CoT). These limitations will be addressed in the final main-conference revision.

\section{Broader Impact}
Communication-efficient multi-LLM systems can reduce compute and latency, but they may also enable higher-throughput deployment of models. We emphasize reproducible evaluation, careful reporting of accuracy/latency tradeoffs, and responsible deployment in sensitive domains.

\section{Conclusion}
We introduce precision-aware C2C and report accuracy vs. bytes curves. This establishes a communication-budget perspective for cross-model KV transfer and opens the door to low-latency, low-bandwidth agent collaboration.

\section*{Acknowledgments}
Placeholder.

\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\end{document}
