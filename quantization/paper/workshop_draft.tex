\documentclass[10pt]{article}

% Minimal workshop-friendly template (replace with official when ready)
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}

\title{Quantized Cache-to-Cache: Communication-Budgeted KV Transfer for Heterogeneous LLMs}
\author{Anonymous Authors}
\date{}

\begin{document}
\maketitle

\begin{abstract}
We study precision-aware communication between heterogeneous LLMs by quantizing KV-cache transfers in Cache-to-Cache (C2C). Our goal is to reduce bandwidth and memory while preserving accuracy. We present post-training quantization, cache-length reduction, and token-level selective transfer with sparse fusion, and show accuracy vs.\ bytes transmitted curves. This draft contains placeholders for results.
\end{abstract}

\section{Introduction}
Large language models (LLMs) often communicate via text, which is slow and lossy. Cache-to-Cache (C2C) communicates via KV-cache projection and fusion, but does not address precision constraints. We ask: \textit{How low can KV precision go before accuracy collapses, and can we recover performance under tight bandwidth budgets?}

\section{Background and Motivation}
C2C projects sharer KV caches into receiver space and fuses them with learned gates. This retains richer semantics than text relay. However, KV caches are large: they scale with sequence length, heads, and hidden size. Quantization and cache-length reduction can shrink the communication footprint.

\section{Related Work}
Layer-selective cache communication has been explored in KVComm and Q-KVComm, which emphasize selecting layers and quantizing shared caches (\href{https://arxiv.org/abs/2510.03346}{KVComm}, \href{https://arxiv.org/abs/2512.17914}{Q-KVComm}). Token-level KV selection has been studied for single-model inference (e.g., ZipCache and TokenSelect), and value-norm criteria have been shown to outperform attention-only scores for token importance (VATP) (\href{https://arxiv.org/abs/2405.14256}{ZipCache}, \href{https://aclanthology.org/2025.emnlp-main.1079/}{TokenSelect}, \href{https://arxiv.org/abs/2406.12335}{VATP}). Our work is grounded in C2Câ€™s cache projection and fusion (\href{https://arxiv.org/abs/2510.03215}{C2C}) and relates to recent KV-cache alignment and latent communication efforts (\href{https://arxiv.org/abs/2601.06123}{KV Cache Alignment}, \href{https://arxiv.org/abs/2511.20639}{Latent Collaboration}). Our contribution is distinct: we apply projector-aware, token-level sparsity within a C2C projector+fuser pipeline for heterogeneous models, and quantify accuracy-per-byte tradeoffs.

\section{Method}
\subsection{Post-Training Quantization (PTQ)}
We quantize the KV caches using INT8 or INT4/NF4 with per-head scaling. We evaluate accuracy and latency under fixed precision budgets.

\subsection{Cache-Length Reduction}
We prune KV tokens using a fixed ratio (e.g., top-50\%, 25\%, 10\%), reducing transmitted bytes further.

\subsection{Selective \& Compressed Cache Transfer (SparseC2C)}
We select a sparse subset of token positions to transfer and fuse. Let $I \subset \{1,\dots,T\}$ be selected tokens and $S_I$ the gather operator. We fuse only selected tokens and scatter updates back:
\[
(\tilde{K}^R_\ell, \tilde{V}^R_\ell) = S_I^\top (K^R_\ell, V^R_\ell),\quad
(\tilde{K}^S_\ell, \tilde{V}^S_\ell) = S_I^\top (K^S_\ell, V^S_\ell)
\]
\[
(\tilde{K}^{R\prime}_\ell, \tilde{V}^{R\prime}_\ell) =
\mathcal{F}_\ell\big(\tilde{K}^R_\ell, \tilde{V}^R_\ell, \Pi^K_\ell(\tilde{K}^S_\ell), \Pi^V_\ell(\tilde{V}^S_\ell)\big)
\]
We then scatter the update to the full cache. We use \emph{projector-aware} token scoring by computing value norms in receiver space (``proj\_vnorm\_topk''), which ties selection to the cross-model mapping.

\subsection{Communication-Budget Curves}
We report accuracy as a function of transmitted bytes, enabling fair comparison under equal communication constraints.

\section{Experiments}
\subsection{Setup}
We evaluate on OpenBookQA and ARC-C with a Qwen3-0.6B receiver and Qwen2.5-0.5B sharer. All models are frozen; only the projector is trained when QAT is enabled.

\subsection{Main Results}
All results below are full runs (OpenBookQA: 500 samples, ARC-C: 1150 samples).
PTQ is effectively lossless relative to FP16, and cache pruning shows a strong front/back asymmetry.

\begin{table}[ht]
\centering
\caption{Baseline vs.\ PTQ (full-cache, \%).}
\begin{tabular}{lcc}
\toprule
Setting & OpenBookQA & ARC-C \\
\midrule
FP16 baseline & 52.8 & 55.1 \\
INT8 PTQ & 52.8 & 55.0 \\
INT4 PTQ & 52.6 & 55.4 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{OpenBookQA accuracy (\%, 500 samples) for cache-length pruning (INT8).}
\begin{tabular}{lcccc}
\toprule
Order mode & 75\% & 50\% & 25\% & 10\% \\
\midrule
Front & 44.6 & 43.0 & 38.8 & 38.6 \\
Back  & 52.2 & 52.0 & 50.8 & 49.2 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\centering
\caption{ARC-C accuracy (\%, 1150 samples) for cache-length pruning (INT8).}
\begin{tabular}{lcccc}
\toprule
Order mode & 75\% & 50\% & 25\% & 10\% \\
\midrule
Front & 40.2 & 46.3 & 38.3 & 40.7 \\
Back  & 55.7 & 57.2 & 56.2 & 53.7 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Communication-Budget Curve}
\begin{figure}[ht]
\centering
\includegraphics[width=0.85\linewidth]{quantization/analysis/m4_budget_curve/budget_curve_openbookqa.png}
\caption{Accuracy vs bytes transmitted (OpenBookQA).}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\linewidth]{quantization/analysis/m4_budget_curve/budget_curve_arc_c.png}
\caption{Accuracy vs bytes transmitted (ARC-C).}
\end{figure}

\section{Discussion}
Quantized C2C provides large bandwidth reductions with limited accuracy drop. Cache pruning further improves the tradeoff, suggesting a practical path to deployable multi-LLM communication.

\section{Conclusion}
We introduce precision-aware C2C and report accuracy vs bytes curves. This establishes a communication-budget perspective for cross-model KV transfer and opens the door to low-latency, low-bandwidth agent collaboration.

\section*{Acknowledgments}
Placeholder.

\end{document}
