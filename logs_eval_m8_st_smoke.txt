Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Samples: 32  |  Max new tokens: 16
Avg prompt tokens (Llama): 35.2 | (Qwen): 29.7 | Latent length M: 8
Compression ratio (Llama): 4.4x | (Qwen): 3.7x
Approx interlingua payload per example: 8192 bytes (dtype float32, shape M=8, d_z=256)

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 11.804
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 11.262
Wall clock: 13.07s for 32 examples

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.613
Qwen   EM: 0.000   F1: 0.013   |  NLL/token (gold): 9.361
Wall clock: 6.70s for 32 examples

— Token-budget baseline (same #prefix tokens as latent)
Llama  EM: 0.000  F1: 0.016
Qwen   EM: 0.000   F1: 0.020
Wall clock: 4.62s for 32 examples

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.006
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.013
