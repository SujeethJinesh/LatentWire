# LatentWire: Cross-Model Communication via Soft Tokens

**PI Presentation Summary - December 2025**

---

## 1. The Problem We're Solving

### Background: How LLMs Work Today

When you give an LLM text like "This movie was great", it:
1. **Tokenizes:** Converts text to numbers (e.g., "This"â†’1234, "movie"â†’5678, etc.)
2. **Embeds:** Converts each token ID to a 4096-dimensional vector
3. **Processes:** Passes through 32 transformer layers, each refining the representation
4. **Outputs:** Produces probability distribution over next token

Each layer produces "hidden states" - the model's internal understanding at that layer. These hidden states are rich continuous vectors (4096 dimensions Ã— sequence length).

### The Multi-Agent Problem

When two LLMs need to collaborate:

**Current Approach (Text-Relay):**
```
User query â†’ Llama processes â†’ Llama GENERATES text summary (slow!) â†’ Mistral reads text â†’ Mistral responds
```

**Why it's slow:** Text generation is autoregressive - Llama must generate one token at a time (~50 tokens Ã— 15ms/token = 750ms just for generation).

**Why it's lossy:** Converting rich 4096-dimensional representations to discrete words loses information. "The sentiment is positive" discards nuance that the hidden states captured.

### Our Solution: LatentWire

```
User query â†’ Llama processes â†’ Bridge COMPRESSES hidden states â†’ Mistral receives soft tokens â†’ Mistral responds
```

**Key insight:** Skip text generation entirely. Extract Llama's hidden states, compress them, and inject them directly into Mistral.

---

## 2. Key Concepts Explained

### What is a "Soft Token"?

**Hard token (normal):** A discrete word mapped to an ID, then to a fixed embedding vector.
- "hello" â†’ token ID 1234 â†’ embedding vector [0.1, -0.3, 0.5, ...] (4096 dims)
- This embedding is looked up from a fixed table (not learned per-input)

**Soft token (ours):** A learned continuous vector that we CREATE and inject into the model.
- Not from the vocabulary - we generate it dynamically
- Each soft token is a 4096-dimensional vector (same size as Mistral's embeddings)
- We create 8 of these, so we inject 8 Ã— 4096 = 32,768 numbers into Mistral

**Why "soft"?** Because it's continuous/differentiable, not discrete. We can train it with gradient descent.

### What is a "Hidden State"?

When Llama processes "This movie was great":
- Layer 0 output: 4 tokens Ã— 4096 dims = basic word meanings
- Layer 16 output: 4 tokens Ã— 4096 dims = contextual understanding
- Layer 31 output: 4 tokens Ã— 4096 dims = high-level semantics

We extract from Layer 16 (middle layer) because it contains good semantic information but isn't too task-specific.

### What is the "Bridge"?

The bridge is a small neural network (6.3M parameters) that:

**Input:** Llama's hidden states (variable length Ã— 4096 dims)
**Output:** Exactly 8 soft tokens (8 Ã— 4096 dims)

**Architecture: Perceiver Resampler**

The Perceiver is an architecture from DeepMind (used in Flamingo for imagesâ†’LLM). It solves: "How do I convert variable-length input to fixed-length output?"

```
Llama hidden states (e.g., 50 tokens Ã— 4096)
              â†“
    [Input Projection: 4096 â†’ 512]
              â†“
    [Cross-Attention Layer 1]
       - 8 learned "query" vectors ask: "what's important?"
       - They attend to all 50 input tokens
       - Output: 8 vectors Ã— 512 dims
              â†“
    [Cross-Attention Layer 2]
       - Refine the 8 vectors further
              â†“
    [Output Projection: 512 â†’ 4096]
              â†“
    8 soft tokens (8 Ã— 4096) ready for Mistral
```

**Why cross-attention?** It lets the bridge LEARN what information to extract. The 8 query vectors learn to ask questions like "what's the sentiment?" or "what's the topic?"

### What Does "Frozen" Mean?

- **Frozen:** We don't update these weights during training. Llama and Mistral stay exactly as downloaded.
- **Trainable:** We DO update these weights. Only the bridge (6.3M params) is trained.

This is important because:
1. We don't need massive GPU memory to store optimizer states for 15B parameters
2. We prove the method works without modifying the base models
3. The bridge is the only thing enabling communication

---

## 3. How We Train the Bridge

### Training Data

We use standard text classification datasets:

| Dataset | Task | Example | Labels |
|---------|------|---------|--------|
| SST-2 | Sentiment | "This movie was great" | positive/negative |
| AG News | Topic | "Stock market rises..." | World/Sports/Business/Sci-Tech |
| TREC | Question type | "What is the capital of France?" | location/person/number/etc. |

### Training Procedure (Step by Step)

For each training example (e.g., "This movie was great" â†’ positive):

**Step 1: Llama Forward Pass**
```python
# Feed text to Llama (frozen, no gradients)
llama_output = llama("This movie was great")
hidden_states = llama_output.hidden_states[16]  # Layer 16
# Shape: [1, 5, 4096] (batch=1, 5 tokens, 4096 dims)
```

**Step 2: Bridge Compression**
```python
# Bridge compresses to 8 soft tokens (trainable, has gradients)
soft_tokens = bridge(hidden_states)
# Shape: [1, 8, 4096]
```

**Step 3: Inject into Mistral**
```python
# Prepend soft tokens to task prompt
task_prompt = "\nIs the sentiment positive or negative?\nAnswer:"
prompt_embeddings = mistral.embed(task_prompt)  # [1, 12, 4096]

# Concatenate: soft tokens + prompt
full_input = concat(soft_tokens, prompt_embeddings)  # [1, 20, 4096]
```

**Step 4: Mistral Forward Pass**
```python
# Mistral predicts next token (frozen, no gradients through Mistral)
logits = mistral(inputs_embeds=full_input)
# Get probability of "positive" vs "negative"
prob_positive = logits[token_id("positive")]
prob_negative = logits[token_id("negative")]
```

**Step 5: Compute Loss & Update Bridge**
```python
# Cross-entropy loss: should predict "positive"
loss = -log(prob_positive / (prob_positive + prob_negative))

# Backpropagate through: Mistral (frozen) â†’ soft_tokens â†’ Bridge (trainable)
loss.backward()
optimizer.step()  # Only updates bridge parameters
```

### Training Configuration

| Setting | Value | Why |
|---------|-------|-----|
| Training examples | 5,000 | Enough to learn the mapping |
| Training steps | 2,000 | ~2.5 epochs over the data |
| Batch size | 8 | Memory constraint |
| Learning rate | 1e-4 | Standard for small models |
| Optimizer | AdamW | Standard choice |

---

## 4. How We Evaluate

### Evaluation Procedure

For each test example:

1. **Llama encodes** the input text (single forward pass, ~17ms)
2. **Bridge compresses** to 8 soft tokens (~1ms)
3. **Mistral receives** soft tokens + task prompt, predicts label (~19ms)
4. **Compare** prediction to ground truth

**Total time per example:** ~37ms

### What We Measure

**Accuracy:** % of test examples where predicted label = true label

**Latency:** Wall-clock time from input to output (measured on NVIDIA H100 GPU)

### Test Sets Used

| Dataset | Test Set Size | We Evaluate On |
|---------|---------------|----------------|
| SST-2 | 872 | All 872 (validation set) |
| AG News | 7,600 | 200 samples |
| TREC | 500 | 200 samples |

---

## 5. Baselines Explained

We compare against multiple baselines to isolate what's causing the improvement:

### Baseline 1: Random Chance

**What:** Guess randomly among the labels.

**Purpose:** Lower bound. Any method must beat this.

| Dataset | Random Chance |
|---------|---------------|
| SST-2 (2 classes) | 50.0% |
| AG News (4 classes) | 25.0% |
| TREC (6 classes) | 16.7% |

### Baseline 2: Zero-Shot (Llama or Mistral alone)

**What:** Give the model the text + task prompt, ask it to classify directly.

**Procedure:**
```
Input: "This movie was great"
Prompt: "Is the sentiment positive or negative? Answer:"
â†’ Model outputs "positive" or "negative"
```

**Purpose:** What can a single model do without any training?

| Model | SST-2 | AG News |
|-------|-------|---------|
| Llama 3.1 8B | 88.4% | 63.8% |
| Mistral 7B | 92.2% | 69.4% |

### Baseline 3: Few-Shot Prompting (5-shot)

**What:** Give the model 5 examples before asking it to classify.

**Procedure:**
```
Input:
"Text: I loved this film. Label: positive
Text: Terrible waste of time. Label: negative
Text: Amazing performances! Label: positive
Text: Boring and slow. Label: negative
Text: Best movie ever. Label: positive
Text: This movie was great. Label:"
â†’ Model outputs "positive"
```

**Purpose:** What can in-context learning achieve? This is a strong baseline.

| Model | SST-2 | AG News |
|-------|-------|---------|
| Mistral 5-shot | 94.5% | 80.3% |

### Baseline 4: Prompt-Tuning (NO Llama)

**What:** Train learnable soft tokens on Mistral ONLY. No Llama involvement.

**Procedure:** Same as our bridge, but:
- No Llama forward pass
- Soft tokens are random parameters that we train
- Mistral sees: [learned soft tokens] + [text] + [task prompt]

**Purpose:** CRITICAL BASELINE. Tests whether improvement comes from:
- (A) Llama's hidden states, or
- (B) Just having trainable soft tokens

**Results:**

| Dataset | Prompt-Tuning | Bridge | Difference |
|---------|---------------|--------|------------|
| SST-2 | 49.5% | 96.7% | +47.2pp |
| AG News | 19.8% | 90.7% | +70.9pp |
| TREC | 19.0% | 95.3% | +76.3pp |

**Conclusion:** Prompt-tuning achieves RANDOM CHANCE. The entire improvement comes from Llama's hidden states.

### Baseline 5: Text-Relay

**What:** Llama generates a text summary, Mistral classifies from that summary.

**Procedure:**
```
Step 1 - Llama generates:
  Input: "Summarize: This movie was great"
  Output: "The reviewer enjoyed the movie." (generated token-by-token, ~750ms)

Step 2 - Mistral classifies:
  Input: "The reviewer enjoyed the movie. Is this positive or negative?"
  Output: "positive"
```

**Purpose:** Tests whether latent transfer is better than text transfer.

**Results:**

| Metric | Text-Relay | Bridge |
|--------|------------|--------|
| SST-2 Accuracy | 71.0% | 96.7% |
| Latency | 834.5ms | 37.3ms |

**Conclusion:** Bridge is 22Ã— faster AND 26pp more accurate.

### Baseline 6: Fine-Tuning Mistral

**What:** Actually train Mistral's weights on the classification task.

**Procedure:** Unfreeze last N layers of Mistral, train with cross-entropy loss.

**Purpose:** Upper bound for single-model performance.

| Method | Params Trained | SST-2 | Latency |
|--------|----------------|-------|---------|
| Fine-tune 2 layers | 570M | 94.0% | 113ms |
| Fine-tune 8 layers | 1.9B | 94.0% | 113ms |
| LoRA rank-8 | 3.4M | 95.3% | 113ms |
| **Bridge (ours)** | **6.3M** | **96.7%** | **37ms** |

**Conclusion:** Bridge beats fine-tuning by 2.7pp while being 3Ã— faster.

---

## 6. Main Results

### Classification Accuracy

| Dataset | Classes | Random | Prompt-Tuning | Mistral 0-shot | Mistral 5-shot | Text-Relay | **Bridge** |
|---------|---------|--------|---------------|----------------|----------------|------------|------------|
| SST-2 | 2 | 50.0% | 49.5% | 92.2% | 94.5% | 71.0% | **96.7%** |
| AG News | 4 | 25.0% | 19.8% | 69.4% | 80.3% | 64.5% | **90.7%** |
| TREC | 6 | 16.7% | 19.0% | 61.8% | -- | 58.0% | **95.3%** |
| Banking77 | 77 | 1.3% | -- | -- | -- | 1.0% | **21.5%** |

### Latency Comparison

| Method | What Happens | Time |
|--------|--------------|------|
| **Bridge** | Llama encode (17ms) + Bridge (1ms) + Mistral (19ms) | **37ms** |
| Text-Relay | Llama encode (17ms) + Llama generate 50 tokens (750ms) + Mistral (68ms) | **835ms** |

**Speedup: 22.4Ã—**

---

## 7. Key Findings

### Finding 1: Sender Model is Essential

**Question:** Is the improvement from Llama's hidden states, or just from training?

**Experiment:** Train soft tokens on Mistral without any Llama involvement.

**Result:** Without Llama, accuracy = random chance. With Llama, accuracy = 96.7%.

**Statistical Significance:**
- t-test p-value: 8.49 Ã— 10â»Â¹â´ (extremely significant)
- Cohen's d: 107 (massive effect size; d > 0.8 is "large")

### Finding 2: Super-Additive Performance

**Question:** Is the bridge just averaging the two models?

**Experiment:** Compare bridge to each model operating alone.

| Configuration | SST-2 |
|---------------|-------|
| Llama alone | 88.4% |
| Mistral alone | 92.2% |
| Best of the two | 92.2% |
| **Bridge (Llamaâ†’Mistral)** | **96.7%** |

**The bridge exceeds both models by 4.5pp.** This is "super-additive" - the combination is better than either part.

### Finding 3: Cross-Model Beats Same-Model

**Question:** Do we need two DIFFERENT models, or would Llamaâ†’Llama work?

**Experiment:** Train bridge to go from Llama hidden states back into Llama.

| Configuration | SST-2 |
|---------------|-------|
| Llama â†’ Llama (same architecture) | 84.5% |
| Mistral â†’ Mistral (same architecture) | 95.5% |
| **Llama â†’ Mistral (different architectures)** | **96.7%** |

**Cross-model beats same-model by 12.2pp!**

**Why?** Hypothesis: When source and target are the same, the bridge can learn "shortcuts" (near-identity mappings). When they're different, it's FORCED to learn abstract, task-relevant features.

### Finding 4: Fewer Tokens = Better Performance

**Question:** Do more soft tokens help?

**Experiment:** Vary number of soft tokens on Banking77 (77 classes).

| Soft Tokens | Accuracy |
|-------------|----------|
| 16 | 21.5% |
| 32 | 13.5% |
| 64 | 7.5% |
| 128 | 1.0% |

**More tokens = WORSE performance!**

**Why?** Information bottleneck principle. Compression forces the bridge to extract only task-relevant features. More capacity allows overfitting to noise.

---

## 8. Limitations: Reasoning Tasks Fail

### Experiment

Test bridge on tasks requiring multi-step reasoning:

| Task | What It Tests | Llama Direct | Bridge | Random |
|------|---------------|--------------|--------|--------|
| CommonsenseQA | Common sense reasoning | 75.0% | 17.0% | 20.0% |
| GSM8K | Math word problems | 76.5% | 2.0% | ~0% |
| BoolQ | Yes/no comprehension | 79.5% | 53.5% | 50.0% |
| PIQA | Physical intuition | 80.0% | 52.5% | 50.0% |

### Why Reasoning Fails

**Classification tasks:** "What is the sentiment?" â†’ Answer is a single label embedded in the hidden states.

**Reasoning tasks:** "If John has 5 apples and gives 2 away, how many remain?" â†’ Requires:
1. Parse the problem
2. Identify quantities
3. Apply operation
4. Track intermediate state
5. Output final answer

8 soft tokens can encode "what the input is about" but cannot encode "how to reason step-by-step."

**This is a fundamental limitation of compression-based communication.**

---

## 9. Comparison to Related Work

### Why Aren't We Comparing to Cross-LoRA, PromptBridge, StitchLLM?

These methods solve **different problems**:

| Method | What It Does | When It Happens |
|--------|--------------|-----------------|
| **Cross-LoRA** | Transfers LoRA adapter weights between models | Offline (before deployment) |
| **PromptBridge** | Optimizes text prompts to work across models | Offline (before deployment) |
| **StitchLLM** | Combines transformer blocks from different models | Offline (model composition) |
| **LatentWire (ours)** | Sends information between models during inference | **Runtime (per-input)** |

**Key difference:** Those methods modify weights/prompts ONCE, then deploy. We enable DYNAMIC communication on each input.

**Analogy:**
- Cross-LoRA: "Let me copy your notes before the exam" (offline transfer)
- LatentWire: "Let me whisper answers to you during the exam" (runtime communication)

They're not directly comparable because they solve different use cases.

---

## 10. Architecture Details

### Bridge Architecture (Perceiver Resampler)

```
Input: Llama hidden states [batch, seq_len, 4096]
                    â†“
         Linear(4096 â†’ 512)              # Project to internal dim
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Cross-Attention Ã—2  â”‚
         â”‚   - 8 query vectors â”‚         # Learned queries
         â”‚   - 8 attention headsâ”‚
         â”‚   - FFN after each  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
         Linear(512 â†’ 4096)              # Project to Mistral dim
                    â†“
         RMS Normalize + Scale           # Match Mistral's statistics
                    â†“
Output: 8 soft tokens [batch, 8, 4096]
```

### Parameter Count

| Component | Parameters |
|-----------|------------|
| Input projection (4096 â†’ 512) | 2.1M |
| Query vectors (8 Ã— 512) | 4K |
| Cross-attention layers (Ã—2) | 2.1M |
| FFN layers (Ã—2) | 2.1M |
| Output projection (512 â†’ 4096) | 2.1M |
| **Total bridge** | **~6.3M** |

**For comparison:**
- Llama 3.1 8B: 8,000M parameters (frozen)
- Mistral 7B: 7,000M parameters (frozen)
- Bridge: 6.3M parameters (trained) = **0.04% of total system**

---

## 11. Reproducibility

### Multi-Seed Results

We ran 3 random seeds to verify stability:

**Bridge on SST-2:**
| Seed | Accuracy |
|------|----------|
| 42 | 96.5% |
| 123 | 96.0% |
| 456 | 97.5% |
| **Mean Â± Std** | **96.7% Â± 0.6%** |

**Prompt-Tuning on SST-2:**
| Seed | Accuracy |
|------|----------|
| 42 | 49.5% |
| 123 | 49.5% |
| 456 | 49.5% |
| **Mean Â± Std** | **49.5% Â± 0.0%** |

The 0.0% std for prompt-tuning confirms it's stuck at random chance regardless of initialization.

---

## 12. What's Next

### Completed
- âœ… Core experiments (SST-2, AG News, TREC, Banking77)
- âœ… All baselines (prompt-tuning, text-relay, few-shot, fine-tuning)
- âœ… Ablations (layer selection, architecture, token count)
- âœ… Reasoning experiments (showing limitation)
- âœ… Paper written (12 pages)
- âœ… Unified comparison script for reproducibility

### In Progress
- ğŸ”„ Running final unified comparison on HPC
- ğŸ”„ Verifying all error bars

### Before Submission
- Final paper polish
- Ensure all claims have statistical backing

### Future Directions (Not for this paper)
- Scale to larger models (70B)
- Bidirectional communication (both models talk)
- Chain of >2 models
- Non-classification tasks where compression works

---

## 13. One-Page Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                           â•‘
â•‘   LatentWire: Cross-Model Communication via Soft Tokens                   â•‘
â•‘                                                                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                           â•‘
â•‘   PROBLEM                                                                 â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€                                                                 â•‘
â•‘   LLM-to-LLM communication via text is slow (835ms) because text          â•‘
â•‘   generation is autoregressive (one token at a time).                     â•‘
â•‘                                                                           â•‘
â•‘   SOLUTION                                                                â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€                                                                â•‘
â•‘   Extract hidden states from Llama â†’ Compress to 8 soft tokens â†’          â•‘
â•‘   Inject directly into Mistral. No text generation needed.                â•‘
â•‘                                                                           â•‘
â•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â•‘
â•‘   â”‚   Llama     â”‚      â”‚   Bridge    â”‚      â”‚   Mistral   â”‚              â•‘
â•‘   â”‚    8B       â”‚ â”€â”€â”€â–º â”‚    6.3M     â”‚ â”€â”€â”€â–º â”‚     7B      â”‚              â•‘
â•‘   â”‚  (frozen)   â”‚      â”‚ (trainable) â”‚      â”‚  (frozen)   â”‚              â•‘
â•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â•‘
â•‘                                                                           â•‘
â•‘   KEY RESULTS                                                             â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â•‘
â•‘   â€¢ Latency:     37ms vs 835ms = 22Ã— faster                              â•‘
â•‘   â€¢ SST-2:       96.7% (vs 92.2% Mistral alone, 49.5% without Llama)     â•‘
â•‘   â€¢ AG News:     90.7% (vs 69.4% Mistral alone)                          â•‘
â•‘   â€¢ Super-additive: Bridge > both individual models                       â•‘
â•‘   â€¢ Cross > Same: Llamaâ†’Mistral beats Llamaâ†’Llama by 12pp                â•‘
â•‘                                                                           â•‘
â•‘   KEY INSIGHT                                                             â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â•‘
â•‘   Prompt-tuning without Llama = random chance (49.5%)                     â•‘
â•‘   Bridge with Llama = 96.7%                                               â•‘
â•‘   â†’ The improvement is ENTIRELY from Llama's hidden states                â•‘
â•‘                                                                           â•‘
â•‘   LIMITATION                                                              â•‘
â•‘   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â•‘
â•‘   Reasoning tasks fail. CommonsenseQA: 17% (vs Llama's 75%)               â•‘
â•‘   8 soft tokens encode "what" but not "how to reason"                     â•‘
â•‘                                                                           â•‘
â•‘   TARGET: MLSys 2025                                                      â•‘
â•‘                                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Contact

- **Author:** Sujeeth Jinesh
- **Advisor:** Thierry Tambe
- **Affiliation:** Stanford University
- **Email:** sujinesh@stanford.edu
