# LatentWire ‚Äî 8B_clean_answer_ftce ‚Äî Experiment Log

**Run ID:** `8B_clean_answer_ftce`  
**Start:** Sun Sep 14 23:54:43 PDT 2025  
**Backbones:** - Llama: `meta-llama/Meta-Llama-3.1-8B-Instruct`  
- Qwen:  `Qwen/Qwen2.5-7B-Instruct`  
**Dataset:** SQuAD (`train` for training subsample, `validation` for eval)  
**Seeds:** train seed = 42; deterministic eval seed = 12345  
**Encoder:** `byte` interlingua (token-level input) ‚Üí `M=32`, `d_z=256`, `BYTE_MAX=2048`  
**Adapters:** 2√ó linear + scale (to each LM) with RMS calibration to input embeddings  
**Eval mode:** Sequential (per‚ÄëLM), `fresh_eval=1` (recompute Z), deterministic first step

---

## 0) Global Flags / Script (for reproducibility)

From `run_pipeline.sh` at time of the baseline and the current re‚Äërun (unless otherwise noted):

- **Training knobs**
  - `EPOCHS=24`, `BATCH_SIZE=64`, `TRAIN_SAMPLES=87599`
  - `ENCODER_TYPE=byte`, `LATENT_LEN=32`, `D_Z=256`, `BYTE_MAX=2048`
  - `LR=5e-5`, `SCALE_L2=0.05`, `ADAPTER_RMS_L2=0.0`, `MAX_GRAD_NORM=1.0`
  - `WARM_ANCHOR_TEXT="Answer: "`
  - `FIRST_TOKEN_CE=0.5` (Œª for first‚Äëtoken CE)
  - `TRAIN_APPEND_BOS="yes"` (BOS appended after prefix+anchor for the **first‚Äëtoken** objective)
  - `SEQUENTIAL_MODELS=1` (train both LMs in the same step, shared encoder)

- **Eval knobs**
  - `DATASET=squad`, `SMOKE_SAMPLES=200`, `SAMPLES=200`
  - `MAX_NEW_TOKENS=12`, `CHUNK_SIZE=8`
  - `SEQUENTIAL_EVAL=1`, `FRESH_EVAL=1`, `LOAD_4BIT=0`
  - **Anchors & BOS:** `LATENT_ANCHOR_MODE="text"`, `LATENT_ANCHOR_TEXT="Answer: "`, `APPEND_BOS_AFTER_PREFIX="yes"`  
  - **Calibration:** `CALIBRATION="embed_rms"`, `PREFIX_GAIN=1.0`
  - **Decode hardening:** `FIRST_TOKEN_TOP_P=1.0`, `FIRST_TOKEN_TEMPERATURE=0.0`  
    (deterministic first token), `min_new_tokens=3`, `eos_ban_steps=6`.

- **Bookkeeping**
  - Saving `state.pt`, `encoder.pt`, `adapter_{llama,qwen}.pt`, `config.json`, and `training_stats.json` every epoch (end step).

---

## 1) Baseline Observations (before PAD fixes)

### 1.1 High‚Äëlevel pattern

- **Text prompting** is strong (F1 ‚âà 0.80‚Äì0.85).
- **Latent prompting** collapses: F1 ‚âà 0.006‚Äì0.022; **first‚Äëtoken top‚Äë1 ‚âà 0.055‚Äì0.075**.
- **Debug generations** show filler loops (‚Äúthe the the ‚Ä¶‚Äù) despite RMS calibration and early EOS ban.

> **Key insight:** Training loss looked reasonable, but gradients were dominated by **left‚Äëpadded tokens** in the teacher‚Äëforced path (PAD/EOS transitions), not by the actual answer tokens.

### 1.2 Concrete snapshots (from eval logs you posted)

| Epoch | Group     | EM   | F1      | NLL/token (gold) | FirstTok@1 | FirstTok@5 |
|------:|-----------|-----:|---------|------------------:|-----------:|-----------:|
| 14    | **Llama (latent)** | 0.000 | **0.006** | 11.370 | **0.060** | 0.105 |
| 14    | **Qwen  (latent)** | 0.000 | **0.022** | 8.226  | **0.065** | 0.145 |
| 20    | **Llama (latent)** | 0.000 | **0.010** | 11.513 | **0.055** | 0.130 |
| 20    | **Qwen  (latent)** | 0.000 | **0.017** | 8.150  | **0.065** | 0.160 |
| 21    | **Llama (latent)** | 0.000 | **0.008** | 11.240 | **0.060** | 0.110 |
| 21    | **Qwen  (latent)** | 0.000 | **0.015** | 8.194  | **0.075** | 0.165 |

**Text baseline** (constant across these epochs):  
- *Llama:* EM 0.58, F1 ~0.799  
- *Qwen:* EM 0.68, F1 ~0.853

**Selected debug generations (latent, first few; representative):**

- *Llama (e.g., ep20):* - `two years after the first successful use of the vaccine, the`  
  - `the 20th century, the 20th century`  
  - `the 1960s and 1970s. The`  
  - `the the the the the the the the the the the the`  
- *Qwen (e.g., ep20):* - `by the of the of thej`  
  - `the of the of the of theJ`  
  - `the of the and the of the and and`  

**Diagnostics showing RMS/scale were *not* the issue** (ep20 excerpts):  
- `prefix_std ‚âà embed_rms` (e.g., Llama: 0.01057 vs 0.01057)  
- `adapter.scale ‚âà 1` (e.g., 0.988‚Äì1.000)  
- So **amplitude/calibration looked healthy**; the problem lay elsewhere.

---

## 2) Root‚ÄëCause Diagnosis

- We globally set the tokenizer to **left padding** (typical for decoder LMs).  
- During training, we formed TF sequences from the **answers** but did **not**:
  1. **Mask PAD tokens** out of the labels (`-100`), **and**
  2. **Zero their attention** so the model wouldn‚Äôt attend to left‚Äëpad noise.
- Result: the CE focused on trivial PAD/EOS transitions instead of content tokens.  
  The model then failed to learn a strong **first token** from the latent prefix, and free‚Äërun decoding collapsed into high‚Äëfrequency fillers.

This matches the empirical signals:
- Low first‚Äëtoken accuracy (~5‚Äì7%),  
- ‚Äúthe ‚Ä¶‚Äù loops despite early EOS ban and good RMS calibration.

---

## 3) Changes Applied (today)

> ‚úÖ **All implemented; optional items are listed in ¬ß4 but not turned on yet.**

### 3.1 PAD‚Äëaware losses (code only; no flag changes)

**File:** `latentwire/models.py` (inside `LMWrapper`)

- **`forward_with_prefix_loss(...)`**
  - Mask labels where `label == pad_token_id` ‚Üí `-100`.
  - Build **attention masks** that **zero out padded TF positions**.
  - Keep ignoring the positions for `[latent prefix]` and optional `[anchor]`.

- **`loss_with_text_prompt(...)`** (used for NLL diagnostics)
  - Same masking for PAD labels.
  - Zero attention at padded TF positions after the prompt.

**Why it should work:** Now the CE is dominated by **real answer tokens**, not padding, so gradients will align the latent prefix + (optional) anchor with the **first content token** and subsequent answer tokens. This is the most common and decisive fix for latent‚Äëprefix training collapse.

### 3.2 Right‚Äëpad **answers** only when building TF labels (code only)

**File:** `latentwire/train.py`  
- Temporarily set `tokenizer.padding_side="right"` just for **answer tokenization** (teacher forcing labels). Everything else stays the same.  
- Rationale: prevents a wall of left PADs at the beginning of TF sequences, further reducing the chance of PAD dominating the loss.

**Why it should work:** Right‚Äëpadding ensures the earliest supervised steps correspond to **actual answer tokens**, aligning the loss with what we want the prefix to control (the start of the answer).

---

## 4) Optional ablations (not applied yet)

These are **off** right now. Enable only if needed after observing the post‚Äëfix epoch.

1) **BOS after prefix+anchor (A/B)** - **Flag:** `APPEND_BOS_AFTER_PREFIX="no"` (eval) and `TRAIN_APPEND_BOS="no"` (for first‚Äëtoken CE)  
   - **Why:** For many chat LMs, a BOS **after** `"Answer: "` can be unnatural and push toward generic fillers. Removing BOS often increases first‚Äëtoken @1.  
   - **Metric to watch:** first_token_top1 ‚Üë, latent F1 ‚Üë.

2) **Increase first‚Äëtoken supervision (short boost)** - **Flag:** `FIRST_TOKEN_CE=1.0` (temporarily)  
   - **Why:** Once PAD masking is correct, a slightly stronger first‚Äëstep CE can accelerate alignment.  
   - **Metric:** first_token_top1 should move noticeably (>0.10‚Äì0.15 in a couple of epochs).

3) **Mild prefix gain at eval** - **Flag:** `PREFIX_GAIN=1.25`  
   - **Why:** Gives the latent prefix slightly more influence at decode time; keep within 1.0‚Äì1.5.  
   - **Metric:** latent F1 ‚Üë without weird phrasing; if outputs over‚Äëshoot or get erratic, roll back.

4) **First‚Äëtoken nucleus sampling (if greedy remains sticky)** - **Flags:** `FIRST_TOKEN_TOP_P=0.9`, `FIRST_TOKEN_TEMPERATURE=0.7`  
   - **Why:** Adds small stochasticity only to the **first** token; often enough to break filler ties. Determinism remains repeatable under fixed seed.  
   - **Metric:** first_token_top1 ‚Üë; inspect first five generations.

5) **Anchor mode A/B** - **Flag:** switch `LATENT_ANCHOR_MODE="text" ‚Üî "chat"` (keep text `"Answer: "` vs. letting the model‚Äôs chat template drive)  
   - **Why:** If an LM strongly expects its chat formatting, aligning the anchor mode can help.  
   - **Metric:** first_token_top1 & latent F1.

---

## 5) What we expect **after the fixes in ¬ß3** (acceptance criteria)

These are *expectations*, not guarantees, to decide next actions:

- **First‚Äëtoken acc (top‚Äë1)** should rise substantially above chance, typically into the **0.15‚Äì0.30** range after 1‚Äì2 epochs.  
- **Latent F1** should move off the floor (no longer ~0.01); any **monotonic** improvement across epochs is the signal we want.
- **Qualitative**: the ‚Äúthe the the ‚Ä¶‚Äù loops should mostly disappear in the first few debug generations.

**If, after one epoch with the fixes, first_token_top1 is still < 0.10**, apply ablation **(1)** (BOS=no). If still flat, try **(2)** FIRST_TOKEN_CE=1.0 for an epoch.

---

## 6) Evidence the issue wasn‚Äôt amplitude/calibration

- Logs consistently showed `prefix_std ‚âà embed_rms` and `adapter.scale ‚âà 1`.  
- CE loss numbers (1.1‚Äì1.6) were **much** lower than the **latent NLL/token** at eval (8‚Äì11), consistent with CE being dominated by easy PAD/EOS.  
- Early EOS was already banned for the first steps (`eos_ban_steps=6`, `min_new_tokens=3`), so sampling wasn‚Äôt the root cause.

---

## 7) Current status

- ‚úÖ **Code fixes applied**: PAD‚Äëaware CE + right‚Äëpadded answers for TF (train + eval loss paths).  
- üö´ **Not applied (yet)**: BOS=no, FIRST_TOKEN_CE bump, PREFIX_GAIN>1, first‚Äëtoken sampling tweaks.

**Next action:** run the provided script unchanged (keeps `APPEND_BOS_AFTER_PREFIX="yes"`, `FIRST_TOKEN_CE=0.5`) to **isolate** the effect of the PAD fixes. Then review:
- `eval_epoch*/metrics.json` ‚Üí `latent.first_token_top1/top5`, `latent.f1`  
- `eval_epoch*/predictions.jsonl` ‚Üí quick scan of first 5 predictions per LM.

---

## 8) Notes, warnings, and environment quirks

- HF Transformers >=4.46 warning: *‚Äú`logits` model output will have the same type as the model ‚Ä¶‚Äù* ‚Äî informational only.  
- KV cache deprecation: *‚Äú`past_key_values` as a tuple of tuples ‚Ä¶ will be removed in v4.47‚Äù*. Our usage is fine for now; unrelated to the collapse.  
- We record `training_stats.json` with prefix RMS stats per LM; these confirm RMS calibration is behaving as intended.

---

## 9) Minimal checklist to avoid running in circles

- [x] Mask PAD in **labels** (train + eval losses)  
- [x] Zero **attention** on padded TF positions  
- [x] **Right‚Äëpad** answers when constructing TF labels  
- [ ] (If needed) BOS after prefix+anchor **OFF** (`APPEND_BOS_AFTER_PREFIX="no"`, `TRAIN_APPEND_BOS="no"`)  
- [ ] (If needed) Temporarily **increase** `FIRST_TOKEN_CE` to `1.0`  
- [ ] (If needed) `PREFIX_GAIN=1.25` at eval  
- [ ] (If needed) First‚Äëtoken `top_p=0.9`, `temperature=0.7`  
- [ ] (If needed) Anchor mode A/B: `text` ‚Üî `chat`

**Stop criteria for each ablation:** keep one change for 1‚Äì2 epochs; if no improvement in `first_token_top1` and latent F1, revert and try the next.

---

## 10) Appendix ‚Äî representative flags & their *why*

- `LATENT_ANCHOR_TEXT="Answer: "`: provides a short, stable context to bias the LM toward concise answers.
- `CALIBRATION="embed_rms"` + `PREFIX_GAIN=1.0`: matches latent amplitude to the LM‚Äôs input embedding RMS (prevents blown logits while keeping signal).
- `FIRST_TOKEN_CE=0.5`: adds explicit supervision on the first step; we may tune this after PAD fixes if first‚Äëtoken acc is still low.
- `APPEND_BOS_AFTER_PREFIX="yes"`: kept **on** initially for continuity with earlier runs; we will A/B `no` if needed.
- `min_new_tokens=3`, `eos_ban_steps=6`: bans early EOS / chat EOT tokens; ensures we observe a proper first token and short continuation.
- `SEQUENTIAL_EVAL=1`, `FRESH_EVAL=1`: recompute Z per model (text alignment) and avoid stale caches; crucial when encoders or wrappers change.

---

### 2025‚Äë09‚Äë15 ‚Äî Run 8B_clean_answer_ftce (SQuAD)
**Goal:** make latent prompting usable by fixing loss target hygiene and first‚Äëtoken alignment, while holding capacity at M=32 (vs prior runs at M=16).

#### Hardware / Models
- **GPUs:** `CUDA_VISIBLE_DEVICES=0,1`
- **LLMs:** `meta-llama/Meta-Llama-3.1-8B-Instruct`, `Qwen/Qwen2.5-7B-Instruct`
- **Encoder:** `byte` (`BYTE_MAX=2048`)
- **Latent shape:** `LATENT_LEN=32`, `D_Z=256`

#### Common eval settings (Epoch 1‚Äì2)
- **Dataset:** `squad`, `samples=200`, `max_new_tokens=12`
- **Latent anchor:** `mode=text`, `text="Answer: "`
- (As run in Epoch 1‚Äì2) `APPEND_BOS_AFTER_PREFIX="yes"` (training matched eval)
- **Calibration:** `embed_rms`, `prefix_gain=1.0`
- **First step decode:** `first_token_top_p=1.0`, `first_token_temperature=0.0` (greedy first token)
- Sequential eval with fresh Z: `--sequential_eval --fresh_eval`

#### Training knobs (Epoch 1‚Äì2)
- `EPOCHS=24`, `BATCH_SIZE=64`, `TRAIN_SAMPLES=87599`
- `LR=5e-5`, `SCALE_L2=0.05`, `ADAPTER_RMS_L2=0.0`, `MAX_GRAD_NORM=1.0`
- **First‚Äëtoken CE:** `first_token_ce_weight=0.5`
- (As run) `train_append_bos_after_prefix="yes"`
- **Save cadence:** end of each epoch; smoke eval each epoch (200 samples)

#### What we changed before this run (code hygiene)
- Cross‚Äëentropy masking & right‚Äëpadding fixes in `train.py`/`models.py`
  - **Why:** avoid training on pad/garbage; align targets with real tokens.
  - **Expected effect:** immediate drop in latent NLL; steadier training curves.
- Anchor consistency `Answer: ` used in both train and eval.
  - **Why:** reduce train/eval mismatch at the first step.
  - **Expected effect:** lower variance in first‚Äëtoken logits; better NLL.

#### Results so far (Epoch 1 ‚Üí Epoch 2)
- **Text baseline** (reference, unchanged across epochs)
  - Llama F1 **0.799**, Qwen F1 **0.853**
- **Latent path** (shared interlingua)
| Metric | Epoch 1 | Epoch 2 | Œî |
| :--- | :--- | :--- | :--- |
| **Llama NLL/token (gold)** | 8.1683 | 7.8636 | ‚Äì0.3047 (‚Äì3.73%) |
| **Qwen NLL/token (gold)** | 7.7830 | 7.4624 | ‚Äì0.3206 (‚Äì4.12%) |
| **Llama F1** | 0.0205 | 0.0312 | +0.0107 |
| **Qwen F1** | 0.0035 | 0.0095 | +0.0060 |
| **Llama FirstTok@1** | 0.030 | 0.025 | ‚Äì0.005 |
| **Llama FirstTok@5** | 0.040 | 0.075 | +0.035 |
| **Qwen FirstTok@1** | 0.060 | 0.055 | ‚Äì0.005 |
| **Qwen FirstTok@5** | 0.125 | 0.140 | +0.015 |

- **Calibration / amplitude (debug)**
  - `Z.std`: 0.606 ‚Üí 0.662 (encoder ‚Äúusing the space‚Äù more)
  - `adapter.scale`: ~1.0 (calibrator doing its job)
  - `rms_mean_raw` (train): Llama 0.632 ‚Üí 0.696, Qwen 0.618 ‚Üí 0.692 (pre‚Äëcalibration scale rose; OK with `embed_rms`)
- **Qualitative:** First generations still dominated by function‚Äëword loops ("the of the ‚Ä¶"), indicating the first‚Äëtoken decision is still under‚Äëaligned despite the NLL gains.

#### Interpretation:
The NLL/F1 improvements are coming from the target hygiene + anchor consistency changes; the bottleneck is first‚Äëtoken alignment. Greedy first step (temp=0.0) plus a BOS inserted after the anchor makes the LM default to high‚Äëfrequency function words when the latent signal isn‚Äôt yet strong.

#### Decision after Epoch 2
Proceed to Epoch 3 to capture one more checkpoint under the ‚ÄúStage‚ÄëA‚Äù settings, then stop and restart with a first‚Äëtoken‚Äìfocused configuration (‚ÄúStage‚ÄëB‚Äù) aimed at breaking the "the/of/and" failure mode.

#### Stage‚ÄëB configuration (to apply after Epoch 3)
- **Exact flag deltas (A ‚Üí B):**
| Old Setting | New Setting |
| :--- | :--- |
| `APPEND_BOS_AFTER_PREFIX="yes"` | `APPEND_BOS_AFTER_PREFIX="no"` |
| `TRAIN_APPEND_BOS="yes"` | `TRAIN_APPEND_BOS="no"` |
| `FIRST_TOKEN_CE=0.5` | `FIRST_TOKEN_CE=1.0` |
| `PREFIX_GAIN=1.0` | `PREFIX_GAIN=1.15` |

- **Rationale:**
  - **Remove BOS after the anchor (train+eval):** keeps the latent+anchor in a single continuous stream so the very next token is conditioned by the latent, not reset toward generic sentence starts.
    - **Hypothesis:** should lift FirstTok@1/@5 noticeably within the next couple of epochs.
  - **Double first‚Äëtoken CE weight:** increases gradient pressure on the first decision.
    - **Hypothesis:** pushes the latent to create a clear margin on the correct first word.
  - **Mild PREFIX_GAIN at decode:** gives the latent a small nudge without destabilizing longer‚Äërange decoding.
- **What stays the same:** `LATENT_LEN=32`, `LR=5e-5`, `SCALE_L2=0.05`, deterministic first step for now (`top_p=1.0`, `temp=0.0`). We‚Äôll revisit decode sampling only if first‚Äëtoken accuracy remains flat after these changes.

#### Measurement plan for Stage‚ÄëB
Track, per epoch (200‚Äësample smoke eval):
- FirstTok@1/@5 (primary success signal)
- Latent NLL/token (should continue trending down or hold)
- Latent F1 (should move up along with FirstTok metrics)
- Debug first generations (expect function‚Äëword loops to fade)
- **Guardrail:** if FirstTok@1 does not improve meaningfully after 1‚Äì2 epochs on Stage‚ÄëB, switch eval first‚Äëstep to `first_token_top_p=0.9`, `first_token_temperature=0.7` and sweep `PREFIX_GAIN` in `[1.10, 1.25]`.

#### Artifacts & paths (for reproducibility)
- **Epoch 1 eval:** `runs/8B_clean_answer_ftce/eval_epoch1/metrics.json`
  - Llama latent: F1 0.021, NLL 8.168; Qwen latent: F1 0.003, NLL 7.783
- **Epoch 2 eval:** `runs/8B_clean_answer_ftce/eval_epoch2/metrics.json`
  - Llama latent: F1 0.031, NLL 7.864; Qwen latent: F1 0.009, NLL 7.462
- Debug snippets show first generations dominated by "the/of/and" patterns in both epochs.
- **Next action:** Stop after Epoch 3 checkpoint is written, then restart training with the Stage‚ÄëB script above (resume from latest ckpt).

### 2025‚Äë09‚Äë15 ‚Äî Latent prompting stalled at first token; fix plan

#### What went wrong (evidence)
- **Latent F1/EM remain near zero** across two successive epoch evals on SQuAD (M=32):
  - *Epoch‚ÄØ1:* Llama EM 0.000 / F1 0.025, Qwen EM 0.000 / F1 0.009
  - *Epoch‚ÄØ2:* Llama EM 0.000 / F1 0.025, Qwen EM 0.000 / F1 0.013
- **First‚Äëtoken accuracy is flat/very low** despite more training:
  - Llama Top‚Äë1 2.5% ‚Üí 4.0%, Qwen ~6.0%; Top‚Äë5 stays <16%.
- **Oracle upper bound is also tiny** (F1 ‚âà 0.025‚Äì0.028), meaning errors are systematic at the first decode steps, not sampling.
- **Degenerate first generations at eval** (debug): e.g., "the of ‚Ä¶", numeric runs ("1919‚Ä¶")‚Äîtypical when the model can‚Äôt read the latent evidence and falls into function‚Äëword attractors.
- **Amplitude calibration looks fine** (RMS near targets; adapter.scale ‚âà 1.0), so the issue is semantic alignment, not scale.

**Diagnosis:** We are supervising only the `t=0` decision (first‚Äëtoken CE) and relying on scalar RMS calibration. That does not provide enough signal for steps 0‚Äì3 to land on the same distribution the model uses under text prompting. As a result, decoding enters a generic basin and never recovers within a 12‚Äëtoken budget.

#### Attempted solution (what we will change)
We are adding early‚Äëstep guidance + a slightly more expressive prefix mapping, plus a guardrail check.

1.  **K‚Äëtoken teacher‚Äëforced CE (K=4) after the "Answer: " anchor**
    - Supervise the first 4 answer tokens under the latent prefix (teacher forcing).
    - Keep the existing first‚Äëtoken CE; fold it into this K‚Äëstep average.
    - Loss weights to start: `Œª_first = 1.0`, `Œª_kce = 0.5`.
2.  **Prefix knowledge distillation (KD) for `t=0..K-1` from the text‚Äëprompted teacher**
    - Run the same LLM with the text prompt and teacher‚Äëforce `t=0..K-1` to get teacher logits.
    - Minimize `KL(teacher || latent‚Äëstudent)` over those steps.
    - Loss weight to start: `Œª_kd = 0.5` (lower to 0.25 if unstable).
3.  **Per‚Äëchannel affine calibration on the prefix (Œ≥, Œ≤)**
    - After RMS calibration, apply a learnable element‚Äëwise scale and bias on the injected prefix to correct directional mismatch (not just magnitude).
    - L2‚Äëregularize `(Œ≥‚àí1, Œ≤)` with weight ‚âà 1e‚Äë4.
4.  **Upgrade the adapter to a tiny 2‚Äëlayer MLP (GELU)**
    - `Linear(d_z ‚Üí 4¬∑d_model) ‚Üí GELU ‚Üí Linear(4¬∑d_model ‚Üí d_model)` with WD ‚âà 1e‚Äë4.
    - This gives the encoder a small nonlinearity to map latent space into the LLM‚Äôs prefix manifold.
5.  **Eval‚Äëonly nudges (temporary, to reflect progress sooner)**
    - *First token decode:* `top_p=0.9`, `temperature=0.7` (`t=0` only), then deterministic.
    - *Prefix gain schedule:* `gain@t0=1.25`, `gain@t1=1.10`, then 1.0.
    - Reduce `eos_ban_steps` from 6 ‚Üí 0‚Äì1 to avoid forced babbling on short answers.
    - *(Optional demo‚Äëonly)* light stop‚Äëlist at `t=0` for `the, of, and, to, in, a, is, was` to remove the most common attractors.
6.  **Sanity check: anchor/label alignment assertion (both tokenizers)**
    - Verify the first gold token after `"Answer: "` is the same id used as `y_gold[:,0]` for each model (Llama/Qwen). An off‚Äëby‚Äëone here would exactly produce the observed flat first‚Äëtoken CE.

#### Why we believe this will work
- **Multi‚Äëstep supervision (K‚Äëtoken CE)** gives the model a short guided runway so it learns not just which token to start with, but also how to stay on the answer manifold through steps 1‚Äì3‚Äîprecisely where we collapse today.
- **Prefix KD** forces the latent‚Äëprompted distribution at early steps to match the text‚Äëprompted distribution, directly transferring the text baseline‚Äôs behavior (our text F1 is good: Llama ‚âà‚ÄØ0.80, Qwen ‚âà‚ÄØ0.85).
- **Per‚Äëchannel affine + tiny MLP** add just enough expressiveness to correct directional/shape mismatches that scalar RMS cannot fix; this is a common failure mode behind ‚Äúfunction‚Äëword first token‚Äù degeneration.
- **Eval nudges** remove decode‚Äëtime headwinds so training gains show up immediately, improving stakeholder confidence while the new losses converge.

#### Expected acceptance signals
- **FirstTok@1** should move from ~3‚Äì6% into the teens (Top‚Äë5 into the 30‚Äì40% range).
- Degenerate "the/of/and" first tokens largely disappear in the debug print.
- Latent F1/EM increase materially above the token‚Äëbudget baseline (currently ~0.04 F1 for Llama), trending toward the text counterpart.

#### Implementation notes (concise)
- **K-step CE under latent prefix (teacher forcing)**
  ```python
  K = 4
  loss_kce = sum(F.cross_entropy(logits_latent[:, t, :], y_gold[:, t]) for t in range(K)) / K
  loss = loss_main + Œª_first*first_token_ce + Œª_kce*loss_kce    ```

### 2025-09-22 ‚Äî Stage C eval crash (chat literal)

- **Error:** `UnboundLocalError: local variable 'strip_literal' referenced before assignment` during Stage‚ÄØC evaluation.
- **Cause:** The chat-mode prompt path stripped the `Answer: ` literal and attempted to reattach it before the literal was initialised in the anchor loop.
- **Fix:** Initialise the literal once (from `config.json` or the default) before building `anchor_info`, then reuse it when constructing prompts and anchors. Evaluation now completes and text baselines recover.

### 2025-09-22 ‚Äî Stage‚ÄØA warm-up & chat-template baseline repair

- **Pipeline update:** `run_scoped_softprompt_multi.sh` now performs a Stage‚ÄØA latent fit (encoder + adapters unfrozen) before the scoped Stage‚ÄØB prefix training, saving the first pass to `ckpt/stageA` and resuming from it with the encoder frozen. This prevents Stage‚ÄØB from starting with random latents.
- **Training sanity:** `_assert_t0_alignment` skips its check when chat templates are active, eliminating false warnings about first-token mismatches under templated prompts.
- **Evaluation fix:** `format_with_chat_template` always routes through the tokenizer‚Äôs own chat template and appends `"Answer: "` afterward, so text baselines retain model-specific headers instead of falling back to plain ‚ÄúAssistant:‚Äù scaffolds.
- **Post-mortem:** The initial Stage‚ÄØC rerun still showed zero text EM/F1 because we reloaded prefix-tuning adapters *before* computing text baselines. Evaluation now measures text prompts using the raw base checkpoints and only attaches prefix adapters afterwards for latent runs.

### 2025-09-22 ‚Äî Stage‚ÄØA instability (fix)

- Stage‚ÄØA gradients were spiking into the 500‚Äì800 range, starving the latent encoder of real progress. We made clipping the default (`--max_grad_norm=1.0`) in `latentwire/train.py` and reduced the Stage‚ÄØA/Stage‚ÄØB first-token + K-token weights in `scripts/run_scoped_softprompt_multi.sh` to stabilise optimisation. These knobs apply automatically for future runs; setting `--max_grad_norm <= 0` still disables clipping for experiments.
- Stage‚ÄØB now keeps the encoder trainable while prefix-tuning so the warmed-up latent model can continue improving instead of freezing at a random initialisation.
- Enabled a gentle cosine schedule for the first-token CE (peaks capped at 2.5/3.0) and turned on KD for the first K steps in both Stage‚ÄØA and Stage‚ÄØB. This keeps gradients in check while distilling the text baseline into the latent path during smoke runs, giving the latent wire a fighting chance before the hero sweep.
- Stage‚ÄØB now resumes from Stage‚ÄØA weights with `--reset_epoch`, so we reuse the learned latent encoder without inheriting Stage‚ÄØA's epoch counter; each stage now cleanly runs its own four epochs.
- Stage‚ÄØB no longer freezes the encoder; instead we resume from Stage‚ÄØA, reset the epoch counter, drop the first-token peak slightly (2.2), and lower the LR (5e-5) so the encoder and prefix continue to improve together without blowing up gradients.
- Both stages now add light state-KD (`state_kd_weight=0.1`) and use a lower LR (`5e-5`) so the latent prefix is nudged toward the text teacher‚Äôs early-layer activations during smoke runs; this should move first-token losses faster and reduce the need for ad-hoc tuning before the hero sweep.
- Default smoke runs now keep Stage‚ÄØA at 4 epochs but extend Stage‚ÄØB to 6 (hero: 6/10), export `TOKENIZERS_PARALLELISM=false`, and disable `use_cache` in eval, which clears the repeated tokenizer/past-key warnings in the logs.
- Stage‚ÄØB now trains on a larger sampled subset (default 1.3k vs 640) while Stage‚ÄØA keeps the smaller 640 batch; the extra data plus longer epoch budget should help the prefix/encoder continue to improve during smoke runs before we scale to hero configurations.
- Stage‚ÄØC now evaluates with a mild prefix gain (`1.1`) to counteract under‚Äëscaling during decode; this will be our default until the latent first-token accuracy stabilises.
- Stage‚ÄØA starts with latent dropout (`keep_start=0.7`) and Stage‚ÄØB starts even lower (`0.5`), annealing to 1.0; combined with state KD it mixes teacher tokens into the latent path early on so first-token learning no longer stalls.
- **Next intervention plan (latent acceptance):**
  1. **Mixed text/latent warm-up.** For the first Stage‚ÄØB epoch alternate batches between text teacher forcing and latent-prefix teacher forcing. This injects clean gold scaffolds at the moment the encoder/adapters are most fragile, which should push first-token top‚Äë1 into double digits and kick latent F1 off the floor.
  2. **Shared + per-model latent slices w/ deeper adapters.** Split `latent_len` into `[shared || llama_private || qwen_private]` (e.g., 32‚Üí20/6/6) and upgrade adapters to 2-layer MLPs with residual. This gives each model enough dedicated bandwidth to interpret the shared wire without fighting the other, particularly important because Qwen‚Äôs first-token acceptance remains 0%.
  3. **Tiny LoRA fallback.** If the above still leaves latent F1 >10 points behind text, attach r=4 LoRA to the first 4 attention blocks on each LLM. This keeps the story scoped while letting the models learn how to read the latent prefix instead of being purely frozen.
  4. **Parallel Llama/Qwen passes.** Once latent learning is healthy, run both LLM updates concurrently (Accelerate or manual threading) so all four GPUs are busy; that roughly halves turn-around time for smoke sweeps and hero runs.
- **Next steps:** Re-run Stage‚ÄØA‚ÜíStage‚ÄØB‚ÜíStage‚ÄØC to confirm text EM/F1 recover, then inspect latent metrics with the warmed-up wire.

### 2025-09-25 ‚Äî Single-model warm-up + runner (today)

- Added optional model selection to `latentwire/train.py` (`--models` now honours `llama`/`qwen` subsets) so we can train a single backend without loading the other 7B checkpoint. Checkpoint loading/saving now adapts to whichever adapters are present.
- Implemented the Stage‚ÄØB text‚Üîlatent warm-up (controlled via `--warmup_text_latent_steps` / `--warmup_text_latent_epochs`). When enabled we alternate full-text and latent teacher forcing for the initial steps; logging now tags each batch `L` (latent) or `T` (text) so we can verify the schedule.
- Updated `scripts/run_scoped_softprompt_multi.sh` to enable a one-epoch warm-up during Stage‚ÄØB, and added `scripts/run_llama_single.sh` for the Llama-only pipeline (Stage‚ÄØA/B/C). The new runner defaults to smoke-sized budgets and accepts `--hero` for longer sweeps.
- Known issue: `pytest -q` currently fails on this workstation because Torch cannot locate `libtorch_cpu.dylib` in the host Anaconda env; rerun inside the project venv/conda env before publishing results.
- Fixed a regression spotted in the latest smoke logs where Stage‚ÄØA aborted with an `IndentationError` (`state_blob` block in `latentwire/train.py`). The periodic checkpoint save now has the correct indentation and we only emit the warm-anchor metadata once per checkpoint record.
- Warm-up now includes an explicit embedding-alignment term: during text-mode steps we match the first few gold answer embeddings (default 4 tokens, weight 0.5) against the adapter output. Both `scripts/run_scoped_softprompt_multi.sh` and `scripts/run_llama_single.sh` wire the new `--warmup_align_tokens/--warmup_align_weight` knobs so the gradient actually reaches the encoder/adapters instead of only exercising the frozen teachers.
- Alignment now skips any leading BOS tokens when computing the warm-up loss so single-token answers still contribute signal; the warm-up path also adds a teacher-forced cross-entropy term during text batches and logs those warm-up steps so we can track `align`/`text_tf` in real time. Stage‚ÄØC summary reports ‚Äújoint‚Äù metrics as `n/a` when only one model is active.
- Upgraded the per-model adapters to a residual two-layer MLP and bumped the single-model runner defaults (`adapter_hidden_mult=4`, `adapter_dropout=0.1`, `latent_private_len=12`). Warm-up now runs for two epochs with stronger alignment/teacher weights (`warmup_text_latent_epochs=2`, `warmup_align_weight=1.0`, `warmup_text_teacher_weight=1.5`) so the adapter can actually absorb the teacher signal before we switch to latent-only batches.
- Default device maps in `run_llama_single.sh` and the multi-model runner stay on HuggingFace's `auto` setting; to encourage a more even split across the listed GPUs set `GPU_MEM_GIB` (e.g., `GPU_MEM_GIB=60`) before launching or override `LLAMA_DEVICE_MAP`/`QWEN_DEVICE_MAP` manually.
- Evaluation now respects the active model subset when loading the encoder (fixes STQuery checkpoints produced with private latent slices for single-model runs).
