/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda
Number of GPUs: 4
Samples: 1000
Batch size: 64
Output: runs/embed_diagnostics

Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3435.84it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.68s/it]
Model loaded! (16.91s)

Caching vocabulary statistics...
  Vocab RMS: 0.0105
  Vocab mean: 0.000023
  Vocab std: 0.0106
  Time: 0.04s
  Saved vocab stats to runs/embed_diagnostics/vocab_stats.json

Loading 1000 examples from squad...
  Loaded 1000 examples (1.97s)

================================================================================
ANALYZING TEXT EMBEDDINGS (Ground Truth)
================================================================================
Processing 1000 examples in 16 batches of size 64...
  Batch 1/16 (examples 0-64)...
  Batch 6/16 (examples 320-384)...
  Batch 11/16 (examples 640-704)...
  Batch 16/16 (examples 960-1000)...

Collected text embeddings: torch.Size([1, 172209, 4096])
  Total tokens: 172209
  Avg tokens/example: 172.2
  Time: 1.98s (86946 tokens/sec)
  Throughput: 504.9 examples/sec
  Moving embeddings to cuda...
    Computing vocab cosine similarity for 172209 tokens in 169 chunks...
      Chunk 1/169...
      Chunk 17/169...
      Chunk 33/169...
      Chunk 49/169...
      Chunk 65/169...
      Chunk 81/169...
      Chunk 97/169...
      Chunk 113/169...
      Chunk 129/169...
      Chunk 145/169...
      Chunk 161/169...

Text Embedding Statistics:
  Per-token RMS: min=0.0053, max=0.0145, mean=0.0084, std=0.0019
  Overall RMS: 0.0086
  Nearest vocab cosine: mean=nan
  Analysis time: 0.63s
  Saved analysis to runs/embed_diagnostics/text_embeddings_analysis.json

Testing transforms on text embeddings...
  Transform testing time: 0.67s
  Saved transforms to runs/embed_diagnostics/text_transforms.json

================================================================================
ANALYZING SYNTHETIC 'BAD' EMBEDDINGS
================================================================================

Generating synthetic embeddings that simulate common failure modes...
Synthetic embeddings: torch.Size([1, 172209, 4096])
  Mode: 100× magnitude + noise
  Generation time: 0.00s
    Computing vocab cosine similarity for 172209 tokens in 169 chunks...
      Chunk 1/169...
      Chunk 17/169...
      Chunk 33/169...
      Chunk 49/169...
      Chunk 65/169...
      Chunk 81/169...
      Chunk 97/169...
      Chunk 113/169...
      Chunk 129/169...
      Chunk 145/169...
      Chunk 161/169...

Synthetic Embedding Statistics:
  Per-token RMS: min=9.5156, max=10.5078, mean=10.0391, std=0.1121
  Overall RMS: 10.0391
  Nearest vocab cosine: mean=nan
  Analysis time: 0.34s
  Saved analysis to runs/embed_diagnostics/learned_embeddings_analysis.json

Testing transforms on synthetic embeddings...
  Transform testing time: 0.66s
  Saved transforms to runs/embed_diagnostics/learned_transforms.json

================================================================================
COMPARISON: Text vs Learned
================================================================================

1. Per-token RMS:
  Text:    mean=0.0084, std=0.0019
  Learned: mean=10.0391, std=0.1121
  Ratio: 1198.40×

2. Overall RMS:
  Text:    0.0086
  Learned: 10.0391
  Ratio: 1168.60×

3. Nearest vocab cosine:
  Text:    nan
  Learned: nan
  Diff: nan

4. Per-token RMS variation:
  Text CV (std/mean):    0.2284
  Learned CV (std/mean): 0.0112
  → DESTROYED

================================================================================
Results saved to: runs/embed_diagnostics
  - diagnostics.json (comprehensive results)
  - experiment_log.json (timing and config)
  - vocab_stats.json
  - text_embeddings_analysis.json
  - text_transforms.json
  - learned_embeddings_analysis.json
  - learned_transforms.json

Total time: 25.48s (0.4min)
================================================================================

KEY INSIGHTS:

1. RMS Scaling Effect:
   Before: per-token RMS std = 0.0019
   After:  per-token RMS std = 0.0000
   → RMS scaling DESTROYS per-token variation

2. Batch Distribution Effect:
   Per-token RMS variation after: 0.0024
   → Batch dist PRESERVES variation

3. Why Learned Embeddings Fail:
   ⚠️  Magnitude is 1168.6× too large
   ⚠️  Per-token variation is wrong: CV=0.0112 vs 0.2284
