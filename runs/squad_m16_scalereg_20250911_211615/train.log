/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Llama hidden size: 2048, Qwen hidden size: 896
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/5
  step 10/256 | loss_L=3.1309 | loss_Q=6.4640 | scale_pen(L)=2.3552e-07 | scale_pen(Q)=8.6747e-07 | grad_norm=24.49 | sec/step~0.40
  step 20/256 | loss_L=2.7487 | loss_Q=3.7520 | scale_pen(L)=1.0329e-07 | scale_pen(Q)=3.3153e-06 | grad_norm=12.81 | sec/step~0.26
  step 30/256 | loss_L=2.3944 | loss_Q=2.5069 | scale_pen(L)=1.2785e-08 | scale_pen(Q)=5.6113e-06 | grad_norm=4.43 | sec/step~0.22
  step 40/256 | loss_L=2.1812 | loss_Q=2.3579 | scale_pen(L)=2.0305e-09 | scale_pen(Q)=6.8350e-06 | grad_norm=3.28 | sec/step~0.19
  step 50/256 | loss_L=2.3827 | loss_Q=2.4263 | scale_pen(L)=7.0531e-09 | scale_pen(Q)=7.4374e-06 | grad_norm=3.05 | sec/step~0.19
  step 60/256 | loss_L=2.0655 | loss_Q=2.1343 | scale_pen(L)=1.4268e-08 | scale_pen(Q)=7.9424e-06 | grad_norm=3.08 | sec/step~0.19
  step 70/256 | loss_L=1.9950 | loss_Q=1.9860 | scale_pen(L)=1.2333e-07 | scale_pen(Q)=8.4932e-06 | grad_norm=2.49 | sec/step~0.19
  step 80/256 | loss_L=2.0019 | loss_Q=1.9719 | scale_pen(L)=2.7738e-07 | scale_pen(Q)=9.0617e-06 | grad_norm=2.31 | sec/step~0.18
  step 90/256 | loss_L=2.2005 | loss_Q=2.1424 | scale_pen(L)=6.3983e-07 | scale_pen(Q)=9.4212e-06 | grad_norm=2.22 | sec/step~0.19
  step 100/256 | loss_L=1.9050 | loss_Q=1.8853 | scale_pen(L)=8.7995e-07 | scale_pen(Q)=9.7080e-06 | grad_norm=2.15 | sec/step~0.19
  [debug:L] a.scale=1.0009 | Z.std=0.9994 Z.mean||=15.9908 | p.std=0.5760 p.mean||=26.0655
  [debug:Q] a.scale=1.0031 | Z.std=0.9994 Z.mean||=15.9908 | p.std=0.5773 p.mean||=17.2785
  step 110/256 | loss_L=2.0972 | loss_Q=2.0766 | scale_pen(L)=1.2704e-06 | scale_pen(Q)=9.8752e-06 | grad_norm=2.43 | sec/step~0.19
  step 120/256 | loss_L=1.8610 | loss_Q=1.7840 | scale_pen(L)=1.5032e-06 | scale_pen(Q)=9.9841e-06 | grad_norm=1.55 | sec/step~0.20
  step 130/256 | loss_L=1.9417 | loss_Q=1.8354 | scale_pen(L)=1.8654e-06 | scale_pen(Q)=1.0077e-05 | grad_norm=1.82 | sec/step~0.19
  step 140/256 | loss_L=2.0016 | loss_Q=1.9954 | scale_pen(L)=2.1068e-06 | scale_pen(Q)=1.0310e-05 | grad_norm=1.51 | sec/step~0.19
  step 150/256 | loss_L=1.8072 | loss_Q=1.7952 | scale_pen(L)=2.7730e-06 | scale_pen(Q)=1.0444e-05 | grad_norm=1.71 | sec/step~0.19
  step 160/256 | loss_L=1.4657 | loss_Q=1.3749 | scale_pen(L)=3.3865e-06 | scale_pen(Q)=1.0492e-05 | grad_norm=1.25 | sec/step~0.20
  step 170/256 | loss_L=1.7830 | loss_Q=1.7018 | scale_pen(L)=4.1729e-06 | scale_pen(Q)=1.0374e-05 | grad_norm=1.46 | sec/step~0.19
  step 180/256 | loss_L=2.0103 | loss_Q=2.0055 | scale_pen(L)=5.0994e-06 | scale_pen(Q)=1.0451e-05 | grad_norm=1.79 | sec/step~0.20
  step 190/256 | loss_L=2.0639 | loss_Q=1.9795 | scale_pen(L)=6.0147e-06 | scale_pen(Q)=1.0349e-05 | grad_norm=1.53 | sec/step~0.19
  step 200/256 | loss_L=1.6376 | loss_Q=1.6231 | scale_pen(L)=6.8899e-06 | scale_pen(Q)=1.0224e-05 | grad_norm=1.25 | sec/step~0.19
  [debug:L] a.scale=1.0026 | Z.std=0.9996 Z.mean||=15.9929 | p.std=0.5902 p.mean||=26.7050
  [debug:Q] a.scale=1.0032 | Z.std=0.9996 Z.mean||=15.9929 | p.std=0.5842 p.mean||=17.4874
  step 210/256 | loss_L=1.7647 | loss_Q=1.7930 | scale_pen(L)=8.5419e-06 | scale_pen(Q)=1.0488e-05 | grad_norm=1.41 | sec/step~0.19
  step 220/256 | loss_L=1.8418 | loss_Q=1.8458 | scale_pen(L)=9.7222e-06 | scale_pen(Q)=1.0359e-05 | grad_norm=1.55 | sec/step~0.19
  step 230/256 | loss_L=1.6905 | loss_Q=1.6711 | scale_pen(L)=1.1828e-05 | scale_pen(Q)=1.0567e-05 | grad_norm=1.31 | sec/step~0.19
  step 240/256 | loss_L=1.7892 | loss_Q=1.7881 | scale_pen(L)=1.4207e-05 | scale_pen(Q)=1.0531e-05 | grad_norm=1.25 | sec/step~0.19
  step 250/256 | loss_L=1.7825 | loss_Q=1.7855 | scale_pen(L)=1.6436e-05 | scale_pen(Q)=1.0695e-05 | grad_norm=1.37 | sec/step~0.18
  step 256/256 | loss_L=1.5910 | loss_Q=1.6078 | scale_pen(L)=1.7865e-05 | scale_pen(Q)=1.0721e-05 | grad_norm=1.15 | sec/step~0.19
  [debug:L] a.scale=1.0042 | Z.std=0.9996 Z.mean||=15.9943 | p.std=0.5976 p.mean||=27.0396
  [debug:Q] a.scale=1.0033 | Z.std=0.9996 Z.mean||=15.9943 | p.std=0.5895 p.mean||=17.6462
Epoch 2/5
  step 10/256 | loss_L=1.6041 | loss_Q=1.5993 | scale_pen(L)=2.0751e-05 | scale_pen(Q)=1.0774e-05 | grad_norm=1.55 | sec/step~0.19
  step 20/256 | loss_L=1.6227 | loss_Q=1.7046 | scale_pen(L)=2.1917e-05 | scale_pen(Q)=1.0834e-05 | grad_norm=1.29 | sec/step~0.20
  step 30/256 | loss_L=1.5297 | loss_Q=1.5089 | scale_pen(L)=2.4869e-05 | scale_pen(Q)=1.0940e-05 | grad_norm=1.37 | sec/step~0.19
  step 40/256 | loss_L=1.7124 | loss_Q=1.7516 | scale_pen(L)=2.6851e-05 | scale_pen(Q)=1.1021e-05 | grad_norm=1.19 | sec/step~0.19
  step 50/256 | loss_L=1.5083 | loss_Q=1.5485 | scale_pen(L)=2.8359e-05 | scale_pen(Q)=1.1054e-05 | grad_norm=1.27 | sec/step~0.19
  step 60/256 | loss_L=1.5831 | loss_Q=1.6051 | scale_pen(L)=3.0349e-05 | scale_pen(Q)=1.1162e-05 | grad_norm=0.98 | sec/step~0.20
  step 70/256 | loss_L=1.8232 | loss_Q=1.7010 | scale_pen(L)=3.3000e-05 | scale_pen(Q)=1.1157e-05 | grad_norm=1.22 | sec/step~0.19
  step 80/256 | loss_L=1.4781 | loss_Q=1.4426 | scale_pen(L)=3.5194e-05 | scale_pen(Q)=1.1254e-05 | grad_norm=0.97 | sec/step~0.20
  step 90/256 | loss_L=1.6078 | loss_Q=1.5721 | scale_pen(L)=3.7984e-05 | scale_pen(Q)=1.1340e-05 | grad_norm=1.16 | sec/step~0.19
  step 100/256 | loss_L=1.4401 | loss_Q=1.3887 | scale_pen(L)=4.1022e-05 | scale_pen(Q)=1.1734e-05 | grad_norm=1.14 | sec/step~0.20
  [debug:L] a.scale=1.0064 | Z.std=0.9997 Z.mean||=15.9954 | p.std=0.6066 p.mean||=27.4447
  [debug:Q] a.scale=1.0034 | Z.std=0.9997 Z.mean||=15.9954 | p.std=0.5947 p.mean||=17.8013
  step 110/256 | loss_L=1.7377 | loss_Q=1.7378 | scale_pen(L)=4.4886e-05 | scale_pen(Q)=1.1895e-05 | grad_norm=1.07 | sec/step~0.19
  step 120/256 | loss_L=1.3231 | loss_Q=1.3265 | scale_pen(L)=4.6872e-05 | scale_pen(Q)=1.2088e-05 | grad_norm=0.88 | sec/step~0.20
  step 130/256 | loss_L=1.6312 | loss_Q=1.6343 | scale_pen(L)=4.8909e-05 | scale_pen(Q)=1.2176e-05 | grad_norm=1.15 | sec/step~0.19
  step 140/256 | loss_L=1.4920 | loss_Q=1.4397 | scale_pen(L)=5.1284e-05 | scale_pen(Q)=1.2002e-05 | grad_norm=1.40 | sec/step~0.19
  step 150/256 | loss_L=1.4073 | loss_Q=1.4026 | scale_pen(L)=5.3681e-05 | scale_pen(Q)=1.2250e-05 | grad_norm=1.05 | sec/step~0.19
  step 160/256 | loss_L=1.3909 | loss_Q=1.3999 | scale_pen(L)=5.6045e-05 | scale_pen(Q)=1.2122e-05 | grad_norm=1.58 | sec/step~0.19
  step 170/256 | loss_L=1.5964 | loss_Q=1.5970 | scale_pen(L)=5.7824e-05 | scale_pen(Q)=1.2370e-05 | grad_norm=1.17 | sec/step~0.19
  step 180/256 | loss_L=1.6152 | loss_Q=1.4963 | scale_pen(L)=6.0183e-05 | scale_pen(Q)=1.2380e-05 | grad_norm=0.95 | sec/step~0.20
  step 190/256 | loss_L=1.8522 | loss_Q=1.8364 | scale_pen(L)=6.4008e-05 | scale_pen(Q)=1.2476e-05 | grad_norm=1.19 | sec/step~0.19
  step 200/256 | loss_L=1.5534 | loss_Q=1.5492 | scale_pen(L)=6.6461e-05 | scale_pen(Q)=1.2709e-05 | grad_norm=1.20 | sec/step~0.19
  [debug:L] a.scale=1.0082 | Z.std=0.9997 Z.mean||=15.9956 | p.std=0.6136 p.mean||=27.7572
  [debug:Q] a.scale=1.0036 | Z.std=0.9997 Z.mean||=15.9956 | p.std=0.5991 p.mean||=17.9312
  step 210/256 | loss_L=1.2120 | loss_Q=1.2078 | scale_pen(L)=6.8265e-05 | scale_pen(Q)=1.2974e-05 | grad_norm=0.89 | sec/step~0.19
  step 220/256 | loss_L=1.4330 | loss_Q=1.3987 | scale_pen(L)=7.0060e-05 | scale_pen(Q)=1.2989e-05 | grad_norm=1.09 | sec/step~0.19
  step 230/256 | loss_L=1.4893 | loss_Q=1.5109 | scale_pen(L)=7.1252e-05 | scale_pen(Q)=1.3241e-05 | grad_norm=0.97 | sec/step~0.19
  step 240/256 | loss_L=1.4147 | loss_Q=1.4091 | scale_pen(L)=7.1163e-05 | scale_pen(Q)=1.3334e-05 | grad_norm=1.10 | sec/step~0.19
  step 250/256 | loss_L=1.2305 | loss_Q=1.2011 | scale_pen(L)=7.3428e-05 | scale_pen(Q)=1.3493e-05 | grad_norm=1.55 | sec/step~0.19
  step 256/256 | loss_L=1.4842 | loss_Q=1.4318 | scale_pen(L)=7.4721e-05 | scale_pen(Q)=1.3558e-05 | grad_norm=1.03 | sec/step~0.20
  [debug:L] a.scale=1.0086 | Z.std=0.9998 Z.mean||=15.9961 | p.std=0.6170 p.mean||=27.9084
  [debug:Q] a.scale=1.0037 | Z.std=0.9998 Z.mean||=15.9961 | p.std=0.6025 p.mean||=18.0340
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 512
Epoch 3/5
  step 10/256 | loss_L=1.6631 | loss_Q=1.5818 | scale_pen(L)=7.6151e-05 | scale_pen(Q)=1.3370e-05 | grad_norm=1.00 | sec/step~0.19
  step 20/256 | loss_L=1.2580 | loss_Q=1.2262 | scale_pen(L)=7.8795e-05 | scale_pen(Q)=1.3150e-05 | grad_norm=0.78 | sec/step~0.20
  step 30/256 | loss_L=1.4560 | loss_Q=1.4033 | scale_pen(L)=7.9181e-05 | scale_pen(Q)=1.3169e-05 | grad_norm=0.79 | sec/step~0.19
  step 40/256 | loss_L=1.4777 | loss_Q=1.4690 | scale_pen(L)=7.9749e-05 | scale_pen(Q)=1.3094e-05 | grad_norm=0.89 | sec/step~0.20
  step 50/256 | loss_L=1.4992 | loss_Q=1.4706 | scale_pen(L)=8.2385e-05 | scale_pen(Q)=1.3024e-05 | grad_norm=1.00 | sec/step~0.19
  step 60/256 | loss_L=1.3248 | loss_Q=1.3509 | scale_pen(L)=8.3551e-05 | scale_pen(Q)=1.3107e-05 | grad_norm=0.94 | sec/step~0.19
  step 70/256 | loss_L=1.5887 | loss_Q=1.6030 | scale_pen(L)=8.4657e-05 | scale_pen(Q)=1.3243e-05 | grad_norm=1.05 | sec/step~0.19
  step 80/256 | loss_L=1.4026 | loss_Q=1.3401 | scale_pen(L)=8.5826e-05 | scale_pen(Q)=1.2921e-05 | grad_norm=1.10 | sec/step~0.20
  step 90/256 | loss_L=1.2045 | loss_Q=1.1635 | scale_pen(L)=8.6395e-05 | scale_pen(Q)=1.3128e-05 | grad_norm=0.85 | sec/step~0.19
  step 100/256 | loss_L=1.5190 | loss_Q=1.4635 | scale_pen(L)=8.7041e-05 | scale_pen(Q)=1.2950e-05 | grad_norm=1.02 | sec/step~0.20
  [debug:L] a.scale=1.0093 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.6235 p.mean||=28.2039
  [debug:Q] a.scale=1.0036 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.6061 p.mean||=18.1404
  step 110/256 | loss_L=1.3486 | loss_Q=1.2716 | scale_pen(L)=8.9214e-05 | scale_pen(Q)=1.2994e-05 | grad_norm=0.77 | sec/step~0.19
  step 120/256 | loss_L=1.4783 | loss_Q=1.4623 | scale_pen(L)=9.3026e-05 | scale_pen(Q)=1.3003e-05 | grad_norm=0.80 | sec/step~0.19
  step 130/256 | loss_L=1.6971 | loss_Q=1.6870 | scale_pen(L)=9.3719e-05 | scale_pen(Q)=1.2695e-05 | grad_norm=1.13 | sec/step~0.19
  step 140/256 | loss_L=1.6347 | loss_Q=1.5583 | scale_pen(L)=9.4469e-05 | scale_pen(Q)=1.2959e-05 | grad_norm=1.11 | sec/step~0.19
  step 150/256 | loss_L=1.3645 | loss_Q=1.3609 | scale_pen(L)=9.6016e-05 | scale_pen(Q)=1.2809e-05 | grad_norm=1.11 | sec/step~0.19
  step 160/256 | loss_L=1.3504 | loss_Q=1.3613 | scale_pen(L)=9.6088e-05 | scale_pen(Q)=1.2646e-05 | grad_norm=1.22 | sec/step~0.19
  step 170/256 | loss_L=1.6184 | loss_Q=1.5759 | scale_pen(L)=9.7035e-05 | scale_pen(Q)=1.2684e-05 | grad_norm=0.80 | sec/step~0.19
  step 180/256 | loss_L=1.1721 | loss_Q=1.1579 | scale_pen(L)=9.8837e-05 | scale_pen(Q)=1.2588e-05 | grad_norm=0.83 | sec/step~0.19
  step 190/256 | loss_L=1.2282 | loss_Q=1.1761 | scale_pen(L)=9.9524e-05 | scale_pen(Q)=1.2318e-05 | grad_norm=1.40 | sec/step~0.19
  step 200/256 | loss_L=1.1049 | loss_Q=1.1229 | scale_pen(L)=9.9745e-05 | scale_pen(Q)=1.2202e-05 | grad_norm=1.16 | sec/step~0.18
  [debug:L] a.scale=1.0100 | Z.std=0.9998 Z.mean||=15.9971 | p.std=0.6296 p.mean||=28.4741
  [debug:Q] a.scale=1.0035 | Z.std=0.9998 Z.mean||=15.9971 | p.std=0.6100 p.mean||=18.2558
  step 210/256 | loss_L=1.5014 | loss_Q=1.5272 | scale_pen(L)=9.9795e-05 | scale_pen(Q)=1.2149e-05 | grad_norm=0.83 | sec/step~0.19
  step 220/256 | loss_L=1.6026 | loss_Q=1.6236 | scale_pen(L)=1.0187e-04 | scale_pen(Q)=1.1916e-05 | grad_norm=0.98 | sec/step~0.18
  step 230/256 | loss_L=1.4167 | loss_Q=1.4163 | scale_pen(L)=1.0151e-04 | scale_pen(Q)=1.1725e-05 | grad_norm=0.90 | sec/step~0.20
  step 240/256 | loss_L=1.3598 | loss_Q=1.3240 | scale_pen(L)=1.0238e-04 | scale_pen(Q)=1.1793e-05 | grad_norm=0.99 | sec/step~0.19
  step 250/256 | loss_L=1.7000 | loss_Q=1.6112 | scale_pen(L)=1.0408e-04 | scale_pen(Q)=1.1669e-05 | grad_norm=1.08 | sec/step~0.18
  step 256/256 | loss_L=1.2378 | loss_Q=1.1736 | scale_pen(L)=1.0329e-04 | scale_pen(Q)=1.1502e-05 | grad_norm=0.95 | sec/step~0.18
  [debug:L] a.scale=1.0102 | Z.std=0.9998 Z.mean||=15.9974 | p.std=0.6323 p.mean||=28.5981
  [debug:Q] a.scale=1.0034 | Z.std=0.9998 Z.mean||=15.9974 | p.std=0.6116 p.mean||=18.3042
Epoch 4/5
  step 10/256 | loss_L=1.3045 | loss_Q=1.2635 | scale_pen(L)=1.0707e-04 | scale_pen(Q)=1.1405e-05 | grad_norm=0.95 | sec/step~0.18
  step 20/256 | loss_L=1.4949 | loss_Q=1.5342 | scale_pen(L)=1.0838e-04 | scale_pen(Q)=1.1358e-05 | grad_norm=1.13 | sec/step~0.18
  step 30/256 | loss_L=1.4064 | loss_Q=1.3599 | scale_pen(L)=1.0755e-04 | scale_pen(Q)=1.1189e-05 | grad_norm=0.77 | sec/step~0.18
  step 40/256 | loss_L=1.6240 | loss_Q=1.6125 | scale_pen(L)=1.0812e-04 | scale_pen(Q)=1.1157e-05 | grad_norm=0.97 | sec/step~0.18
  step 50/256 | loss_L=1.4308 | loss_Q=1.3992 | scale_pen(L)=1.0610e-04 | scale_pen(Q)=1.1036e-05 | grad_norm=0.79 | sec/step~0.18
  step 60/256 | loss_L=1.5132 | loss_Q=1.4743 | scale_pen(L)=1.0648e-04 | scale_pen(Q)=1.0782e-05 | grad_norm=0.93 | sec/step~0.18
  step 70/256 | loss_L=1.5869 | loss_Q=1.5854 | scale_pen(L)=1.0685e-04 | scale_pen(Q)=1.0504e-05 | grad_norm=0.86 | sec/step~0.18
  step 80/256 | loss_L=1.3889 | loss_Q=1.3868 | scale_pen(L)=1.0695e-04 | scale_pen(Q)=1.0225e-05 | grad_norm=0.88 | sec/step~0.18
  step 90/256 | loss_L=1.2859 | loss_Q=1.3118 | scale_pen(L)=1.0762e-04 | scale_pen(Q)=1.0126e-05 | grad_norm=0.77 | sec/step~0.18
  step 100/256 | loss_L=1.3112 | loss_Q=1.2974 | scale_pen(L)=1.0988e-04 | scale_pen(Q)=9.9939e-06 | grad_norm=0.70 | sec/step~0.19
  [debug:L] a.scale=1.0105 | Z.std=0.9998 Z.mean||=15.9965 | p.std=0.6355 p.mean||=28.7380
  [debug:Q] a.scale=1.0032 | Z.std=0.9998 Z.mean||=15.9965 | p.std=0.6129 p.mean||=18.3408
  step 110/256 | loss_L=1.2629 | loss_Q=1.2534 | scale_pen(L)=1.0929e-04 | scale_pen(Q)=9.7869e-06 | grad_norm=0.94 | sec/step~0.18
  step 120/256 | loss_L=1.4040 | loss_Q=1.3929 | scale_pen(L)=1.0907e-04 | scale_pen(Q)=9.6073e-06 | grad_norm=0.91 | sec/step~0.18
  step 130/256 | loss_L=1.3864 | loss_Q=1.3195 | scale_pen(L)=1.0811e-04 | scale_pen(Q)=9.3861e-06 | grad_norm=0.88 | sec/step~0.18
  step 140/256 | loss_L=1.4542 | loss_Q=1.4075 | scale_pen(L)=1.0765e-04 | scale_pen(Q)=9.1928e-06 | grad_norm=0.79 | sec/step~0.19
  step 150/256 | loss_L=1.1855 | loss_Q=1.1410 | scale_pen(L)=1.0952e-04 | scale_pen(Q)=9.0876e-06 | grad_norm=1.08 | sec/step~0.19
  step 160/256 | loss_L=1.5733 | loss_Q=1.4925 | scale_pen(L)=1.1128e-04 | scale_pen(Q)=9.1070e-06 | grad_norm=1.06 | sec/step~0.18
  step 170/256 | loss_L=1.4055 | loss_Q=1.4135 | scale_pen(L)=1.1142e-04 | scale_pen(Q)=9.1603e-06 | grad_norm=1.10 | sec/step~0.18
  step 180/256 | loss_L=1.3406 | loss_Q=1.3323 | scale_pen(L)=1.1436e-04 | scale_pen(Q)=8.9837e-06 | grad_norm=1.06 | sec/step~0.19
  step 190/256 | loss_L=1.2474 | loss_Q=1.2526 | scale_pen(L)=1.1375e-04 | scale_pen(Q)=8.9715e-06 | grad_norm=0.75 | sec/step~0.18
  step 200/256 | loss_L=1.5432 | loss_Q=1.5151 | scale_pen(L)=1.1167e-04 | scale_pen(Q)=8.9074e-06 | grad_norm=0.74 | sec/step~0.18
  [debug:L] a.scale=1.0106 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.6400 p.mean||=28.9416
  [debug:Q] a.scale=1.0030 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.6167 p.mean||=18.4535
  step 210/256 | loss_L=1.3068 | loss_Q=1.3084 | scale_pen(L)=1.1262e-04 | scale_pen(Q)=8.8825e-06 | grad_norm=0.71 | sec/step~0.19
  step 220/256 | loss_L=1.5472 | loss_Q=1.4991 | scale_pen(L)=1.1345e-04 | scale_pen(Q)=8.6650e-06 | grad_norm=1.07 | sec/step~0.18
  step 230/256 | loss_L=1.2121 | loss_Q=1.2394 | scale_pen(L)=1.1421e-04 | scale_pen(Q)=8.5684e-06 | grad_norm=1.07 | sec/step~0.19
  step 240/256 | loss_L=1.2671 | loss_Q=1.2669 | scale_pen(L)=1.1660e-04 | scale_pen(Q)=8.3921e-06 | grad_norm=1.94 | sec/step~0.19
  step 250/256 | loss_L=1.2290 | loss_Q=1.2440 | scale_pen(L)=1.1554e-04 | scale_pen(Q)=8.5579e-06 | grad_norm=0.76 | sec/step~0.19
  step 256/256 | loss_L=1.3406 | loss_Q=1.2909 | scale_pen(L)=1.1351e-04 | scale_pen(Q)=8.4717e-06 | grad_norm=0.77 | sec/step~0.18
  [debug:L] a.scale=1.0107 | Z.std=0.9997 Z.mean||=15.9957 | p.std=0.6409 p.mean||=28.9802
  [debug:Q] a.scale=1.0029 | Z.std=0.9997 Z.mean||=15.9957 | p.std=0.6158 p.mean||=18.4278
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1024
Epoch 5/5
  step 10/256 | loss_L=1.3075 | loss_Q=1.2923 | scale_pen(L)=1.1154e-04 | scale_pen(Q)=8.3059e-06 | grad_norm=0.82 | sec/step~0.19
  step 20/256 | loss_L=1.3341 | loss_Q=1.3134 | scale_pen(L)=1.1512e-04 | scale_pen(Q)=8.3362e-06 | grad_norm=1.08 | sec/step~0.19
  step 30/256 | loss_L=1.5324 | loss_Q=1.5268 | scale_pen(L)=1.1445e-04 | scale_pen(Q)=8.0564e-06 | grad_norm=0.92 | sec/step~0.18
  step 40/256 | loss_L=1.4664 | loss_Q=1.4369 | scale_pen(L)=1.1440e-04 | scale_pen(Q)=7.6205e-06 | grad_norm=0.77 | sec/step~0.19
  step 50/256 | loss_L=1.4000 | loss_Q=1.3809 | scale_pen(L)=1.1598e-04 | scale_pen(Q)=7.4888e-06 | grad_norm=0.77 | sec/step~0.19
  step 60/256 | loss_L=1.3477 | loss_Q=1.3147 | scale_pen(L)=1.1570e-04 | scale_pen(Q)=7.4745e-06 | grad_norm=0.92 | sec/step~0.19
  step 70/256 | loss_L=1.3414 | loss_Q=1.3083 | scale_pen(L)=1.1447e-04 | scale_pen(Q)=7.3466e-06 | grad_norm=0.73 | sec/step~0.19
  step 80/256 | loss_L=1.6837 | loss_Q=1.6246 | scale_pen(L)=1.1302e-04 | scale_pen(Q)=7.1400e-06 | grad_norm=0.92 | sec/step~0.19
  step 90/256 | loss_L=1.4809 | loss_Q=1.4124 | scale_pen(L)=1.1204e-04 | scale_pen(Q)=6.9426e-06 | grad_norm=0.87 | sec/step~0.19
  step 100/256 | loss_L=1.1753 | loss_Q=1.1786 | scale_pen(L)=1.1059e-04 | scale_pen(Q)=6.8462e-06 | grad_norm=0.80 | sec/step~0.19
  [debug:L] a.scale=1.0105 | Z.std=0.9997 Z.mean||=15.9956 | p.std=0.6438 p.mean||=29.1083
  [debug:Q] a.scale=1.0026 | Z.std=0.9997 Z.mean||=15.9956 | p.std=0.6181 p.mean||=18.4943
  step 110/256 | loss_L=1.4716 | loss_Q=1.4997 | scale_pen(L)=1.1041e-04 | scale_pen(Q)=6.8918e-06 | grad_norm=0.73 | sec/step~0.19
  step 120/256 | loss_L=1.4643 | loss_Q=1.4328 | scale_pen(L)=1.1238e-04 | scale_pen(Q)=6.8144e-06 | grad_norm=0.98 | sec/step~0.19
  step 130/256 | loss_L=1.2639 | loss_Q=1.2319 | scale_pen(L)=1.1385e-04 | scale_pen(Q)=6.6998e-06 | grad_norm=0.67 | sec/step~0.19
  step 140/256 | loss_L=1.4259 | loss_Q=1.3911 | scale_pen(L)=1.1423e-04 | scale_pen(Q)=6.5165e-06 | grad_norm=0.87 | sec/step~0.19
  step 150/256 | loss_L=1.4476 | loss_Q=1.4469 | scale_pen(L)=1.1240e-04 | scale_pen(Q)=6.4020e-06 | grad_norm=0.78 | sec/step~0.19
  step 160/256 | loss_L=1.6442 | loss_Q=1.5907 | scale_pen(L)=1.1140e-04 | scale_pen(Q)=6.3172e-06 | grad_norm=0.87 | sec/step~0.19
  step 170/256 | loss_L=1.3215 | loss_Q=1.2752 | scale_pen(L)=1.1042e-04 | scale_pen(Q)=6.3352e-06 | grad_norm=0.70 | sec/step~0.19
  step 180/256 | loss_L=1.2159 | loss_Q=1.1841 | scale_pen(L)=1.0803e-04 | scale_pen(Q)=6.1636e-06 | grad_norm=0.97 | sec/step~0.18
  step 190/256 | loss_L=1.5385 | loss_Q=1.4672 | scale_pen(L)=1.0667e-04 | scale_pen(Q)=6.0065e-06 | grad_norm=0.76 | sec/step~0.19
  step 200/256 | loss_L=1.3879 | loss_Q=1.3673 | scale_pen(L)=1.0970e-04 | scale_pen(Q)=5.8147e-06 | grad_norm=0.89 | sec/step~0.19
  [debug:L] a.scale=1.0105 | Z.std=0.9997 Z.mean||=15.9957 | p.std=0.6483 p.mean||=29.3100
  [debug:Q] a.scale=1.0024 | Z.std=0.9997 Z.mean||=15.9957 | p.std=0.6198 p.mean||=18.5448
  step 210/256 | loss_L=1.2977 | loss_Q=1.2376 | scale_pen(L)=1.0834e-04 | scale_pen(Q)=5.7105e-06 | grad_norm=0.92 | sec/step~0.19
  step 220/256 | loss_L=1.3712 | loss_Q=1.3244 | scale_pen(L)=1.1054e-04 | scale_pen(Q)=5.5431e-06 | grad_norm=0.79 | sec/step~0.18
  step 230/256 | loss_L=1.3882 | loss_Q=1.3709 | scale_pen(L)=1.1065e-04 | scale_pen(Q)=5.3572e-06 | grad_norm=0.90 | sec/step~0.19
  step 240/256 | loss_L=1.3249 | loss_Q=1.3494 | scale_pen(L)=1.1456e-04 | scale_pen(Q)=5.1501e-06 | grad_norm=0.77 | sec/step~0.19
  step 250/256 | loss_L=1.2318 | loss_Q=1.2453 | scale_pen(L)=1.1281e-04 | scale_pen(Q)=5.1908e-06 | grad_norm=0.90 | sec/step~0.19
  step 256/256 | loss_L=1.3950 | loss_Q=1.3749 | scale_pen(L)=1.1120e-04 | scale_pen(Q)=5.1647e-06 | grad_norm=0.86 | sec/step~0.18
  [debug:L] a.scale=1.0105 | Z.std=0.9997 Z.mean||=15.9950 | p.std=0.6481 p.mean||=29.2964
  [debug:Q] a.scale=1.0023 | Z.std=0.9997 Z.mean||=15.9950 | p.std=0.6213 p.mean||=18.5904
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/squad_m16_scalereg_20250911_211615/ckpt
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Llama hidden size: 2048, Qwen hidden size: 896
⏪ Resuming from: runs/squad_m16_scalereg_20250911_211615/ckpt/state.pt
/projects/m000066/sujinesh/LatentWire/latentwire/train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(path, map_location="cpu")
   -> start_epoch=4, global_step=1280
Epoch 5/5
  step 10/256 | loss_L=2.9781 | loss_Q=2.9302 | scale_pen(L)=7.0185e-06 | scale_pen(Q)=4.5005e-06 | grad_norm=6.49 | sec/step~0.55
  step 20/256 | loss_L=2.5271 | loss_Q=2.1500 | scale_pen(L)=1.0587e-05 | scale_pen(Q)=1.0795e-05 | grad_norm=2.99 | sec/step~0.31
  step 30/256 | loss_L=2.1713 | loss_Q=1.9994 | scale_pen(L)=1.4891e-05 | scale_pen(Q)=1.3862e-05 | grad_norm=2.46 | sec/step~0.23
  step 40/256 | loss_L=2.1719 | loss_Q=2.0006 | scale_pen(L)=1.6036e-05 | scale_pen(Q)=1.5227e-05 | grad_norm=1.46 | sec/step~0.20
  step 50/256 | loss_L=2.3632 | loss_Q=2.1980 | scale_pen(L)=1.3740e-05 | scale_pen(Q)=1.5561e-05 | grad_norm=1.72 | sec/step~0.20
  step 60/256 | loss_L=1.9230 | loss_Q=1.7475 | scale_pen(L)=1.3276e-05 | scale_pen(Q)=1.5451e-05 | grad_norm=1.54 | sec/step~0.19
  step 70/256 | loss_L=1.6909 | loss_Q=1.6276 | scale_pen(L)=1.3202e-05 | scale_pen(Q)=1.5015e-05 | grad_norm=1.55 | sec/step~0.19
  step 80/256 | loss_L=2.0502 | loss_Q=1.9304 | scale_pen(L)=1.3396e-05 | scale_pen(Q)=1.5108e-05 | grad_norm=1.72 | sec/step~0.19
  step 90/256 | loss_L=1.9851 | loss_Q=1.7204 | scale_pen(L)=1.3934e-05 | scale_pen(Q)=1.5419e-05 | grad_norm=1.17 | sec/step~0.18
  step 100/256 | loss_L=1.9536 | loss_Q=1.7788 | scale_pen(L)=1.4424e-05 | scale_pen(Q)=1.5754e-05 | grad_norm=1.62 | sec/step~0.19
  [debug:L] a.scale=0.9962 | Z.std=1.0007 Z.mean||=16.0107 | p.std=0.6386 p.mean||=28.8878
  [debug:Q] a.scale=1.0040 | Z.std=1.0007 Z.mean||=16.0107 | p.std=0.6539 p.mean||=19.5630
  step 110/256 | loss_L=1.8811 | loss_Q=1.7456 | scale_pen(L)=1.4546e-05 | scale_pen(Q)=1.6006e-05 | grad_norm=1.13 | sec/step~0.19
  step 120/256 | loss_L=1.4350 | loss_Q=1.3510 | scale_pen(L)=1.5569e-05 | scale_pen(Q)=1.6406e-05 | grad_norm=1.01 | sec/step~0.19
  step 130/256 | loss_L=1.8142 | loss_Q=1.7275 | scale_pen(L)=1.5655e-05 | scale_pen(Q)=1.6122e-05 | grad_norm=1.50 | sec/step~0.19
  step 140/256 | loss_L=1.7639 | loss_Q=1.6506 | scale_pen(L)=1.5926e-05 | scale_pen(Q)=1.5801e-05 | grad_norm=0.98 | sec/step~0.19
  step 150/256 | loss_L=1.6457 | loss_Q=1.4405 | scale_pen(L)=1.5898e-05 | scale_pen(Q)=1.5723e-05 | grad_norm=0.98 | sec/step~0.19
  step 160/256 | loss_L=1.9214 | loss_Q=1.8139 | scale_pen(L)=1.6138e-05 | scale_pen(Q)=1.5644e-05 | grad_norm=1.20 | sec/step~0.19
  step 170/256 | loss_L=1.5331 | loss_Q=1.4410 | scale_pen(L)=1.7458e-05 | scale_pen(Q)=1.5965e-05 | grad_norm=1.06 | sec/step~0.19
  step 180/256 | loss_L=1.5619 | loss_Q=1.4024 | scale_pen(L)=1.7751e-05 | scale_pen(Q)=1.5590e-05 | grad_norm=1.18 | sec/step~0.19
  step 190/256 | loss_L=1.6084 | loss_Q=1.5124 | scale_pen(L)=1.6768e-05 | scale_pen(Q)=1.5307e-05 | grad_norm=1.03 | sec/step~0.18
  step 200/256 | loss_L=1.6064 | loss_Q=1.4646 | scale_pen(L)=1.6785e-05 | scale_pen(Q)=1.5621e-05 | grad_norm=0.99 | sec/step~0.19
  [debug:L] a.scale=0.9959 | Z.std=1.0008 Z.mean||=16.0124 | p.std=0.6482 p.mean||=29.3180
  [debug:Q] a.scale=1.0040 | Z.std=1.0008 Z.mean||=16.0124 | p.std=0.6583 p.mean||=19.6920
  step 210/256 | loss_L=1.6019 | loss_Q=1.4306 | scale_pen(L)=1.6581e-05 | scale_pen(Q)=1.5219e-05 | grad_norm=1.07 | sec/step~0.19
  step 220/256 | loss_L=1.9949 | loss_Q=1.8366 | scale_pen(L)=1.5747e-05 | scale_pen(Q)=1.5484e-05 | grad_norm=1.07 | sec/step~0.19
  step 230/256 | loss_L=1.4676 | loss_Q=1.3726 | scale_pen(L)=1.5003e-05 | scale_pen(Q)=1.5445e-05 | grad_norm=0.91 | sec/step~0.19
  step 240/256 | loss_L=1.6569 | loss_Q=1.5030 | scale_pen(L)=1.5399e-05 | scale_pen(Q)=1.5334e-05 | grad_norm=0.91 | sec/step~0.19
  step 250/256 | loss_L=1.4632 | loss_Q=1.3524 | scale_pen(L)=1.4946e-05 | scale_pen(Q)=1.4682e-05 | grad_norm=0.81 | sec/step~0.18
  step 256/256 | loss_L=1.6457 | loss_Q=1.5171 | scale_pen(L)=1.4838e-05 | scale_pen(Q)=1.4453e-05 | grad_norm=1.02 | sec/step~0.19
  [debug:L] a.scale=0.9961 | Z.std=1.0009 Z.mean||=16.0138 | p.std=0.6532 p.mean||=29.5402
  [debug:Q] a.scale=1.0038 | Z.std=1.0009 Z.mean||=16.0138 | p.std=0.6623 p.mean||=19.8110
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1536
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/squad_m16_scalereg_20250911_211615/ckpt
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Llama hidden size: 2048, Qwen hidden size: 896
⏪ Resuming from: runs/squad_m16_scalereg_20250911_211615/ckpt/state.pt
/projects/m000066/sujinesh/LatentWire/latentwire/train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(path, map_location="cpu")
   -> start_epoch=4, global_step=1536
Epoch 5/5
  step 10/256 | loss_L=3.1260 | loss_Q=2.7933 | scale_pen(L)=5.0632e-07 | scale_pen(Q)=8.1349e-07 | grad_norm=3.47 | sec/step~0.39
  step 20/256 | loss_L=2.4985 | loss_Q=2.2678 | scale_pen(L)=1.0110e-06 | scale_pen(Q)=8.2168e-07 | grad_norm=2.36 | sec/step~0.24
  step 30/256 | loss_L=2.3204 | loss_Q=2.0615 | scale_pen(L)=1.6097e-06 | scale_pen(Q)=2.8040e-07 | grad_norm=1.37 | sec/step~0.19
  step 40/256 | loss_L=2.4282 | loss_Q=2.1663 | scale_pen(L)=1.1227e-06 | scale_pen(Q)=7.0416e-08 | grad_norm=1.07 | sec/step~0.17
  step 50/256 | loss_L=2.1282 | loss_Q=1.8121 | scale_pen(L)=4.5052e-07 | scale_pen(Q)=4.3272e-08 | grad_norm=1.01 | sec/step~0.16
  step 60/256 | loss_L=1.9351 | loss_Q=1.6020 | scale_pen(L)=2.2477e-07 | scale_pen(Q)=6.4413e-08 | grad_norm=1.41 | sec/step~0.16
  step 70/256 | loss_L=2.2158 | loss_Q=1.9537 | scale_pen(L)=1.4343e-07 | scale_pen(Q)=1.0040e-07 | grad_norm=0.91 | sec/step~0.16
  step 80/256 | loss_L=2.0085 | loss_Q=1.6959 | scale_pen(L)=2.3924e-08 | scale_pen(Q)=1.4872e-07 | grad_norm=0.92 | sec/step~0.16
  step 90/256 | loss_L=1.8089 | loss_Q=1.6925 | scale_pen(L)=2.5441e-08 | scale_pen(Q)=1.8213e-07 | grad_norm=0.71 | sec/step~0.16
  step 100/256 | loss_L=2.0796 | loss_Q=1.7950 | scale_pen(L)=1.2333e-07 | scale_pen(Q)=2.0048e-07 | grad_norm=0.72 | sec/step~0.17
  [debug:L] a.scale=1.0004 | Z.std=0.9997 Z.mean||=15.9952 | p.std=0.6076 p.mean||=27.4914
  [debug:Q] a.scale=1.0004 | Z.std=0.9997 Z.mean||=15.9952 | p.std=0.5939 p.mean||=17.7795
  step 110/256 | loss_L=1.9184 | loss_Q=1.6462 | scale_pen(L)=2.8141e-07 | scale_pen(Q)=2.1526e-07 | grad_norm=1.15 | sec/step~0.16
  step 120/256 | loss_L=2.0194 | loss_Q=1.8457 | scale_pen(L)=7.0551e-07 | scale_pen(Q)=2.1394e-07 | grad_norm=0.65 | sec/step~0.16
  step 130/256 | loss_L=1.7830 | loss_Q=1.5541 | scale_pen(L)=1.2217e-06 | scale_pen(Q)=2.0176e-07 | grad_norm=0.55 | sec/step~0.16
  step 140/256 | loss_L=1.8714 | loss_Q=1.6848 | scale_pen(L)=1.6699e-06 | scale_pen(Q)=1.8479e-07 | grad_norm=0.58 | sec/step~0.16
  step 150/256 | loss_L=1.7472 | loss_Q=1.5457 | scale_pen(L)=2.0624e-06 | scale_pen(Q)=1.6370e-07 | grad_norm=0.62 | sec/step~0.16
  step 160/256 | loss_L=1.8283 | loss_Q=1.6396 | scale_pen(L)=2.7945e-06 | scale_pen(Q)=1.6360e-07 | grad_norm=0.53 | sec/step~0.16
  step 170/256 | loss_L=1.8773 | loss_Q=1.6689 | scale_pen(L)=3.5162e-06 | scale_pen(Q)=1.4597e-07 | grad_norm=0.59 | sec/step~0.16
  step 180/256 | loss_L=1.8456 | loss_Q=1.6309 | scale_pen(L)=3.8273e-06 | scale_pen(Q)=1.3560e-07 | grad_norm=0.56 | sec/step~0.16
  step 190/256 | loss_L=1.4410 | loss_Q=1.3081 | scale_pen(L)=4.5193e-06 | scale_pen(Q)=1.3411e-07 | grad_norm=0.61 | sec/step~0.16
  step 200/256 | loss_L=2.2955 | loss_Q=2.1058 | scale_pen(L)=5.2163e-06 | scale_pen(Q)=1.2688e-07 | grad_norm=0.72 | sec/step~0.16
  [debug:L] a.scale=1.0023 | Z.std=0.9998 Z.mean||=15.9962 | p.std=0.6159 p.mean||=27.8638
  [debug:Q] a.scale=1.0004 | Z.std=0.9998 Z.mean||=15.9962 | p.std=0.5994 p.mean||=17.9455
  step 210/256 | loss_L=1.6038 | loss_Q=1.4144 | scale_pen(L)=5.9204e-06 | scale_pen(Q)=1.1993e-07 | grad_norm=0.55 | sec/step~0.16
  step 220/256 | loss_L=2.2256 | loss_Q=2.0455 | scale_pen(L)=6.6321e-06 | scale_pen(Q)=1.1405e-07 | grad_norm=0.73 | sec/step~0.17
  step 230/256 | loss_L=1.9079 | loss_Q=1.7186 | scale_pen(L)=7.0441e-06 | scale_pen(Q)=1.1014e-07 | grad_norm=0.64 | sec/step~0.16
  step 240/256 | loss_L=1.6481 | loss_Q=1.5251 | scale_pen(L)=7.9511e-06 | scale_pen(Q)=1.0677e-07 | grad_norm=0.64 | sec/step~0.16
  step 250/256 | loss_L=1.7280 | loss_Q=1.5670 | scale_pen(L)=8.7445e-06 | scale_pen(Q)=1.1141e-07 | grad_norm=0.66 | sec/step~0.16
  step 256/256 | loss_L=1.8761 | loss_Q=1.7044 | scale_pen(L)=9.0152e-06 | scale_pen(Q)=8.5719e-08 | grad_norm=0.56 | sec/step~0.16
  [debug:L] a.scale=1.0030 | Z.std=0.9998 Z.mean||=15.9974 | p.std=0.6212 p.mean||=28.1005
  [debug:Q] a.scale=1.0003 | Z.std=0.9998 Z.mean||=15.9974 | p.std=0.6035 p.mean||=18.0663
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/squad_m16_scalereg_20250911_211615/ckpt
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Llama hidden size: 2048, Qwen hidden size: 896
⏪ Resuming from: runs/squad_m16_scalereg_20250911_211615/ckpt/state.pt
/projects/m000066/sujinesh/LatentWire/latentwire/train.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(path, map_location="cpu")
   -> start_epoch=4, global_step=1792
Epoch 5/5
  step 10/256 | loss_L=2.6127 | loss_Q=3.1921 | scale_pen(L)=4.4521e-10 | scale_pen(Q)=1.3242e-06 | grad_norm=5.09 | sec/step~0.35
  step 20/256 | loss_L=2.5819 | loss_Q=2.6765 | scale_pen(L)=4.8900e-08 | scale_pen(Q)=3.1342e-06 | grad_norm=2.53 | sec/step~0.23
  step 30/256 | loss_L=2.1891 | loss_Q=2.3672 | scale_pen(L)=1.1794e-08 | scale_pen(Q)=4.3446e-06 | grad_norm=1.60 | sec/step~0.18
  step 40/256 | loss_L=1.8812 | loss_Q=1.8707 | scale_pen(L)=4.7641e-09 | scale_pen(Q)=5.2218e-06 | grad_norm=1.12 | sec/step~0.17
  step 50/256 | loss_L=2.1047 | loss_Q=2.0166 | scale_pen(L)=3.3790e-08 | scale_pen(Q)=5.2622e-06 | grad_norm=1.43 | sec/step~0.16
  step 60/256 | loss_L=1.9069 | loss_Q=1.7803 | scale_pen(L)=3.5297e-08 | scale_pen(Q)=5.0083e-06 | grad_norm=0.83 | sec/step~0.16
  step 70/256 | loss_L=2.1052 | loss_Q=1.9955 | scale_pen(L)=9.7623e-08 | scale_pen(Q)=4.4070e-06 | grad_norm=0.71 | sec/step~0.17
  step 80/256 | loss_L=2.2150 | loss_Q=2.1625 | scale_pen(L)=1.0786e-07 | scale_pen(Q)=3.7743e-06 | grad_norm=0.74 | sec/step~0.16
  step 90/256 | loss_L=1.8741 | loss_Q=1.7501 | scale_pen(L)=5.6900e-08 | scale_pen(Q)=3.1753e-06 | grad_norm=0.66 | sec/step~0.16
  step 100/256 | loss_L=1.7477 | loss_Q=1.6524 | scale_pen(L)=7.8948e-08 | scale_pen(Q)=2.6211e-06 | grad_norm=0.78 | sec/step~0.16
  [debug:L] a.scale=1.0003 | Z.std=0.9997 Z.mean||=15.9949 | p.std=0.5892 p.mean||=26.6599
  [debug:Q] a.scale=1.0016 | Z.std=0.9997 Z.mean||=15.9949 | p.std=0.6014 p.mean||=17.9947
  step 110/256 | loss_L=1.5291 | loss_Q=1.4347 | scale_pen(L)=8.3983e-08 | scale_pen(Q)=2.1879e-06 | grad_norm=0.80 | sec/step~0.15
  step 120/256 | loss_L=1.7992 | loss_Q=1.6474 | scale_pen(L)=7.1432e-08 | scale_pen(Q)=1.8339e-06 | grad_norm=0.59 | sec/step~0.16
  step 130/256 | loss_L=1.7319 | loss_Q=1.6667 | scale_pen(L)=5.6333e-08 | scale_pen(Q)=1.5070e-06 | grad_norm=0.55 | sec/step~0.15
  step 140/256 | loss_L=1.6426 | loss_Q=1.5673 | scale_pen(L)=2.5861e-08 | scale_pen(Q)=1.2280e-06 | grad_norm=0.62 | sec/step~0.17
  step 150/256 | loss_L=1.5925 | loss_Q=1.5332 | scale_pen(L)=2.1500e-08 | scale_pen(Q)=1.0503e-06 | grad_norm=0.56 | sec/step~0.16
  step 160/256 | loss_L=1.6560 | loss_Q=1.5726 | scale_pen(L)=7.2006e-08 | scale_pen(Q)=9.0201e-07 | grad_norm=0.66 | sec/step~0.15
  step 170/256 | loss_L=1.5611 | loss_Q=1.4937 | scale_pen(L)=1.5570e-07 | scale_pen(Q)=7.1294e-07 | grad_norm=0.86 | sec/step~0.16
  step 180/256 | loss_L=2.0135 | loss_Q=1.8806 | scale_pen(L)=2.1031e-07 | scale_pen(Q)=6.6739e-07 | grad_norm=0.58 | sec/step~0.16
  step 190/256 | loss_L=1.6177 | loss_Q=1.6615 | scale_pen(L)=3.4455e-07 | scale_pen(Q)=6.3090e-07 | grad_norm=0.67 | sec/step~0.16
  step 200/256 | loss_L=1.6131 | loss_Q=1.4909 | scale_pen(L)=5.7320e-07 | scale_pen(Q)=5.3052e-07 | grad_norm=0.59 | sec/step~0.16
  [debug:L] a.scale=1.0008 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.5974 p.mean||=27.0309
  [debug:Q] a.scale=1.0007 | Z.std=0.9998 Z.mean||=15.9969 | p.std=0.6055 p.mean||=18.1155
  step 210/256 | loss_L=1.4771 | loss_Q=1.4377 | scale_pen(L)=8.5486e-07 | scale_pen(Q)=4.1793e-07 | grad_norm=0.59 | sec/step~0.15
  step 220/256 | loss_L=1.9109 | loss_Q=1.7569 | scale_pen(L)=1.0469e-06 | scale_pen(Q)=3.7180e-07 | grad_norm=0.68 | sec/step~0.16
  step 230/256 | loss_L=1.5496 | loss_Q=1.4881 | scale_pen(L)=1.3151e-06 | scale_pen(Q)=3.6832e-07 | grad_norm=0.62 | sec/step~0.15
  step 240/256 | loss_L=1.7167 | loss_Q=1.6558 | scale_pen(L)=1.6909e-06 | scale_pen(Q)=3.4652e-07 | grad_norm=0.58 | sec/step~0.16
  step 250/256 | loss_L=1.4470 | loss_Q=1.3816 | scale_pen(L)=1.8902e-06 | scale_pen(Q)=3.2023e-07 | grad_norm=0.46 | sec/step~0.16
  step 256/256 | loss_L=1.8398 | loss_Q=1.7707 | scale_pen(L)=2.1360e-06 | scale_pen(Q)=3.0899e-07 | grad_norm=0.61 | sec/step~0.16
  [debug:L] a.scale=1.0015 | Z.std=0.9999 Z.mean||=15.9984 | p.std=0.6013 p.mean||=27.2051
  [debug:Q] a.scale=1.0006 | Z.std=0.9999 Z.mean||=15.9984 | p.std=0.6088 p.mean||=18.2126
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2048
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/squad_m16_scalereg_20250911_211615/ckpt
