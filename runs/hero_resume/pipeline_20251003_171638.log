=== Resume Hero Stage B Training (Clean Architecture) ===
Run tag: hero_resume
Checkpoint directory: runs/hero_resume/ckpt_stageb (using --auto_resume)
Epochs: 8 (extended for consolidation)

ARCHITECTURE FIX (2025-10-03):
  - Removed redundant PEFT Prefix-tuning (--use_prefix)
  - Kept DeepPrefixGenerator (Z-conditional, core functionality)
  - Kept LoRA (task adaptation)
  - Fixes PEFT adapter stacking bug causing eval mode collapse

Previous bug: PEFT Prefix + LoRA stacking incorrectly
  - Training: 272.8M params (LoRA 42M + PEFT Prefix 231M)
  - Eval: Only 41.9M loaded (Prefix lost, caused 100% 'the' mode collapse)
  - Root cause: save_pretrained() on stacked adapters lost Prefix weights

Retained config from v1:
  - LATENT_KEEP_END=0.85 (freeze dropout at sweet spot)
  - FIRST_TOKEN_CE_WEIGHT=11.0, KD_WEIGHT=0.5
  - OOM fixes: expandable_segments, KD_TEACHER_CHUNK=1

Archiving existing diagnostics to runs/hero_resume/diagnostics_20251003_171638.jsonl.bak
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage B: Training with frozen dropout (keep_probâ†’0.85) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3613.44it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.14s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.31it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.09it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
âª Resuming from: runs/hero_resume/ckpt_stageb/state.pt
   -> loaded encoder/adapters/deep_prefix/refiner FROM state.pt
[WARN] Optimizer state incompatible; continuing with fresh optimizer (loaded state dict contains a parameter group that doesn't match the size of optimizer's group)
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=14, global_step=6675
[warmup] alternating text/latent for first 890 steps
Epoch 15/8
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/445 | grad_norm=497.81 | sec/step~4.81 | keep=0.85 | K=8 | llama(L): tf=19.6063 first=20.6075 kCE=17.5092 KD=2.6851 acc=0.000 state=24.1377 align=0.0000 latA=0.7449 latP=0.1136 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | (tail text) | align=0.0003 | text_tf=15.6403 | latent_scale=1.00
  step  20/445 | grad_norm=200.05 | sec/step~15.24 | keep=0.85 | K=8 | llama(T): tf=16.0307 first=12.6110 kCE=12.3437 KD=0.0000 acc=0.000 state=23.4895 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=124.85 | sec/step~5.41 | keep=0.85 | K=8 | llama(L): tf=12.9417 first=10.3008 kCE=11.4028 KD=8.8726 acc=0.000 state=23.9896 align=0.0000 latA=0.7593 latP=0.1485 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=32.62 | sec/step~4.90 | keep=0.85 | K=8 | llama(L): tf=11.2511 first=9.4456 kCE=9.4742 KD=12.5851 acc=0.056 state=24.6335 align=0.0000 latA=0.7813 latP=0.1302 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0017 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=21.91 | sec/step~4.43 | keep=0.85 | K=8 | llama(L): tf=10.4754 first=8.5387 kCE=10.1682 KD=14.5818 acc=0.028 state=22.6550 align=0.0000 latA=0.7758 latP=0.1236 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0017 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=120.14 | sec/step~4.87 | keep=0.85 | K=8 | llama(L): tf=10.7215 first=8.8214 kCE=10.4195 KD=13.9697 acc=0.056 state=24.2986 align=0.0000 latA=0.7733 latP=0.1308 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0017 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc_ema=5.2% (raw_batch=11.1%) at step 6741 â†’ saved to runs/hero_resume/ckpt_stageb_best
  ðŸŒŸ NEW PEAK: first_acc_ema=5.2% (raw_batch=5.6%) at step 6742 â†’ saved to runs/hero_resume/ckpt_stageb_best
  ðŸŒŸ NEW PEAK: first_acc_ema=5.2% (raw_batch=5.6%) at step 6743 â†’ saved to runs/hero_resume/ckpt_stageb_best
  ðŸŒŸ NEW PEAK: first_acc_ema=5.3% (raw_batch=5.6%) at step 6744 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  70/445 | grad_norm=51.66 | sec/step~5.50 | keep=0.85 | K=8 | llama(L): tf=10.1184 first=8.0517 kCE=9.8627 KD=15.7052 acc=0.083 state=24.9951 align=0.0000 latA=0.7501 latP=0.1236 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0016 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc_ema=5.6% (raw_batch=8.3%) at step 6745 â†’ saved to runs/hero_resume/ckpt_stageb_best
  ðŸŒŸ NEW PEAK: first_acc_ema=5.6% (raw_batch=8.3%) at step 6748 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  80/445 | grad_norm=34.79 | sec/step~5.15 | keep=0.85 | K=8 | llama(L): tf=9.6814 first=8.2530 kCE=9.7103 KD=16.5554 acc=0.083 state=24.1888 align=0.0000 latA=0.7664 latP=0.1273 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0016 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc_ema=6.0% (raw_batch=13.9%) at step 6758 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  90/445 | grad_norm=32.51 | sec/step~4.89 | keep=0.85 | K=8 | llama(L): tf=9.6926 first=7.6986 kCE=9.8827 KD=14.5488 acc=0.028 state=23.0444 align=0.0000 latA=0.7635 latP=0.1295 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0016 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=28.47 | sec/step~4.53 | keep=0.85 | K=8 | llama(L): tf=9.6275 first=7.3063 kCE=9.9222 KD=13.2259 acc=0.111 state=22.7823 align=0.0000 latA=0.7383 latP=0.1495 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0015 rms_cal~0.0106 embed_rms~0.01057]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2504, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1897, in main
    text_teacher_loss, _, _ = _loss_with_text_prompt_chunked(ctx.wrapper, scaffold, targets)
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 90, in _loss_with_text_prompt_chunked
    loss, _, _ = wrapper.loss_with_text_prompt(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/models.py", line 1120, in loss_with_text_prompt
    out = self.model(**model_kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/peft_model.py", line 1850, in forward
    return self.base_model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
    return self.model.forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1000, in forward
    layer_outputs = decoder_layer(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 745, in forward
    hidden_states = self.mlp(hidden_states)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 311, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 771, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 130.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 121.00 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 78.06 GiB is allocated by PyTorch, and 269.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
