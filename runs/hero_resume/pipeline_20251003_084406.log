=== Resume Hero Stage B Training (Schedule Fix) ===
Run tag: hero_resume
Checkpoint directory: runs/hero_resume/ckpt_stageb (using --auto_resume)
Epochs: 6 (extended for consolidation)

SCHEDULE FIXES (based on v1 analysis):
  - LATENT_KEEP_END: 1.0 → 0.85 (freeze at sweet spot)
  - EPOCHS extended to 8 (consolidate with frozen dropout)
  - Peak checkpointing: train.py saves '_best' automatically

Analysis showed peak at keep_prob=0.6-0.85:
  - Peak: 19.4% first_acc at keep_prob=0.613
  - 26 steps achieved ≥10% (all at keep_prob 0.55-0.82)
  - Final eval (keep_prob=1.0) only 4.4% - model never learned full latents

Retained from v1:
  - FIRST_TOKEN_CE_WEIGHT=11.0
  - KD_WEIGHT=0.5
  - OOM fixes: expandable_segments, KD_TEACHER_CHUNK=1
  - Performance: TEXT_TEACHER_CHUNK=4

Archiving existing diagnostics to runs/hero_resume/diagnostics_20251003_084406.jsonl.bak
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage B: Training with frozen dropout (keep_prob→0.85) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1909.76it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:18<00:18,  9.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:27<00:08,  8.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  6.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.29s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
trainable params: 272,801,792 || all params: 8,345,006,080 || trainable%: 3.2690
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⏪ Resuming from: runs/hero_resume/ckpt_stageb/state.pt
   -> loaded encoder/adapters/deep_prefix/refiner FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=8, global_step=4005
[warmup] alternating text/latent for first 890 steps
Epoch 9/6
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  step  10/445 | grad_norm=1.29 | sec/step~7.45 | keep=0.85 | K=8 | llama(L): tf=12.5735 first=14.3938 kCE=12.2767 KD=2.6451 acc=0.056 state=40.6946 align=0.0000 latA=0.7706 latP=0.1131 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | (tail text) | align=0.0003 | text_tf=11.0502 | latent_scale=1.00
  step  20/445 | grad_norm=0.59 | sec/step~8.88 | keep=0.85 | K=8 | llama(T): tf=11.8856 first=12.7568 kCE=11.8824 KD=0.0000 acc=0.000 state=42.0063 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.68 | sec/step~7.76 | keep=0.85 | K=8 | llama(L): tf=11.5747 first=11.0574 kCE=12.8114 KD=2.9451 acc=0.000 state=38.9115 align=0.0000 latA=0.7756 latP=0.1083 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.50 | sec/step~7.65 | keep=0.85 | K=8 | llama(L): tf=11.6863 first=9.7752 kCE=14.2849 KD=2.8651 acc=0.000 state=25.5982 align=0.0000 latA=0.7639 latP=0.1200 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.39 | sec/step~7.56 | keep=0.85 | K=8 | llama(L): tf=10.8014 first=9.1148 kCE=12.2349 KD=2.8773 acc=0.056 state=16.8970 align=0.0000 latA=0.7867 latP=0.1272 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=1.15 | sec/step~7.59 | keep=0.85 | K=8 | llama(L): tf=10.5498 first=8.7391 kCE=11.8400 KD=3.2497 acc=0.000 state=17.2718 align=0.0000 latA=0.7724 latP=0.1243 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=1.64 | sec/step~8.57 | keep=0.85 | K=8 | llama(L): tf=9.8756 first=8.7979 kCE=11.9057 KD=2.7288 acc=0.000 state=16.0369 align=0.0000 latA=0.7832 latP=0.1302 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=0.91 | sec/step~7.51 | keep=0.85 | K=8 | llama(L): tf=10.4626 first=8.8623 kCE=13.0826 KD=2.5821 acc=0.028 state=13.7870 align=0.0000 latA=0.7755 latP=0.1201 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=1.42 | sec/step~7.71 | keep=0.85 | K=8 | llama(L): tf=11.0840 first=7.8710 kCE=13.5234 KD=2.9999 acc=0.167 state=12.2881 align=0.0000 latA=0.7624 latP=0.1291 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  🌟 NEW PEAK: first_acc_ema=5.3% (raw_batch=16.7%) at step 4095 → saved to runs/hero_resume/ckpt_stageb_best
  step  100/445 | grad_norm=0.42 | sec/step~8.56 | keep=0.85 | K=8 | llama(L): tf=10.8445 first=8.9054 kCE=14.0337 KD=2.6090 acc=0.028 state=11.0237 align=0.0000 latA=0.7975 latP=0.1227 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  101/445 | (tail text) | align=0.0003 | text_tf=10.9305 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=6.7% (raw_batch=25.0%) at step 4113 → saved to runs/hero_resume/ckpt_stageb_best
  step  110/445 | grad_norm=0.26 | sec/step~7.72 | keep=0.85 | K=8 | llama(L): tf=10.8105 first=7.7436 kCE=13.7687 KD=2.7020 acc=0.028 state=10.3215 align=0.0000 latA=0.7837 latP=0.1387 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.98 | sec/step~7.49 | keep=0.85 | K=8 | llama(L): tf=11.1500 first=8.1717 kCE=13.8842 KD=2.7551 acc=0.028 state=8.6800 align=0.0000 latA=0.7920 latP=0.1518 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  125/445 | (tail text) | align=0.0003 | text_tf=10.7011 | latent_scale=1.00
  step  127/445 | (tail text) | align=0.0002 | text_tf=10.4545 | latent_scale=1.00
  step  130/445 | grad_norm=0.82 | sec/step~8.79 | keep=0.85 | K=8 | llama(L): tf=10.9485 first=7.4878 kCE=13.8925 KD=2.7415 acc=0.056 state=9.7609 align=0.0000 latA=0.7698 latP=0.1270 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=0.89 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=10.8352 first=7.3861 kCE=13.7592 KD=2.5318 acc=0.028 state=4.5879 align=0.0000 latA=0.7647 latP=0.1404 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=1.10 | sec/step~8.43 | keep=0.85 | K=8 | llama(L): tf=10.8449 first=7.4524 kCE=13.8337 KD=3.0885 acc=0.056 state=6.1216 align=0.0000 latA=0.7871 latP=0.1353 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=0.34 | sec/step~8.22 | keep=0.85 | K=8 | llama(L): tf=10.9113 first=7.5633 kCE=13.8056 KD=3.6847 acc=0.028 state=3.5119 align=0.0000 latA=0.7762 latP=0.1307 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.35 | sec/step~9.18 | keep=0.85 | K=8 | llama(L): tf=10.9909 first=7.9729 kCE=13.9512 KD=2.6738 acc=0.028 state=1.9931 align=0.0000 latA=0.7909 latP=0.1455 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=1.91 | sec/step~8.54 | keep=0.85 | K=8 | llama(L): tf=10.3081 first=7.7683 kCE=13.5986 KD=3.3884 acc=0.028 state=5.6878 align=0.0000 latA=0.7986 latP=0.1227 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=1.53 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=10.4098 first=7.4957 kCE=13.6888 KD=3.9401 acc=0.083 state=2.6438 align=0.0000 latA=0.7776 latP=0.1152 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  200/445 | grad_norm=1.21 | sec/step~7.33 | keep=0.85 | K=8 | llama(L): tf=10.3201 first=7.7817 kCE=13.6374 KD=3.3704 acc=0.028 state=1.2039 align=0.0000 latA=0.7823 latP=0.1145 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=1.16 | sec/step~8.31 | keep=0.85 | K=8 | llama(L): tf=10.1291 first=7.9521 kCE=13.0462 KD=2.5078 acc=0.056 state=0.9120 align=0.0000 latA=0.7919 latP=0.1459 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.51 | sec/step~8.03 | keep=0.85 | K=8 | llama(L): tf=9.9965 first=7.1304 kCE=13.2663 KD=2.5179 acc=0.083 state=1.5762 align=0.0000 latA=0.7563 latP=0.1281 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.67 | sec/step~7.38 | keep=0.85 | K=8 | llama(L): tf=9.7607 first=7.1188 kCE=13.1474 KD=3.5313 acc=0.028 state=0.3973 align=0.0000 latA=0.7564 latP=0.1154 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  🌟 NEW PEAK: first_acc_ema=6.7% (raw_batch=13.9%) at step 4242 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=6.9% (raw_batch=8.3%) at step 4243 → saved to runs/hero_resume/ckpt_stageb_best
  step  240/445 | grad_norm=1.78 | sec/step~9.29 | keep=0.85 | K=8 | llama(L): tf=9.5124 first=6.9602 kCE=13.1444 KD=2.5717 acc=0.083 state=2.8179 align=0.0000 latA=0.7469 latP=0.1119 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  🌟 NEW PEAK: first_acc_ema=7.0% (raw_batch=11.1%) at step 4247 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=7.1% (raw_batch=8.3%) at step 4248 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=7.3% (raw_batch=8.3%) at step 4249 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=7.7% (raw_batch=11.1%) at step 4250 → saved to runs/hero_resume/ckpt_stageb_best
  step  250/445 | grad_norm=1.18 | sec/step~12.53 | keep=0.85 | K=8 | llama(L): tf=9.7091 first=7.3986 kCE=12.9561 KD=3.4258 acc=0.056 state=4.8525 align=0.0000 latA=0.7643 latP=0.1610 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.15 | sec/step~8.59 | keep=0.85 | K=8 | llama(L): tf=9.6952 first=7.6773 kCE=13.0866 KD=2.9252 acc=0.000 state=1.5152 align=0.0000 latA=0.7744 latP=0.1195 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | (tail text) | align=0.0003 | text_tf=9.8397 | latent_scale=1.00
  step  270/445 | grad_norm=0.50 | sec/step~6.80 | keep=0.85 | K=8 | llama(T): tf=9.7359 first=8.2958 kCE=12.8231 KD=0.0000 acc=0.000 state=0.1842 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.46 | sec/step~7.72 | keep=0.85 | K=8 | llama(L): tf=9.7330 first=7.5554 kCE=12.6236 KD=2.4942 acc=0.028 state=0.5369 align=0.0000 latA=0.7890 latP=0.1257 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.31 | sec/step~8.51 | keep=0.85 | K=8 | llama(L): tf=9.7276 first=6.9326 kCE=12.3600 KD=3.2764 acc=0.111 state=0.1020 align=0.0000 latA=0.7581 latP=0.1239 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  291/445 | (tail text) | align=0.0003 | text_tf=9.9076 | latent_scale=1.00
  step  298/445 | (tail text) | align=0.0002 | text_tf=9.1672 | latent_scale=1.00
  step  300/445 | grad_norm=1.30 | sec/step~7.36 | keep=0.85 | K=8 | llama(L): tf=10.0838 first=7.9639 kCE=12.6679 KD=2.9914 acc=0.056 state=0.1259 align=0.0000 latA=0.7900 latP=0.1218 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.70 | sec/step~8.51 | keep=0.85 | K=8 | llama(L): tf=9.4633 first=7.8828 kCE=12.2064 KD=3.2983 acc=0.028 state=0.1980 align=0.0000 latA=0.7957 latP=0.1266 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=1.10 | sec/step~8.00 | keep=0.85 | K=8 | llama(L): tf=8.9826 first=7.4483 kCE=11.8959 KD=2.4346 acc=0.083 state=0.1902 align=0.0000 latA=0.7880 latP=0.1147 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=0.93 | sec/step~8.20 | keep=0.85 | K=8 | llama(L): tf=9.2287 first=7.4080 kCE=11.4224 KD=3.3422 acc=0.056 state=0.3719 align=0.0000 latA=0.7697 latP=0.1445 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=0.75 | sec/step~8.18 | keep=0.85 | K=8 | llama(L): tf=9.0355 first=7.5225 kCE=11.5934 KD=2.5586 acc=0.056 state=0.1730 align=0.0000 latA=0.7698 latP=0.1138 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.62 | sec/step~10.36 | keep=0.85 | K=8 | llama(L): tf=8.8233 first=6.9009 kCE=10.7468 KD=2.9072 acc=0.111 state=0.0944 align=0.0000 latA=0.7503 latP=0.1329 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=3.18 | sec/step~8.30 | keep=0.85 | K=8 | llama(L): tf=8.8267 first=7.2776 kCE=11.2597 KD=2.4916 acc=0.000 state=0.5211 align=0.0000 latA=0.7821 latP=0.1088 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  369/445 | (tail text) | align=0.0002 | text_tf=9.3926 | latent_scale=1.00
  step  370/445 | grad_norm=0.68 | sec/step~7.92 | keep=0.85 | K=8 | llama(L): tf=9.0342 first=7.6242 kCE=10.5927 KD=2.7471 acc=0.056 state=0.4800 align=0.0000 latA=0.7964 latP=0.1500 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=2.11 | sec/step~7.87 | keep=0.85 | K=8 | llama(L): tf=9.3587 first=7.4808 kCE=10.7191 KD=4.5565 acc=0.083 state=0.2079 align=0.0000 latA=0.7883 latP=0.1064 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.39 | sec/step~7.48 | keep=0.85 | K=8 | llama(L): tf=8.8741 first=7.4789 kCE=10.2465 KD=2.8078 acc=0.056 state=0.0641 align=0.0000 latA=0.7651 latP=0.1313 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  395/445 | (tail text) | align=0.0002 | text_tf=9.6230 | latent_scale=1.00
  step  400/445 | grad_norm=0.48 | sec/step~8.53 | keep=0.85 | K=8 | llama(L): tf=8.5340 first=7.0262 kCE=9.8502 KD=2.4276 acc=0.111 state=0.1576 align=0.0000 latA=0.7399 latP=0.1473 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.46 | sec/step~7.67 | keep=0.85 | K=8 | llama(L): tf=9.0715 first=7.5647 kCE=9.7519 KD=2.8800 acc=0.056 state=0.0837 align=0.0000 latA=0.7777 latP=0.1153 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=1.72 | sec/step~8.43 | keep=0.85 | K=8 | llama(L): tf=8.3853 first=7.0759 kCE=9.5600 KD=4.7001 acc=0.083 state=0.2187 align=0.0000 latA=0.7601 latP=0.1260 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  428/445 | (tail text) | align=0.0003 | text_tf=9.8324 | latent_scale=1.00
  step  430/445 | grad_norm=1.16 | sec/step~13.08 | keep=0.85 | K=8 | llama(L): tf=8.9415 first=7.5477 kCE=9.3902 KD=2.4797 acc=0.056 state=0.2116 align=0.0000 latA=0.7638 latP=0.1379 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=1.14 | sec/step~11.31 | keep=0.85 | K=8 | llama(L): tf=8.8083 first=7.4907 kCE=8.8102 KD=2.9053 acc=0.028 state=0.2404 align=0.0000 latA=0.7808 latP=0.1179 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.13 | sec/step~5.67 | keep=0.85 | K=8 | llama(L): tf=9.0213 first=7.1807 kCE=8.6438 KD=2.4891 acc=0.062 state=0.1138 align=0.0000 latA=0.7509 latP=0.1510 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 50.0MB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4450
Epoch 10/6
  step  10/445 | grad_norm=1.31 | sec/step~9.34 | keep=0.85 | K=8 | llama(L): tf=8.7655 first=6.6222 kCE=8.1683 KD=2.7041 acc=0.083 state=0.2957 align=0.0000 latA=0.7404 latP=0.1182 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=2.08 | sec/step~7.67 | keep=0.85 | K=8 | llama(L): tf=9.0757 first=7.5240 kCE=7.9718 KD=3.8493 acc=0.111 state=0.0561 align=0.0000 latA=0.7742 latP=0.1260 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.48 | sec/step~10.02 | keep=0.85 | K=8 | llama(L): tf=8.7882 first=7.5190 kCE=7.5799 KD=3.4215 acc=0.028 state=0.4165 align=0.0000 latA=0.7860 latP=0.1093 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  37/445 | (tail text) | align=0.0002 | text_tf=9.3296 | latent_scale=1.00
  step  40/445 | grad_norm=0.36 | sec/step~7.93 | keep=0.85 | K=8 | llama(L): tf=8.7618 first=7.0848 kCE=7.4851 KD=2.7626 acc=0.083 state=0.0461 align=0.0000 latA=0.7817 latP=0.1111 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.24 | sec/step~7.71 | keep=0.85 | K=8 | llama(L): tf=9.0557 first=7.7196 kCE=7.6002 KD=2.7224 acc=0.028 state=0.1115 align=0.0000 latA=0.7883 latP=0.1387 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=0.87 | sec/step~7.64 | keep=0.85 | K=8 | llama(L): tf=8.3276 first=6.8003 kCE=7.3496 KD=3.0931 acc=0.139 state=0.0591 align=0.0000 latA=0.7529 latP=0.1267 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=1.26 | sec/step~8.26 | keep=0.85 | K=8 | llama(L): tf=8.9209 first=8.0526 kCE=7.2173 KD=4.1158 acc=0.000 state=0.0624 align=0.0000 latA=0.7867 latP=0.1267 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  74/445 | (tail text) | align=0.0003 | text_tf=9.7892 | latent_scale=1.00
  step  80/445 | grad_norm=1.39 | sec/step~8.61 | keep=0.85 | K=8 | llama(L): tf=9.0722 first=7.4755 kCE=7.0007 KD=2.8701 acc=0.056 state=0.4322 align=0.0000 latA=0.7792 latP=0.1128 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.37 | sec/step~7.86 | keep=0.85 | K=8 | llama(L): tf=8.4472 first=7.1457 kCE=6.8531 KD=2.6349 acc=0.056 state=0.2206 align=0.0000 latA=0.7586 latP=0.1366 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=0.48 | sec/step~8.06 | keep=0.85 | K=8 | llama(L): tf=8.6007 first=7.0800 kCE=6.1908 KD=2.9099 acc=0.028 state=0.2626 align=0.0000 latA=0.7715 latP=0.1143 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  🌟 NEW PEAK: first_acc_ema=7.7% (raw_batch=13.9%) at step 4555 → saved to runs/hero_resume/ckpt_stageb_best
  step  110/445 | grad_norm=0.17 | sec/step~7.69 | keep=0.85 | K=8 | llama(L): tf=8.8641 first=6.7791 kCE=7.0412 KD=3.0565 acc=0.056 state=0.0933 align=0.0000 latA=0.7575 latP=0.1306 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.34 | sec/step~8.03 | keep=0.85 | K=8 | llama(L): tf=9.2775 first=7.5013 kCE=6.7980 KD=2.4154 acc=0.083 state=0.1244 align=0.0000 latA=0.7605 latP=0.1154 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=0.88 | sec/step~7.83 | keep=0.85 | K=8 | llama(L): tf=9.2098 first=7.8868 kCE=6.2874 KD=2.6322 acc=0.028 state=0.2388 align=0.0000 latA=0.7789 latP=0.1099 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=1.41 | sec/step~8.55 | keep=0.85 | K=8 | llama(L): tf=8.7106 first=7.4920 kCE=6.4916 KD=2.5624 acc=0.028 state=0.1551 align=0.0000 latA=0.7813 latP=0.1208 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=0.88 | sec/step~7.69 | keep=0.85 | K=8 | llama(L): tf=9.0158 first=7.4686 kCE=7.4077 KD=3.5902 acc=0.056 state=0.1625 align=0.0000 latA=0.7915 latP=0.1518 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  154/445 | (tail text) | align=0.0003 | text_tf=10.0342 | latent_scale=1.00
  step  160/445 | grad_norm=0.44 | sec/step~7.53 | keep=0.85 | K=8 | llama(L): tf=8.4446 first=7.7067 kCE=6.6832 KD=4.0505 acc=0.056 state=0.0349 align=0.0000 latA=0.7887 latP=0.1177 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.24 | sec/step~8.54 | keep=0.85 | K=8 | llama(L): tf=8.8527 first=7.7977 kCE=7.0177 KD=2.5299 acc=0.028 state=0.1513 align=0.0000 latA=0.8095 latP=0.1359 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=1.05 | sec/step~7.47 | keep=0.85 | K=8 | llama(L): tf=8.7019 first=7.2395 kCE=6.7604 KD=3.1060 acc=0.083 state=0.0655 align=0.0000 latA=0.7625 latP=0.1409 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=0.94 | sec/step~7.95 | keep=0.85 | K=8 | llama(L): tf=8.7051 first=7.4498 kCE=7.0196 KD=2.4960 acc=0.028 state=0.0441 align=0.0000 latA=0.7817 latP=0.1403 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  193/445 | (tail text) | align=0.0003 | text_tf=9.8681 | latent_scale=1.00
  step  200/445 | grad_norm=0.96 | sec/step~7.84 | keep=0.85 | K=8 | llama(L): tf=8.8549 first=8.1459 kCE=6.3324 KD=2.4820 acc=0.028 state=0.1085 align=0.0000 latA=0.8077 latP=0.1150 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=0.86 | sec/step~9.09 | keep=0.85 | K=8 | llama(L): tf=8.6582 first=7.1743 kCE=6.7233 KD=2.9520 acc=0.028 state=0.1131 align=0.0000 latA=0.7732 latP=0.1121 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.49 | sec/step~9.61 | keep=0.85 | K=8 | llama(L): tf=8.9150 first=7.9795 kCE=7.0577 KD=2.8570 acc=0.028 state=0.2031 align=0.0000 latA=0.8105 latP=0.1320 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.32 | sec/step~8.20 | keep=0.85 | K=8 | llama(L): tf=8.8245 first=7.3374 kCE=7.4306 KD=2.9615 acc=0.000 state=0.2212 align=0.0000 latA=0.7823 latP=0.1341 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | (tail text) | align=0.0003 | text_tf=9.7794 | latent_scale=1.00
  step  240/445 | grad_norm=1.76 | sec/step~6.65 | keep=0.85 | K=8 | llama(T): tf=9.1974 first=7.7315 kCE=6.6179 KD=0.0000 acc=0.083 state=0.0893 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=1.17 | sec/step~8.76 | keep=0.85 | K=8 | llama(L): tf=8.7318 first=7.1539 kCE=6.9328 KD=2.4645 acc=0.111 state=0.0462 align=0.0000 latA=0.7599 latP=0.1312 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.56 | sec/step~8.76 | keep=0.85 | K=8 | llama(L): tf=8.6849 first=8.1797 kCE=6.8431 KD=3.2581 acc=0.028 state=0.3866 align=0.0000 latA=0.8016 latP=0.1307 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=0.42 | sec/step~8.28 | keep=0.85 | K=8 | llama(L): tf=8.5459 first=7.3458 kCE=6.2015 KD=2.5085 acc=0.083 state=0.1586 align=0.0000 latA=0.7582 latP=0.1326 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.47 | sec/step~7.58 | keep=0.85 | K=8 | llama(L): tf=8.6967 first=7.1503 kCE=6.5545 KD=2.6323 acc=0.028 state=0.0943 align=0.0000 latA=0.7578 latP=0.1245 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.22 | sec/step~7.75 | keep=0.85 | K=8 | llama(L): tf=8.5343 first=7.1783 kCE=5.9146 KD=2.5920 acc=0.083 state=0.1108 align=0.0000 latA=0.7600 latP=0.1084 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  300/445 | grad_norm=1.41 | sec/step~8.12 | keep=0.85 | K=8 | llama(L): tf=8.7952 first=7.4596 kCE=6.9069 KD=3.2558 acc=0.111 state=0.0579 align=0.0000 latA=0.7824 latP=0.1526 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=0.95 | sec/step~7.53 | keep=0.85 | K=8 | llama(L): tf=8.4097 first=7.5547 kCE=6.6410 KD=2.7054 acc=0.028 state=0.0386 align=0.0000 latA=0.7975 latP=0.1257 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=0.86 | sec/step~7.71 | keep=0.85 | K=8 | llama(L): tf=8.5337 first=7.1195 kCE=6.4271 KD=3.2855 acc=0.056 state=0.0857 align=0.0000 latA=0.7710 latP=0.1186 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=0.94 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.7627 first=7.3038 kCE=6.4400 KD=2.6044 acc=0.028 state=0.1095 align=0.0000 latA=0.7647 latP=0.1088 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  🌟 NEW PEAK: first_acc_ema=8.0% (raw_batch=11.1%) at step 4785 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=8.3% (raw_batch=11.1%) at step 4786 → saved to runs/hero_resume/ckpt_stageb_best
  🌟 NEW PEAK: first_acc_ema=8.3% (raw_batch=8.3%) at step 4787 → saved to runs/hero_resume/ckpt_stageb_best
  step  340/445 | grad_norm=0.43 | sec/step~7.55 | keep=0.85 | K=8 | llama(L): tf=8.4123 first=7.3564 kCE=6.7745 KD=2.5950 acc=0.000 state=0.0646 align=0.0000 latA=0.7567 latP=0.1372 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.15 | sec/step~7.15 | keep=0.85 | K=8 | llama(L): tf=8.7615 first=7.3613 kCE=6.5026 KD=2.5233 acc=0.056 state=0.0327 align=0.0000 latA=0.7695 latP=0.1108 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=1.03 | sec/step~11.48 | keep=0.85 | K=8 | llama(L): tf=8.5468 first=7.4923 kCE=6.2560 KD=2.5163 acc=0.028 state=0.0434 align=0.0000 latA=0.7848 latP=0.1292 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=1.20 | sec/step~8.74 | keep=0.85 | K=8 | llama(L): tf=8.3697 first=7.0801 kCE=6.3822 KD=2.7272 acc=0.000 state=0.0718 align=0.0000 latA=0.7556 latP=0.1373 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=0.48 | sec/step~9.51 | keep=0.85 | K=8 | llama(L): tf=8.4497 first=7.2403 kCE=6.2435 KD=4.1041 acc=0.083 state=1.0696 align=0.0000 latA=0.7686 latP=0.1163 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.63 | sec/step~8.57 | keep=0.85 | K=8 | llama(L): tf=9.0725 first=6.6569 kCE=6.8829 KD=3.6041 acc=0.111 state=0.0415 align=0.0000 latA=0.7628 latP=0.1314 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.49 | sec/step~11.77 | keep=0.85 | K=8 | llama(L): tf=8.8040 first=7.6560 kCE=5.8948 KD=3.0427 acc=0.028 state=0.6659 align=0.0000 latA=0.7946 latP=0.1081 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.22 | sec/step~8.42 | keep=0.85 | K=8 | llama(L): tf=8.7231 first=7.5683 kCE=6.5899 KD=2.5638 acc=0.056 state=0.0752 align=0.0000 latA=0.7577 latP=0.1175 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0020 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=0.67 | sec/step~12.28 | keep=0.85 | K=8 | llama(L): tf=8.4934 first=7.3192 kCE=6.3095 KD=2.2271 acc=0.000 state=0.2187 align=0.0000 latA=0.7763 latP=0.1180 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=1.18 | sec/step~9.88 | keep=0.85 | K=8 | llama(L): tf=8.5046 first=7.4469 kCE=5.6657 KD=2.5687 acc=0.028 state=0.0667 align=0.0000 latA=0.7959 latP=0.1120 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  438/445 | (tail text) | align=0.0003 | text_tf=9.0328 | latent_scale=1.00
  step  440/445 | grad_norm=1.02 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.5457 first=6.8925 kCE=6.3446 KD=3.0007 acc=0.083 state=0.0958 align=0.0000 latA=0.7575 latP=0.1202 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.12 | sec/step~6.49 | keep=0.85 | K=8 | llama(L): tf=8.4131 first=7.4227 kCE=6.5347 KD=2.8963 acc=0.062 state=0.1338 align=0.0000 latA=0.8022 latP=0.1180 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4895
Epoch 11/6
  step  10/445 | grad_norm=1.35 | sec/step~8.42 | keep=0.85 | K=8 | llama(L): tf=8.7396 first=8.0097 kCE=6.3227 KD=2.6600 acc=0.056 state=0.0931 align=0.0000 latA=0.7952 latP=0.1293 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=0.92 | sec/step~8.86 | keep=0.85 | K=8 | llama(L): tf=8.7013 first=6.8625 kCE=6.4497 KD=2.6949 acc=0.056 state=0.2943 align=0.0000 latA=0.7701 latP=0.1214 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.87 | sec/step~8.23 | keep=0.85 | K=8 | llama(L): tf=8.2774 first=6.9542 kCE=6.6179 KD=3.4413 acc=0.028 state=0.1035 align=0.0000 latA=0.7642 latP=0.1421 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.71 | sec/step~8.72 | keep=0.85 | K=8 | llama(L): tf=8.8519 first=6.7313 kCE=7.2732 KD=2.2895 acc=0.083 state=0.1928 align=0.0000 latA=0.7344 latP=0.1538 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.31 | sec/step~7.94 | keep=0.85 | K=8 | llama(L): tf=8.1855 first=7.8343 kCE=5.5774 KD=2.4920 acc=0.028 state=0.1050 align=0.0000 latA=0.7974 latP=0.1108 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=1.35 | sec/step~10.08 | keep=0.85 | K=8 | llama(L): tf=8.7207 first=7.3030 kCE=6.2146 KD=3.3092 acc=0.028 state=0.0563 align=0.0000 latA=0.7820 latP=0.1140 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  67/445 | (tail text) | align=0.0002 | text_tf=8.9144 | latent_scale=1.00
  step  70/445 | grad_norm=2.35 | sec/step~8.12 | keep=0.85 | K=8 | llama(L): tf=8.5447 first=7.1227 kCE=5.9473 KD=2.9880 acc=0.083 state=0.0648 align=0.0000 latA=0.7738 latP=0.1170 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=1.11 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=8.3276 first=7.3186 kCE=5.5567 KD=3.1500 acc=0.028 state=0.0756 align=0.0000 latA=0.7673 latP=0.1167 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.98 | sec/step~8.33 | keep=0.85 | K=8 | llama(L): tf=8.5245 first=7.5405 kCE=5.7767 KD=3.2227 acc=0.083 state=0.0749 align=0.0000 latA=0.7891 latP=0.1047 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=0.58 | sec/step~7.74 | keep=0.85 | K=8 | llama(L): tf=8.4660 first=6.5461 kCE=5.5878 KD=2.1891 acc=0.139 state=0.0423 align=0.0000 latA=0.7379 latP=0.1030 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  110/445 | grad_norm=0.19 | sec/step~9.46 | keep=0.85 | K=8 | llama(L): tf=8.5943 first=8.2296 kCE=6.3708 KD=3.0070 acc=0.056 state=0.1198 align=0.0000 latA=0.8130 latP=0.1280 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=2.05 | sec/step~8.38 | keep=0.85 | K=8 | llama(L): tf=8.1656 first=6.7652 kCE=5.3971 KD=2.5228 acc=0.083 state=0.0965 align=0.0000 latA=0.7499 latP=0.1026 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=0.90 | sec/step~8.00 | keep=0.85 | K=8 | llama(L): tf=8.3587 first=7.9331 kCE=6.2621 KD=2.5522 acc=0.056 state=0.1715 align=0.0000 latA=0.7822 latP=0.1428 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  137/445 | (tail text) | align=0.0002 | text_tf=9.0789 | latent_scale=1.00
  step  140/445 | grad_norm=0.97 | sec/step~8.78 | keep=0.85 | K=8 | llama(L): tf=8.3260 first=7.8945 kCE=6.0137 KD=2.8772 acc=0.056 state=0.3094 align=0.0000 latA=0.7903 latP=0.1290 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  145/445 | (tail text) | align=0.0003 | text_tf=9.3669 | latent_scale=1.00
  step  150/445 | grad_norm=1.00 | sec/step~8.26 | keep=0.85 | K=8 | llama(L): tf=8.7004 first=7.3534 kCE=5.7135 KD=4.4453 acc=0.111 state=0.1585 align=0.0000 latA=0.7690 latP=0.1036 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=0.57 | sec/step~8.88 | keep=0.85 | K=8 | llama(L): tf=8.4013 first=6.4746 kCE=5.9262 KD=2.8564 acc=0.111 state=0.4096 align=0.0000 latA=0.7480 latP=0.1084 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.30 | sec/step~8.45 | keep=0.85 | K=8 | llama(L): tf=8.2324 first=7.4333 kCE=6.7758 KD=2.7999 acc=0.083 state=0.2636 align=0.0000 latA=0.7750 latP=0.1722 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=1.82 | sec/step~8.68 | keep=0.85 | K=8 | llama(L): tf=8.9146 first=7.9098 kCE=5.9553 KD=3.0049 acc=0.028 state=0.1766 align=0.0000 latA=0.8010 latP=0.1124 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=0.89 | sec/step~7.70 | keep=0.85 | K=8 | llama(L): tf=8.1626 first=7.0453 kCE=6.0532 KD=3.5213 acc=0.000 state=0.0518 align=0.0000 latA=0.7763 latP=0.1317 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  200/445 | grad_norm=1.72 | sec/step~7.63 | keep=0.85 | K=8 | llama(L): tf=8.6541 first=8.2096 kCE=5.3058 KD=2.8324 acc=0.000 state=0.0438 align=0.0000 latA=0.8120 latP=0.1059 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=1.74 | sec/step~7.84 | keep=0.85 | K=8 | llama(L): tf=8.2690 first=7.3220 kCE=5.7226 KD=2.3286 acc=0.028 state=0.0775 align=0.0000 latA=0.7614 latP=0.1156 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.60 | sec/step~7.88 | keep=0.85 | K=8 | llama(L): tf=8.4561 first=7.5485 kCE=6.2698 KD=2.6666 acc=0.028 state=0.0506 align=0.0000 latA=0.7795 latP=0.1425 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.22 | sec/step~7.91 | keep=0.85 | K=8 | llama(L): tf=8.8208 first=7.9248 kCE=5.9981 KD=2.9778 acc=0.000 state=0.1812 align=0.0000 latA=0.8037 latP=0.1236 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=1.50 | sec/step~7.75 | keep=0.85 | K=8 | llama(L): tf=8.3307 first=7.1114 kCE=6.6393 KD=2.6132 acc=0.056 state=0.0734 align=0.0000 latA=0.7424 latP=0.1573 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  244/445 | (tail text) | align=0.0003 | text_tf=9.4729 | latent_scale=1.00
  step  250/445 | grad_norm=0.84 | sec/step~8.38 | keep=0.85 | K=8 | llama(L): tf=8.6400 first=7.7394 kCE=5.4904 KD=3.2151 acc=0.028 state=0.0637 align=0.0000 latA=0.7920 latP=0.0969 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=0.99 | sec/step~8.73 | keep=0.85 | K=8 | llama(L): tf=8.2336 first=7.5754 kCE=6.3231 KD=3.1258 acc=0.056 state=0.2021 align=0.0000 latA=0.7804 latP=0.1429 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=0.74 | sec/step~8.82 | keep=0.85 | K=8 | llama(L): tf=8.0242 first=7.1731 kCE=5.7485 KD=2.5610 acc=0.056 state=0.4429 align=0.0000 latA=0.7790 latP=0.1257 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  279/445 | (tail text) | align=0.0003 | text_tf=9.4614 | latent_scale=1.00
  step  280/445 | grad_norm=0.34 | sec/step~10.00 | keep=0.85 | K=8 | llama(L): tf=8.4010 first=6.8364 kCE=5.7255 KD=2.7024 acc=0.083 state=0.2505 align=0.0000 latA=0.7587 latP=0.1187 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  285/445 | (tail text) | align=0.0003 | text_tf=9.0562 | latent_scale=1.00
  step  290/445 | grad_norm=0.33 | sec/step~8.35 | keep=0.85 | K=8 | llama(L): tf=8.8920 first=7.7104 kCE=5.6795 KD=2.2929 acc=0.056 state=0.0388 align=0.0000 latA=0.7717 latP=0.1177 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  296/445 | (tail text) | align=0.0003 | text_tf=9.3962 | latent_scale=1.00
  step  300/445 | grad_norm=1.16 | sec/step~8.55 | keep=0.85 | K=8 | llama(L): tf=8.5176 first=7.7061 kCE=6.0330 KD=2.5249 acc=0.000 state=0.1031 align=0.0000 latA=0.7911 latP=0.1284 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=0.80 | sec/step~7.74 | keep=0.85 | K=8 | llama(L): tf=8.4040 first=7.1969 kCE=6.0631 KD=3.7976 acc=0.083 state=0.0628 align=0.0000 latA=0.7803 latP=0.1477 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=0.84 | sec/step~8.04 | keep=0.85 | K=8 | llama(L): tf=8.3157 first=6.6981 kCE=5.8429 KD=2.5018 acc=0.083 state=0.0619 align=0.0000 latA=0.7555 latP=0.1290 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=0.41 | sec/step~7.88 | keep=0.85 | K=8 | llama(L): tf=8.7234 first=7.4990 kCE=6.4955 KD=2.5960 acc=0.028 state=0.0474 align=0.0000 latA=0.7866 latP=0.1361 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=0.40 | sec/step~9.12 | keep=0.85 | K=8 | llama(L): tf=8.8075 first=7.6665 kCE=5.4819 KD=3.0130 acc=0.056 state=0.2857 align=0.0000 latA=0.7753 latP=0.1002 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.23 | sec/step~8.87 | keep=0.85 | K=8 | llama(L): tf=8.5515 first=7.9308 kCE=5.6460 KD=2.5613 acc=0.000 state=0.0306 align=0.0000 latA=0.8084 latP=0.1030 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=1.09 | sec/step~8.22 | keep=0.85 | K=8 | llama(L): tf=8.4273 first=7.6981 kCE=5.6242 KD=2.4836 acc=0.000 state=0.1162 align=0.0000 latA=0.7901 latP=0.1220 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=0.78 | sec/step~8.16 | keep=0.85 | K=8 | llama(L): tf=8.1641 first=7.2291 kCE=5.1475 KD=2.9223 acc=0.056 state=0.1079 align=0.0000 latA=0.7658 latP=0.0945 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=1.11 | sec/step~9.99 | keep=0.85 | K=8 | llama(L): tf=8.3772 first=7.8635 kCE=5.6950 KD=2.5710 acc=0.000 state=0.0363 align=0.0000 latA=0.7903 latP=0.1122 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.65 | sec/step~9.04 | keep=0.85 | K=8 | llama(L): tf=8.2301 first=7.4840 kCE=6.4343 KD=3.0493 acc=0.056 state=0.2038 align=0.0000 latA=0.7840 latP=0.1343 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.42 | sec/step~8.02 | keep=0.85 | K=8 | llama(L): tf=8.2915 first=7.2688 kCE=6.5774 KD=2.6798 acc=0.056 state=0.0511 align=0.0000 latA=0.7589 latP=0.1431 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.20 | sec/step~8.74 | keep=0.85 | K=8 | llama(L): tf=8.2129 first=7.4842 kCE=6.0041 KD=2.9710 acc=0.000 state=0.0623 align=0.0000 latA=0.7868 latP=0.1303 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=0.86 | sec/step~9.42 | keep=0.85 | K=8 | llama(L): tf=8.1775 first=7.5502 kCE=5.4217 KD=3.4188 acc=0.111 state=0.1210 align=0.0000 latA=0.7733 latP=0.1125 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=0.69 | sec/step~9.22 | keep=0.85 | K=8 | llama(L): tf=8.0324 first=6.9141 kCE=5.5802 KD=2.4759 acc=0.111 state=0.0644 align=0.0000 latA=0.7514 latP=0.1201 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=0.53 | sec/step~9.14 | keep=0.85 | K=8 | llama(L): tf=8.8084 first=7.7871 kCE=5.8197 KD=6.3026 acc=0.056 state=0.0336 align=0.0000 latA=0.7884 latP=0.1160 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.21 | sec/step~5.52 | keep=0.85 | K=8 | llama(L): tf=8.6633 first=8.0693 kCE=5.6248 KD=2.8849 acc=0.062 state=0.2867 align=0.0000 latA=0.7824 latP=0.1013 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5340
Epoch 12/6
  step  10/445 | grad_norm=1.54 | sec/step~8.32 | keep=0.85 | K=8 | llama(L): tf=8.4259 first=8.0159 kCE=6.1720 KD=2.5287 acc=0.028 state=0.0326 align=0.0000 latA=0.8141 latP=0.1287 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=0.77 | sec/step~8.51 | keep=0.85 | K=8 | llama(L): tf=8.4283 first=6.9676 kCE=5.5058 KD=3.3334 acc=0.083 state=0.0629 align=0.0000 latA=0.7615 latP=0.1036 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.77 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=8.3674 first=7.1651 kCE=5.9382 KD=3.8749 acc=0.056 state=0.0448 align=0.0000 latA=0.7687 latP=0.1477 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.51 | sec/step~9.08 | keep=0.85 | K=8 | llama(L): tf=8.8138 first=7.4787 kCE=5.9127 KD=2.6215 acc=0.000 state=0.2718 align=0.0000 latA=0.7958 latP=0.1211 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.29 | sec/step~7.96 | keep=0.85 | K=8 | llama(L): tf=8.5275 first=7.0968 kCE=5.7478 KD=2.7834 acc=0.028 state=0.0537 align=0.0000 latA=0.7671 latP=0.1179 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  57/445 | (tail text) | align=0.0002 | text_tf=8.9598 | latent_scale=1.00
  step  60/445 | grad_norm=1.35 | sec/step~8.22 | keep=0.85 | K=8 | llama(L): tf=8.2096 first=6.9284 kCE=5.3568 KD=2.5014 acc=0.139 state=0.0751 align=0.0000 latA=0.7523 latP=0.1110 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=1.25 | sec/step~8.87 | keep=0.85 | K=8 | llama(L): tf=8.0583 first=7.4708 kCE=5.3540 KD=2.3293 acc=0.000 state=0.0651 align=0.0000 latA=0.7980 latP=0.1094 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  74/445 | (tail text) | align=0.0003 | text_tf=9.6054 | latent_scale=1.00
  step  80/445 | grad_norm=1.30 | sec/step~8.13 | keep=0.85 | K=8 | llama(L): tf=8.8342 first=7.4330 kCE=6.2491 KD=2.7457 acc=0.083 state=0.0454 align=0.0000 latA=0.7891 latP=0.1290 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.70 | sec/step~7.63 | keep=0.85 | K=8 | llama(L): tf=8.2790 first=7.2374 kCE=5.5304 KD=4.5312 acc=0.028 state=0.0327 align=0.0000 latA=0.7724 latP=0.1131 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  95/445 | (tail text) | align=0.0003 | text_tf=9.1455 | latent_scale=1.00
  step  100/445 | grad_norm=0.28 | sec/step~8.80 | keep=0.85 | K=8 | llama(L): tf=8.7176 first=7.4441 kCE=5.5906 KD=2.4945 acc=0.083 state=0.0640 align=0.0000 latA=0.7644 latP=0.1135 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  110/445 | grad_norm=0.34 | sec/step~8.18 | keep=0.85 | K=8 | llama(L): tf=8.4481 first=7.3288 kCE=5.2443 KD=2.3358 acc=0.083 state=0.0650 align=0.0000 latA=0.7652 latP=0.1055 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.27 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.7414 first=7.2064 kCE=5.9697 KD=3.3304 acc=0.028 state=0.0635 align=0.0000 latA=0.7543 latP=0.1231 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=0.84 | sec/step~8.10 | keep=0.85 | K=8 | llama(L): tf=8.6397 first=7.5133 kCE=5.5817 KD=2.7605 acc=0.028 state=0.0536 align=0.0000 latA=0.7893 latP=0.1066 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=2.09 | sec/step~9.36 | keep=0.85 | K=8 | llama(L): tf=8.3142 first=6.8295 kCE=5.5430 KD=2.5974 acc=0.083 state=0.2589 align=0.0000 latA=0.7589 latP=0.1185 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=1.24 | sec/step~10.74 | keep=0.85 | K=8 | llama(L): tf=8.8385 first=7.4525 kCE=5.1003 KD=3.4031 acc=0.000 state=0.6927 align=0.0000 latA=0.7775 latP=0.1030 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=0.60 | sec/step~9.12 | keep=0.85 | K=8 | llama(L): tf=8.3264 first=7.0977 kCE=5.3730 KD=2.3693 acc=0.028 state=0.1679 align=0.0000 latA=0.7805 latP=0.1232 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.74 | sec/step~8.23 | keep=0.85 | K=8 | llama(L): tf=8.0587 first=7.2938 kCE=5.3141 KD=3.4177 acc=0.028 state=0.0697 align=0.0000 latA=0.7761 latP=0.1161 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=2.01 | sec/step~9.16 | keep=0.85 | K=8 | llama(L): tf=8.3136 first=7.4527 kCE=5.2914 KD=2.3582 acc=0.028 state=0.0687 align=0.0000 latA=0.7815 latP=0.1291 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=1.31 | sec/step~8.06 | keep=0.85 | K=8 | llama(L): tf=8.6143 first=6.7698 kCE=5.4514 KD=2.3000 acc=0.111 state=0.0466 align=0.0000 latA=0.7488 latP=0.1318 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  200/445 | grad_norm=1.23 | sec/step~8.86 | keep=0.85 | K=8 | llama(L): tf=8.3827 first=7.0606 kCE=5.4665 KD=2.8037 acc=0.028 state=0.0685 align=0.0000 latA=0.7808 latP=0.1162 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=0.81 | sec/step~8.41 | keep=0.85 | K=8 | llama(L): tf=8.5057 first=7.5000 kCE=5.2487 KD=2.5377 acc=0.028 state=0.0753 align=0.0000 latA=0.7866 latP=0.1054 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.50 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=8.0398 first=7.2468 kCE=4.8809 KD=3.0570 acc=0.056 state=0.0543 align=0.0000 latA=0.7925 latP=0.1035 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.30 | sec/step~8.67 | keep=0.85 | K=8 | llama(L): tf=8.2746 first=7.4077 kCE=5.7690 KD=3.0045 acc=0.028 state=0.0425 align=0.0000 latA=0.7795 latP=0.1232 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=1.55 | sec/step~7.90 | keep=0.85 | K=8 | llama(L): tf=8.6226 first=7.9085 kCE=5.5597 KD=3.9110 acc=0.028 state=0.0409 align=0.0000 latA=0.7831 latP=0.1078 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=4.24 | sec/step~7.98 | keep=0.85 | K=8 | llama(L): tf=8.3798 first=7.6332 kCE=5.9051 KD=2.8973 acc=0.028 state=0.1009 align=0.0000 latA=0.7830 latP=0.1409 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.73 | sec/step~9.18 | keep=0.85 | K=8 | llama(L): tf=8.3732 first=7.0019 kCE=5.3038 KD=2.3643 acc=0.056 state=0.1215 align=0.0000 latA=0.7553 latP=0.1212 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=0.85 | sec/step~7.81 | keep=0.85 | K=8 | llama(L): tf=8.2958 first=7.3626 kCE=5.2431 KD=2.6111 acc=0.000 state=0.0427 align=0.0000 latA=0.7831 latP=0.1101 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.52 | sec/step~8.04 | keep=0.85 | K=8 | llama(L): tf=8.9844 first=7.5489 kCE=5.7977 KD=2.8489 acc=0.028 state=0.0536 align=0.0000 latA=0.7755 latP=0.1091 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.45 | sec/step~8.73 | keep=0.85 | K=8 | llama(L): tf=8.4397 first=7.0926 kCE=5.6724 KD=2.5770 acc=0.083 state=0.0837 align=0.0000 latA=0.7708 latP=0.1177 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  300/445 | grad_norm=1.88 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=8.2382 first=7.3825 kCE=5.3607 KD=2.4710 acc=0.056 state=0.0573 align=0.0000 latA=0.7666 latP=0.1169 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.43 | sec/step~7.89 | keep=0.85 | K=8 | llama(L): tf=8.0807 first=7.4432 kCE=4.8830 KD=2.5474 acc=0.028 state=0.0688 align=0.0000 latA=0.7892 latP=0.1110 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=1.19 | sec/step~7.71 | keep=0.85 | K=8 | llama(L): tf=8.8422 first=8.1275 kCE=5.1881 KD=3.0821 acc=0.000 state=0.0354 align=0.0000 latA=0.8126 latP=0.1029 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=1.16 | sec/step~8.63 | keep=0.85 | K=8 | llama(L): tf=8.4815 first=7.6224 kCE=5.5588 KD=2.9203 acc=0.028 state=0.0801 align=0.0000 latA=0.7976 latP=0.1449 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=0.50 | sec/step~8.08 | keep=0.85 | K=8 | llama(L): tf=8.5793 first=7.6957 kCE=5.3516 KD=3.4922 acc=0.056 state=0.0340 align=0.0000 latA=0.7844 latP=0.1112 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.30 | sec/step~7.91 | keep=0.85 | K=8 | llama(L): tf=8.2642 first=7.5737 kCE=5.1730 KD=3.6987 acc=0.083 state=0.0334 align=0.0000 latA=0.7712 latP=0.1133 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=2.60 | sec/step~7.89 | keep=0.85 | K=8 | llama(L): tf=8.2807 first=7.7343 kCE=5.5349 KD=2.8053 acc=0.028 state=0.0367 align=0.0000 latA=0.7885 latP=0.1405 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=4.70 | sec/step~8.84 | keep=0.85 | K=8 | llama(L): tf=8.1623 first=7.4538 kCE=5.7430 KD=3.8352 acc=0.056 state=0.1052 align=0.0000 latA=0.7771 latP=0.1334 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=1.29 | sec/step~8.65 | keep=0.85 | K=8 | llama(L): tf=8.4765 first=8.1630 kCE=5.6335 KD=3.5743 acc=0.028 state=0.0452 align=0.0000 latA=0.8029 latP=0.1334 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.73 | sec/step~8.14 | keep=0.85 | K=8 | llama(L): tf=8.3257 first=7.2680 kCE=5.0545 KD=2.6889 acc=0.028 state=0.0389 align=0.0000 latA=0.7649 latP=0.1115 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.48 | sec/step~7.86 | keep=0.85 | K=8 | llama(L): tf=7.6703 first=7.3229 kCE=4.9067 KD=4.5179 acc=0.056 state=0.0529 align=0.0000 latA=0.7872 latP=0.1138 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  404/445 | (tail text) | align=0.0003 | text_tf=9.7884 | latent_scale=1.00
  step  410/445 | grad_norm=0.33 | sec/step~7.87 | keep=0.85 | K=8 | llama(L): tf=8.0516 first=7.4367 kCE=5.2778 KD=2.5029 acc=0.111 state=0.0345 align=0.0000 latA=0.7554 latP=0.1138 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=1.58 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=8.2708 first=7.3733 kCE=5.6394 KD=3.9999 acc=0.028 state=0.0319 align=0.0000 latA=0.7727 latP=0.1450 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=0.84 | sec/step~8.38 | keep=0.85 | K=8 | llama(L): tf=8.5311 first=6.8646 kCE=6.0245 KD=3.1261 acc=0.083 state=0.0460 align=0.0000 latA=0.7554 latP=0.1317 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=0.74 | sec/step~9.16 | keep=0.85 | K=8 | llama(L): tf=8.3270 first=7.3026 kCE=5.2527 KD=3.0078 acc=0.083 state=0.1601 align=0.0000 latA=0.7745 latP=0.1135 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.39 | sec/step~5.10 | keep=0.85 | K=8 | llama(L): tf=8.4891 first=7.3611 kCE=5.2392 KD=2.6758 acc=0.000 state=0.0694 align=0.0000 latA=0.7675 latP=0.1211 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5785
Epoch 13/6
  step  10/445 | grad_norm=1.54 | sec/step~7.80 | keep=0.85 | K=8 | llama(L): tf=8.5568 first=7.3688 kCE=4.8679 KD=2.7684 acc=0.056 state=0.0428 align=0.0000 latA=0.7757 latP=0.1008 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=0.88 | sec/step~8.45 | keep=0.85 | K=8 | llama(L): tf=8.1644 first=7.1213 kCE=5.1369 KD=2.7698 acc=0.083 state=0.0605 align=0.0000 latA=0.7651 latP=0.1216 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  28/445 | (tail text) | align=0.0003 | text_tf=9.0858 | latent_scale=1.00
  step  30/445 | grad_norm=0.48 | sec/step~8.71 | keep=0.85 | K=8 | llama(L): tf=8.3267 first=6.8097 kCE=5.4381 KD=2.6608 acc=0.111 state=0.0428 align=0.0000 latA=0.7649 latP=0.1152 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.37 | sec/step~8.68 | keep=0.85 | K=8 | llama(L): tf=8.8837 first=7.7184 kCE=5.3091 KD=3.4802 acc=0.083 state=0.0931 align=0.0000 latA=0.7884 latP=0.1157 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.23 | sec/step~9.00 | keep=0.85 | K=8 | llama(L): tf=8.2676 first=7.2089 kCE=5.7381 KD=3.1451 acc=0.028 state=0.0528 align=0.0000 latA=0.7776 latP=0.1248 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  57/445 | (tail text) | align=0.0002 | text_tf=9.5447 | latent_scale=1.00
  step  60/445 | grad_norm=1.09 | sec/step~8.09 | keep=0.85 | K=8 | llama(L): tf=8.1461 first=8.0318 kCE=5.0029 KD=2.5015 acc=0.028 state=0.0432 align=0.0000 latA=0.7859 latP=0.1105 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=1.14 | sec/step~8.04 | keep=0.85 | K=8 | llama(L): tf=8.2066 first=6.8512 kCE=4.9004 KD=5.0412 acc=0.083 state=0.0685 align=0.0000 latA=0.7623 latP=0.1031 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=0.80 | sec/step~8.14 | keep=0.85 | K=8 | llama(L): tf=7.8751 first=6.8631 kCE=5.3752 KD=2.8056 acc=0.056 state=0.0813 align=0.0000 latA=0.7625 latP=0.1297 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.70 | sec/step~8.22 | keep=0.85 | K=8 | llama(L): tf=7.9847 first=7.0822 kCE=5.0082 KD=2.8811 acc=0.056 state=0.0525 align=0.0000 latA=0.7560 latP=0.1290 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  95/445 | (tail text) | align=0.0002 | text_tf=9.2829 | latent_scale=1.00
  step  100/445 | grad_norm=0.35 | sec/step~8.72 | keep=0.85 | K=8 | llama(L): tf=8.2654 first=8.2272 kCE=5.4772 KD=3.4388 acc=0.028 state=0.0515 align=0.0000 latA=0.7968 latP=0.1341 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  110/445 | grad_norm=0.52 | sec/step~8.07 | keep=0.85 | K=8 | llama(L): tf=8.6677 first=7.4759 kCE=5.6031 KD=3.1385 acc=0.056 state=0.0312 align=0.0000 latA=0.7733 latP=0.1332 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.30 | sec/step~8.13 | keep=0.85 | K=8 | llama(L): tf=8.7303 first=7.5298 kCE=5.6602 KD=3.0843 acc=0.083 state=0.0493 align=0.0000 latA=0.7737 latP=0.1232 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=1.05 | sec/step~8.23 | keep=0.85 | K=8 | llama(L): tf=8.3624 first=7.7462 kCE=5.5461 KD=3.2485 acc=0.056 state=0.0376 align=0.0000 latA=0.7918 latP=0.1349 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=1.50 | sec/step~7.96 | keep=0.85 | K=8 | llama(L): tf=8.2548 first=7.3788 kCE=5.1307 KD=2.7922 acc=0.028 state=0.0549 align=0.0000 latA=0.7672 latP=0.1118 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=0.89 | sec/step~7.96 | keep=0.85 | K=8 | llama(L): tf=8.3642 first=7.4831 kCE=4.4875 KD=2.7251 acc=0.028 state=0.0461 align=0.0000 latA=0.7851 latP=0.0908 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=0.67 | sec/step~8.09 | keep=0.85 | K=8 | llama(L): tf=7.7685 first=7.1546 kCE=5.3080 KD=2.4475 acc=0.056 state=0.0324 align=0.0000 latA=0.7640 latP=0.1538 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.51 | sec/step~9.11 | keep=0.85 | K=8 | llama(L): tf=8.6665 first=7.9859 kCE=5.3512 KD=2.8338 acc=0.000 state=0.0645 align=0.0000 latA=0.7806 latP=0.1140 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=0.96 | sec/step~8.33 | keep=0.85 | K=8 | llama(L): tf=8.5181 first=7.2828 kCE=5.4152 KD=2.5454 acc=0.083 state=0.0557 align=0.0000 latA=0.7564 latP=0.1248 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  182/445 | (tail text) | align=0.0003 | text_tf=8.7963 | latent_scale=1.00
  step  190/445 | grad_norm=0.91 | sec/step~8.16 | keep=0.85 | K=8 | llama(L): tf=8.0099 first=6.9822 kCE=5.2410 KD=2.2859 acc=0.028 state=0.0326 align=0.0000 latA=0.7511 latP=0.1234 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  199/445 | (tail text) | align=0.0002 | text_tf=8.6769 | latent_scale=1.00
  step  200/445 | grad_norm=1.05 | sec/step~7.90 | keep=0.85 | K=8 | llama(L): tf=8.7254 first=7.2530 kCE=5.1559 KD=3.7914 acc=0.111 state=0.0326 align=0.0000 latA=0.7755 latP=0.1188 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=0.66 | sec/step~8.11 | keep=0.85 | K=8 | llama(L): tf=8.0586 first=7.1809 kCE=4.8584 KD=3.0583 acc=0.000 state=0.0541 align=0.0000 latA=0.7916 latP=0.1045 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.48 | sec/step~8.21 | keep=0.85 | K=8 | llama(L): tf=8.5992 first=7.5146 kCE=5.3695 KD=2.7942 acc=0.083 state=0.0321 align=0.0000 latA=0.7902 latP=0.1169 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.25 | sec/step~9.10 | keep=0.85 | K=8 | llama(L): tf=8.7110 first=7.6187 kCE=5.4243 KD=2.6357 acc=0.028 state=0.1044 align=0.0000 latA=0.7747 latP=0.1207 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=1.21 | sec/step~8.29 | keep=0.85 | K=8 | llama(L): tf=8.4597 first=7.7072 kCE=5.3593 KD=2.7219 acc=0.111 state=0.0365 align=0.0000 latA=0.7844 latP=0.1168 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=0.80 | sec/step~8.07 | keep=0.85 | K=8 | llama(L): tf=8.4135 first=7.3764 kCE=5.1964 KD=2.4867 acc=0.083 state=0.0367 align=0.0000 latA=0.7765 latP=0.1202 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=2.32 | sec/step~8.79 | keep=0.85 | K=8 | llama(L): tf=8.2519 first=7.2770 kCE=5.4107 KD=3.0794 acc=0.111 state=0.1081 align=0.0000 latA=0.7652 latP=0.1226 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=1.23 | sec/step~8.16 | keep=0.85 | K=8 | llama(L): tf=8.5817 first=7.3416 kCE=5.2228 KD=2.6946 acc=0.000 state=0.0581 align=0.0000 latA=0.7741 latP=0.1076 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  271/445 | (tail text) | align=0.0003 | text_tf=9.1381 | latent_scale=1.00
  step  280/445 | grad_norm=0.38 | sec/step~8.50 | keep=0.85 | K=8 | llama(L): tf=8.4982 first=7.1905 kCE=5.2224 KD=2.6907 acc=0.056 state=0.0336 align=0.0000 latA=0.7552 latP=0.1077 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.33 | sec/step~8.27 | keep=0.85 | K=8 | llama(L): tf=8.1458 first=6.8720 kCE=5.0927 KD=3.5741 acc=0.028 state=0.0308 align=0.0000 latA=0.7535 latP=0.1063 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  300/445 | grad_norm=1.20 | sec/step~9.21 | keep=0.85 | K=8 | llama(L): tf=8.3875 first=7.1905 kCE=5.7934 KD=2.4073 acc=0.083 state=0.1669 align=0.0000 latA=0.7747 latP=0.1376 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.21 | sec/step~7.90 | keep=0.85 | K=8 | llama(L): tf=8.4042 first=6.7937 kCE=5.5394 KD=2.6686 acc=0.111 state=0.0400 align=0.0000 latA=0.7582 latP=0.1165 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=0.74 | sec/step~8.35 | keep=0.85 | K=8 | llama(L): tf=8.7567 first=7.1776 kCE=6.2032 KD=2.7333 acc=0.167 state=0.0946 align=0.0000 latA=0.7587 latP=0.1412 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=0.83 | sec/step~9.75 | keep=0.85 | K=8 | llama(L): tf=8.1627 first=7.4654 kCE=5.4150 KD=2.8551 acc=0.056 state=0.3690 align=0.0000 latA=0.7735 latP=0.1226 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  333/445 | (tail text) | align=0.0002 | text_tf=8.9431 | latent_scale=1.00
  step  340/445 | grad_norm=0.44 | sec/step~8.83 | keep=0.85 | K=8 | llama(L): tf=8.6095 first=7.4577 kCE=4.6416 KD=2.8434 acc=0.028 state=0.0919 align=0.0000 latA=0.7984 latP=0.0978 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  341/445 | (tail text) | align=0.0002 | text_tf=8.3270 | latent_scale=1.00
  step  343/445 | (tail text) | align=0.0003 | text_tf=9.0117 | latent_scale=1.00
  step  347/445 | (tail text) | align=0.0003 | text_tf=8.7094 | latent_scale=1.00
  step  350/445 | grad_norm=0.45 | sec/step~8.75 | keep=0.85 | K=8 | llama(L): tf=8.3575 first=7.7088 kCE=5.6239 KD=3.1662 acc=0.028 state=0.0505 align=0.0000 latA=0.7803 latP=0.1433 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=1.68 | sec/step~8.64 | keep=0.85 | K=8 | llama(L): tf=8.1531 first=6.8737 kCE=5.2113 KD=2.4046 acc=0.111 state=0.0900 align=0.0000 latA=0.7429 latP=0.1155 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=1.64 | sec/step~7.75 | keep=0.85 | K=8 | llama(L): tf=8.3928 first=7.1766 kCE=5.3751 KD=2.5976 acc=0.083 state=0.0313 align=0.0000 latA=0.7740 latP=0.1209 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  374/445 | (tail text) | align=0.0003 | text_tf=9.0347 | latent_scale=1.00
  step  380/445 | grad_norm=0.89 | sec/step~8.73 | keep=0.85 | K=8 | llama(L): tf=7.7680 first=7.0537 kCE=4.8390 KD=2.2570 acc=0.056 state=0.1185 align=0.0000 latA=0.7472 latP=0.1068 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.94 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.4320 first=7.8235 kCE=5.0656 KD=2.6106 acc=0.028 state=0.0458 align=0.0000 latA=0.7852 latP=0.1141 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.66 | sec/step~7.99 | keep=0.85 | K=8 | llama(L): tf=8.7448 first=8.0117 kCE=5.1879 KD=2.5297 acc=0.000 state=0.0614 align=0.0000 latA=0.8197 latP=0.1020 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.72 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=8.5424 first=7.0072 kCE=5.2809 KD=2.5131 acc=0.139 state=0.0365 align=0.0000 latA=0.7424 latP=0.1227 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=3.09 | sec/step~8.15 | keep=0.85 | K=8 | llama(L): tf=8.4367 first=7.0072 kCE=4.7684 KD=2.5716 acc=0.056 state=0.0505 align=0.0000 latA=0.7605 latP=0.0995 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=0.72 | sec/step~8.97 | keep=0.85 | K=8 | llama(L): tf=8.2817 first=7.4045 kCE=4.8074 KD=2.5214 acc=0.111 state=0.1656 align=0.0000 latA=0.7780 latP=0.1055 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=1.22 | sec/step~8.11 | keep=0.85 | K=8 | llama(L): tf=8.3738 first=7.8335 kCE=5.6530 KD=3.0187 acc=0.056 state=0.0756 align=0.0000 latA=0.7812 latP=0.1343 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  442/445 | (tail text) | align=0.0003 | text_tf=9.3550 | latent_scale=1.00
  step  445/445 | grad_norm=0.17 | sec/step~4.63 | keep=0.85 | K=8 | llama(L): tf=8.3017 first=8.3007 kCE=4.6990 KD=2.1412 acc=0.000 state=0.0305 align=0.0000 latA=0.7935 latP=0.1018 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 6230
Epoch 14/6
  step  10/445 | grad_norm=1.91 | sec/step~8.45 | keep=0.85 | K=8 | llama(L): tf=8.5041 first=7.8315 kCE=4.9963 KD=2.6605 acc=0.028 state=0.0591 align=0.0000 latA=0.7825 latP=0.1068 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=1.77 | sec/step~8.19 | keep=0.85 | K=8 | llama(L): tf=8.3608 first=7.7869 kCE=4.7582 KD=3.1344 acc=0.000 state=0.0791 align=0.0000 latA=0.7856 latP=0.1078 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.84 | sec/step~8.18 | keep=0.85 | K=8 | llama(L): tf=8.1682 first=6.8562 kCE=5.1455 KD=2.5665 acc=0.139 state=0.0557 align=0.0000 latA=0.7407 latP=0.1243 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.64 | sec/step~8.19 | keep=0.85 | K=8 | llama(L): tf=8.5033 first=7.9162 kCE=4.8831 KD=2.7818 acc=0.028 state=0.0708 align=0.0000 latA=0.7968 latP=0.1148 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.32 | sec/step~8.16 | keep=0.85 | K=8 | llama(L): tf=8.1849 first=7.0618 kCE=5.1714 KD=2.5195 acc=0.028 state=0.1022 align=0.0000 latA=0.7629 latP=0.1127 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=1.71 | sec/step~9.01 | keep=0.85 | K=8 | llama(L): tf=7.7057 first=7.2137 kCE=4.8256 KD=2.4929 acc=0.056 state=0.0889 align=0.0000 latA=0.7864 latP=0.1214 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=0.99 | sec/step~7.89 | keep=0.85 | K=8 | llama(L): tf=8.4071 first=7.8758 kCE=5.7159 KD=3.6866 acc=0.028 state=0.0326 align=0.0000 latA=0.7898 latP=0.1321 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=0.80 | sec/step~8.14 | keep=0.85 | K=8 | llama(L): tf=8.1132 first=7.2181 kCE=4.7985 KD=3.4481 acc=0.028 state=0.0699 align=0.0000 latA=0.7658 latP=0.1125 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=1.37 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=8.0748 first=7.0287 kCE=5.6056 KD=3.6083 acc=0.056 state=0.0374 align=0.0000 latA=0.7583 latP=0.1204 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=0.93 | sec/step~7.99 | keep=0.85 | K=8 | llama(L): tf=8.1833 first=7.1357 kCE=5.4809 KD=2.9770 acc=0.083 state=0.0562 align=0.0000 latA=0.7623 latP=0.1368 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  106/445 | (tail text) | align=0.0003 | text_tf=9.0907 | latent_scale=1.00
  step  110/445 | grad_norm=0.48 | sec/step~8.33 | keep=0.85 | K=8 | llama(L): tf=8.6065 first=6.6007 kCE=4.7490 KD=2.7260 acc=0.083 state=0.0336 align=0.0000 latA=0.7448 latP=0.0976 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.26 | sec/step~8.09 | keep=0.85 | K=8 | llama(L): tf=8.6883 first=7.4575 kCE=4.9399 KD=3.3865 acc=0.083 state=0.0415 align=0.0000 latA=0.7694 latP=0.1093 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=2.09 | sec/step~7.99 | keep=0.85 | K=8 | llama(L): tf=8.2902 first=7.7064 kCE=5.1398 KD=3.5944 acc=0.056 state=0.0446 align=0.0000 latA=0.7927 latP=0.1169 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=1.31 | sec/step~7.98 | keep=0.85 | K=8 | llama(L): tf=8.0568 first=6.8865 kCE=5.0166 KD=2.5938 acc=0.056 state=0.0358 align=0.0000 latA=0.7507 latP=0.1207 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=1.03 | sec/step~7.92 | keep=0.85 | K=8 | llama(L): tf=8.5413 first=7.2177 kCE=4.6683 KD=3.8905 acc=0.028 state=0.0337 align=0.0000 latA=0.7740 latP=0.0947 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=1.28 | sec/step~8.84 | keep=0.85 | K=8 | llama(L): tf=8.1681 first=7.0119 kCE=5.0592 KD=3.9733 acc=0.111 state=0.1300 align=0.0000 latA=0.7642 latP=0.1230 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.28 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.5679 first=7.4955 kCE=5.3492 KD=2.4113 acc=0.111 state=0.0439 align=0.0000 latA=0.7572 latP=0.1339 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=1.04 | sec/step~8.21 | keep=0.85 | K=8 | llama(L): tf=8.2963 first=6.9599 kCE=5.1824 KD=2.6225 acc=0.139 state=0.0431 align=0.0000 latA=0.7706 latP=0.1102 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=1.27 | sec/step~8.80 | keep=0.85 | K=8 | llama(L): tf=8.7580 first=7.3422 kCE=5.3847 KD=2.7314 acc=0.056 state=0.0317 align=0.0000 latA=0.7801 latP=0.1195 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  200/445 | grad_norm=0.89 | sec/step~8.17 | keep=0.85 | K=8 | llama(L): tf=8.5159 first=7.2811 kCE=5.9183 KD=3.7311 acc=0.083 state=0.0301 align=0.0000 latA=0.7676 latP=0.1302 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  202/445 | (tail text) | align=0.0003 | text_tf=9.4986 | latent_scale=1.00
  step  210/445 | grad_norm=0.70 | sec/step~8.10 | keep=0.85 | K=8 | llama(L): tf=8.1913 first=7.0026 kCE=5.3159 KD=2.2823 acc=0.056 state=0.0358 align=0.0000 latA=0.7701 latP=0.1216 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.83 | sec/step~8.07 | keep=0.85 | K=8 | llama(L): tf=8.7223 first=7.8358 kCE=5.6814 KD=2.3889 acc=0.000 state=0.0360 align=0.0000 latA=0.7840 latP=0.1284 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.29 | sec/step~7.82 | keep=0.85 | K=8 | llama(L): tf=8.4803 first=7.6044 kCE=5.2160 KD=2.5363 acc=0.028 state=0.0307 align=0.0000 latA=0.7947 latP=0.1108 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=1.46 | sec/step~8.96 | keep=0.85 | K=8 | llama(L): tf=8.3878 first=6.9533 kCE=5.0456 KD=2.7159 acc=0.028 state=0.1360 align=0.0000 latA=0.7541 latP=0.1100 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=2.20 | sec/step~9.20 | keep=0.85 | K=8 | llama(L): tf=8.0322 first=7.3923 kCE=5.3815 KD=2.5406 acc=0.056 state=0.0826 align=0.0000 latA=0.8065 latP=0.1339 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.39 | sec/step~7.80 | keep=0.85 | K=8 | llama(L): tf=8.5277 first=7.5229 kCE=5.5773 KD=2.7948 acc=0.028 state=0.0307 align=0.0000 latA=0.7843 latP=0.1358 | scale_pen(llama)=6.0041e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=0.74 | sec/step~8.03 | keep=0.85 | K=8 | llama(L): tf=8.6678 first=7.1469 kCE=5.2773 KD=2.7634 acc=0.111 state=0.0364 align=0.0000 latA=0.7717 latP=0.1053 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.69 | sec/step~9.00 | keep=0.85 | K=8 | llama(L): tf=7.8741 first=7.2523 kCE=4.7062 KD=2.4585 acc=0.028 state=0.0868 align=0.0000 latA=0.7626 latP=0.1100 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.31 | sec/step~8.78 | keep=0.85 | K=8 | llama(L): tf=8.4115 first=8.2622 kCE=4.9790 KD=2.5850 acc=0.028 state=0.0915 align=0.0000 latA=0.7981 latP=0.1069 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  292/445 | (tail text) | align=0.0002 | text_tf=9.2951 | latent_scale=1.00
  step  300/445 | grad_norm=0.94 | sec/step~8.50 | keep=0.85 | K=8 | llama(L): tf=8.5893 first=8.2518 kCE=5.6948 KD=3.8522 acc=0.028 state=0.0411 align=0.0000 latA=0.7927 latP=0.1381 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.04 | sec/step~8.37 | keep=0.85 | K=8 | llama(L): tf=8.3009 first=7.6063 kCE=5.4966 KD=2.5964 acc=0.028 state=0.0517 align=0.0000 latA=0.8003 latP=0.1238 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  319/445 | (tail text) | align=0.0002 | text_tf=8.9502 | latent_scale=1.00
  step  320/445 | grad_norm=0.56 | sec/step~8.10 | keep=0.85 | K=8 | llama(L): tf=8.4752 first=7.6802 kCE=4.8443 KD=2.8164 acc=0.000 state=0.0310 align=0.0000 latA=0.7900 latP=0.0994 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=1.12 | sec/step~9.08 | keep=0.85 | K=8 | llama(L): tf=8.4902 first=7.1644 kCE=5.9528 KD=3.0702 acc=0.083 state=0.0862 align=0.0000 latA=0.7642 latP=0.1320 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  339/445 | (tail text) | align=0.0002 | text_tf=9.1552 | latent_scale=1.00
  step  340/445 | grad_norm=0.41 | sec/step~8.15 | keep=0.85 | K=8 | llama(L): tf=8.6343 first=7.7693 kCE=5.4583 KD=2.4547 acc=0.111 state=0.0324 align=0.0000 latA=0.7759 latP=0.1146 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  344/445 | (tail text) | align=0.0003 | text_tf=9.3133 | latent_scale=1.00
  step  350/445 | grad_norm=0.39 | sec/step~8.24 | keep=0.85 | K=8 | llama(L): tf=8.3155 first=7.0517 kCE=4.8340 KD=2.6035 acc=0.083 state=0.0677 align=0.0000 latA=0.7719 latP=0.1129 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=0.78 | sec/step~9.50 | keep=0.85 | K=8 | llama(L): tf=8.1833 first=7.0717 kCE=4.9920 KD=2.3840 acc=0.056 state=0.0989 align=0.0000 latA=0.7683 latP=0.1035 | scale_pen(llama)=1.2825e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  363/445 | (tail text) | align=0.0003 | text_tf=9.3511 | latent_scale=1.00
  step  370/445 | grad_norm=0.97 | sec/step~8.08 | keep=0.85 | K=8 | llama(L): tf=8.4583 first=7.5492 kCE=5.1816 KD=2.8916 acc=0.056 state=0.0312 align=0.0000 latA=0.7805 latP=0.1147 | scale_pen(llama)=1.2825e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=0.51 | sec/step~8.43 | keep=0.85 | K=8 | llama(L): tf=8.2122 first=7.3818 kCE=5.2851 KD=3.4938 acc=0.083 state=0.0445 align=0.0000 latA=0.7770 latP=0.1182 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.92 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=7.6986 first=6.8314 kCE=4.5412 KD=2.6035 acc=0.083 state=0.0329 align=0.0000 latA=0.7624 latP=0.1062 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.34 | sec/step~8.01 | keep=0.85 | K=8 | llama(L): tf=8.3149 first=7.5525 kCE=4.6737 KD=2.4685 acc=0.028 state=0.0378 align=0.0000 latA=0.8147 latP=0.1069 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.37 | sec/step~8.30 | keep=0.85 | K=8 | llama(L): tf=8.3554 first=7.5688 kCE=4.6368 KD=2.5213 acc=0.028 state=0.0424 align=0.0000 latA=0.7868 latP=0.0966 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  413/445 | (tail text) | align=0.0003 | text_tf=8.8066 | latent_scale=1.00
  step  420/445 | grad_norm=1.32 | sec/step~8.60 | keep=0.85 | K=8 | llama(L): tf=8.1100 first=7.3874 kCE=5.1860 KD=2.9348 acc=0.056 state=0.0479 align=0.0000 latA=0.7733 latP=0.1142 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=1.28 | sec/step~8.43 | keep=0.85 | K=8 | llama(L): tf=8.2682 first=7.1302 kCE=6.1256 KD=2.3043 acc=0.083 state=0.0359 align=0.0000 latA=0.7503 latP=0.1429 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=0.70 | sec/step~8.05 | keep=0.85 | K=8 | llama(L): tf=8.3052 first=7.7202 kCE=5.4612 KD=3.3811 acc=0.056 state=0.0323 align=0.0000 latA=0.7937 latP=0.1261 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.24 | sec/step~5.32 | keep=0.85 | K=8 | llama(L): tf=8.5541 first=7.2517 kCE=5.0983 KD=2.8253 acc=0.000 state=0.0289 align=0.0000 latA=0.7559 latP=0.1039 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 6675
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/hero_resume/ckpt_stageb
📝 Saved LoRA adapters for Llama
📝 Saved Prefix-Tuning adapters for Llama
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0018956597824669, 'rms_mean_cal': 0.010571175224186106, 'embed_rms': 0.010568971745669842, 'count': 2670}}

=== Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/hero_resume/ckpt_stageb/training_stats.json
Encoder input alignment: mode=neutral_chat | strip_anchor=yes | samples=1000
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2315.38it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
✓ Loaded deep prefix generator for llama
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

— Text baseline summary:
llama: EM=0.585 F1=0.789
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
✓ Loaded LoRA adapters for llama
✓ Loaded Prefix-Tuning adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 1000  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.5 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 65536000 bytes (fp32); fp16 reference: 32768000 bytes; fp32 reference: 65536000 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.585  F1: 0.789  |  NLL/token (gold): 13.715866133954693
Wall clock: 38.37s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 8.394945857265837
       First-token acc: top1=0.044  top5=0.087
Wall clock: 11.63s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Wall clock: 17.57s

— 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 1000,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.507
  },
  "compression": {
    "llama": 3.836046875
  },
  "payload_bytes": 65536000,
  "payload_bytes_detail": {
    "fp32": 65536000,
    "fp16": 32768000,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 1243739
    },
    "prompt_count": 1000,
    "latent_shape": [
      1000,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 65536000,
      "fp16": 32768000
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.585,
      "f1": 0.7894288979645536,
      "nll_token": 13.715866133954693
    },
    "wall_clock_sec": 38.37172794342041
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 8.394945857265837,
      "first_token_top1": 0.044,
      "first_token_top5": 0.087,
      "nll_token": 8.394945857265837
    },
    "wall_clock_sec": 11.625567197799683
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 17.570786714553833
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
