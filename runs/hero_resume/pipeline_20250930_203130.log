=== Resume Hero Stage B Training ===
Run tag: hero_resume
Resuming from checkpoint: runs/hero/ckpt_stageb
Saving to: runs/hero_resume/ckpt_stageb
Remaining epochs: 7

OOM fixes applied:
  - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  - KD_TEACHER_CHUNK=1 (reduced from 2)

Quality improvements:
  - FIRST_TOKEN_CE_WEIGHT_STAGEB=11.0 (increased from 9.0)
  - KD_WEIGHT_STAGEB=0.5 (reduced from 1.0 to let CE dominate)

torch: 2.4.0+cu121 cuda: 12.1 available: False
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage B Resume: Llama prefix training (epochs 4-10) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: train.py [-h] [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
                [--llama_device_map LLAMA_DEVICE_MAP]
                [--qwen_device_map QWEN_DEVICE_MAP] [--require_cuda {yes,no}]
                [--dataset {hotpot,squad,squad_v2}] [--models MODELS]
                [--hotpot_config HOTPOT_CONFIG] [--samples SAMPLES]
                [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                [--grad_accum_steps GRAD_ACCUM_STEPS] [--seed SEED]
                [--data_seed DATA_SEED] [--latent_len LATENT_LEN]
                [--latent_shared_len LATENT_SHARED_LEN]
                [--latent_private_len LATENT_PRIVATE_LEN] [--d_z D_Z]
                [--max_bytes MAX_BYTES] [--encoder_type {byte,simple-st,stq}]
                [--encoder_use_chat_template]
                [--encoder_backbone ENCODER_BACKBONE]
                [--hf_encoder_id HF_ENCODER_ID]
                [--max_enc_tokens MAX_ENC_TOKENS] [--freeze_encoder]
                [--use_chat_template] [--max_answer_tokens MAX_ANSWER_TOKENS]
                [--lr LR] [--scale_l2 SCALE_L2]
                [--adapter_rms_l2 ADAPTER_RMS_L2]
                [--max_grad_norm MAX_GRAD_NORM]
                [--grad_diag_interval GRAD_DIAG_INTERVAL]
                [--grad_diag_components GRAD_DIAG_COMPONENTS]
                [--diagnostic_log DIAGNOSTIC_LOG] [--adapter_freeze_scale]
                [--first_token_ce_weight FIRST_TOKEN_CE_WEIGHT]
                [--first_token_ce_schedule {none,cosine}]
                [--first_token_ce_peak FIRST_TOKEN_CE_PEAK]
                [--first_token_ce_warmup_frac FIRST_TOKEN_CE_WARMUP_FRAC]
                [--first_token_autoscale {yes,no}]
                [--train_append_bos_after_prefix {auto,yes,no}]
                [--adapter_hidden_mult ADAPTER_HIDDEN_MULT]
                [--adapter_dropout ADAPTER_DROPOUT] [--adapter_colorize]
                [--no_adapter_metadata]
                [--manifold_stat_weight MANIFOLD_STAT_WEIGHT]
                [--state_kd_weight STATE_KD_WEIGHT]
                [--state_kd_layers STATE_KD_LAYERS] [--use_gist_head]
                [--gist_target_len GIST_TARGET_LEN]
                [--gist_hidden GIST_HIDDEN] [--gist_layers GIST_LAYERS]
                [--gist_dropout GIST_DROPOUT] [--gist_weight GIST_WEIGHT]
                [--gist_mask_prob GIST_MASK_PROB] [--use_lora]
                [--lora_r LORA_R] [--lora_alpha LORA_ALPHA]
                [--lora_dropout LORA_DROPOUT] [--lora_firstN LORA_FIRSTN]
                [--lora_target_modules LORA_TARGET_MODULES] [--use_prefix]
                [--prefix_tokens PREFIX_TOKENS] [--prefix_projection]
                [--peft_prefix_all_layers PEFT_PREFIX_ALL_LAYERS]
                [--use_deep_prefix] [--deep_prefix_len DEEP_PREFIX_LEN]
                [--deep_prefix_dropout DEEP_PREFIX_DROPOUT] [--K K]
                [--adaptive_k_start ADAPTIVE_K_START]
                [--adaptive_k_end ADAPTIVE_K_END]
                [--latent_keep_start LATENT_KEEP_START]
                [--latent_keep_end LATENT_KEEP_END]
                [--latent_keep_power LATENT_KEEP_POWER]
                [--warmup_text_latent_steps WARMUP_TEXT_LATENT_STEPS]
                [--warmup_text_latent_epochs WARMUP_TEXT_LATENT_EPOCHS]
                [--warmup_align_tokens WARMUP_ALIGN_TOKENS]
                [--warmup_align_weight WARMUP_ALIGN_WEIGHT]
                [--warmup_text_teacher_weight WARMUP_TEXT_TEACHER_WEIGHT]
                [--warmup_text_latent_weight WARMUP_TEXT_LATENT_WEIGHT]
                [--warmup_text_latent_weight_end WARMUP_TEXT_LATENT_WEIGHT_END]
                [--warmup_tail_prob WARMUP_TAIL_PROB]
                [--latent_align_weight LATENT_ALIGN_WEIGHT]
                [--latent_prefix_align_weight LATENT_PREFIX_ALIGN_WEIGHT]
                [--latent_align_metric {mse,cosine,both}]
                [--k_ce_weight K_CE_WEIGHT]
                [--kd_first_k_weight KD_FIRST_K_WEIGHT] [--kd_tau KD_TAU]
                [--teacher_llama_id TEACHER_LLAMA_ID]
                [--teacher_qwen_id TEACHER_QWEN_ID] [--kd_skip_text]
                [--latent_refiner_layers LATENT_REFINER_LAYERS]
                [--latent_refiner_heads LATENT_REFINER_HEADS] [--load_4bit]
                [--sequential_models] [--llama_devices LLAMA_DEVICES]
                [--qwen_devices QWEN_DEVICES] [--gpu_mem_gib GPU_MEM_GIB]
                [--grad_ckpt] [--fp16_mps]
                [--warm_anchor_text WARM_ANCHOR_TEXT]
                [--warm_anchor_mode {auto,text,chat,none}]
                [--max_anchor_tokens MAX_ANCHOR_TOKENS] [--debug]
                [--save_dir SAVE_DIR] [--save_every SAVE_EVERY]
                [--resume_from RESUME_FROM] [--auto_resume]
                [--no_load_optimizer] [--no_load_lr_scheduler] [--reset_epoch]
                [--save_training_stats]
train.py: error: unrecognized arguments: --refiner_layers 2 --refiner_heads 4
