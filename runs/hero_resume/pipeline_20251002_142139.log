=== Resume Hero Stage B Training (Schedule Fix) ===
Run tag: hero_resume
Checkpoint directory: runs/hero_resume/ckpt_stageb (using --auto_resume)
Epochs: 8 (extended for consolidation)

SCHEDULE FIXES (based on v1 analysis):
  - LATENT_KEEP_END: 1.0 â†’ 0.85 (freeze at sweet spot)
  - EPOCHS extended to 8 (consolidate with frozen dropout)
  - Peak checkpointing: train.py saves '_best' automatically

Analysis showed peak at keep_prob=0.6-0.85:
  - Peak: 19.4% first_acc at keep_prob=0.613
  - 26 steps achieved â‰¥10% (all at keep_prob 0.55-0.82)
  - Final eval (keep_prob=1.0) only 4.4% - model never learned full latents

Retained from v1:
  - FIRST_TOKEN_CE_WEIGHT=11.0
  - KD_WEIGHT=0.5
  - OOM fixes: expandable_segments, KD_TEACHER_CHUNK=1
  - Performance: TEXT_TEACHER_CHUNK=4

torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage B: Training with frozen dropout (keep_probâ†’0.85) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3656.76it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.08s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
trainable params: 272,801,792 || all params: 8,345,006,080 || trainable%: 3.2690
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
âª Resuming from: runs/hero_resume/ckpt_stageb/state.pt
   -> loaded encoder/adapters/deep_prefix/refiner FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=7, global_step=4005
[warmup] alternating text/latent for first 890 steps
Epoch 8/8
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  step  10/445 | grad_norm=0.71 | sec/step~7.55 | keep=0.77 | K=8 | llama(L): tf=13.0197 first=14.8538 kCE=13.0713 KD=2.5748 acc=0.028 state=40.4132 align=0.0000 latA=0.7987 latP=0.1072 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | (tail text) | align=0.0003 | text_tf=10.7847 | latent_scale=1.00
  step  20/445 | grad_norm=1.02 | sec/step~8.42 | keep=0.77 | K=8 | llama(T): tf=12.6231 first=13.3439 kCE=11.8747 KD=0.0000 acc=0.000 state=41.5378 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.68 | sec/step~8.30 | keep=0.77 | K=8 | llama(L): tf=10.7671 first=9.8817 kCE=12.6871 KD=2.6086 acc=0.000 state=42.2159 align=0.0000 latA=0.7813 latP=0.1284 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.31 | sec/step~7.51 | keep=0.77 | K=8 | llama(L): tf=10.7850 first=9.4911 kCE=13.2584 KD=2.5433 acc=0.000 state=33.5621 align=0.0000 latA=0.7995 latP=0.1175 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.24 | sec/step~7.50 | keep=0.78 | K=8 | llama(L): tf=13.4697 first=10.8806 kCE=14.7364 KD=3.0796 acc=0.000 state=20.4118 align=0.0000 latA=0.7745 latP=0.1258 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=1.21 | sec/step~7.77 | keep=0.78 | K=8 | llama(L): tf=13.7642 first=11.0207 kCE=14.7553 KD=4.1078 acc=0.000 state=20.4504 align=0.0000 latA=0.7633 latP=0.1507 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=1.23 | sec/step~8.31 | keep=0.78 | K=8 | llama(L): tf=12.8013 first=9.9093 kCE=16.0562 KD=2.7092 acc=0.000 state=12.4441 align=0.0000 latA=0.7869 latP=0.1127 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=1.06 | sec/step~8.49 | keep=0.78 | K=8 | llama(L): tf=14.8729 first=9.1170 kCE=13.0251 KD=2.7913 acc=0.000 state=57.1459 align=0.0000 latA=0.7993 latP=0.1448 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.73 | sec/step~9.12 | keep=0.78 | K=8 | llama(L): tf=19.7880 first=12.8582 kCE=13.3455 KD=2.8105 acc=0.000 state=15.9910 align=0.0000 latA=0.7979 latP=0.1278 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=2.16 | sec/step~7.29 | keep=0.79 | K=8 | llama(L): tf=15.8310 first=11.7145 kCE=12.2733 KD=2.6031 acc=0.000 state=36.0678 align=0.0000 latA=0.7762 latP=0.1144 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  101/445 | (tail text) | align=0.0003 | text_tf=23.1356 | latent_scale=1.00
  step  110/445 | grad_norm=0.55 | sec/step~8.14 | keep=0.79 | K=8 | llama(L): tf=14.3970 first=11.2248 kCE=12.3851 KD=3.6545 acc=0.000 state=33.2185 align=0.0000 latA=0.7819 latP=0.1320 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.17 | sec/step~7.51 | keep=0.79 | K=8 | llama(L): tf=14.5142 first=11.4120 kCE=12.3466 KD=3.5382 acc=0.000 state=34.5833 align=0.0000 latA=0.7925 latP=0.1194 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  125/445 | (tail text) | align=0.0003 | text_tf=16.6547 | latent_scale=1.00
  step  127/445 | (tail text) | align=0.0002 | text_tf=16.6369 | latent_scale=1.00
  step  130/445 | grad_norm=0.98 | sec/step~7.98 | keep=0.79 | K=8 | llama(L): tf=14.1875 first=10.9992 kCE=12.0090 KD=2.6378 acc=0.000 state=46.4704 align=0.0000 latA=0.7600 latP=0.1348 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=0.59 | sec/step~8.57 | keep=0.79 | K=8 | llama(L): tf=13.9987 first=11.2568 kCE=12.4918 KD=2.2910 acc=0.000 state=51.6067 align=0.0000 latA=0.7610 latP=0.1355 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=1.22 | sec/step~8.78 | keep=0.79 | K=8 | llama(L): tf=13.1767 first=11.2026 kCE=12.7841 KD=3.1811 acc=0.000 state=51.9518 align=0.0000 latA=0.8225 latP=0.1084 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  160/445 | grad_norm=0.38 | sec/step~7.70 | keep=0.80 | K=8 | llama(L): tf=14.2703 first=10.9928 kCE=13.5890 KD=2.5155 acc=0.000 state=77.0159 align=0.0000 latA=0.7646 latP=0.1333 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.51 | sec/step~8.92 | keep=0.80 | K=8 | llama(L): tf=13.6132 first=10.9374 kCE=14.0410 KD=2.7247 acc=0.000 state=54.1747 align=0.0000 latA=0.7787 latP=0.1175 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=1.31 | sec/step~8.40 | keep=0.80 | K=8 | llama(L): tf=13.2145 first=10.8391 kCE=13.7979 KD=4.5144 acc=0.000 state=62.5556 align=0.0000 latA=0.7831 latP=0.1223 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=1.80 | sec/step~7.72 | keep=0.80 | K=8 | llama(L): tf=13.5684 first=10.5316 kCE=14.3172 KD=2.7099 acc=0.000 state=65.2095 align=0.0000 latA=0.7860 latP=0.1170 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  200/445 | grad_norm=1.45 | sec/step~7.87 | keep=0.80 | K=8 | llama(L): tf=13.7164 first=10.9467 kCE=14.7044 KD=3.6315 acc=0.000 state=66.1390 align=0.0000 latA=0.7931 latP=0.1324 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=0.73 | sec/step~8.69 | keep=0.81 | K=8 | llama(L): tf=13.8566 first=10.1643 kCE=15.0082 KD=2.9201 acc=0.000 state=57.4493 align=0.0000 latA=0.7578 latP=0.1239 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=0.30 | sec/step~7.66 | keep=0.81 | K=8 | llama(L): tf=13.8138 first=9.5618 kCE=15.0687 KD=2.7663 acc=0.000 state=58.2435 align=0.0000 latA=0.7668 latP=0.1452 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.35 | sec/step~7.89 | keep=0.81 | K=8 | llama(L): tf=13.7320 first=10.4783 kCE=15.5617 KD=2.7244 acc=0.000 state=70.5376 align=0.0000 latA=0.8083 latP=0.1004 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=2.30 | sec/step~7.51 | keep=0.81 | K=8 | llama(L): tf=15.1759 first=10.1148 kCE=15.6096 KD=2.8108 acc=0.000 state=68.9731 align=0.0000 latA=0.8093 latP=0.1286 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=0.95 | sec/step~7.69 | keep=0.81 | K=8 | llama(L): tf=13.7905 first=9.9168 kCE=15.2043 KD=2.8948 acc=0.000 state=73.2411 align=0.0000 latA=0.7926 latP=0.1245 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.07 | sec/step~7.64 | keep=0.81 | K=8 | llama(L): tf=13.8619 first=9.4385 kCE=15.1462 KD=2.6883 acc=0.000 state=65.8891 align=0.0000 latA=0.7918 latP=0.1165 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | (tail text) | align=0.0002 | text_tf=13.8736 | latent_scale=1.00
  step  270/445 | grad_norm=0.61 | sec/step~6.23 | keep=0.82 | K=8 | llama(T): tf=13.2526 first=8.0648 kCE=14.6540 KD=0.0000 acc=0.083 state=53.0294 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.66 | sec/step~8.83 | keep=0.82 | K=8 | llama(L): tf=13.8007 first=8.8164 kCE=14.3303 KD=3.0367 acc=0.028 state=65.0486 align=0.0000 latA=0.7887 latP=0.1381 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.33 | sec/step~7.43 | keep=0.82 | K=8 | llama(L): tf=13.8421 first=8.3023 kCE=14.2775 KD=2.7609 acc=0.028 state=83.7354 align=0.0000 latA=0.7674 latP=0.1204 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  291/445 | (tail text) | align=0.0002 | text_tf=15.6939 | latent_scale=1.00
  step  298/445 | (tail text) | align=0.0003 | text_tf=15.3940 | latent_scale=1.00
  step  300/445 | grad_norm=1.14 | sec/step~7.40 | keep=0.82 | K=8 | llama(L): tf=14.7580 first=8.6489 kCE=14.3965 KD=2.5056 acc=0.000 state=59.8557 align=0.0000 latA=0.7690 latP=0.1445 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.54 | sec/step~7.67 | keep=0.82 | K=8 | llama(L): tf=13.7106 first=8.5093 kCE=14.1409 KD=3.3025 acc=0.028 state=92.4510 align=0.0000 latA=0.7722 latP=0.1287 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc=11.1% at step 4321 â†’ saved to runs/hero_resume/ckpt_stageb_best
  ðŸŒŸ NEW PEAK: first_acc=13.9% at step 4322 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  320/445 | grad_norm=0.68 | sec/step~7.41 | keep=0.83 | K=8 | llama(L): tf=13.7049 first=8.3788 kCE=14.5521 KD=3.0058 acc=0.083 state=88.9924 align=0.0000 latA=0.7910 latP=0.1270 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=0.53 | sec/step~8.41 | keep=0.83 | K=8 | llama(L): tf=13.7521 first=8.1495 kCE=14.7151 KD=2.6464 acc=0.111 state=94.8007 align=0.0000 latA=0.7691 latP=0.1303 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=0.47 | sec/step~8.10 | keep=0.83 | K=8 | llama(L): tf=13.2311 first=7.4119 kCE=14.7148 KD=3.6280 acc=0.083 state=84.3038 align=0.0000 latA=0.7423 latP=0.1199 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.51 | sec/step~8.09 | keep=0.83 | K=8 | llama(L): tf=12.4748 first=8.1818 kCE=14.2469 KD=4.7649 acc=0.056 state=94.7107 align=0.0000 latA=0.7815 latP=0.1289 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=1.88 | sec/step~9.10 | keep=0.83 | K=8 | llama(L): tf=13.5652 first=8.0031 kCE=14.9121 KD=3.0194 acc=0.083 state=82.7139 align=0.0000 latA=0.7749 latP=0.1158 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  369/445 | (tail text) | align=0.0003 | text_tf=17.2400 | latent_scale=1.00
  step  370/445 | grad_norm=1.61 | sec/step~8.44 | keep=0.84 | K=8 | llama(L): tf=13.4139 first=7.2785 kCE=14.6455 KD=2.8920 acc=0.111 state=99.7393 align=0.0000 latA=0.7420 latP=0.1460 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=1.25 | sec/step~7.65 | keep=0.84 | K=8 | llama(L): tf=13.5039 first=7.6205 kCE=14.9503 KD=2.9454 acc=0.056 state=94.9676 align=0.0000 latA=0.7606 latP=0.1110 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.94 | sec/step~7.92 | keep=0.84 | K=8 | llama(L): tf=12.8227 first=7.5621 kCE=14.7691 KD=2.7147 acc=0.000 state=48.5386 align=0.0000 latA=0.7768 latP=0.1113 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  395/445 | (tail text) | align=0.0002 | text_tf=15.1855 | latent_scale=1.00
  step  400/445 | grad_norm=0.57 | sec/step~7.70 | keep=0.84 | K=8 | llama(L): tf=13.6904 first=8.0455 kCE=14.7787 KD=2.5224 acc=0.056 state=89.6590 align=0.0000 latA=0.7732 latP=0.1234 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.40 | sec/step~8.12 | keep=0.84 | K=8 | llama(L): tf=14.5372 first=7.7197 kCE=15.1647 KD=2.7995 acc=0.056 state=82.8324 align=0.0000 latA=0.7926 latP=0.1475 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=1.73 | sec/step~7.89 | keep=0.85 | K=8 | llama(L): tf=12.8183 first=7.6644 kCE=14.7492 KD=2.5869 acc=0.083 state=52.3316 align=0.0000 latA=0.7907 latP=0.1134 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  428/445 | (tail text) | align=0.0002 | text_tf=14.4223 | latent_scale=1.00
  step  430/445 | grad_norm=0.91 | sec/step~7.99 | keep=0.85 | K=8 | llama(L): tf=12.8789 first=7.6035 kCE=14.6290 KD=2.2468 acc=0.000 state=34.3336 align=0.0000 latA=0.7802 latP=0.1247 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  440/445 | grad_norm=0.78 | sec/step~8.20 | keep=0.85 | K=8 | llama(L): tf=14.4934 first=8.1233 kCE=15.3709 KD=2.8309 acc=0.083 state=73.5168 align=0.0000 latA=0.7937 latP=0.1323 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.62 | sec/step~8.29 | keep=0.85 | K=8 | llama(L): tf=12.6949 first=6.1224 kCE=15.1069 KD=4.0406 acc=0.188 state=91.0553 align=0.0000 latA=0.7107 latP=0.1076 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc=18.8% at step 4450 â†’ saved to runs/hero_resume/ckpt_stageb_best
[checkpoint] Freed 3.6KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  âœ… Saved (and pruned to) latest at step 4450
Epoch 9/8
  step  10/445 | grad_norm=0.77 | sec/step~8.28 | keep=0.85 | K=8 | llama(L): tf=13.6225 first=7.7204 kCE=15.3326 KD=2.5443 acc=0.083 state=94.4517 align=0.0000 latA=0.7745 latP=0.1141 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=1.49 | sec/step~8.20 | keep=0.85 | K=8 | llama(L): tf=13.0596 first=6.8341 kCE=15.0731 KD=2.6637 acc=0.056 state=71.4405 align=0.0000 latA=0.7689 latP=0.1178 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc=22.2% at step 4477 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  30/445 | grad_norm=0.93 | sec/step~8.12 | keep=0.85 | K=8 | llama(L): tf=12.5510 first=7.8344 kCE=14.7673 KD=2.8604 acc=0.000 state=75.2331 align=0.0000 latA=0.7821 latP=0.1100 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  37/445 | (tail text) | align=0.0003 | text_tf=15.4717 | latent_scale=1.00
  step  40/445 | grad_norm=0.52 | sec/step~8.22 | keep=0.85 | K=8 | llama(L): tf=14.1769 first=7.4769 kCE=15.1757 KD=2.8215 acc=0.083 state=71.7680 align=0.0000 latA=0.7710 latP=0.1196 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.45 | sec/step~7.90 | keep=0.85 | K=8 | llama(L): tf=14.3296 first=7.5925 kCE=14.9588 KD=2.7692 acc=0.056 state=72.5204 align=0.0000 latA=0.7820 latP=0.1275 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=1.10 | sec/step~8.32 | keep=0.85 | K=8 | llama(L): tf=13.3692 first=7.4766 kCE=14.4556 KD=3.2819 acc=0.083 state=78.2733 align=0.0000 latA=0.7736 latP=0.1224 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  70/445 | grad_norm=2.21 | sec/step~8.71 | keep=0.85 | K=8 | llama(L): tf=14.1796 first=7.6014 kCE=14.8899 KD=2.6673 acc=0.083 state=87.9609 align=0.0000 latA=0.7823 latP=0.1326 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  74/445 | (tail text) | align=0.0003 | text_tf=14.2123 | latent_scale=1.00
  step  80/445 | grad_norm=0.97 | sec/step~7.87 | keep=0.85 | K=8 | llama(L): tf=14.2059 first=7.6848 kCE=14.5305 KD=2.5807 acc=0.028 state=77.6405 align=0.0000 latA=0.7624 latP=0.1189 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=1.71 | sec/step~8.04 | keep=0.85 | K=8 | llama(L): tf=14.0915 first=7.2824 kCE=14.7482 KD=2.9323 acc=0.167 state=72.2808 align=0.0000 latA=0.7529 latP=0.1305 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=1.02 | sec/step~7.53 | keep=0.85 | K=8 | llama(L): tf=13.0279 first=8.1715 kCE=14.1278 KD=2.5339 acc=0.028 state=98.9251 align=0.0000 latA=0.8061 latP=0.1218 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  ðŸŒŸ NEW PEAK: first_acc=25.0% at step 4558 â†’ saved to runs/hero_resume/ckpt_stageb_best
  step  110/445 | grad_norm=0.32 | sec/step~8.00 | keep=0.85 | K=8 | llama(L): tf=13.7403 first=7.3730 kCE=14.2680 KD=2.6465 acc=0.028 state=70.2577 align=0.0000 latA=0.7805 latP=0.1389 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=2.16 | sec/step~7.39 | keep=0.85 | K=8 | llama(L): tf=14.0130 first=7.5765 kCE=14.1026 KD=2.7194 acc=0.028 state=69.8889 align=0.0000 latA=0.7881 latP=0.1533 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=1.08 | sec/step~8.35 | keep=0.85 | K=8 | llama(L): tf=13.5749 first=7.1911 kCE=14.2802 KD=2.6729 acc=0.056 state=84.5116 align=0.0000 latA=0.7491 latP=0.1278 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  140/445 | grad_norm=1.66 | sec/step~7.62 | keep=0.85 | K=8 | llama(L): tf=13.5968 first=7.2072 kCE=13.8944 KD=2.5602 acc=0.028 state=34.0061 align=0.0000 latA=0.7572 latP=0.1401 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  150/445 | grad_norm=1.35 | sec/step~8.25 | keep=0.85 | K=8 | llama(L): tf=13.7819 first=7.5019 kCE=14.1964 KD=2.9937 acc=0.056 state=70.1955 align=0.0000 latA=0.7768 latP=0.1348 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  154/445 | (tail text) | align=0.0003 | text_tf=15.3490 | latent_scale=1.00
  step  160/445 | grad_norm=0.59 | sec/step~7.51 | keep=0.85 | K=8 | llama(L): tf=13.5175 first=7.3804 kCE=14.1640 KD=3.6610 acc=0.028 state=74.5477 align=0.0000 latA=0.7732 latP=0.1317 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  170/445 | grad_norm=0.47 | sec/step~7.62 | keep=0.85 | K=8 | llama(L): tf=15.0867 first=7.9115 kCE=14.5437 KD=2.6184 acc=0.028 state=34.3779 align=0.0000 latA=0.7878 latP=0.1472 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  180/445 | grad_norm=2.61 | sec/step~8.87 | keep=0.85 | K=8 | llama(L): tf=12.8866 first=7.4836 kCE=13.9646 KD=3.3207 acc=0.028 state=60.4951 align=0.0000 latA=0.8050 latP=0.1223 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  190/445 | grad_norm=0.95 | sec/step~7.86 | keep=0.85 | K=8 | llama(L): tf=13.2773 first=7.2471 kCE=14.1813 KD=3.8265 acc=0.083 state=51.7745 align=0.0000 latA=0.7748 latP=0.1150 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  193/445 | (tail text) | align=0.0002 | text_tf=16.3369 | latent_scale=1.00
  step  200/445 | grad_norm=1.06 | sec/step~8.15 | keep=0.85 | K=8 | llama(L): tf=13.2171 first=7.6498 kCE=14.1029 KD=3.3295 acc=0.028 state=62.3318 align=0.0000 latA=0.7827 latP=0.1162 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  210/445 | grad_norm=0.52 | sec/step~8.32 | keep=0.85 | K=8 | llama(L): tf=13.2754 first=7.6509 kCE=13.7961 KD=2.4134 acc=0.056 state=26.5446 align=0.0000 latA=0.7959 latP=0.1458 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  220/445 | grad_norm=1.06 | sec/step~8.14 | keep=0.85 | K=8 | llama(L): tf=13.9273 first=6.9065 kCE=14.1176 KD=2.4722 acc=0.083 state=61.2954 align=0.0000 latA=0.7538 latP=0.1292 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  230/445 | grad_norm=0.77 | sec/step~7.72 | keep=0.85 | K=8 | llama(L): tf=12.1338 first=6.8324 kCE=13.7149 KD=3.5069 acc=0.028 state=77.7122 align=0.0000 latA=0.7523 latP=0.1168 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | (tail text) | align=0.0002 | text_tf=14.9539 | latent_scale=1.00
  step  240/445 | grad_norm=2.29 | sec/step~7.96 | keep=0.85 | K=8 | llama(T): tf=12.4273 first=6.6840 kCE=13.8351 KD=0.0000 acc=0.083 state=61.3999 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  250/445 | grad_norm=0.90 | sec/step~9.77 | keep=0.85 | K=8 | llama(L): tf=14.2674 first=7.1972 kCE=14.4784 KD=3.3617 acc=0.056 state=75.1066 align=0.0000 latA=0.7801 latP=0.1630 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=1.40 | sec/step~9.09 | keep=0.85 | K=8 | llama(L): tf=12.9192 first=7.3313 kCE=13.9153 KD=2.7811 acc=0.000 state=42.6810 align=0.0000 latA=0.7761 latP=0.1205 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=0.72 | sec/step~8.35 | keep=0.85 | K=8 | llama(L): tf=14.3405 first=8.1709 kCE=14.8251 KD=2.7251 acc=0.000 state=63.5986 align=0.0000 latA=0.8075 latP=0.1471 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=0.76 | sec/step~7.66 | keep=0.85 | K=8 | llama(L): tf=13.1845 first=7.4156 kCE=14.3337 KD=2.4755 acc=0.028 state=40.0673 align=0.0000 latA=0.7671 latP=0.1242 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=0.30 | sec/step~7.35 | keep=0.85 | K=8 | llama(L): tf=12.9501 first=6.9927 kCE=14.2467 KD=3.2138 acc=0.111 state=30.9301 align=0.0000 latA=0.7675 latP=0.1244 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  300/445 | grad_norm=1.47 | sec/step~7.60 | keep=0.85 | K=8 | llama(L): tf=12.9386 first=7.8937 kCE=14.1676 KD=2.9357 acc=0.056 state=32.8127 align=0.0000 latA=0.7836 latP=0.1222 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=1.91 | sec/step~8.06 | keep=0.85 | K=8 | llama(L): tf=13.4539 first=7.7436 kCE=14.0246 KD=3.2589 acc=0.028 state=55.7110 align=0.0000 latA=0.7865 latP=0.1268 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=1.27 | sec/step~7.88 | keep=0.85 | K=8 | llama(L): tf=12.5041 first=7.5048 kCE=13.6017 KD=2.3667 acc=0.083 state=61.1865 align=0.0000 latA=0.7838 latP=0.1145 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  330/445 | grad_norm=1.22 | sec/step~8.06 | keep=0.85 | K=8 | llama(L): tf=13.3945 first=7.3266 kCE=13.6809 KD=3.2351 acc=0.056 state=64.6258 align=0.0000 latA=0.7723 latP=0.1443 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=0.71 | sec/step~7.97 | keep=0.85 | K=8 | llama(L): tf=12.2268 first=7.4050 kCE=13.4110 KD=2.5253 acc=0.056 state=35.3470 align=0.0000 latA=0.7727 latP=0.1138 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  350/445 | grad_norm=0.40 | sec/step~7.81 | keep=0.85 | K=8 | llama(L): tf=12.1106 first=6.8952 kCE=13.0979 KD=2.8866 acc=0.111 state=58.6207 align=0.0000 latA=0.7421 latP=0.1323 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=2.80 | sec/step~8.73 | keep=0.85 | K=8 | llama(L): tf=12.1286 first=7.3448 kCE=13.4019 KD=2.4566 acc=0.000 state=29.6130 align=0.0000 latA=0.7787 latP=0.1114 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=1.20 | sec/step~8.29 | keep=0.85 | K=8 | llama(L): tf=12.8733 first=7.5589 kCE=13.2034 KD=2.6996 acc=0.056 state=53.3013 align=0.0000 latA=0.7802 latP=0.1504 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=2.01 | sec/step~7.97 | keep=0.85 | K=8 | llama(L): tf=12.6732 first=7.3571 kCE=13.5770 KD=4.2966 acc=0.083 state=33.5891 align=0.0000 latA=0.7714 latP=0.1077 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=0.37 | sec/step~7.85 | keep=0.85 | K=8 | llama(L): tf=12.6024 first=7.3751 kCE=13.2490 KD=2.7407 acc=0.056 state=49.6295 align=0.0000 latA=0.7686 latP=0.1303 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=0.53 | sec/step~8.35 | keep=0.85 | K=8 | llama(L): tf=13.0552 first=6.7799 kCE=13.3843 KD=2.3613 acc=0.111 state=57.3616 align=0.0000 latA=0.7361 latP=0.1498 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.42 | sec/step~7.93 | keep=0.85 | K=8 | llama(L): tf=12.1297 first=7.4598 kCE=13.0636 KD=2.7806 acc=0.056 state=21.1850 align=0.0000 latA=0.7838 latP=0.1134 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=1.84 | sec/step~8.55 | keep=0.85 | K=8 | llama(L): tf=12.1085 first=7.1742 kCE=12.8862 KD=4.6432 acc=0.083 state=52.4523 align=0.0000 latA=0.7751 latP=0.1262 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=1.56 | sec/step~8.19 | keep=0.85 | K=8 | llama(L): tf=12.4911 first=7.4975 kCE=12.7510 KD=2.4197 acc=0.056 state=30.3014 align=0.0000 latA=0.7753 latP=0.1399 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  438/445 | (tail text) | align=0.0003 | text_tf=12.5945 | latent_scale=1.00
  step  440/445 | grad_norm=1.76 | sec/step~7.84 | keep=0.85 | K=8 | llama(L): tf=12.6731 first=7.3331 kCE=12.8494 KD=2.8829 acc=0.028 state=41.4953 align=0.0000 latA=0.7818 latP=0.1173 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0019 rms_cal~0.0106 embed_rms~0.01057]
  step  445/445 | grad_norm=0.13 | sec/step~4.88 | keep=0.85 | K=8 | llama(L): tf=12.8456 first=7.2226 kCE=12.5339 KD=2.4260 acc=0.062 state=51.2507 align=0.0000 latA=0.7453 latP=0.1518 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  âœ… Saved (and pruned to) latest at step 4895
Epoch 10/8
  step  10/445 | grad_norm=1.52 | sec/step~7.86 | keep=0.85 | K=8 | llama(L): tf=11.8960 first=6.6065 kCE=12.6333 KD=2.6374 acc=0.083 state=23.1564 align=0.0000 latA=0.7283 latP=0.1183 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  20/445 | grad_norm=1.85 | sec/step~7.61 | keep=0.85 | K=8 | llama(L): tf=13.1349 first=7.4872 kCE=12.8150 KD=3.7991 acc=0.111 state=22.8834 align=0.0000 latA=0.7774 latP=0.1259 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  30/445 | grad_norm=0.72 | sec/step~8.48 | keep=0.85 | K=8 | llama(L): tf=12.1578 first=7.5496 kCE=12.5231 KD=3.3673 acc=0.028 state=47.9826 align=0.0000 latA=0.7755 latP=0.1081 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  40/445 | grad_norm=0.67 | sec/step~7.65 | keep=0.85 | K=8 | llama(L): tf=11.8280 first=6.9539 kCE=12.5983 KD=2.7537 acc=0.083 state=50.7373 align=0.0000 latA=0.7650 latP=0.1119 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  50/445 | grad_norm=0.21 | sec/step~7.56 | keep=0.85 | K=8 | llama(L): tf=12.6167 first=7.5445 kCE=12.6804 KD=2.6693 acc=0.028 state=45.6607 align=0.0000 latA=0.7897 latP=0.1417 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  60/445 | grad_norm=0.61 | sec/step~7.79 | keep=0.85 | K=8 | llama(L): tf=12.4550 first=6.7311 kCE=12.6395 KD=2.9880 acc=0.139 state=35.3864 align=0.0000 latA=0.7457 latP=0.1293 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  67/445 | (tail text) | align=0.0003 | text_tf=12.8959 | latent_scale=1.00
  step  70/445 | grad_norm=0.97 | sec/step~7.57 | keep=0.85 | K=8 | llama(L): tf=12.0603 first=7.8753 kCE=12.4680 KD=3.9779 acc=0.000 state=34.6928 align=0.0000 latA=0.8007 latP=0.1270 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  80/445 | grad_norm=1.31 | sec/step~8.57 | keep=0.85 | K=8 | llama(L): tf=11.9521 first=7.3841 kCE=12.6335 KD=2.8040 acc=0.056 state=15.0800 align=0.0000 latA=0.7757 latP=0.1113 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  90/445 | grad_norm=0.69 | sec/step~8.41 | keep=0.85 | K=8 | llama(L): tf=12.5764 first=7.2091 kCE=12.3755 KD=2.5794 acc=0.056 state=17.6600 align=0.0000 latA=0.7648 latP=0.1372 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  100/445 | grad_norm=0.49 | sec/step~7.90 | keep=0.85 | K=8 | llama(L): tf=11.2399 first=7.0543 kCE=12.0569 KD=2.7350 acc=0.028 state=26.9722 align=0.0000 latA=0.7671 latP=0.1138 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  110/445 | grad_norm=0.19 | sec/step~7.50 | keep=0.85 | K=8 | llama(L): tf=11.9483 first=6.7812 kCE=12.1810 KD=3.0233 acc=0.056 state=31.6192 align=0.0000 latA=0.7547 latP=0.1309 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  120/445 | grad_norm=1.28 | sec/step~8.52 | keep=0.85 | K=8 | llama(L): tf=11.8839 first=7.4167 kCE=12.2300 KD=2.3719 acc=0.083 state=38.4575 align=0.0000 latA=0.7657 latP=0.1166 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  130/445 | grad_norm=1.06 | sec/step~7.99 | keep=0.85 | K=8 | llama(L): tf=11.4947 first=7.8513 kCE=12.1061 KD=2.5640 acc=0.028 state=48.5953 align=0.0000 latA=0.7767 latP=0.1099 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  137/445 | (tail text) | align=0.0003 | text_tf=12.4924 | latent_scale=1.00
  step  140/445 | grad_norm=1.87 | sec/step~7.92 | keep=0.85 | K=8 | llama(L): tf=11.4671 first=7.4616 kCE=12.1234 KD=2.4770 acc=0.028 state=45.1192 align=0.0000 latA=0.7766 latP=0.1214 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0018 rms_cal~0.0106 embed_rms~0.01057]
  step  145/445 | (tail text) | align=0.0003 | text_tf=12.9881 | latent_scale=1.00
