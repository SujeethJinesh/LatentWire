
=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1948.80it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:09<00:27,  9.31s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:17<00:17,  8.60s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:25<00:08,  8.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:27<00:00,  5.79s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:27<00:00,  6.84s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
‚ö†Ô∏è  No valid checkpoint found to resume; starting fresh.
Epoch 1/4
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/40 | grad_norm=325.32 | sec/step~1.87 | keep=0.70 | K=4 | first_w=2.00 | llama(L): tf=11.6341 first=18.8435 kCE=9.9332 KD=7.2675 state=26.1357 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=117.73 | sec/step~2.10 | keep=0.70 | K=4 | first_w=2.00 | llama(L): tf=10.7975 first=15.5171 kCE=8.8687 KD=9.1148 state=26.6979 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=437.82 | sec/step~2.18 | keep=0.71 | K=4 | first_w=2.00 | llama(L): tf=10.4144 first=15.1674 kCE=7.9785 KD=8.0263 state=26.6354 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=143.81 | sec/step~2.08 | keep=0.72 | K=4 | first_w=2.00 | llama(L): tf=10.4090 first=11.6861 kCE=8.6599 KD=9.4586 state=25.5762 align=0.0000 | scale_pen(llama)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 2/4
  step  10/40 | grad_norm=52.98 | sec/step~2.09 | keep=0.73 | K=4 | first_w=2.00 | llama(L): tf=9.9474 first=10.5256 kCE=8.7152 KD=9.3534 state=25.3241 align=0.0000 | scale_pen(llama)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=17.95 | sec/step~2.71 | keep=0.74 | K=4 | first_w=1.98 | llama(L): tf=10.0756 first=9.8903 kCE=9.3824 KD=8.4864 state=26.7609 align=0.0000 | scale_pen(llama)=6.2670e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=50.83 | sec/step~2.33 | keep=0.76 | K=4 | first_w=1.92 | llama(L): tf=10.2078 first=9.3517 kCE=9.3220 KD=9.2647 state=25.7609 align=0.0000 | scale_pen(llama)=6.2670e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=24.38 | sec/step~1.81 | keep=0.77 | K=4 | first_w=1.82 | llama(L): tf=10.0049 first=9.5066 kCE=9.5084 KD=9.6542 state=23.2000 align=0.0000 | scale_pen(llama)=8.1855e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 3/4
  step  10/40 | grad_norm=28.36 | sec/step~2.11 | keep=0.79 | K=4 | first_w=1.70 | llama(L): tf=9.8564 first=8.8944 kCE=8.8508 KD=9.3330 state=24.3847 align=0.0000 | scale_pen(llama)=8.1855e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=9.17 | sec/step~2.13 | keep=0.82 | K=4 | first_w=1.57 | llama(L): tf=9.7145 first=8.4150 kCE=8.9047 KD=9.6031 state=23.6344 align=0.0000 | scale_pen(llama)=7.9936e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=21.87 | sec/step~1.97 | keep=0.84 | K=4 | first_w=1.43 | llama(L): tf=10.1696 first=8.5697 kCE=8.7181 KD=9.0118 state=23.4477 align=0.0000 | scale_pen(llama)=7.9936e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=10.14 | sec/step~2.70 | keep=0.87 | K=4 | first_w=1.30 | llama(L): tf=9.4257 first=9.1359 kCE=8.1134 KD=7.6905 state=24.4457 align=0.0000 | scale_pen(llama)=8.8818e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 4/4
  step  10/40 | grad_norm=12.77 | sec/step~1.91 | keep=0.90 | K=4 | first_w=1.18 | llama(L): tf=9.3851 first=8.0175 kCE=8.3025 KD=8.6871 state=21.3220 align=0.0000 | scale_pen(llama)=8.8818e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=5.23 | sec/step~2.12 | keep=0.93 | K=4 | first_w=1.08 | llama(L): tf=9.6335 first=8.0586 kCE=8.6493 KD=9.1992 state=20.0096 align=0.0000 | scale_pen(llama)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=16.34 | sec/step~1.83 | keep=0.96 | K=4 | first_w=1.02 | llama(L): tf=9.7005 first=8.3178 kCE=9.0831 KD=9.3442 state=20.1343 align=0.0000 | scale_pen(llama)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=8.34 | sec/step~2.02 | keep=1.00 | K=4 | first_w=1.00 | llama(L): tf=10.0154 first=7.9900 kCE=8.2629 KD=8.4415 state=20.5707 align=0.0000 | scale_pen(llama)=5.1159e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250925_100218/ckpt/stageA
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 1.000006415694952, 'rms_mean_cal': 0.010571220761630685, 'embed_rms': 0.01057521253824234, 'count': 160}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2990.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.22s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 272,723,968 || all params: 8,302,985,216 || trainable%: 3.2846
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
‚è™ Resuming from: runs/llama_single_20250925_100218/ckpt/stageA/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 240 steps
Epoch 1/6
[warmup] step=0 mode=text (warm-up)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=9.3322 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=9.9616 | latent_scale=0.00
[warmup] step=2 mode=text (warm-up)
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=9.6052 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=9.6944 | latent_scale=0.01
[warmup] step=4 mode=text (warm-up)
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=11.1511 | latent_scale=0.02
[warmup] step=5 mode=text (warm-up)
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=9.9139 | latent_scale=0.02
[warmup] step=6 mode=text (warm-up)
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=10.7789 | latent_scale=0.03
[warmup] step=7 mode=text (warm-up)
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=10.8019 | latent_scale=0.03
[warmup] step=8 mode=text (warm-up)
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=11.1129 | latent_scale=0.03
[warmup] step=9 mode=text (warm-up)
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=9.4162 | latent_scale=0.04
  step  10/80 | grad_norm=3.58 | sec/step~3.21 | keep=0.50 | K=4 | first_w=1.20 | llama(T): tf=0.4019 first=0.5269 kCE=0.3630 KD=0.1015 state=0.9542 align=0.0003 | scale_pen(llama)=5.1159e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=9.9096 | latent_scale=0.04
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=10.1798 | latent_scale=0.05
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=8.9452 | latent_scale=0.05
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=10.9025 | latent_scale=0.05
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=10.3443 | latent_scale=0.06
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=9.8095 | latent_scale=0.06
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=7.2402 | latent_scale=0.07
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=8.1601 | latent_scale=0.07
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=7.1597 | latent_scale=0.07
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=7.3541 | latent_scale=0.08
  step  20/80 | grad_norm=0.53 | sec/step~2.79 | keep=0.50 | K=4 | first_w=1.20 | llama(T): tf=0.7470 first=0.7842 kCE=0.8046 KD=0.1090 state=1.9007 align=0.0003 | scale_pen(llama)=4.9694e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=8.1863 | latent_scale=0.08
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=7.7524 | latent_scale=0.09
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=8.5676 | latent_scale=0.09
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=8.3319 | latent_scale=0.10
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=8.9781 | latent_scale=0.10
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=8.2922 | latent_scale=0.10
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=8.4216 | latent_scale=0.11
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=7.9068 | latent_scale=0.11
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=8.2107 | latent_scale=0.12
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=8.3013 | latent_scale=0.12
  step  30/80 | grad_norm=2.27 | sec/step~3.33 | keep=0.50 | K=4 | first_w=1.20 | llama(T): tf=1.2247 first=1.1681 kCE=1.2209 KD=0.1331 state=2.9539 align=0.0003 | scale_pen(llama)=4.9694e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=8.2589 | latent_scale=0.12
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=8.1176 | latent_scale=0.13
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=7.4264 | latent_scale=0.13
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=6.7829 | latent_scale=0.14
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=6.8707 | latent_scale=0.14
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=7.9943 | latent_scale=0.15
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=7.2446 | latent_scale=0.15
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=8.2814 | latent_scale=0.15
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=7.7225 | latent_scale=0.16
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=6.5802 | latent_scale=0.16
  step  40/80 | grad_norm=2.78 | sec/step~2.61 | keep=0.50 | K=4 | first_w=1.20 | llama(T): tf=1.5968 first=1.4632 kCE=1.7281 KD=0.2657 state=3.2923 align=0.0003 | scale_pen(llama)=1.8146e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=7.2930 | latent_scale=0.17
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=7.5710 | latent_scale=0.17
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=7.2796 | latent_scale=0.17
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=7.7819 | latent_scale=0.18
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=6.7608 | latent_scale=0.18
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=6.6687 | latent_scale=0.19
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=7.2461 | latent_scale=0.19
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=6.9033 | latent_scale=0.20
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=6.4803 | latent_scale=0.20
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=5.4066 | latent_scale=0.20
  step  50/80 | grad_norm=0.45 | sec/step~2.75 | keep=0.51 | K=4 | first_w=1.20 | llama(T): tf=1.8066 first=1.5008 kCE=2.0577 KD=0.4192 state=3.4923 align=0.0003 | scale_pen(llama)=1.0591e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=6.0884 | latent_scale=0.21
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=7.3411 | latent_scale=0.21
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=5.9240 | latent_scale=0.22
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=6.4681 | latent_scale=0.22
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=7.8819 | latent_scale=0.23
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=7.0249 | latent_scale=0.23
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=6.1035 | latent_scale=0.23
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=6.3106 | latent_scale=0.24
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=6.8180 | latent_scale=0.24
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=6.5897 | latent_scale=0.25
  step  60/80 | grad_norm=2.93 | sec/step~2.76 | keep=0.51 | K=4 | first_w=1.20 | llama(T): tf=2.5520 first=2.2060 kCE=2.7806 KD=0.5776 state=4.0590 align=0.0003 | scale_pen(llama)=1.0591e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=6.7916 | latent_scale=0.25
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=5.6915 | latent_scale=0.25
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=6.1312 | latent_scale=0.26
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=6.4491 | latent_scale=0.26
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=6.1028 | latent_scale=0.27
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=6.5592 | latent_scale=0.27
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=5.7090 | latent_scale=0.28
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=6.4666 | latent_scale=0.28
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=7.7237 | latent_scale=0.28
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=7.1867 | latent_scale=0.29
  step  70/80 | grad_norm=1.07 | sec/step~3.15 | keep=0.51 | K=4 | first_w=1.20 | llama(T): tf=2.8345 first=2.4575 kCE=2.9165 KD=0.6033 state=3.9885 align=0.0003 | scale_pen(llama)=5.9721e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=6.4680 | latent_scale=0.29
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=7.0210 | latent_scale=0.30
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=6.6334 | latent_scale=0.30
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=6.4604 | latent_scale=0.30
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=5.6978 | latent_scale=0.31
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=5.7844 | latent_scale=0.31
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=5.6400 | latent_scale=0.32
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=6.8969 | latent_scale=0.32
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=6.4823 | latent_scale=0.33
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=6.7293 | latent_scale=0.33
  step  80/80 | grad_norm=4.83 | sec/step~3.18 | keep=0.51 | K=4 | first_w=1.20 | llama(T): tf=3.2199 first=2.7337 kCE=3.3330 KD=0.7489 state=4.5475 align=0.0003 | scale_pen(llama)=5.4627e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 2/6
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=6.3481 | latent_scale=0.33
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=6.8143 | latent_scale=0.34
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=5.8630 | latent_scale=0.34
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=5.6426 | latent_scale=0.35
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=5.6313 | latent_scale=0.35
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=4.9657 | latent_scale=0.35
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=6.0126 | latent_scale=0.36
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=5.8325 | latent_scale=0.36
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=6.0080 | latent_scale=0.37
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=5.1076 | latent_scale=0.37
  step  10/80 | grad_norm=4.67 | sec/step~3.02 | keep=0.52 | K=4 | first_w=1.20 | llama(T): tf=3.1380 first=2.9907 kCE=3.3425 KD=0.8975 state=5.2368 align=0.0003 | scale_pen(llama)=5.4627e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=5.6836 | latent_scale=0.38
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=5.3724 | latent_scale=0.38
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=4.8628 | latent_scale=0.38
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=5.9838 | latent_scale=0.39
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=6.3012 | latent_scale=0.39
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=5.8349 | latent_scale=0.40
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=5.8371 | latent_scale=0.40
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=5.5270 | latent_scale=0.40
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=4.9989 | latent_scale=0.41
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=4.8361 | latent_scale=0.41
  step  20/80 | grad_norm=1.45 | sec/step~3.67 | keep=0.52 | K=4 | first_w=1.20 | llama(T): tf=3.8622 first=2.8369 kCE=3.6603 KD=1.2807 state=5.6247 align=0.0003 | scale_pen(llama)=8.8690e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=5.8400 | latent_scale=0.42
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=5.8624 | latent_scale=0.42
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=4.8989 | latent_scale=0.42
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=5.7547 | latent_scale=0.43
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=5.6562 | latent_scale=0.43
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=5.8581 | latent_scale=0.44
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=6.3573 | latent_scale=0.44
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=5.5241 | latent_scale=0.45
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=5.4000 | latent_scale=0.45
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=6.0647 | latent_scale=0.45
  step  30/80 | grad_norm=5.02 | sec/step~3.06 | keep=0.53 | K=4 | first_w=1.20 | llama(T): tf=4.4762 first=3.8609 kCE=4.3777 KD=1.5078 state=5.6250 align=0.0003 | scale_pen(llama)=8.8690e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=5.7468 | latent_scale=0.46
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=5.9344 | latent_scale=0.46
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=6.7275 | latent_scale=0.47
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=5.8281 | latent_scale=0.47
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=5.1912 | latent_scale=0.47
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=5.5753 | latent_scale=0.48
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=4.9247 | latent_scale=0.48
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=5.0390 | latent_scale=0.49
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=5.3126 | latent_scale=0.49
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=5.1414 | latent_scale=0.50
  step  40/80 | grad_norm=4.36 | sec/step~2.76 | keep=0.53 | K=4 | first_w=1.20 | llama(T): tf=4.5871 first=4.2916 kCE=4.3871 KD=1.4545 state=6.2029 align=0.0003 | scale_pen(llama)=4.4020e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=4.7363 | latent_scale=0.50
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=5.4678 | latent_scale=0.50
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=5.7927 | latent_scale=0.51
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=5.2361 | latent_scale=0.51
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=5.7536 | latent_scale=0.52
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=5.4577 | latent_scale=0.52
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=5.6026 | latent_scale=0.53
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=5.4189 | latent_scale=0.53
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=4.5558 | latent_scale=0.53
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=5.9619 | latent_scale=0.54
  step  50/80 | grad_norm=0.78 | sec/step~3.26 | keep=0.54 | K=4 | first_w=1.20 | llama(T): tf=4.6421 first=4.5859 kCE=4.3862 KD=1.3832 state=7.1606 align=0.0003 | scale_pen(llama)=7.1623e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=4.7372 | latent_scale=0.54
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=6.4794 | latent_scale=0.55
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=5.2078 | latent_scale=0.55
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=4.3188 | latent_scale=0.55
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=5.3113 | latent_scale=0.56
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=5.1734 | latent_scale=0.56
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=4.4421 | latent_scale=0.57
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=5.4478 | latent_scale=0.57
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=4.9478 | latent_scale=0.57
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=4.4511 | latent_scale=0.58
  step  60/80 | grad_norm=5.51 | sec/step~2.59 | keep=0.54 | K=4 | first_w=1.20 | llama(T): tf=5.0133 first=4.3034 kCE=4.7142 KD=1.5970 state=6.1238 align=0.0003 | scale_pen(llama)=7.1623e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=5.1654 | latent_scale=0.58
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=5.1564 | latent_scale=0.59
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=5.1337 | latent_scale=0.59
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=4.2003 | latent_scale=0.60
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=5.6005 | latent_scale=0.60
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=3.6594 | latent_scale=0.60
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=3.9633 | latent_scale=0.61
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=3.9188 | latent_scale=0.61
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=4.4190 | latent_scale=0.62
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=4.1334 | latent_scale=0.62
  step  70/80 | grad_norm=4.26 | sec/step~2.63 | keep=0.55 | K=4 | first_w=1.20 | llama(T): tf=5.3017 first=4.0079 kCE=4.4746 KD=1.8221 state=7.4560 align=0.0003 | scale_pen(llama)=6.5996e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=5.7249 | latent_scale=0.62
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=3.9542 | latent_scale=0.63
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=4.3189 | latent_scale=0.63
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=5.0140 | latent_scale=0.64
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=5.0677 | latent_scale=0.64
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=4.8695 | latent_scale=0.65
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=4.1567 | latent_scale=0.65
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=5.0970 | latent_scale=0.65
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=4.5175 | latent_scale=0.66
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=4.9630 | latent_scale=0.66
  step  80/80 | grad_norm=10.57 | sec/step~3.09 | keep=0.56 | K=4 | first_w=1.20 | llama(T): tf=5.1561 first=5.3069 kCE=4.6787 KD=1.7524 state=8.6597 align=0.0003 | scale_pen(llama)=4.2286e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 3/6
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=5.1628 | latent_scale=0.67
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=4.0990 | latent_scale=0.67
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=4.1011 | latent_scale=0.68
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=4.4818 | latent_scale=0.68
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=3.9000 | latent_scale=0.68
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=3.8532 | latent_scale=0.69
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=5.8180 | latent_scale=0.69
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=4.3267 | latent_scale=0.70
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=4.1548 | latent_scale=0.70
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=4.2719 | latent_scale=0.70
  step  10/80 | grad_norm=16.28 | sec/step~2.62 | keep=0.56 | K=4 | first_w=1.20 | llama(T): tf=6.1073 first=5.8523 kCE=6.0023 KD=2.4205 state=8.6766 align=0.0003 | scale_pen(llama)=4.2286e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=4.0911 | latent_scale=0.71
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=4.4748 | latent_scale=0.71
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=5.4440 | latent_scale=0.72
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=4.6632 | latent_scale=0.72
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=4.1583 | latent_scale=0.72
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=5.3145 | latent_scale=0.73
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=4.9050 | latent_scale=0.73
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=4.2360 | latent_scale=0.74
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=4.4882 | latent_scale=0.74
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=3.9074 | latent_scale=0.75
  step  20/80 | grad_norm=2.75 | sec/step~2.50 | keep=0.57 | K=4 | first_w=1.20 | llama(T): tf=6.3345 first=5.5374 kCE=4.4029 KD=2.5714 state=8.3610 align=0.0003 | scale_pen(llama)=2.1675e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=4.1703 | latent_scale=0.75
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=4.3317 | latent_scale=0.75
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=4.3607 | latent_scale=0.76
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=4.5554 | latent_scale=0.76
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=4.6302 | latent_scale=0.77
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=4.2793 | latent_scale=0.77
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=4.2621 | latent_scale=0.78
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=4.9073 | latent_scale=0.78
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=4.3233 | latent_scale=0.78
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=4.3871 | latent_scale=0.79
  step  30/80 | grad_norm=10.62 | sec/step~3.12 | keep=0.58 | K=4 | first_w=1.20 | llama(T): tf=6.7638 first=5.9079 kCE=4.6434 KD=2.5691 state=11.0310 align=0.0003 | scale_pen(llama)=2.1675e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=4.8167 | latent_scale=0.79
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=5.5623 | latent_scale=0.80
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=5.1624 | latent_scale=0.80
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=5.1130 | latent_scale=0.80
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=4.7659 | latent_scale=0.81
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=4.5934 | latent_scale=0.81
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=3.9013 | latent_scale=0.82
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=4.3897 | latent_scale=0.82
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=4.6405 | latent_scale=0.82
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=3.8931 | latent_scale=0.83
  step  40/80 | grad_norm=47.63 | sec/step~3.42 | keep=0.59 | K=4 | first_w=1.20 | llama(T): tf=7.3080 first=6.6005 kCE=4.5729 KD=3.2859 state=11.1603 align=0.0003 | scale_pen(llama)=5.6403e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=6.1921 | latent_scale=0.83
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=3.8957 | latent_scale=0.84
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=4.0253 | latent_scale=0.84
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=4.5944 | latent_scale=0.85
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=5.7328 | latent_scale=0.85
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=4.8187 | latent_scale=0.85
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=4.0641 | latent_scale=0.86
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=3.9006 | latent_scale=0.86
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=4.1079 | latent_scale=0.87
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=4.2351 | latent_scale=0.87
  step  50/80 | grad_norm=0.51 | sec/step~3.29 | keep=0.60 | K=4 | first_w=1.20 | llama(T): tf=8.1652 first=6.9640 kCE=4.7697 KD=3.2229 state=11.9262 align=0.0003 | scale_pen(llama)=7.9936e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=4.8107 | latent_scale=0.88
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=3.2025 | latent_scale=0.88
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=4.4856 | latent_scale=0.88
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=3.7924 | latent_scale=0.89
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=5.4235 | latent_scale=0.89
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=5.0653 | latent_scale=0.90
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=5.5855 | latent_scale=0.90
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=4.6217 | latent_scale=0.90
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=4.2542 | latent_scale=0.91
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=4.6683 | latent_scale=0.91
  step  60/80 | grad_norm=3.69 | sec/step~2.97 | keep=0.60 | K=4 | first_w=1.20 | llama(T): tf=8.2388 first=7.1180 kCE=4.9979 KD=3.3595 state=11.8127 align=0.0003 | scale_pen(llama)=7.9936e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=4.2978 | latent_scale=0.92
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=4.3601 | latent_scale=0.92
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=4.7601 | latent_scale=0.93
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=3.6254 | latent_scale=0.93
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=4.1431 | latent_scale=0.93
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=5.3926 | latent_scale=0.94
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=4.2465 | latent_scale=0.94
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=4.1303 | latent_scale=0.95
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=4.0312 | latent_scale=0.95
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=3.9588 | latent_scale=0.95
  step  70/80 | grad_norm=2.00 | sec/step~2.73 | keep=0.61 | K=4 | first_w=1.20 | llama(T): tf=8.2788 first=7.3042 kCE=5.6906 KD=3.2851 state=10.5640 align=0.0003 | scale_pen(llama)=2.8777e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=4.4447 | latent_scale=0.96
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=3.7029 | latent_scale=0.96
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=4.3743 | latent_scale=0.97
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=3.8935 | latent_scale=0.97
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=4.2984 | latent_scale=0.97
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=4.7424 | latent_scale=0.98
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=3.9358 | latent_scale=0.98
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=4.9370 | latent_scale=0.99
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=4.1489 | latent_scale=0.99
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=4.0880 | latent_scale=1.00
  step  80/80 | grad_norm=4.82 | sec/step~3.26 | keep=0.62 | K=4 | first_w=1.19 | llama(T): tf=8.5972 first=7.4474 kCE=5.8557 KD=3.2527 state=12.9538 align=0.0003 | scale_pen(llama)=8.2082e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 4/6
[warmup] step=241 mode=text (tail)
  step  2/80 | (tail text) | align=0.0003 | text_tf=3.5128 | latent_scale=1.00
[warmup] step=247 mode=text (tail)
  step  8/80 | (tail text) | align=0.0003 | text_tf=4.2547 | latent_scale=1.00
[warmup] step=249 mode=text (tail)
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.9854 | latent_scale=1.00
  step  10/80 | grad_norm=2.77 | sec/step~3.02 | keep=0.64 | K=4 | first_w=1.19 | llama(T): tf=7.9505 first=7.2656 kCE=6.2034 KD=3.4958 state=12.9456 align=0.0003 | scale_pen(llama)=8.2082e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=252 mode=text (tail)
  step  13/80 | (tail text) | align=0.0003 | text_tf=4.4408 | latent_scale=1.00
[warmup] step=253 mode=text (tail)
  step  14/80 | (tail text) | align=0.0003 | text_tf=4.3640 | latent_scale=1.00
[warmup] step=259 mode=text (tail)
  step  20/80 | (tail text) | align=0.0003 | text_tf=3.5352 | latent_scale=1.00
  step  20/80 | grad_norm=2.26 | sec/step~3.61 | keep=0.65 | K=4 | first_w=1.19 | llama(T): tf=7.7364 first=7.2147 kCE=5.9504 KD=3.9263 state=13.5081 align=0.0003 | scale_pen(llama)=7.5730e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=263 mode=text (tail)
  step  24/80 | (tail text) | align=0.0003 | text_tf=4.8760 | latent_scale=1.00
[warmup] step=266 mode=text (tail)
  step  27/80 | (tail text) | align=0.0003 | text_tf=4.1971 | latent_scale=1.00
[warmup] step=267 mode=text (tail)
  step  28/80 | (tail text) | align=0.0003 | text_tf=5.6367 | latent_scale=1.00
  step  30/80 | grad_norm=9.92 | sec/step~2.53 | keep=0.66 | K=4 | first_w=1.18 | llama(L): tf=8.5274 first=6.8896 kCE=6.2867 KD=3.7824 state=12.9455 align=0.0000 | scale_pen(llama)=7.5730e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  40/80 | grad_norm=1.82 | sec/step~2.54 | keep=0.67 | K=4 | first_w=1.18 | llama(L): tf=8.1377 first=7.9649 kCE=5.9293 KD=3.6251 state=13.0083 align=0.0000 | scale_pen(llama)=4.7805e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=281 mode=text (tail)
  step  42/80 | (tail text) | align=0.0003 | text_tf=4.6778 | latent_scale=1.00
[warmup] step=284 mode=text (tail)
  step  45/80 | (tail text) | align=0.0003 | text_tf=4.4467 | latent_scale=1.00
[warmup] step=286 mode=text (tail)
  step  47/80 | (tail text) | align=0.0003 | text_tf=4.3333 | latent_scale=1.00
  step  50/80 | grad_norm=8.29 | sec/step~2.52 | keep=0.68 | K=4 | first_w=1.17 | llama(L): tf=8.5342 first=7.7043 kCE=5.9342 KD=4.0539 state=12.8830 align=0.0000 | scale_pen(llama)=2.7853e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  57/80 | (tail text) | align=0.0003 | text_tf=4.7798 | latent_scale=1.00
  step  59/80 | (tail text) | align=0.0003 | text_tf=4.8565 | latent_scale=1.00
  step  60/80 | grad_norm=51.80 | sec/step~2.50 | keep=0.69 | K=4 | first_w=1.17 | llama(L): tf=8.0781 first=7.1787 kCE=5.8501 KD=4.3842 state=12.0720 align=0.0000 | scale_pen(llama)=2.7853e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  68/80 | (tail text) | align=0.0003 | text_tf=3.9935 | latent_scale=1.00
  step  70/80 | grad_norm=1.48 | sec/step~2.65 | keep=0.71 | K=4 | first_w=1.16 | llama(L): tf=8.7359 first=6.9683 kCE=5.1194 KD=4.3435 state=12.6338 align=0.0000 | scale_pen(llama)=9.9796e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  80/80 | grad_norm=3.59 | sec/step~2.52 | keep=0.72 | K=4 | first_w=1.16 | llama(L): tf=8.4278 first=7.2731 kCE=5.3874 KD=4.2030 state=12.4462 align=0.0000 | scale_pen(llama)=3.6241e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
Epoch 5/6
  step  9/80 | (tail text) | align=0.0003 | text_tf=3.4891 | latent_scale=1.00
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.9918 | latent_scale=1.00
  step  10/80 | grad_norm=1.92 | sec/step~2.88 | keep=0.74 | K=4 | first_w=1.15 | llama(T): tf=8.0559 first=7.6883 kCE=4.6364 KD=4.0441 state=12.1332 align=0.0003 | scale_pen(llama)=3.6241e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (tail text) | align=0.0003 | text_tf=4.3984 | latent_scale=1.00
  step  15/80 | (tail text) | align=0.0003 | text_tf=3.4751 | latent_scale=1.00
  step  20/80 | grad_norm=1.89 | sec/step~2.79 | keep=0.75 | K=4 | first_w=1.15 | llama(L): tf=7.9715 first=6.7984 kCE=5.5097 KD=4.0453 state=14.0071 align=0.0000 | scale_pen(llama)=2.4475e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (tail text) | align=0.0003 | text_tf=4.7640 | latent_scale=1.00
  step  27/80 | (tail text) | align=0.0003 | text_tf=3.3385 | latent_scale=1.00
  step  30/80 | grad_norm=6.13 | sec/step~3.23 | keep=0.77 | K=4 | first_w=1.14 | llama(L): tf=7.7782 first=7.5393 kCE=5.2933 KD=4.1918 state=13.7576 align=0.0000 | scale_pen(llama)=2.4475e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  34/80 | (tail text) | align=0.0003 | text_tf=4.3319 | latent_scale=1.00
  step  40/80 | (tail text) | align=0.0003 | text_tf=4.2696 | latent_scale=1.00
  step  40/80 | grad_norm=2.40 | sec/step~3.09 | keep=0.78 | K=4 | first_w=1.14 | llama(T): tf=7.9960 first=7.1413 kCE=5.0442 KD=3.8252 state=11.4468 align=0.0003 | scale_pen(llama)=7.5175e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  45/80 | (tail text) | align=0.0003 | text_tf=5.5407 | latent_scale=1.00
  step  47/80 | (tail text) | align=0.0003 | text_tf=4.6602 | latent_scale=1.00
  step  50/80 | grad_norm=0.58 | sec/step~2.35 | keep=0.80 | K=4 | first_w=1.13 | llama(L): tf=8.3956 first=7.1493 kCE=5.3578 KD=4.4678 state=11.2584 align=0.0000 | scale_pen(llama)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  52/80 | (tail text) | align=0.0003 | text_tf=3.3967 | latent_scale=1.00
  step  55/80 | (tail text) | align=0.0003 | text_tf=4.0379 | latent_scale=1.00
  step  57/80 | (tail text) | align=0.0003 | text_tf=4.0241 | latent_scale=1.00
  step  60/80 | (tail text) | align=0.0003 | text_tf=5.0777 | latent_scale=1.00
  step  60/80 | grad_norm=2.79 | sec/step~3.09 | keep=0.81 | K=4 | first_w=1.13 | llama(T): tf=7.8383 first=7.6730 kCE=4.5367 KD=3.9793 state=12.0081 align=0.0003 | scale_pen(llama)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  70/80 | grad_norm=2.67 | sec/step~2.72 | keep=0.83 | K=4 | first_w=1.12 | llama(L): tf=7.9889 first=7.5434 kCE=5.0324 KD=3.9115 state=12.9451 align=0.0000 | scale_pen(llama)=8.1855e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  75/80 | (tail text) | align=0.0003 | text_tf=4.5358 | latent_scale=1.00
  step  80/80 | grad_norm=6.89 | sec/step~2.38 | keep=0.85 | K=4 | first_w=1.12 | llama(L): tf=7.6719 first=7.1348 kCE=5.0911 KD=3.9188 state=10.8216 align=0.0000 | scale_pen(llama)=1.9455e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
Epoch 6/6
  step  1/80 | (tail text) | align=0.0003 | text_tf=3.9750 | latent_scale=1.00
  step  6/80 | (tail text) | align=0.0003 | text_tf=4.0348 | latent_scale=1.00
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.6998 | latent_scale=1.00
  step  10/80 | grad_norm=2.58 | sec/step~3.05 | keep=0.86 | K=4 | first_w=1.11 | llama(T): tf=8.3560 first=7.4505 kCE=4.9219 KD=4.1076 state=12.5071 align=0.0003 | scale_pen(llama)=1.9455e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  16/80 | (tail text) | align=0.0003 | text_tf=4.4144 | latent_scale=1.00
  step  17/80 | (tail text) | align=0.0003 | text_tf=4.9291 | latent_scale=1.00
  step  20/80 | grad_norm=1.41 | sec/step~2.20 | keep=0.88 | K=4 | first_w=1.11 | llama(L): tf=8.2791 first=7.0763 kCE=4.6020 KD=4.0835 state=10.5713 align=0.0000 | scale_pen(llama)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  24/80 | (tail text) | align=0.0003 | text_tf=4.0274 | latent_scale=1.00
  step  29/80 | (tail text) | align=0.0003 | text_tf=3.9197 | latent_scale=1.00
  step  30/80 | grad_norm=4.83 | sec/step~2.73 | keep=0.90 | K=4 | first_w=1.11 | llama(L): tf=7.8205 first=6.9251 kCE=4.4428 KD=3.9303 state=12.8822 align=0.0000 | scale_pen(llama)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  38/80 | (tail text) | align=0.0003 | text_tf=4.1963 | latent_scale=1.00
  step  40/80 | grad_norm=2.09 | sec/step~2.62 | keep=0.92 | K=4 | first_w=1.10 | llama(L): tf=7.9926 first=7.5664 kCE=5.0310 KD=3.6156 state=11.0714 align=0.0000 | scale_pen(llama)=2.0464e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  44/80 | (tail text) | align=0.0003 | text_tf=3.7909 | latent_scale=1.00
  step  48/80 | (tail text) | align=0.0003 | text_tf=3.9810 | latent_scale=1.00
  step  50/80 | grad_norm=0.75 | sec/step~2.52 | keep=0.94 | K=4 | first_w=1.10 | llama(L): tf=7.3875 first=7.0956 kCE=5.1917 KD=3.7323 state=12.1947 align=0.0000 | scale_pen(llama)=1.1511e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  59/80 | (tail text) | align=0.0003 | text_tf=3.1556 | latent_scale=1.00
  step  60/80 | grad_norm=1.97 | sec/step~2.26 | keep=0.96 | K=4 | first_w=1.10 | llama(L): tf=7.6434 first=6.4341 kCE=5.3448 KD=3.4426 state=10.4462 align=0.0000 | scale_pen(llama)=1.1511e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  65/80 | (tail text) | align=0.0003 | text_tf=3.7402 | latent_scale=1.00
  step  69/80 | (tail text) | align=0.0003 | text_tf=4.1189 | latent_scale=1.00
  step  70/80 | grad_norm=1.27 | sec/step~2.14 | keep=0.98 | K=4 | first_w=1.10 | llama(L): tf=8.1140 first=7.6453 kCE=4.3079 KD=4.0136 state=10.0090 align=0.0000 | scale_pen(llama)=6.5690e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (tail text) | align=0.0003 | text_tf=3.6647 | latent_scale=1.00
  step  72/80 | (tail text) | align=0.0003 | text_tf=3.9225 | latent_scale=1.00
  step  76/80 | (tail text) | align=0.0003 | text_tf=3.3153 | latent_scale=1.00
  step  80/80 | grad_norm=3.33 | sec/step~2.33 | keep=1.00 | K=4 | first_w=1.10 | llama(L): tf=7.9445 first=7.0148 kCE=4.7787 KD=3.8130 state=11.2579 align=0.0000 | scale_pen(llama)=5.9721e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 2.4KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250925_100218/ckpt/stageB
üìù Saved Prefix-Tuning adapters for Llama
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 1.000205461680889, 'rms_mean_cal': 0.010571547450187306, 'embed_rms': 0.01056710910052061, 'count': 480}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/llama_single_20250925_100218/ckpt/stageB/training_stats.json
Encoder input alignment: mode=neutral_chat | strip_anchor=yes | samples=200
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2859.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  1.35s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:03<00:01,  1.29s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.04s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

‚Äî Text baseline summary:
llama: EM=0.590 F1=0.796
‚úì Loaded Prefix-Tuning adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 246.0 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 13107200 bytes (fp32); fp16 reference: 6553600 bytes; fp32 reference: 13107200 bytes
latent/text bytes (one-copy, fp16): n/a

‚Äî Baseline: Text prompting
Llama  EM: 0.590  F1: 0.796  |  NLL/token (gold): 13.675748455854526
Wall clock: 6.68s

‚Äî Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 8.256595855397162
       First-token acc: top1=0.000  top5=0.050
Wall clock: 1.43s

‚Äî Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Wall clock: 1.89s

‚Äî 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 246.03
  },
  "compression": {
    "llama": 3.84421875
  },
  "payload_bytes": 13107200,
  "payload_bytes_detail": {
    "fp32": 13107200,
    "fp16": 6553600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251558
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 13107200,
      "fp16": 6553600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.7961021295152534,
      "nll_token": 13.675748455854526
    },
    "wall_clock_sec": 6.684671640396118
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 8.256595855397162,
      "first_token_top1": 0.0,
      "first_token_top5": 0.05,
      "nll_token": 8.256595855397162
    },
    "wall_clock_sec": 1.428495168685913
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 1.8878355026245117
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
