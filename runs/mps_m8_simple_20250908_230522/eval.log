Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Samples: 200  |  Max new tokens: 24
Avg prompt tokens (Llama): 76.1 | (Qwen): 59.9 | Latent length M: 8
Compression ratio (Llama): 9.5x | (Qwen): 7.5x
Approx interlingua payload per example: 8192 bytes (dtype float32, shape M=8, d_z=256)

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.009  |  NLL/token (gold): 14.314
Qwen   EM: 0.000   F1: 0.003   |  NLL/token (gold): 11.754
Wall clock: 141.55s for 200 examples

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 9.601
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 10.044
Wall clock: 17.36s for 200 examples

— Token-budget baseline (same #prefix tokens as latent)
Llama  EM: 0.000  F1: 0.011
Qwen   EM: 0.000   F1: 0.005
Wall clock: 11.36s for 200 examples

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.840
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 24,
  "latent_len": 8,
  "avg_prompt_tokens": {
    "llama": 76.095,
    "qwen": 59.875
  },
  "compression": {
    "llama": 9.511875,
    "qwen": 7.484375
  },
  "payload_bytes": 8192,
  "text": {
    "llama": {
      "em": 0.0,
      "f1": 0.008771054326936681,
      "nll_token": 14.314151741986606
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002543738896102188,
      "nll_token": 11.753927846023123
    },
    "wall_clock_sec": 141.5504801273346
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 9.600510046443837
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 10.044482185654875
    },
    "wall_clock_sec": 17.363337993621826
  },
  "token_budget": {
    "llama": {
      "em": 0.0,
      "f1": 0.01076005894362357
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.004907781076781872
    },
    "wall_clock_sec": 11.35565710067749
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.84,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  }
}
Wrote per-example predictions to runs/mps_m8_simple_20250908_230522/predictions.jsonl
