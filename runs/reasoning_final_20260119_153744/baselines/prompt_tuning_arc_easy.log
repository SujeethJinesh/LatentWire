/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Device: cuda:0

======================================================================
PROMPT TUNING BASELINE (8 tokens): ARC_EASY
======================================================================
This proves whether Llama (sender) actually helps.

--- Seed 42 ---
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 335.66it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:02<00:04,  2.17s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:06<00:03,  3.17s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.29s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.16s/it]
Training:   0%|          | 0/1500 [00:00<?, ?it/s]Training:   0%|          | 0/1500 [00:00<?, ?it/s, loss=6.604]Training:   0%|          | 1/1500 [00:00<19:59,  1.25it/s, loss=6.604]Training:   0%|          | 1/1500 [00:00<19:59,  1.25it/s, loss=6.493]Training:   0%|          | 2/1500 [00:00<11:05,  2.25it/s, loss=6.493]Training:   0%|          | 2/1500 [00:01<11:05,  2.25it/s, loss=6.523]Training:   0%|          | 3/1500 [00:01<07:18,  3.41it/s, loss=6.523]Training:   0%|          | 3/1500 [00:01<07:18,  3.41it/s, loss=6.074]Training:   0%|          | 4/1500 [00:01<05:34,  4.47it/s, loss=6.074]Training:   0%|          | 4/1500 [00:01<05:34,  4.47it/s, loss=6.036]Training:   0%|          | 5/1500 [00:01<04:38,  5.37it/s, loss=6.036]Training:   0%|          | 5/1500 [00:01<04:38,  5.37it/s, loss=6.292]Training:   0%|          | 6/1500 [00:01<04:04,  6.10it/s, loss=6.292]Training:   0%|          | 6/1500 [00:01<04:04,  6.10it/s, loss=6.138]Training:   0%|          | 7/1500 [00:01<03:46,  6.58it/s, loss=6.138]Training:   0%|          | 7/1500 [00:01<03:46,  6.58it/s, loss=6.097]Training:   1%|          | 8/1500 [00:01<04:06,  6.06it/s, loss=6.097]Training:   1%|          | 8/1500 [00:01<04:06,  6.06it/s, loss=5.948]Training:   1%|          | 9/1500 [00:01<03:42,  6.69it/s, loss=5.948]Training:   1%|          | 9/1500 [00:02<03:42,  6.69it/s, loss=5.973]Training:   1%|          | 10/1500 [00:02<03:26,  7.23it/s, loss=5.973]Training:   1%|          | 10/1500 [00:02<03:26,  7.23it/s, loss=5.785]Training:   1%|          | 11/1500 [00:02<03:20,  7.41it/s, loss=5.785]Training:   1%|          | 11/1500 [00:02<03:20,  7.41it/s, loss=5.316]Training:   1%|          | 12/1500 [00:02<03:12,  7.71it/s, loss=5.316]Training:   1%|          | 12/1500 [00:02<03:12,  7.71it/s, loss=5.461]Training:   1%|          | 13/1500 [00:02<03:07,  7.93it/s, loss=5.461]Training:   1%|          | 13/1500 [00:02<03:07,  7.93it/s, loss=5.481]Training:   1%|          | 14/1500 [00:02<03:04,  8.04it/s, loss=5.481]Training:   1%|          | 14/1500 [00:02<03:04,  8.04it/s, loss=5.535]Training:   1%|          | 15/1500 [00:02<03:01,  8.18it/s, loss=5.535]Training:   1%|          | 15/1500 [00:02<03:01,  8.18it/s, loss=5.202]Training:   1%|          | 15/1500 [00:02<03:01,  8.18it/s, loss=5.063]Training:   1%|          | 17/1500 [00:02<03:18,  7.47it/s, loss=5.063]Training:   1%|          | 17/1500 [00:03<03:18,  7.47it/s, loss=5.070]Training:   1%|          | 17/1500 [00:03<03:18,  7.47it/s, loss=4.934]Training:   1%|▏         | 19/1500 [00:03<03:03,  8.06it/s, loss=4.934]Training:   1%|▏         | 19/1500 [00:03<03:03,  8.06it/s, loss=5.142]Training:   1%|▏         | 20/1500 [00:03<02:59,  8.23it/s, loss=5.142]Training:   1%|▏         | 20/1500 [00:03<02:59,  8.23it/s, loss=5.083]Training:   1%|▏         | 20/1500 [00:03<02:59,  8.23it/s, loss=4.569]Training:   1%|▏         | 22/1500 [00:03<02:47,  8.82it/s, loss=4.569]Training:   1%|▏         | 22/1500 [00:03<02:47,  8.82it/s, loss=4.410]Training:   2%|▏         | 23/1500 [00:03<02:54,  8.44it/s, loss=4.410]Training:   2%|▏         | 23/1500 [00:03<02:54,  8.44it/s, loss=4.259]Training:   2%|▏         | 24/1500 [00:03<03:01,  8.14it/s, loss=4.259]Training:   2%|▏         | 24/1500 [00:03<03:01,  8.14it/s, loss=3.801]Training:   2%|▏         | 25/1500 [00:03<02:56,  8.36it/s, loss=3.801]Training:   2%|▏         | 25/1500 [00:03<02:56,  8.36it/s, loss=4.221]Training:   2%|▏         | 26/1500 [00:03<02:48,  8.74it/s, loss=4.221]Training:   2%|▏         | 26/1500 [00:04<02:48,  8.74it/s, loss=3.868]Training:   2%|▏         | 27/1500 [00:04<02:48,  8.72it/s, loss=3.868]Training:   2%|▏         | 27/1500 [00:04<02:48,  8.72it/s, loss=4.301]Training:   2%|▏         | 28/1500 [00:04<02:44,  8.96it/s, loss=4.301]Training:   2%|▏         | 28/1500 [00:04<02:44,  8.96it/s, loss=4.022]Training:   2%|▏         | 29/1500 [00:04<02:49,  8.68it/s, loss=4.022]Training:   2%|▏         | 29/1500 [00:04<02:49,  8.68it/s, loss=3.650]Training:   2%|▏         | 30/1500 [00:04<02:52,  8.51it/s, loss=3.650]Training:   2%|▏         | 30/1500 [00:04<02:52,  8.51it/s, loss=3.908]Training:   2%|▏         | 31/1500 [00:04<02:51,  8.54it/s, loss=3.908]Training:   2%|▏         | 31/1500 [00:04<02:51,  8.54it/s, loss=3.714]Training:   2%|▏         | 32/1500 [00:04<02:50,  8.62it/s, loss=3.714]Training:   2%|▏         | 32/1500 [00:04<02:50,  8.62it/s, loss=3.299]Training:   2%|▏         | 33/1500 [00:04<02:49,  8.63it/s, loss=3.299]Training:   2%|▏         | 33/1500 [00:04<02:49,  8.63it/s, loss=3.865]Training:   2%|▏         | 34/1500 [00:04<02:51,  8.54it/s, loss=3.865]Training:   2%|▏         | 34/1500 [00:04<02:51,  8.54it/s, loss=3.454]Training:   2%|▏         | 35/1500 [00:04<02:53,  8.45it/s, loss=3.454]Training:   2%|▏         | 35/1500 [00:05<02:53,  8.45it/s, loss=3.176]Training:   2%|▏         | 36/1500 [00:05<02:50,  8.56it/s, loss=3.176]Training:   2%|▏         | 36/1500 [00:05<02:50,  8.56it/s, loss=4.147]Training:   2%|▏         | 37/1500 [00:05<02:54,  8.40it/s, loss=4.147]Training:   2%|▏         | 37/1500 [00:05<02:54,  8.40it/s, loss=3.251]Training:   3%|▎         | 38/1500 [00:05<02:58,  8.18it/s, loss=3.251]Training:   3%|▎         | 38/1500 [00:05<02:58,  8.18it/s, loss=3.077]Training:   3%|▎         | 39/1500 [00:05<03:05,  7.89it/s, loss=3.077]Training:   3%|▎         | 39/1500 [00:05<03:05,  7.89it/s, loss=3.329]Training:   3%|▎         | 40/1500 [00:05<02:58,  8.18it/s, loss=3.329]Training:   3%|▎         | 40/1500 [00:05<02:58,  8.18it/s, loss=2.801]Training:   3%|▎         | 41/1500 [00:05<03:04,  7.92it/s, loss=2.801]Training:   3%|▎         | 41/1500 [00:05<03:04,  7.92it/s, loss=3.163]Training:   3%|▎         | 42/1500 [00:05<02:59,  8.12it/s, loss=3.163]Training:   3%|▎         | 42/1500 [00:05<02:59,  8.12it/s, loss=3.104]Training:   3%|▎         | 43/1500 [00:05<02:58,  8.18it/s, loss=3.104]Training:   3%|▎         | 43/1500 [00:06<02:58,  8.18it/s, loss=2.747]Training:   3%|▎         | 44/1500 [00:06<02:58,  8.14it/s, loss=2.747]Training:   3%|▎         | 44/1500 [00:06<02:58,  8.14it/s, loss=2.767]Training:   3%|▎         | 45/1500 [00:06<02:55,  8.30it/s, loss=2.767]Training:   3%|▎         | 45/1500 [00:06<02:55,  8.30it/s, loss=2.620]Training:   3%|▎         | 46/1500 [00:06<02:53,  8.36it/s, loss=2.620]Training:   3%|▎         | 46/1500 [00:06<02:53,  8.36it/s, loss=2.426]Training:   3%|▎         | 47/1500 [00:06<02:48,  8.61it/s, loss=2.426]Training:   3%|▎         | 47/1500 [00:06<02:48,  8.61it/s, loss=2.179]Training:   3%|▎         | 48/1500 [00:06<02:50,  8.52it/s, loss=2.179]Training:   3%|▎         | 48/1500 [00:06<02:50,  8.52it/s, loss=2.487]Training:   3%|▎         | 49/1500 [00:06<02:48,  8.62it/s, loss=2.487]Training:   3%|▎         | 49/1500 [00:06<02:48,  8.62it/s, loss=2.346]Training:   3%|▎         | 50/1500 [00:06<02:49,  8.55it/s, loss=2.346]Training:   3%|▎         | 50/1500 [00:06<02:49,  8.55it/s, loss=2.507]Training:   3%|▎         | 51/1500 [00:06<03:06,  7.78it/s, loss=2.507]Training:   3%|▎         | 51/1500 [00:07<03:06,  7.78it/s, loss=2.816]Training:   3%|▎         | 52/1500 [00:07<03:04,  7.87it/s, loss=2.816]Training:   3%|▎         | 52/1500 [00:07<03:04,  7.87it/s, loss=2.113]Training:   4%|▎         | 53/1500 [00:07<02:59,  8.05it/s, loss=2.113]Training:   4%|▎         | 53/1500 [00:07<02:59,  8.05it/s, loss=2.217]Training:   4%|▎         | 54/1500 [00:07<02:56,  8.19it/s, loss=2.217]Training:   4%|▎         | 54/1500 [00:07<02:56,  8.19it/s, loss=2.326]Training:   4%|▎         | 55/1500 [00:07<02:58,  8.10it/s, loss=2.326]Training:   4%|▎         | 55/1500 [00:07<02:58,  8.10it/s, loss=1.822]Training:   4%|▎         | 56/1500 [00:07<02:49,  8.50it/s, loss=1.822]Training:   4%|▎         | 56/1500 [00:07<02:49,  8.50it/s, loss=2.027]Training:   4%|▍         | 57/1500 [00:07<02:53,  8.33it/s, loss=2.027]Training:   4%|▍         | 57/1500 [00:07<02:53,  8.33it/s, loss=1.817]Training:   4%|▍         | 58/1500 [00:07<02:57,  8.11it/s, loss=1.817]Training:   4%|▍         | 58/1500 [00:07<02:57,  8.11it/s, loss=1.808]Training:   4%|▍         | 59/1500 [00:07<02:48,  8.55it/s, loss=1.808]Training:   4%|▍         | 59/1500 [00:08<02:48,  8.55it/s, loss=2.169]Training:   4%|▍         | 60/1500 [00:08<02:55,  8.19it/s, loss=2.169]Training:   4%|▍         | 60/1500 [00:08<03:12,  7.48it/s, loss=2.169]
Traceback (most recent call last):
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 1651, in <module>
    main()
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 1601, in main
    results = run_prompt_tuning_baseline(args, config, device)
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 946, in run_prompt_tuning_baseline
    batch = next(iter_dl)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 398, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 171, in collate
    {
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 172, in <dictcomp>
    key: collate(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 171, in collate
    {
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 172, in <dictcomp>
    key: collate(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py", line 207, in collate
    raise RuntimeError("each element in list of batch should be of equal size")
RuntimeError: each element in list of batch should be of equal size
