Starting experiment suite at Sun Oct 12 19:21:23 PDT 2025

========================================================================
PHASE 1/5: TEXT BASELINE - Upper Bound
========================================================================

Evaluating both LLMs with full text prompts...
This establishes the best possible performance.

[1a] Running Llama text baseline...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: eval.py [-h] --ckpt CKPT [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
               [--llama_device_map LLAMA_DEVICE_MAP]
               [--qwen_device_map QWEN_DEVICE_MAP]
               [--dataset {hotpot,squad,squad_v2}] [--samples SAMPLES]
               [--max_new_tokens MAX_NEW_TOKENS] [--load_4bit]
               [--device DEVICE] [--hotpot_config HOTPOT_CONFIG]
               [--out_dir OUT_DIR]
               [--token_budget_mode {chat_full,content_only}]
               [--token_budget_k TOKEN_BUDGET_K] [--gpu_mem_gib GPU_MEM_GIB]
               [--latent_anchor_mode {auto,chat,text,none}]
               [--latent_anchor_text LATENT_ANCHOR_TEXT]
               [--append_bos_after_prefix {auto,yes,no}] [--skip_prefix_acc]
               [--use_chat_template {yes,no}] [--sequential_eval]
               [--models MODELS] [--chunk_size CHUNK_SIZE]
               [--hf_encoder_id HF_ENCODER_ID]
               [--max_enc_tokens MAX_ENC_TOKENS] [--fresh_eval]
               [--embedding_replay]
               [--embedding_baseline_modes EMBEDDING_BASELINE_MODES] [--debug]
               [--debug_print_first DEBUG_PRINT_FIRST]
               [--debug_topk DEBUG_TOPK]
               [--debug_topk_examples DEBUG_TOPK_EXAMPLES]
               [--min_new_tokens MIN_NEW_TOKENS]
               [--eos_ban_steps EOS_BAN_STEPS]
               [--first_token_top_p FIRST_TOKEN_TOP_P]
               [--first_token_temperature FIRST_TOKEN_TEMPERATURE]
               [--latent_quant_bits {16,8,6,4}]
               [--latent_quant_group_size LATENT_QUANT_GROUP_SIZE]
               [--latent_quant_scale_bits LATENT_QUANT_SCALE_BITS]
               [--prefix_gain PREFIX_GAIN]
               [--calibration {none,embed_rms,fixed,train_stats}]
               [--prefix_target_rms PREFIX_TARGET_RMS]
               [--adapter_dropout ADAPTER_DROPOUT]
               [--encoder_text_mode {auto,raw,neutral_chat,llama_chat,qwen_chat}]
               [--seed SEED]
eval.py: error: the following arguments are required: --ckpt
