Starting experiment suite at Sun Oct 12 20:44:18 PDT 2025

========================================================================
PHASE 1/5: TEXT BASELINE - Upper Bound
========================================================================

Evaluating both LLMs with full text prompts...
This establishes the best possible performance.

[1a] Running Llama text baseline (batch_size=256)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3587.94it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.88s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.85s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10000 examples from squad...
Loaded 10000 examples

Generating answers with full text prompts (batch_size=256)...
  0/10000...
  6400/10000...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Samples: 10000
  EM: 0.43%
  F1: 36.28%
================================================================================

Results saved to runs/full_suite/baselines/text_llama/results.json
Total time: 258.0s


[1b] Skipping Qwen text baseline for speed...

✓ Phase 1 complete in 262s (4m)

========================================================================
PHASE 2/5: TOKEN BUDGET BASELINE - Fair Comparison
========================================================================

Evaluating with text truncated to M=32 tokens...
This is the critical fairness baseline.

[2a] Running Llama token budget (M=32, batch_size=256)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3913.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.81s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.86s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10000 examples from squad...
Loaded 10000 examples

Generating answers with text truncated to 32 tokens (batch_size=256)...
  0/10000...
  6400/10000...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Token Budget: 32
  Samples: 10000
  EM: 0.00%
  F1: 4.26%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_llama_m32/results.json
Total time: 53.6s


[2b] Skipping Qwen token budget for speed...

✓ Phase 2 complete in 57s (0m)

========================================================================
PHASE 3/5: PCA BASELINE - Linear Compression
========================================================================

Testing if PCA (linear projection) is sufficient...

[3] Running PCA baseline (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda
GPUs: 4

Loading meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7898.88it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.84s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.81s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]
scripts/run_all_experiments.sh: line 304: 3997832 Killed                  python scripts/baselines/pca_baseline.py --llama_id "$LLAMA_ID" --dataset "$DATASET" --samples "$EVAL_SAMPLES" --latent_len "$LATENT_LEN" --max_new_tokens "$MAX_NEW_TOKENS" --save_dir "$BASE_OUTPUT_DIR/baselines/pca_m${LATENT_LEN}"
