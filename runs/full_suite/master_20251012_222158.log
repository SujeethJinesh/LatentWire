Starting experiment suite at Sun Oct 12 22:21:58 PDT 2025

========================================================================
PHASE 1/5: TEXT BASELINE - Upper Bound
========================================================================

Evaluating both LLMs with full text prompts...
This establishes the best possible performance.

[1a] Running Llama text baseline (batch_size=256)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3744.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.90s/it]
Model loaded!

Loading 10000 examples from squad...
Loaded 10000 examples

Generating answers with full text prompts (batch_size=256)...
  0/10000...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  6400/10000...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Samples: 10000
  EM: 0.43%
  F1: 36.28%
================================================================================

Results saved to runs/full_suite/baselines/text_llama/results.json
Total time: 258.8s


[1b] Skipping Qwen text baseline for speed...

✓ Phase 1 complete in 262s (4m)

========================================================================
PHASE 2/5: TOKEN BUDGET BASELINE - Fair Comparison
========================================================================

Evaluating with text truncated to M=32 tokens...
This is the critical fairness baseline.

[2a] Running Llama token budget (M=32, batch_size=256)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3767.62it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.93s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.77s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.92s/it]
Model loaded!

Loading 10000 examples from squad...
Loaded 10000 examples

Generating answers with text truncated to 32 tokens (batch_size=256)...
  0/10000...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  6400/10000...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Token Budget: 32
  Samples: 10000
  EM: 0.00%
  F1: 4.26%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_llama_m32/results.json
Total time: 54.3s


[2b] Skipping Qwen token budget for speed...

✓ Phase 2 complete in 58s (0m)

========================================================================
PHASE 3/5: PCA BASELINE - Linear Compression
========================================================================

Testing if PCA (linear projection) is sufficient...

[3] Running PCA baseline (M=32, samples=10000)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda
GPUs: 4

Loading meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6792.40it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.94s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.88s/it]
Model loaded!

Loading 10000 examples from squad...
Loaded 10000 examples

Fitting IncrementalPCA (batched GPU extraction)...
Embedding layer device: cuda:0
  Processing examples 0/10000...
