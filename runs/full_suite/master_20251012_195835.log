Starting experiment suite at Sun Oct 12 19:58:35 PDT 2025

========================================================================
PHASE 1/5: TEXT BASELINE - Upper Bound
========================================================================

Evaluating both LLMs with full text prompts...
This establishes the best possible performance.

[1a] Running Llama text baseline...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 565.16it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.64s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with full text prompts...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Samples: 10
  EM: 0.00%
  F1: 26.92%
================================================================================

Results saved to runs/full_suite/baselines/text_llama/results.json
Total time: 24.8s


[1b] Running Qwen text baseline...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: Qwen/Qwen2.5-7B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3634.58it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with full text prompts...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: Qwen/Qwen2.5-7B-Instruct
  Samples: 10
  EM: 0.00%
  F1: 17.72%
================================================================================

Results saved to runs/full_suite/baselines/text_qwen/results.json
Total time: 22.6s


✓ Phase 1 complete in 55s (0m)

========================================================================
PHASE 2/5: TOKEN BUDGET BASELINE - Fair Comparison
========================================================================

Evaluating with text truncated to M=32 tokens...
This is the critical fairness baseline.

[2a] Running Llama token budget (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1780.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with text truncated to 32 tokens...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Token Budget: 32
  Samples: 10
  EM: 0.00%
  F1: 0.00%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_llama_m32/results.json
Total time: 24.6s


[2b] Running Qwen token budget (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: Qwen/Qwen2.5-7B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8652.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.63s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with text truncated to 32 tokens...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: Qwen/Qwen2.5-7B-Instruct
  Token Budget: 32
  Samples: 10
  EM: 0.00%
  F1: 0.00%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_qwen_m32/results.json
Total time: 23.7s


✓ Phase 2 complete in 54s (0m)

========================================================================
PHASE 3/5: PCA BASELINE - Linear Compression
========================================================================

Testing if PCA (linear projection) is sufficient...

[3] Running PCA baseline (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda
GPUs: 4

Loading meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7499.87it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.99s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.82s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.84s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Extracting text embeddings for PCA...
  Processing 0/10...
  Collected 1724 token embeddings
  Embedding dim: 4096

Fitting PCA to reduce 4096D → 32D...
  PCA fitted!
  Explained variance: 25.11%
  First 5 components: [0.03828779 0.0211216  0.01419981 0.01213682 0.01053391]

Saved PCA to runs/full_suite/baselines/pca_m32/pca_M32.pkl

================================================================================
EVALUATION: Generating answers with PCA-compressed embeddings
================================================================================

  Example 0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Samples: 10
  Latent dim (M): 32
  PCA explained variance: 25.11%
  EM: 0.00%
  F1: 1.43%
================================================================================

Results saved to runs/full_suite/baselines/pca_m32/results.json
Total time: 24.6s

INTERPRETATION:
  If PCA F1 ≈ Text baseline → Linear compression is sufficient
  If PCA F1 << Text baseline → Need learned non-linear encoder


✓ Phase 3 complete in 29s (0m)

========================================================================
PHASE 4/5: LATENTWIRE TRAINING - Learned Compression
========================================================================

Training encoder + adapters for compressed interlingua...
  Samples: 100
  Epochs: 1
  Batch size: 8
  Latent: M=32, d_z=256

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5187.76it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 386.09it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
[Qwen/Qwen2.5-7B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Llama hidden size: 4096, Qwen hidden size: 3584
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[DeviceMap] Qwen : {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:6.2GB(7%), GPU1:8.6GB(10%), GPU2:8.6GB(10%), GPU3:8.0GB(9%) | Total: 31.4GB allocated, 31.5GB reserved, 308.6GB free, Peak: 31.4GB
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[INFO] llama anchor tokens: 3
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
[INFO] qwen anchor tokens: 3
[Optimizer] Gathering latent adapter parameters (direct wrappers fallback)...
[Optimizer]   Meta-Llama-3.1-8B-Instruct: skipped (use_latent_adapters=False)
[Optimizer]   Qwen2.5-7B-Instruct: skipped (use_latent_adapters=False)
[Optimizer] Latent adapter summary: 0 params in 0 tensors
[Optimizer] No latent adapters enabled (expected)
[Optimization] Using fused AdamW optimizer
[Optimizer] Created 3 parameter groups:
  [1] encoder(90 tensors)
  [2] llama_adapter(20 tensors)
  [3] qwen_adapter(20 tensors)
[INFO] LR scheduler: CosineAnnealingLR (T_max=12, eta_min=2.00e-06)
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
[Epoch 1 Start] [GPU Memory] GPU0:6.3GB(14%), GPU1:8.6GB(10%), GPU2:8.6GB(10%), GPU3:8.0GB(9%) | Total: 31.4GB allocated, 37.1GB reserved, 303.0GB free, Peak: 34.5GB
    [Memory after encoder] 31.9GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] KD teacher: adapters NOT disabled - KD may be contaminated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 31.9GB allocated, peak 70.3GB
    [Memory after optimizer] 32.0GB allocated
  [Step 1] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(22%), GPU2:8.6GB(22%), GPU3:8.1GB(21%) | Total: 32.0GB allocated, 73.4GB reserved, 266.7GB free, Peak: 70.3GB
    [Memory after encoder] 32.5GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 32.2GB allocated, peak 70.4GB
    [Memory after optimizer] 32.2GB allocated
  [Step 2] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 74.4GB reserved, 265.7GB free, Peak: 70.4GB
    [Memory after encoder] 32.5GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 32.2GB allocated, peak 70.5GB
    [Memory after optimizer] 32.2GB allocated
  [Step 3] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 75.0GB reserved, 265.1GB free, Peak: 70.5GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 4] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 75.3GB reserved, 264.8GB free, Peak: 70.5GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 5] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 75.5GB reserved, 264.6GB free, Peak: 70.7GB
  [Batch Size Suggestion after 5 steps] Low peak memory (20.8%), can cautiously increase batch size
    Current: 8, Suggested: 9
    To apply: set BATCH_SIZE_STAGEA/B=9 in run script
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 6] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.8GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 7] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 8] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 9] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  step  10/13 | grad_norm=16.56 | sec/step~1.48 | lr=8.56e-06 | keep=1.00 | K=4 | llama(L): tf=9.8224 first=9.3741 kCE=8.5349 KD=6.2283 acc=0.000 align=0.0000 | scale_pen(llama)=8.1005e-11 | qwen(L): tf=11.1577 first=14.2890 kCE=8.0972 KD=13.7489 acc=0.125 [✓'1'] align=0.0000 | scale_pen(qwen)=1.9455e-11 | feature_grads[encoder=8.282e+00, adapter_llama=7.949e+00, adapter_qwen=1.193e+01] | K=4 tau=2.00
  [Step 10] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 11] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 12] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.4GB reserved, 263.7GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  step  13/13 | grad_norm=15.61 | sec/step~1.39 | lr=3.67e-06 | keep=1.00 | K=4 | llama(L): tf=10.4558 first=8.3421 kCE=8.3952 KD=4.2070 acc=0.000 align=0.0000 | scale_pen(llama)=7.7819e-11 | qwen(L): tf=12.8111 first=13.5228 kCE=9.4972 KD=15.8443 acc=0.250 [✓'1'] align=0.0000 | scale_pen(qwen)=8.2082e-11 | feature_grads[encoder=7.586e+00, adapter_llama=1.007e+01, adapter_qwen=9.207e+00] | K=4 tau=2.00
  [Step 13] [GPU Memory] GPU0:6.5GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 31.9GB allocated, 76.9GB reserved, 263.2GB free, Peak: 70.8GB
[checkpoint] Freed 1.0KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, adapter_qwen.pt
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/full_suite/latentwire

✓ Phase 4 complete in 39s (0m)

========================================================================
PHASE 5/5: LATENTWIRE EVALUATION
========================================================================

Evaluating trained LatentWire system...

WARNING: Best checkpoint not found, using latest
Using checkpoint: runs/full_suite/latentwire

[5a] Evaluating LatentWire on Llama...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Encoder input alignment: mode=raw | strip_anchor=yes | samples=10
Building encoder and computing Z...
Saved Z to runs/full_suite/latentwire/eval_llama/Z.pt

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6935.60it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.24s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

— Text baseline summary:
llama: EM=0.500 F1=0.694

==== LatentWire Evaluation ====
Dataset: squad
Samples: 10  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 251.4 | (Qwen): - | Latent length M: 32
Compression ratio (Llama): 7.9x | (Qwen): -x
Approx interlingua payload per example: 327680 bytes (fp32); fp16 reference: 163840 bytes; fp32 reference: 327680 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.500  F1: 0.694  |  NLL/token (gold): 12.929239363897414
Wall clock: 0.87s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 9.109343347095308
       First-token acc: top1=0.000  top5=0.000
Wall clock: 0.36s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Wall clock: 0.66s

— 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 10,
  "max_new_tokens": 12,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 251.4
  },
  "compression": {
    "llama": 7.85625
  },
  "payload_bytes": 327680,
  "payload_bytes_detail": {
    "fp32": 327680,
    "fp16": 163840,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 12564
    },
    "prompt_count": 10,
    "latent_shape": [
      10,
      32,
      256
    ],
    "latent_bytes": {
      "fp32": 327680,
      "fp16": 163840
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.5,
      "f1": 0.6939560396878883,
      "nll_token": 12.929239363897414
    },
    "wall_clock_sec": 0.8719072341918945
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.109343347095308,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 9.109343347095308
    },
    "wall_clock_sec": 0.36260080337524414
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 0.6616518497467041
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.0,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      },
      "embedding_replay": false,
      "embedding_baseline_modes": []
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/full_suite/latentwire/eval_llama/predictions.jsonl

[5b] Evaluating LatentWire on Qwen...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Encoder input alignment: mode=raw | strip_anchor=yes | samples=10
Building encoder and computing Z...
Saved Z to runs/full_suite/latentwire/eval_qwen/Z.pt

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6260.16it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.34it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[Qwen/Qwen2.5-7B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

— Text baseline summary:
qwen: EM=0.400 F1=0.583

==== LatentWire Evaluation ====
Dataset: squad
Samples: 10  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): - | (Qwen): 236.5 | Latent length M: 32
Compression ratio (Llama): -x | (Qwen): 7.4x
Approx interlingua payload per example: 327680 bytes (fp32); fp16 reference: 163840 bytes; fp32 reference: 327680 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Qwen   EM: 0.400   F1: 0.583   |  NLL/token (gold): 21.862741708755493
Wall clock: 1.13s

— Latent prompting (shared interlingua)
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 10.74841758939955
       First-token acc: top1=0.000  top5=0.000
Wall clock: 0.32s

— Token-budget baseline (mode: content_only)
Qwen   EM: 0.100   F1: 0.195
Wall clock: 0.32s

— 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 10,
  "max_new_tokens": 12,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "qwen": 236.5
  },
  "compression": {
    "qwen": 7.390625
  },
  "payload_bytes": 327680,
  "payload_bytes_detail": {
    "fp32": 327680,
    "fp16": 163840,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "qwen": 11044
    },
    "prompt_count": 10,
    "latent_shape": [
      10,
      32,
      256
    ],
    "latent_bytes": {
      "fp32": 327680,
      "fp16": 163840
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "qwen": {
      "em": 0.4,
      "f1": 0.5833333298333334,
      "nll_token": 21.862741708755493
    },
    "wall_clock_sec": 1.1322762966156006
  },
  "latent": {
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.74841758939955,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 10.74841758939955
    },
    "wall_clock_sec": 0.32138705253601074
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "qwen": {
      "em": 0.1,
      "f1": 0.1953846139785799
    },
    "wall_clock_sec": 0.318178653717041
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "qwen": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.0,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      },
      "embedding_replay": false,
      "embedding_baseline_modes": []
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/full_suite/latentwire/eval_qwen/predictions.jsonl

✓ Phase 5 complete in 30s (0m)

========================================================================
COMPREHENSIVE ANALYSIS
========================================================================

Analyzing all results...

Loading results...

================================================================================
COMPREHENSIVE EXPERIMENT RESULTS
================================================================================

Analysis timestamp: 2025-10-12T20:02:02.542992
Results directory: runs/full_suite

================================================================================
RESULTS SUMMARY
================================================================================

Experiment                                EM       F1 Status    
--------------------------------------------------------------------------------
TEXT BASELINE (Upper Bound):
  Llama                                0.00%   26.92% success   
  Qwen                                 0.00%   17.72% success   

TOKEN BUDGET BASELINE (Fair Comparison):
  Llama                                0.00%    0.00% success   
  Qwen                                 0.00%    0.00% success   

PCA BASELINE (Linear Compression):
  Llama (PCA-compressed)              0.00%    1.43% success   

LATENTWIRE (Learned Compression):

================================================================================
KEY COMPARISONS
================================================================================

1. LatentWire vs Token Budget (same M) - CRITICAL TEST
   Does learned compression beat simple truncation?

   Data not available for comparison

2. LatentWire vs PCA
   Is non-linear encoding necessary?

   Data not available for comparison

3. LatentWire vs Text Baseline
   How much information loss from compression?

   Data not available for comparison

================================================================================
CROSS-MODEL GENERALIZATION
================================================================================

Does the learned interlingua work for both models?

   Data not available for cross-model comparison

================================================================================
OVERALL CONCLUSION
================================================================================


   🎉 SUCCESS: Compressed interlingua is working!
      Learned compression beats baselines and retains quality.

================================================================================

Report saved to: runs/full_suite/comparison_report.txt
JSON summary saved to: runs/full_suite/comparison_report.json


========================================================================
EXPERIMENT SUITE COMPLETE!
========================================================================

Total time: 207s (3m / 0h)

Results saved to: runs/full_suite
  - baselines/text_llama/
  - baselines/text_qwen/
  - baselines/token_budget_llama_m32/
  - baselines/token_budget_qwen_m32/
  - baselines/pca_m32/
  - latentwire/ (checkpoints)
  - latentwire/eval_llama/
  - latentwire/eval_qwen/
  - comparison_report.json
  - comparison_report.txt

View report:
  cat runs/full_suite/comparison_report.txt

