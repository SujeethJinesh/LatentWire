================================================================================
COMPREHENSIVE EXPERIMENT RESULTS
================================================================================

Analysis timestamp: 2025-10-12T22:52:48.871745
Results directory: runs/full_suite

================================================================================
RESULTS SUMMARY
================================================================================

Experiment                                EM       F1 Status    
--------------------------------------------------------------------------------
TEXT BASELINE (Upper Bound):
  Llama                                0.00%   26.92% success   

TOKEN BUDGET BASELINE (Fair Comparison):
  Llama                                0.00%    0.00% success   

PCA BASELINE (Linear Compression):
  Llama (PCA-compressed)              0.00%    1.77% success   

LATENTWIRE (Learned Compression):
  Llama                                0.00%    0.00% success   

================================================================================
KEY COMPARISONS
================================================================================

1. LatentWire vs Token Budget (same M) - CRITICAL TEST
   Does learned compression beat simple truncation?

   Llama:
     LatentWire:   F1 = 0.00%
     Token Budget: F1 = 0.00%
     Difference:   +0.00% (+0.0% relative)
     ✗ FAILURE: Not learning to compress effectively

2. LatentWire vs PCA
   Is non-linear encoding necessary?

   LatentWire: F1 = 0.00%
   PCA:        F1 = 1.77%
   Difference: -1.77% (-100.0% relative)
   ✗ Linear compression (PCA) is competitive

3. LatentWire vs Text Baseline
   How much information loss from compression?

   Text (full):  F1 = 26.92%
   LatentWire:   F1 = 0.00%
   Retention:    0.0% of full-text performance
   ✗ Poor: Severe information loss

================================================================================
CROSS-MODEL GENERALIZATION
================================================================================

Does the learned interlingua work for both models?

   Data not available for cross-model comparison

================================================================================
OVERALL CONCLUSION
================================================================================

   ✗ Does not beat token budget baseline
   ~ PCA is competitive (linear may suffice)
   ⚠ Only retains 0% of full-text performance

   ✗ NEEDS WORK: Compressed interlingua not yet effective.
     Debug training, check embeddings, tune hyperparameters.

================================================================================