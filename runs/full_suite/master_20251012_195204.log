Starting experiment suite at Sun Oct 12 19:52:04 PDT 2025

========================================================================
PHASE 1/5: TEXT BASELINE - Upper Bound
========================================================================

Evaluating both LLMs with full text prompts...
This establishes the best possible performance.

[1a] Running Llama text baseline...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3739.07it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.56s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.33s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.52s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with full text prompts...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Samples: 10
  EM: 0.00%
  F1: 26.92%
================================================================================

Results saved to runs/full_suite/baselines/text_llama/results.json
Total time: 25.9s


[1b] Running Qwen text baseline...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Text Baseline Evaluation
Model: Qwen/Qwen2.5-7B-Instruct
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3536.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.38s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with full text prompts...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: Qwen/Qwen2.5-7B-Instruct
  Samples: 10
  EM: 0.00%
  F1: 17.72%
================================================================================

Results saved to runs/full_suite/baselines/text_qwen/results.json
Total time: 21.9s


✓ Phase 1 complete in 55s (0m)

========================================================================
PHASE 2/5: TOKEN BUDGET BASELINE - Fair Comparison
========================================================================

Evaluating with text truncated to M=32 tokens...
This is the critical fairness baseline.

[2a] Running Llama token budget (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7547.11it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.58s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with text truncated to 32 tokens...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: meta-llama/Meta-Llama-3.1-8B-Instruct
  Token Budget: 32
  Samples: 10
  EM: 0.00%
  F1: 0.00%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_llama_m32/results.json
Total time: 23.3s


[2b] Running Qwen token budget (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Token Budget Baseline Evaluation
Model: Qwen/Qwen2.5-7B-Instruct
Token Budget: 32
Device: cuda
GPUs: 4

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4380.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:10,  3.53s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:10<00:03,  3.56s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Generating answers with text truncated to 32 tokens...
  0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Model: Qwen/Qwen2.5-7B-Instruct
  Token Budget: 32
  Samples: 10
  EM: 0.00%
  F1: 0.00%
================================================================================

Results saved to runs/full_suite/baselines/token_budget_qwen_m32/results.json
Total time: 22.4s


✓ Phase 2 complete in 52s (0m)

========================================================================
PHASE 3/5: PCA BASELINE - Linear Compression
========================================================================

Testing if PCA (linear projection) is sufficient...

[3] Running PCA baseline (M=32)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda
GPUs: 4

Loading meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6170.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:13<00:04,  4.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Model loaded!

Loading 10 examples from squad...
Loaded 10 examples

Extracting text embeddings for PCA...
  Processing 0/10...
  Collected 1724 token embeddings
  Embedding dim: 4096

Fitting PCA to reduce 4096D → 32D...
  PCA fitted!
  Explained variance: 25.11%
  First 5 components: [0.03828776 0.02112162 0.0141998  0.01213679 0.01053391]

Saved PCA to runs/full_suite/baselines/pca_m32/pca_M32.pkl

================================================================================
EVALUATION: Generating answers with PCA-compressed embeddings
================================================================================

  Example 0/10...

Computing metrics...

================================================================================
RESULTS
================================================================================
  Samples: 10
  Latent dim (M): 32
  PCA explained variance: 25.11%
  EM: 0.00%
  F1: 0.00%
================================================================================

Results saved to runs/full_suite/baselines/pca_m32/results.json
Total time: 24.2s

INTERPRETATION:
  If PCA F1 ≈ Text baseline → Linear compression is sufficient
  If PCA F1 << Text baseline → Need learned non-linear encoder


✓ Phase 3 complete in 28s (0m)

========================================================================
PHASE 4/5: LATENTWIRE TRAINING - Learned Compression
========================================================================

Training encoder + adapters for compressed interlingua...
  Samples: 100
  Epochs: 1
  Batch size: 8
  Latent: M=32, d_z=256

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6410.86it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5053.38it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.48it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
[Qwen/Qwen2.5-7B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Llama hidden size: 4096, Qwen hidden size: 3584
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[DeviceMap] Qwen : {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:6.2GB(7%), GPU1:8.6GB(10%), GPU2:8.6GB(10%), GPU3:8.0GB(9%) | Total: 31.4GB allocated, 31.5GB reserved, 308.6GB free, Peak: 31.4GB
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[INFO] llama anchor tokens: 3
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
[INFO] qwen anchor tokens: 3
[Optimizer] Gathering latent adapter parameters (direct wrappers fallback)...
[Optimizer]   Meta-Llama-3.1-8B-Instruct: skipped (use_latent_adapters=False)
[Optimizer]   Qwen2.5-7B-Instruct: skipped (use_latent_adapters=False)
[Optimizer] Latent adapter summary: 0 params in 0 tensors
[Optimizer] No latent adapters enabled (expected)
[Optimization] Using fused AdamW optimizer
[Optimizer] Created 3 parameter groups:
  [1] encoder(90 tensors)
  [2] llama_adapter(20 tensors)
  [3] qwen_adapter(20 tensors)
[INFO] LR scheduler: CosineAnnealingLR (T_max=12, eta_min=2.00e-06)
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
[Epoch 1 Start] [GPU Memory] GPU0:6.3GB(14%), GPU1:8.6GB(10%), GPU2:8.6GB(10%), GPU3:8.0GB(9%) | Total: 31.4GB allocated, 37.1GB reserved, 303.0GB free, Peak: 34.5GB
    [Memory after encoder] 31.9GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] KD teacher: adapters NOT disabled - KD may be contaminated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 31.9GB allocated, peak 70.4GB
    [Memory after optimizer] 32.0GB allocated
  [Step 1] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(22%), GPU2:8.6GB(22%), GPU3:8.1GB(21%) | Total: 32.0GB allocated, 73.4GB reserved, 266.7GB free, Peak: 70.4GB
    [Memory after encoder] 32.5GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 32.2GB allocated, peak 70.4GB
    [Memory after optimizer] 32.2GB allocated
  [Step 2] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 74.5GB reserved, 265.6GB free, Peak: 70.4GB
    [Memory after encoder] 32.5GB allocated
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
    [Memory after backward] 32.2GB allocated, peak 70.4GB
    [Memory after optimizer] 32.2GB allocated
  [Step 3] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 74.9GB reserved, 265.2GB free, Peak: 70.4GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 4] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 75.2GB reserved, 264.9GB free, Peak: 70.5GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 5] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(22%) | Total: 32.0GB allocated, 75.4GB reserved, 264.7GB free, Peak: 70.5GB
  [Batch Size Suggestion after 5 steps] Low peak memory (20.7%), can cautiously increase batch size
    Current: 8, Suggested: 9
    To apply: set BATCH_SIZE_STAGEA/B=9 in run script
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 6] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.9GB free, Peak: 70.5GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 7] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.9GB free, Peak: 70.5GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 8] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.9GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 9] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.8GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  step  10/13 | grad_norm=11.24 | sec/step~1.12 | lr=8.56e-06 | keep=1.00 | K=4 | llama(L): tf=9.6966 first=9.3113 kCE=8.7074 KD=6.3462 acc=0.000 align=0.0000 | scale_pen(llama)=3.6380e-12 | qwen(L): tf=10.8957 first=15.0199 kCE=8.0629 KD=13.3454 acc=0.125 [✓'1'] align=0.0000 | scale_pen(qwen)=3.2063e-11 | feature_grads[encoder=5.137e+00, adapter_llama=8.190e+00, adapter_qwen=5.729e+00] | K=4 tau=2.00
  [Step 10] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.8GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 11] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.8GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  [Step 12] [GPU Memory] GPU0:6.6GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 32.0GB allocated, 76.3GB reserved, 263.8GB free, Peak: 70.8GB
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
[WARN] Failed to disable KD teacher adapters: No adapter loaded. Please load an adapter first.
  step  13/13 | grad_norm=16.09 | sec/step~1.88 | lr=3.67e-06 | keep=1.00 | K=4 | llama(L): tf=10.5624 first=8.5344 kCE=8.3978 KD=4.2788 acc=0.000 align=0.0000 | scale_pen(llama)=9.6065e-12 | qwen(L): tf=12.9869 first=13.9338 kCE=9.5663 KD=15.5198 acc=0.250 [✓'1'] align=0.0000 | scale_pen(qwen)=3.6241e-11 | feature_grads[encoder=7.947e+00, adapter_llama=9.396e+00, adapter_qwen=1.037e+01] | K=4 tau=2.00
  [Step 13] [GPU Memory] GPU0:6.5GB(20%), GPU1:8.6GB(23%), GPU2:8.6GB(23%), GPU3:8.1GB(23%) | Total: 31.9GB allocated, 76.8GB reserved, 263.3GB free, Peak: 70.8GB
[checkpoint] Freed 1.0KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, adapter_qwen.pt
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/full_suite/latentwire

✓ Phase 4 complete in 42s (0m)

========================================================================
PHASE 5/5: LATENTWIRE EVALUATION
========================================================================

Evaluating trained LatentWire system...

WARNING: Best checkpoint not found, using latest
Using checkpoint: runs/full_suite/latentwire

[5a] Evaluating LatentWire on Llama...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: eval.py [-h] --ckpt CKPT [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
               [--llama_device_map LLAMA_DEVICE_MAP]
               [--qwen_device_map QWEN_DEVICE_MAP]
               [--dataset {hotpot,squad,squad_v2}] [--samples SAMPLES]
               [--max_new_tokens MAX_NEW_TOKENS] [--load_4bit]
               [--device DEVICE] [--hotpot_config HOTPOT_CONFIG]
               [--out_dir OUT_DIR]
               [--token_budget_mode {chat_full,content_only}]
               [--token_budget_k TOKEN_BUDGET_K] [--gpu_mem_gib GPU_MEM_GIB]
               [--latent_anchor_mode {auto,chat,text,none}]
               [--latent_anchor_text LATENT_ANCHOR_TEXT]
               [--append_bos_after_prefix {auto,yes,no}] [--skip_prefix_acc]
               [--use_chat_template {yes,no}] [--sequential_eval]
               [--models MODELS] [--chunk_size CHUNK_SIZE]
               [--hf_encoder_id HF_ENCODER_ID]
               [--max_enc_tokens MAX_ENC_TOKENS] [--fresh_eval]
               [--embedding_replay]
               [--embedding_baseline_modes EMBEDDING_BASELINE_MODES] [--debug]
               [--debug_print_first DEBUG_PRINT_FIRST]
               [--debug_topk DEBUG_TOPK]
               [--debug_topk_examples DEBUG_TOPK_EXAMPLES]
               [--min_new_tokens MIN_NEW_TOKENS]
               [--eos_ban_steps EOS_BAN_STEPS]
               [--first_token_top_p FIRST_TOKEN_TOP_P]
               [--first_token_temperature FIRST_TOKEN_TEMPERATURE]
               [--latent_quant_bits {16,8,6,4}]
               [--latent_quant_group_size LATENT_QUANT_GROUP_SIZE]
               [--latent_quant_scale_bits LATENT_QUANT_SCALE_BITS]
               [--prefix_gain PREFIX_GAIN]
               [--calibration {none,embed_rms,fixed,train_stats}]
               [--prefix_target_rms PREFIX_TARGET_RMS]
               [--adapter_dropout ADAPTER_DROPOUT]
               [--encoder_text_mode {auto,raw,neutral_chat,llama_chat,qwen_chat}]
               [--seed SEED]
eval.py: error: unrecognized arguments: --save_dir runs/full_suite/latentwire/eval_llama
