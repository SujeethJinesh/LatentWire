
>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=hero

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2653.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⚠️  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 72 steps
Epoch 1/6
[warmup] step=0 mode=text (warm-up)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/286 | (warm-up text) | align=0.0002 | text_tf=17.0353 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/286 | (warm-up text) | align=0.0002 | text_tf=15.2612 | latent_scale=0.01
[warmup] step=2 mode=text (warm-up)
  step  3/286 | (warm-up text) | align=0.0002 | text_tf=16.4562 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/286 | (warm-up text) | align=0.0002 | text_tf=14.7206 | latent_scale=0.02
[warmup] step=4 mode=text (warm-up)
  step  5/286 | (warm-up text) | align=0.0002 | text_tf=16.2263 | latent_scale=0.03
[warmup] step=5 mode=text (warm-up)
  step  6/286 | (warm-up text) | align=0.0002 | text_tf=13.0608 | latent_scale=0.03
[warmup] step=6 mode=text (warm-up)
  step  7/286 | (warm-up text) | align=0.0002 | text_tf=14.1786 | latent_scale=0.04
[warmup] step=7 mode=text (warm-up)
  step  8/286 | (warm-up text) | align=0.0002 | text_tf=14.1742 | latent_scale=0.05
[warmup] step=8 mode=text (warm-up)
  step  9/286 | (warm-up text) | align=0.0002 | text_tf=15.7358 | latent_scale=0.06
[warmup] step=9 mode=text (warm-up)
  step  10/286 | (warm-up text) | align=0.0002 | text_tf=14.2791 | latent_scale=0.06
  step  10/286 | grad_norm=11.04 | sec/step~5.65 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=0.9253 first=0.8956 kCE=0.9219 KD=0.0000 acc=0.000 state=0.8358 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  11/286 | (warm-up text) | align=0.0002 | text_tf=16.7581 | latent_scale=0.07
  step  12/286 | (warm-up text) | align=0.0002 | text_tf=15.9838 | latent_scale=0.08
  step  13/286 | (warm-up text) | align=0.0002 | text_tf=13.5855 | latent_scale=0.08
  step  14/286 | (warm-up text) | align=0.0002 | text_tf=15.1276 | latent_scale=0.09
  step  15/286 | (warm-up text) | align=0.0002 | text_tf=15.0641 | latent_scale=0.10
  step  16/286 | (warm-up text) | align=0.0002 | text_tf=12.0901 | latent_scale=0.10
  step  17/286 | (warm-up text) | align=0.0002 | text_tf=13.8445 | latent_scale=0.11
  step  18/286 | (warm-up text) | align=0.0002 | text_tf=11.6775 | latent_scale=0.12
  step  19/286 | (warm-up text) | align=0.0002 | text_tf=13.4289 | latent_scale=0.12
  step  20/286 | (warm-up text) | align=0.0002 | text_tf=13.6131 | latent_scale=0.13
  step  20/286 | grad_norm=79.14 | sec/step~6.13 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=5.0958 first=5.0874 kCE=6.7679 KD=0.0000 acc=0.000 state=1.9463 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  21/286 | (warm-up text) | align=0.0002 | text_tf=13.2673 | latent_scale=0.14
  step  22/286 | (warm-up text) | align=0.0002 | text_tf=13.2701 | latent_scale=0.15
  step  23/286 | (warm-up text) | align=0.0002 | text_tf=15.4642 | latent_scale=0.15
  step  24/286 | (warm-up text) | align=0.0002 | text_tf=13.0951 | latent_scale=0.16
  step  25/286 | (warm-up text) | align=0.0002 | text_tf=14.3320 | latent_scale=0.17
  step  26/286 | (warm-up text) | align=0.0002 | text_tf=12.0079 | latent_scale=0.17
  step  27/286 | (warm-up text) | align=0.0002 | text_tf=13.9639 | latent_scale=0.18
  step  28/286 | (warm-up text) | align=0.0002 | text_tf=14.7691 | latent_scale=0.19
  step  29/286 | (warm-up text) | align=0.0002 | text_tf=9.4885 | latent_scale=0.19
  step  30/286 | (warm-up text) | align=0.0002 | text_tf=10.2495 | latent_scale=0.20
  step  30/286 | grad_norm=15.43 | sec/step~5.83 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=4.1395 first=4.0326 kCE=5.9552 KD=0.0000 acc=0.000 state=2.8451 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  31/286 | (warm-up text) | align=0.0002 | text_tf=10.9408 | latent_scale=0.21
  step  32/286 | (warm-up text) | align=0.0002 | text_tf=11.1937 | latent_scale=0.22
  step  33/286 | (warm-up text) | align=0.0002 | text_tf=11.2423 | latent_scale=0.22
  step  34/286 | (warm-up text) | align=0.0002 | text_tf=10.7325 | latent_scale=0.23
  step  35/286 | (warm-up text) | align=0.0002 | text_tf=11.5816 | latent_scale=0.24
  step  36/286 | (warm-up text) | align=0.0002 | text_tf=10.7645 | latent_scale=0.24
  step  37/286 | (warm-up text) | align=0.0002 | text_tf=11.7692 | latent_scale=0.25
  step  38/286 | (warm-up text) | align=0.0002 | text_tf=11.3940 | latent_scale=0.26
  step  39/286 | (warm-up text) | align=0.0002 | text_tf=10.9872 | latent_scale=0.26
  step  40/286 | (warm-up text) | align=0.0002 | text_tf=10.7717 | latent_scale=0.27
  step  40/286 | grad_norm=101.49 | sec/step~6.96 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=6.1637 first=5.0032 kCE=8.0180 KD=0.0000 acc=0.000 state=3.8227 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  41/286 | (warm-up text) | align=0.0002 | text_tf=9.5657 | latent_scale=0.28
  step  42/286 | (warm-up text) | align=0.0002 | text_tf=12.3194 | latent_scale=0.28
  step  43/286 | (warm-up text) | align=0.0002 | text_tf=9.2695 | latent_scale=0.29
  step  44/286 | (warm-up text) | align=0.0002 | text_tf=9.7101 | latent_scale=0.30
  step  45/286 | (warm-up text) | align=0.0002 | text_tf=9.2396 | latent_scale=0.31
  step  46/286 | (warm-up text) | align=0.0002 | text_tf=9.2033 | latent_scale=0.31
  step  47/286 | (warm-up text) | align=0.0002 | text_tf=9.1881 | latent_scale=0.32
  step  48/286 | (warm-up text) | align=0.0002 | text_tf=10.2592 | latent_scale=0.33
  step  49/286 | (warm-up text) | align=0.0002 | text_tf=10.7027 | latent_scale=0.33
  step  50/286 | (warm-up text) | align=0.0002 | text_tf=9.7803 | latent_scale=0.34
  step  50/286 | grad_norm=66.09 | sec/step~6.89 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=4.9800 first=3.6009 kCE=5.7352 KD=0.0000 acc=0.000 state=5.2464 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  51/286 | (warm-up text) | align=0.0002 | text_tf=10.0123 | latent_scale=0.35
  step  52/286 | (warm-up text) | align=0.0002 | text_tf=10.3788 | latent_scale=0.35
  step  53/286 | (warm-up text) | align=0.0002 | text_tf=9.6999 | latent_scale=0.36
  step  54/286 | (warm-up text) | align=0.0002 | text_tf=9.8978 | latent_scale=0.37
  step  55/286 | (warm-up text) | align=0.0002 | text_tf=8.6992 | latent_scale=0.38
  step  56/286 | (warm-up text) | align=0.0002 | text_tf=9.7133 | latent_scale=0.38
  step  57/286 | (warm-up text) | align=0.0002 | text_tf=9.4838 | latent_scale=0.39
  step  58/286 | (warm-up text) | align=0.0002 | text_tf=8.6456 | latent_scale=0.40
  step  59/286 | (warm-up text) | align=0.0002 | text_tf=9.0515 | latent_scale=0.40
  step  60/286 | (warm-up text) | align=0.0002 | text_tf=8.8755 | latent_scale=0.41
  step  60/286 | grad_norm=23.26 | sec/step~6.99 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=6.0281 first=4.7775 kCE=5.6651 KD=0.0000 acc=0.000 state=6.1848 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  61/286 | (warm-up text) | align=0.0002 | text_tf=8.9680 | latent_scale=0.42
  step  62/286 | (warm-up text) | align=0.0002 | text_tf=9.3875 | latent_scale=0.42
  step  63/286 | (warm-up text) | align=0.0002 | text_tf=9.6672 | latent_scale=0.43
  step  64/286 | (warm-up text) | align=0.0002 | text_tf=9.4637 | latent_scale=0.44
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2374, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1889, in main
    text_teacher_loss, _, _ = _loss_with_text_prompt_chunked(ctx.wrapper, scaffold, targets)
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 90, in _loss_with_text_prompt_chunked
    loss, _, _ = wrapper.loss_with_text_prompt(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/models.py", line 1120, in loss_with_text_prompt
    out = self.model(**model_kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/peft_model.py", line 1850, in forward
    return self.base_model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
    return self.model.forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1000, in forward
    layer_outputs = decoder_layer(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 745, in forward
    hidden_states = self.mlp(hidden_states)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 311, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/lora/layer.py", line 771, in forward
    result = result + lora_B(lora_A(dropout(x))) * scaling
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 3 has a total capacity of 79.19 GiB of which 27.00 MiB is free. Including non-PyTorch memory, this process has 79.15 GiB memory in use. Of the allocated memory 78.08 GiB is allocated by PyTorch, and 352.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
