
>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=hero

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7225.33it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⚠️  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 84 steps
Epoch 1/6
[warmup] step=0 mode=text (warm-up)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/334 | (warm-up text) | align=0.0002 | text_tf=14.1440 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/334 | (warm-up text) | align=0.0002 | text_tf=16.1462 | latent_scale=0.01
[warmup] step=2 mode=text (warm-up)
  step  3/334 | (warm-up text) | align=0.0002 | text_tf=16.1955 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/334 | (warm-up text) | align=0.0002 | text_tf=13.8191 | latent_scale=0.02
[warmup] step=4 mode=text (warm-up)
  step  5/334 | (warm-up text) | align=0.0002 | text_tf=16.4431 | latent_scale=0.02
[warmup] step=5 mode=text (warm-up)
  step  6/334 | (warm-up text) | align=0.0002 | text_tf=13.6501 | latent_scale=0.03
[warmup] step=6 mode=text (warm-up)
  step  7/334 | (warm-up text) | align=0.0002 | text_tf=13.0606 | latent_scale=0.04
[warmup] step=7 mode=text (warm-up)
  step  8/334 | (warm-up text) | align=0.0002 | text_tf=14.0702 | latent_scale=0.04
[warmup] step=8 mode=text (warm-up)
  step  9/334 | (warm-up text) | align=0.0002 | text_tf=14.7693 | latent_scale=0.05
[warmup] step=9 mode=text (warm-up)
  step  10/334 | (warm-up text) | align=0.0002 | text_tf=14.9338 | latent_scale=0.05
  step  10/334 | grad_norm=9.70 | sec/step~5.98 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=0.7748 first=0.7090 kCE=0.7791 KD=0.0000 acc=0.000 state=0.7590 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  11/334 | (warm-up text) | align=0.0002 | text_tf=16.6695 | latent_scale=0.06
  step  12/334 | (warm-up text) | align=0.0002 | text_tf=13.7340 | latent_scale=0.07
  step  13/334 | (warm-up text) | align=0.0002 | text_tf=17.3862 | latent_scale=0.07
  step  14/334 | (warm-up text) | align=0.0002 | text_tf=15.8871 | latent_scale=0.08
  step  15/334 | (warm-up text) | align=0.0002 | text_tf=12.4290 | latent_scale=0.08
  step  16/334 | (warm-up text) | align=0.0002 | text_tf=13.4528 | latent_scale=0.09
  step  17/334 | (warm-up text) | align=0.0002 | text_tf=15.1455 | latent_scale=0.10
  step  18/334 | (warm-up text) | align=0.0002 | text_tf=13.0329 | latent_scale=0.10
  step  19/334 | (warm-up text) | align=0.0002 | text_tf=13.3482 | latent_scale=0.11
  step  20/334 | (warm-up text) | align=0.0002 | text_tf=12.5245 | latent_scale=0.11
  step  20/334 | grad_norm=68.26 | sec/step~5.90 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=4.2643 first=4.0747 kCE=5.7345 KD=0.0000 acc=0.000 state=1.6337 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  21/334 | (warm-up text) | align=0.0002 | text_tf=11.4630 | latent_scale=0.12
  step  22/334 | (warm-up text) | align=0.0002 | text_tf=13.2524 | latent_scale=0.12
  step  23/334 | (warm-up text) | align=0.0002 | text_tf=14.2178 | latent_scale=0.13
  step  24/334 | (warm-up text) | align=0.0002 | text_tf=12.4431 | latent_scale=0.14
  step  25/334 | (warm-up text) | align=0.0002 | text_tf=13.1236 | latent_scale=0.14
  step  26/334 | (warm-up text) | align=0.0002 | text_tf=15.2182 | latent_scale=0.15
  step  27/334 | (warm-up text) | align=0.0002 | text_tf=15.0127 | latent_scale=0.15
  step  28/334 | (warm-up text) | align=0.0002 | text_tf=13.5143 | latent_scale=0.16
  step  29/334 | (warm-up text) | align=0.0002 | text_tf=11.1611 | latent_scale=0.17
  step  30/334 | (warm-up text) | align=0.0002 | text_tf=10.5535 | latent_scale=0.17
  step  30/334 | grad_norm=14.90 | sec/step~6.25 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=3.8874 first=3.3225 kCE=4.9568 KD=0.0000 acc=0.000 state=2.3933 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  31/334 | (warm-up text) | align=0.0002 | text_tf=11.4586 | latent_scale=0.18
  step  32/334 | (warm-up text) | align=0.0002 | text_tf=11.1695 | latent_scale=0.18
  step  33/334 | (warm-up text) | align=0.0002 | text_tf=10.2919 | latent_scale=0.19
  step  34/334 | (warm-up text) | align=0.0002 | text_tf=10.1658 | latent_scale=0.20
  step  35/334 | (warm-up text) | align=0.0002 | text_tf=10.2722 | latent_scale=0.20
  step  36/334 | (warm-up text) | align=0.0002 | text_tf=10.8611 | latent_scale=0.21
  step  37/334 | (warm-up text) | align=0.0002 | text_tf=11.4092 | latent_scale=0.21
  step  38/334 | (warm-up text) | align=0.0002 | text_tf=11.3575 | latent_scale=0.22
  step  39/334 | (warm-up text) | align=0.0002 | text_tf=10.6854 | latent_scale=0.23
  step  40/334 | (warm-up text) | align=0.0002 | text_tf=9.8275 | latent_scale=0.23
  step  40/334 | grad_norm=102.21 | sec/step~6.13 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=4.9759 first=4.6362 kCE=6.9924 KD=0.0000 acc=0.000 state=3.4844 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  41/334 | (warm-up text) | align=0.0002 | text_tf=11.9459 | latent_scale=0.24
  step  42/334 | (warm-up text) | align=0.0002 | text_tf=11.1302 | latent_scale=0.24
  step  43/334 | (warm-up text) | align=0.0002 | text_tf=10.3597 | latent_scale=0.25
  step  44/334 | (warm-up text) | align=0.0002 | text_tf=9.7843 | latent_scale=0.26
  step  45/334 | (warm-up text) | align=0.0002 | text_tf=10.6631 | latent_scale=0.26
  step  46/334 | (warm-up text) | align=0.0002 | text_tf=9.8216 | latent_scale=0.27
  step  47/334 | (warm-up text) | align=0.0002 | text_tf=10.3664 | latent_scale=0.27
  step  48/334 | (warm-up text) | align=0.0002 | text_tf=8.8489 | latent_scale=0.28
  step  49/334 | (warm-up text) | align=0.0002 | text_tf=10.6818 | latent_scale=0.29
  step  50/334 | (warm-up text) | align=0.0002 | text_tf=9.5410 | latent_scale=0.29
  step  50/334 | grad_norm=40.35 | sec/step~6.52 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=4.3640 first=3.0013 kCE=5.0173 KD=0.0000 acc=0.000 state=4.4863 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  51/334 | (warm-up text) | align=0.0002 | text_tf=8.9104 | latent_scale=0.30
  step  52/334 | (warm-up text) | align=0.0002 | text_tf=9.4364 | latent_scale=0.30
  step  53/334 | (warm-up text) | align=0.0002 | text_tf=8.7956 | latent_scale=0.31
  step  54/334 | (warm-up text) | align=0.0002 | text_tf=9.5876 | latent_scale=0.32
  step  55/334 | (warm-up text) | align=0.0002 | text_tf=9.8029 | latent_scale=0.32
  step  56/334 | (warm-up text) | align=0.0002 | text_tf=10.5246 | latent_scale=0.33
  step  57/334 | (warm-up text) | align=0.0002 | text_tf=9.8110 | latent_scale=0.33
  step  58/334 | (warm-up text) | align=0.0002 | text_tf=10.0541 | latent_scale=0.34
  step  59/334 | (warm-up text) | align=0.0002 | text_tf=9.5101 | latent_scale=0.35
  step  60/334 | (warm-up text) | align=0.0002 | text_tf=10.3782 | latent_scale=0.35
  step  60/334 | grad_norm=20.74 | sec/step~5.97 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=5.3851 first=4.5227 kCE=5.0569 KD=0.0000 acc=0.000 state=5.0872 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  61/334 | (warm-up text) | align=0.0002 | text_tf=9.2659 | latent_scale=0.36
  step  62/334 | (warm-up text) | align=0.0002 | text_tf=8.9871 | latent_scale=0.36
  step  63/334 | (warm-up text) | align=0.0002 | text_tf=9.6769 | latent_scale=0.37
  step  64/334 | (warm-up text) | align=0.0002 | text_tf=7.9844 | latent_scale=0.38
  step  65/334 | (warm-up text) | align=0.0002 | text_tf=10.2835 | latent_scale=0.38
  step  66/334 | (warm-up text) | align=0.0002 | text_tf=9.0365 | latent_scale=0.39
  step  67/334 | (warm-up text) | align=0.0002 | text_tf=9.0490 | latent_scale=0.39
  step  68/334 | (warm-up text) | align=0.0002 | text_tf=9.5648 | latent_scale=0.40
  step  69/334 | (warm-up text) | align=0.0002 | text_tf=8.1671 | latent_scale=0.40
  step  70/334 | (warm-up text) | align=0.0002 | text_tf=9.2853 | latent_scale=0.41
  step  70/334 | grad_norm=77.52 | sec/step~6.58 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=6.2586 first=5.2025 kCE=5.8161 KD=0.0000 acc=0.000 state=5.7681 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  71/334 | (warm-up text) | align=0.0002 | text_tf=8.6811 | latent_scale=0.42
  step  72/334 | (warm-up text) | align=0.0002 | text_tf=9.4229 | latent_scale=0.42
  step  73/334 | (warm-up text) | align=0.0002 | text_tf=8.3811 | latent_scale=0.43
  step  74/334 | (warm-up text) | align=0.0002 | text_tf=9.7552 | latent_scale=0.43
  step  75/334 | (warm-up text) | align=0.0002 | text_tf=9.3400 | latent_scale=0.44
  step  76/334 | (warm-up text) | align=0.0002 | text_tf=8.5320 | latent_scale=0.45
  step  77/334 | (warm-up text) | align=0.0002 | text_tf=8.2473 | latent_scale=0.45
  step  78/334 | (warm-up text) | align=0.0002 | text_tf=7.7942 | latent_scale=0.46
  step  79/334 | (warm-up text) | align=0.0002 | text_tf=8.5767 | latent_scale=0.46
  step  80/334 | (warm-up text) | align=0.0002 | text_tf=8.9894 | latent_scale=0.47
  step  80/334 | grad_norm=51.55 | sec/step~6.35 | keep=0.70 | K=8 | first_w=8.00 | llama(T): tf=7.4854 first=5.8768 kCE=6.7372 KD=0.0000 acc=0.000 state=7.2502 align=0.0002 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  81/334 | (warm-up text) | align=0.0002 | text_tf=8.9108 | latent_scale=0.48
  step  82/334 | (warm-up text) | align=0.0002 | text_tf=9.5190 | latent_scale=0.48
  step  83/334 | (warm-up text) | align=0.0002 | text_tf=9.4642 | latent_scale=0.49
  step  84/334 | (warm-up text) | align=0.0002 | text_tf=8.9847 | latent_scale=0.49
  step  90/334 | grad_norm=52.28 | sec/step~3.78 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=13.5075 first=10.5517 kCE=13.1131 KD=26.1861 acc=0.042 state=14.1009 align=0.0000 latA=0.4991 latP=0.2478 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=18.23 | sec/step~4.30 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=11.8904 first=8.8136 kCE=13.1612 KD=23.5568 acc=0.125 state=14.7746 align=0.0000 latA=0.5009 latP=0.2479 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=103.78 | sec/step~4.55 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=12.0721 first=9.5072 kCE=13.4734 KD=23.2367 acc=0.042 state=14.2265 align=0.0000 latA=0.4998 latP=0.2480 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=40.79 | sec/step~4.54 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=11.7294 first=9.3412 kCE=12.9658 KD=22.7672 acc=0.000 state=15.1409 align=0.0000 latA=0.4987 latP=0.2482 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=13.97 | sec/step~3.87 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=11.6103 first=8.3177 kCE=12.5937 KD=20.7673 acc=0.000 state=15.4045 align=0.0000 latA=0.5033 latP=0.2483 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=42.98 | sec/step~3.92 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.9245 first=8.4197 kCE=12.4260 KD=19.9575 acc=0.000 state=14.9485 align=0.0000 latA=0.5009 latP=0.2480 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=19.85 | sec/step~3.87 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=11.0330 first=8.6555 kCE=11.8686 KD=19.8759 acc=0.000 state=15.1550 align=0.0000 latA=0.5009 latP=0.2484 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=12.63 | sec/step~4.56 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.8167 first=8.6345 kCE=11.1832 KD=18.7803 acc=0.000 state=15.3972 align=0.0000 latA=0.5004 latP=0.2486 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=6.22 | sec/step~3.80 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.8929 first=9.5762 kCE=11.0229 KD=17.5357 acc=0.000 state=13.9427 align=0.0000 latA=0.5009 latP=0.2487 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=33.04 | sec/step~4.01 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=9.7409 first=8.3141 kCE=10.4175 KD=18.7442 acc=0.083 state=14.8703 align=0.0000 latA=0.4987 latP=0.2486 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=11.73 | sec/step~3.64 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=9.5136 first=8.8261 kCE=10.5752 KD=20.5154 acc=0.000 state=14.7243 align=0.0000 latA=0.4988 latP=0.2485 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=20.59 | sec/step~4.02 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.8040 first=8.1699 kCE=11.3584 KD=15.3091 acc=0.000 state=15.1267 align=0.0000 latA=0.5007 latP=0.2486 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=71.17 | sec/step~4.15 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.9216 first=9.7600 kCE=11.5392 KD=15.4972 acc=0.000 state=14.7691 align=0.0000 latA=0.4980 latP=0.2483 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=19.55 | sec/step~4.32 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.0010 first=9.0989 kCE=10.7323 KD=15.3951 acc=0.000 state=14.5611 align=0.0000 latA=0.5020 latP=0.2484 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=9.82 | sec/step~4.26 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.0788 first=8.6637 kCE=10.3398 KD=16.5898 acc=0.083 state=14.7159 align=0.0000 latA=0.4997 latP=0.2485 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=4.96 | sec/step~4.27 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=10.4829 first=8.9174 kCE=10.2974 KD=16.1109 acc=0.042 state=14.0820 align=0.0000 latA=0.5010 latP=0.2485 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=23.68 | sec/step~3.89 | keep=0.70 | K=8 | first_w=8.00 | llama(L): tf=9.9984 first=8.6321 kCE=10.0712 KD=17.0003 acc=0.000 state=14.6353 align=0.0000 latA=0.4984 latP=0.2484 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=13.45 | sec/step~3.58 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.1473 first=8.9868 kCE=10.2870 KD=16.4655 acc=0.000 state=13.6367 align=0.0000 latA=0.4990 latP=0.2482 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=8.16 | sec/step~3.98 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.2081 first=7.7893 kCE=10.4564 KD=15.0832 acc=0.250 state=14.6039 align=0.0000 latA=0.4971 latP=0.2482 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=23.15 | sec/step~3.91 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.2370 first=7.4105 kCE=10.2506 KD=16.7663 acc=0.125 state=14.3239 align=0.0000 latA=0.4981 latP=0.2483 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=12.66 | sec/step~4.51 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.5474 first=8.5892 kCE=10.4421 KD=14.4450 acc=0.042 state=13.9779 align=0.0000 latA=0.4975 latP=0.2481 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=12.16 | sec/step~3.99 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=11.0360 first=8.3533 kCE=10.9098 KD=12.5737 acc=0.000 state=13.1486 align=0.0000 latA=0.4996 latP=0.2480 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=5.13 | sec/step~4.01 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.6804 first=8.3472 kCE=10.9355 KD=15.4001 acc=0.000 state=13.3221 align=0.0000 latA=0.4970 latP=0.2480 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=16.94 | sec/step~4.04 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.9589 first=7.8720 kCE=11.0081 KD=14.4791 acc=0.000 state=13.5290 align=0.0000 latA=0.4983 latP=0.2480 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=12.35 | sec/step~4.30 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.2385 first=7.6987 kCE=10.3865 KD=13.6245 acc=0.083 state=14.1114 align=0.0000 latA=0.4986 latP=0.2479 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=16.29 | sec/step~5.10 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.8441 first=8.3994 kCE=10.6678 KD=13.9503 acc=0.000 state=14.1133 align=0.0000 latA=0.4991 latP=0.2478 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 3.5KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 334
Epoch 2/6
  step  10/334 | grad_norm=16.01 | sec/step~4.79 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.2955 first=7.4770 kCE=10.1522 KD=14.3979 acc=0.042 state=14.9807 align=0.0000 latA=0.4963 latP=0.2479 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=9.16 | sec/step~3.78 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=9.9007 first=7.4675 kCE=9.6116 KD=14.6692 acc=0.000 state=13.3441 align=0.0000 latA=0.4991 latP=0.2479 | scale_pen(llama)=7.9936e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=4.60 | sec/step~3.78 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.4397 first=7.1061 kCE=9.8600 KD=12.6017 acc=0.125 state=13.2763 align=0.0000 latA=0.4975 latP=0.2478 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=12.97 | sec/step~3.77 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.5294 first=7.7748 kCE=10.0503 KD=13.3112 acc=0.083 state=13.9773 align=0.0000 latA=0.4975 latP=0.2478 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=11.45 | sec/step~3.90 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.1515 first=6.9591 kCE=9.4958 KD=11.3181 acc=0.125 state=13.9437 align=0.0000 latA=0.4980 latP=0.2475 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=8.13 | sec/step~3.76 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.0933 first=7.4143 kCE=9.0531 KD=12.5908 acc=0.000 state=12.1164 align=0.0000 latA=0.4963 latP=0.2475 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  70/334 | grad_norm=22.75 | sec/step~4.19 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.8405 first=7.4497 kCE=9.4426 KD=12.5544 acc=0.000 state=12.6269 align=0.0000 latA=0.4969 latP=0.2474 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  80/334 | grad_norm=17.36 | sec/step~4.01 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.2358 first=7.3403 kCE=9.2449 KD=10.5634 acc=0.042 state=14.4233 align=0.0000 latA=0.4984 latP=0.2476 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  90/334 | grad_norm=11.01 | sec/step~4.25 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=9.7277 first=7.3691 kCE=8.5826 KD=9.8625 acc=0.125 state=13.9416 align=0.0000 latA=0.4954 latP=0.2473 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=5.43 | sec/step~4.55 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.7082 first=7.9423 kCE=9.5594 KD=9.9645 acc=0.083 state=14.6910 align=0.0000 latA=0.4977 latP=0.2474 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=16.64 | sec/step~4.61 | keep=0.71 | K=8 | first_w=8.00 | llama(L): tf=10.6668 first=7.8878 kCE=8.9879 KD=10.2161 acc=0.083 state=13.8000 align=0.0000 latA=0.4957 latP=0.2474 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=15.18 | sec/step~3.71 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=10.4776 first=8.5123 kCE=8.4971 KD=9.9483 acc=0.000 state=13.9139 align=0.0000 latA=0.4978 latP=0.2470 | scale_pen(llama)=1.5667e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=11.38 | sec/step~4.86 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.8493 first=7.8435 kCE=8.2198 KD=9.2156 acc=0.042 state=14.6759 align=0.0000 latA=0.4957 latP=0.2471 | scale_pen(llama)=1.7195e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=32.05 | sec/step~4.84 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.8566 first=9.0324 kCE=7.7010 KD=9.2834 acc=0.000 state=14.3371 align=0.0000 latA=0.4976 latP=0.2472 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=27.97 | sec/step~4.16 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.5936 first=7.2515 kCE=7.7332 KD=9.7842 acc=0.042 state=13.0982 align=0.0000 latA=0.4962 latP=0.2472 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=13.83 | sec/step~4.34 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=10.1869 first=6.8605 kCE=8.1440 KD=8.7843 acc=0.042 state=12.7003 align=0.0000 latA=0.4987 latP=0.2470 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=5.74 | sec/step~3.93 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=10.7803 first=7.2591 kCE=8.3775 KD=9.8789 acc=0.042 state=11.8501 align=0.0000 latA=0.4982 latP=0.2471 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=23.22 | sec/step~4.16 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.8419 first=7.4626 kCE=7.3238 KD=7.4280 acc=0.125 state=14.1927 align=0.0000 latA=0.4976 latP=0.2469 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=15.25 | sec/step~3.91 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.2585 first=6.8901 kCE=6.7645 KD=8.6740 acc=0.083 state=13.5577 align=0.0000 latA=0.4954 latP=0.2467 | scale_pen(llama)=1.7195e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=11.92 | sec/step~4.00 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.4004 first=7.9529 kCE=6.2748 KD=7.7136 acc=0.000 state=13.6800 align=0.0000 latA=0.4983 latP=0.2468 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=33.17 | sec/step~3.99 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.7492 first=7.0474 kCE=6.9387 KD=8.8261 acc=0.083 state=13.6812 align=0.0000 latA=0.4993 latP=0.2469 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=19.78 | sec/step~3.88 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=10.0492 first=7.5459 kCE=6.9176 KD=7.5528 acc=0.042 state=13.9323 align=0.0000 latA=0.4976 latP=0.2467 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=11.59 | sec/step~4.54 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=9.8740 first=7.8447 kCE=6.8238 KD=6.9405 acc=0.042 state=14.8514 align=0.0000 latA=0.4963 latP=0.2469 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=4.91 | sec/step~4.22 | keep=0.72 | K=8 | first_w=8.00 | llama(L): tf=8.8283 first=8.1749 kCE=6.4572 KD=7.1646 acc=0.000 state=14.5775 align=0.0000 latA=0.4991 latP=0.2467 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=18.74 | sec/step~4.16 | keep=0.73 | K=8 | first_w=8.00 | llama(L): tf=9.0298 first=8.2986 kCE=6.3701 KD=7.4202 acc=0.042 state=12.9814 align=0.0000 latA=0.4979 latP=0.2465 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=14.80 | sec/step~4.82 | keep=0.73 | K=8 | first_w=8.00 | llama(L): tf=9.1875 first=7.7739 kCE=6.9875 KD=7.5836 acc=0.125 state=14.0385 align=0.0000 latA=0.4974 latP=0.2466 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=7.06 | sec/step~4.12 | keep=0.73 | K=8 | first_w=8.00 | llama(L): tf=9.0369 first=7.6253 kCE=6.2670 KD=7.2665 acc=0.000 state=13.9431 align=0.0000 latA=0.4992 latP=0.2467 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=18.82 | sec/step~4.34 | keep=0.73 | K=8 | first_w=8.00 | llama(L): tf=9.2173 first=6.6665 kCE=6.5099 KD=8.4902 acc=0.083 state=12.2579 align=0.0000 latA=0.4983 latP=0.2466 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=14.96 | sec/step~4.14 | keep=0.73 | K=8 | first_w=8.00 | llama(L): tf=9.9450 first=7.7858 kCE=5.9899 KD=7.1364 acc=0.000 state=12.8875 align=0.0000 latA=0.4981 latP=0.2463 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=11.10 | sec/step~4.01 | keep=0.73 | K=8 | first_w=7.99 | llama(L): tf=9.0627 first=7.1287 kCE=6.3813 KD=7.1836 acc=0.042 state=14.2896 align=0.0000 latA=0.4999 latP=0.2464 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=5.68 | sec/step~4.36 | keep=0.73 | K=8 | first_w=7.99 | llama(L): tf=9.3321 first=8.5849 kCE=7.1116 KD=8.0465 acc=0.000 state=13.8419 align=0.0000 latA=0.4983 latP=0.2467 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=22.23 | sec/step~3.92 | keep=0.73 | K=8 | first_w=7.98 | llama(L): tf=9.4713 first=7.7791 kCE=6.8648 KD=6.9268 acc=0.000 state=12.8063 align=0.0000 latA=0.4968 latP=0.2465 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=14.50 | sec/step~3.82 | keep=0.73 | K=8 | first_w=7.98 | llama(L): tf=9.4592 first=8.0045 kCE=7.0641 KD=6.7456 acc=0.000 state=13.8264 align=0.0000 latA=0.4955 latP=0.2463 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=21.02 | sec/step~3.05 | keep=0.73 | K=8 | first_w=7.97 | llama(L): tf=9.5941 first=8.9463 kCE=6.4647 KD=6.5807 acc=0.000 state=12.3794 align=0.0000 latA=0.4972 latP=0.2463 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 668
Epoch 3/6
  step  10/334 | grad_norm=19.05 | sec/step~3.96 | keep=0.73 | K=8 | first_w=7.96 | llama(L): tf=9.2666 first=7.5378 kCE=6.5397 KD=7.0043 acc=0.000 state=13.5912 align=0.0000 latA=0.4952 latP=0.2464 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=12.97 | sec/step~4.36 | keep=0.74 | K=8 | first_w=7.95 | llama(L): tf=9.1384 first=6.8051 kCE=6.1803 KD=6.2460 acc=0.000 state=14.4232 align=0.0000 latA=0.4983 latP=0.2462 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=5.53 | sec/step~3.80 | keep=0.74 | K=8 | first_w=7.94 | llama(L): tf=9.3192 first=7.9214 kCE=6.5951 KD=7.8075 acc=0.042 state=13.6888 align=0.0000 latA=0.4967 latP=0.2461 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=18.69 | sec/step~3.86 | keep=0.74 | K=8 | first_w=7.93 | llama(L): tf=9.8974 first=7.0303 kCE=6.4816 KD=6.7638 acc=0.042 state=13.4496 align=0.0000 latA=0.4958 latP=0.2463 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=18.15 | sec/step~3.82 | keep=0.74 | K=8 | first_w=7.92 | llama(L): tf=9.1022 first=7.6711 kCE=5.8146 KD=8.3270 acc=0.083 state=13.5454 align=0.0000 latA=0.4968 latP=0.2461 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=12.07 | sec/step~4.31 | keep=0.74 | K=8 | first_w=7.90 | llama(L): tf=9.3318 first=7.5952 kCE=6.3629 KD=7.9748 acc=0.042 state=13.8063 align=0.0000 latA=0.4950 latP=0.2463 | scale_pen(llama)=6.0041e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/334 | grad_norm=33.79 | sec/step~3.94 | keep=0.74 | K=8 | first_w=7.88 | llama(L): tf=9.3547 first=7.7435 kCE=6.1809 KD=6.8176 acc=0.083 state=12.7683 align=0.0000 latA=0.4960 latP=0.2463 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/334 | grad_norm=13.00 | sec/step~3.86 | keep=0.74 | K=8 | first_w=7.87 | llama(L): tf=8.9990 first=8.5261 kCE=5.1833 KD=7.1956 acc=0.000 state=13.2793 align=0.0000 latA=0.4983 latP=0.2462 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  90/334 | grad_norm=11.94 | sec/step~3.89 | keep=0.74 | K=8 | first_w=7.85 | llama(L): tf=9.2103 first=7.0096 kCE=5.5107 KD=6.5713 acc=0.042 state=12.9956 align=0.0000 latA=0.4982 latP=0.2463 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=5.53 | sec/step~3.99 | keep=0.74 | K=8 | first_w=7.83 | llama(L): tf=9.2064 first=7.8419 kCE=5.4160 KD=5.9924 acc=0.083 state=13.9732 align=0.0000 latA=0.4973 latP=0.2462 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=28.32 | sec/step~4.05 | keep=0.75 | K=8 | first_w=7.81 | llama(L): tf=9.4440 first=7.8847 kCE=6.1793 KD=5.8057 acc=0.042 state=13.6485 align=0.0000 latA=0.4960 latP=0.2465 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=14.78 | sec/step~3.79 | keep=0.75 | K=8 | first_w=7.79 | llama(L): tf=8.8940 first=7.9450 kCE=5.7842 KD=5.6743 acc=0.042 state=13.8237 align=0.0000 latA=0.4965 latP=0.2463 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=7.06 | sec/step~4.22 | keep=0.75 | K=8 | first_w=7.76 | llama(L): tf=8.9709 first=7.2367 kCE=5.9770 KD=6.0140 acc=0.125 state=13.4095 align=0.0000 latA=0.4970 latP=0.2461 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=20.18 | sec/step~3.90 | keep=0.75 | K=8 | first_w=7.74 | llama(L): tf=8.9378 first=7.2156 kCE=6.5318 KD=6.3668 acc=0.000 state=13.0407 align=0.0000 latA=0.4978 latP=0.2465 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=20.69 | sec/step~3.71 | keep=0.75 | K=8 | first_w=7.71 | llama(L): tf=9.0269 first=8.3352 kCE=6.0663 KD=8.1036 acc=0.042 state=12.3947 align=0.0000 latA=0.4960 latP=0.2463 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=14.43 | sec/step~3.92 | keep=0.75 | K=8 | first_w=7.69 | llama(L): tf=9.1528 first=7.5411 kCE=5.9322 KD=5.4209 acc=0.083 state=13.6494 align=0.0000 latA=0.4978 latP=0.2462 | scale_pen(llama)=2.4016e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=4.63 | sec/step~3.76 | keep=0.75 | K=8 | first_w=7.66 | llama(L): tf=9.5171 first=7.9349 kCE=5.7327 KD=8.0380 acc=0.000 state=13.0731 align=0.0000 latA=0.5001 latP=0.2462 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=21.23 | sec/step~3.70 | keep=0.75 | K=8 | first_w=7.63 | llama(L): tf=9.2389 first=7.0602 kCE=6.4006 KD=5.1010 acc=0.042 state=13.3625 align=0.0000 latA=0.4996 latP=0.2465 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=19.32 | sec/step~4.05 | keep=0.75 | K=8 | first_w=7.60 | llama(L): tf=9.3154 first=7.7035 kCE=6.0612 KD=6.8958 acc=0.000 state=12.9054 align=0.0000 latA=0.4970 latP=0.2461 | scale_pen(llama)=1.5667e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=7.25 | sec/step~3.72 | keep=0.76 | K=8 | first_w=7.57 | llama(L): tf=8.8563 first=7.0249 kCE=5.9217 KD=5.0843 acc=0.125 state=12.9774 align=0.0000 latA=0.4973 latP=0.2460 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=17.12 | sec/step~3.92 | keep=0.76 | K=8 | first_w=7.54 | llama(L): tf=9.5323 first=7.9103 kCE=5.8942 KD=5.3466 acc=0.042 state=13.4444 align=0.0000 latA=0.4993 latP=0.2462 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=11.72 | sec/step~4.29 | keep=0.76 | K=8 | first_w=7.50 | llama(L): tf=9.4644 first=7.7540 kCE=6.4171 KD=5.2553 acc=0.042 state=13.6905 align=0.0000 latA=0.4994 latP=0.2461 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=10.67 | sec/step~4.15 | keep=0.76 | K=8 | first_w=7.47 | llama(L): tf=8.8552 first=8.2320 kCE=5.8260 KD=5.1699 acc=0.042 state=13.7745 align=0.0000 latA=0.4996 latP=0.2460 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=4.31 | sec/step~3.81 | keep=0.76 | K=8 | first_w=7.44 | llama(L): tf=8.9893 first=7.1707 kCE=6.6961 KD=6.4616 acc=0.083 state=12.6078 align=0.0000 latA=0.4979 latP=0.2461 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=15.19 | sec/step~5.00 | keep=0.76 | K=8 | first_w=7.40 | llama(L): tf=8.7773 first=7.4755 kCE=4.9870 KD=5.0943 acc=0.000 state=14.4483 align=0.0000 latA=0.4991 latP=0.2457 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=14.64 | sec/step~4.24 | keep=0.76 | K=8 | first_w=7.36 | llama(L): tf=9.2921 first=7.7379 kCE=5.8721 KD=5.5789 acc=0.000 state=13.0809 align=0.0000 latA=0.4971 latP=0.2457 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=7.14 | sec/step~3.82 | keep=0.77 | K=8 | first_w=7.33 | llama(L): tf=9.1685 first=8.0539 kCE=5.7746 KD=4.7809 acc=0.000 state=13.9102 align=0.0000 latA=0.5000 latP=0.2455 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=18.84 | sec/step~3.98 | keep=0.77 | K=8 | first_w=7.29 | llama(L): tf=9.1134 first=7.9873 kCE=5.7183 KD=4.7882 acc=0.042 state=13.0553 align=0.0000 latA=0.4962 latP=0.2458 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=11.17 | sec/step~4.33 | keep=0.77 | K=8 | first_w=7.25 | llama(L): tf=9.0924 first=7.4356 kCE=5.3251 KD=5.4497 acc=0.000 state=12.3020 align=0.0000 latA=0.4963 latP=0.2454 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=8.17 | sec/step~3.77 | keep=0.77 | K=8 | first_w=7.21 | llama(L): tf=8.6598 first=7.8915 kCE=6.1145 KD=4.7938 acc=0.000 state=12.9099 align=0.0000 latA=0.4974 latP=0.2458 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=3.95 | sec/step~3.62 | keep=0.77 | K=8 | first_w=7.17 | llama(L): tf=9.2373 first=8.3511 kCE=6.1783 KD=5.8383 acc=0.000 state=12.8937 align=0.0000 latA=0.4983 latP=0.2454 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=13.86 | sec/step~3.75 | keep=0.77 | K=8 | first_w=7.12 | llama(L): tf=8.8891 first=7.1782 kCE=5.6524 KD=6.2573 acc=0.042 state=13.4759 align=0.0000 latA=0.4980 latP=0.2455 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=11.23 | sec/step~4.60 | keep=0.77 | K=8 | first_w=7.08 | llama(L): tf=8.8740 first=8.1563 kCE=4.9249 KD=4.5275 acc=0.000 state=13.4062 align=0.0000 latA=0.4982 latP=0.2452 | scale_pen(llama)=1.0267e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=14.56 | sec/step~3.02 | keep=0.77 | K=8 | first_w=7.06 | llama(L): tf=9.6071 first=8.6575 kCE=5.7599 KD=11.9352 acc=0.000 state=13.0193 align=0.0000 latA=0.4977 latP=0.2453 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1002
Epoch 4/6
  step  10/334 | grad_norm=13.31 | sec/step~4.52 | keep=0.78 | K=8 | first_w=7.02 | llama(L): tf=8.5122 first=7.5811 kCE=6.2183 KD=5.2970 acc=0.042 state=13.6175 align=0.0000 latA=0.4999 latP=0.2456 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=8.59 | sec/step~3.82 | keep=0.78 | K=8 | first_w=6.97 | llama(L): tf=9.1234 first=6.8691 kCE=5.5904 KD=4.5032 acc=0.125 state=13.0306 align=0.0000 latA=0.4970 latP=0.2453 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=4.29 | sec/step~4.27 | keep=0.78 | K=8 | first_w=6.93 | llama(L): tf=8.9854 first=7.2328 kCE=5.5876 KD=5.7936 acc=0.000 state=13.4903 align=0.0000 latA=0.5003 latP=0.2454 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=14.47 | sec/step~3.76 | keep=0.78 | K=8 | first_w=6.88 | llama(L): tf=8.7109 first=6.7093 kCE=5.7252 KD=4.2881 acc=0.125 state=12.8226 align=0.0000 latA=0.5002 latP=0.2455 | scale_pen(llama)=4.2988e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=10.60 | sec/step~4.25 | keep=0.78 | K=8 | first_w=6.83 | llama(L): tf=9.2288 first=6.9068 kCE=5.7833 KD=5.8981 acc=0.167 state=12.4948 align=0.0000 latA=0.4981 latP=0.2452 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=8.93 | sec/step~4.31 | keep=0.78 | K=8 | first_w=6.79 | llama(L): tf=9.3778 first=7.2735 kCE=5.4631 KD=4.7236 acc=0.042 state=13.0745 align=0.0000 latA=0.4982 latP=0.2450 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/334 | grad_norm=25.93 | sec/step~3.95 | keep=0.79 | K=8 | first_w=6.74 | llama(L): tf=8.8450 first=6.8705 kCE=6.1851 KD=4.3885 acc=0.125 state=13.0587 align=0.0000 latA=0.4993 latP=0.2453 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/334 | grad_norm=10.53 | sec/step~4.01 | keep=0.79 | K=8 | first_w=6.69 | llama(L): tf=8.5730 first=7.8579 kCE=5.0320 KD=5.0473 acc=0.083 state=13.4650 align=0.0000 latA=0.4955 latP=0.2450 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  90/334 | grad_norm=10.56 | sec/step~3.88 | keep=0.79 | K=8 | first_w=6.64 | llama(L): tf=9.6912 first=7.8733 kCE=4.9385 KD=5.4477 acc=0.042 state=12.9705 align=0.0000 latA=0.4957 latP=0.2449 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=4.89 | sec/step~3.82 | keep=0.79 | K=8 | first_w=6.59 | llama(L): tf=9.0508 first=7.4796 kCE=5.6401 KD=7.2905 acc=0.083 state=12.8276 align=0.0000 latA=0.4976 latP=0.2447 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=17.48 | sec/step~4.05 | keep=0.79 | K=8 | first_w=6.54 | llama(L): tf=9.0544 first=6.9043 kCE=4.6310 KD=4.2958 acc=0.042 state=12.9484 align=0.0000 latA=0.4929 latP=0.2443 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=9.00 | sec/step~4.08 | keep=0.79 | K=8 | first_w=6.49 | llama(L): tf=9.4024 first=7.5610 kCE=5.8157 KD=6.8002 acc=0.125 state=12.4144 align=0.0000 latA=0.4969 latP=0.2447 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=10.28 | sec/step~4.08 | keep=0.80 | K=8 | first_w=6.44 | llama(L): tf=8.6989 first=7.6796 kCE=5.7323 KD=5.2277 acc=0.000 state=12.8170 align=0.0000 latA=0.4958 latP=0.2451 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=35.12 | sec/step~4.05 | keep=0.80 | K=8 | first_w=6.38 | llama(L): tf=8.5905 first=7.8858 kCE=4.7834 KD=5.6185 acc=0.083 state=13.0207 align=0.0000 latA=0.4968 latP=0.2442 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=14.79 | sec/step~3.76 | keep=0.80 | K=8 | first_w=6.33 | llama(L): tf=8.9957 first=7.7650 kCE=5.4701 KD=5.6855 acc=0.083 state=12.7783 align=0.0000 latA=0.4946 latP=0.2442 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=7.20 | sec/step~4.12 | keep=0.80 | K=8 | first_w=6.28 | llama(L): tf=8.3060 first=7.2653 kCE=4.4906 KD=6.5185 acc=0.000 state=12.9456 align=0.0000 latA=0.4942 latP=0.2443 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=8.63 | sec/step~3.95 | keep=0.80 | K=8 | first_w=6.23 | llama(L): tf=8.2014 first=6.7976 kCE=5.3478 KD=5.8106 acc=0.083 state=12.9869 align=0.0000 latA=0.4940 latP=0.2441 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=24.63 | sec/step~3.78 | keep=0.80 | K=8 | first_w=6.17 | llama(L): tf=8.2172 first=6.5349 kCE=4.9408 KD=4.8884 acc=0.083 state=13.1390 align=0.0000 latA=0.4957 latP=0.2444 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=22.85 | sec/step~3.96 | keep=0.81 | K=8 | first_w=6.12 | llama(L): tf=8.9009 first=7.3714 kCE=5.3820 KD=4.3763 acc=0.083 state=12.6188 align=0.0000 latA=0.5002 latP=0.2441 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=5.67 | sec/step~4.30 | keep=0.81 | K=8 | first_w=6.06 | llama(L): tf=8.8443 first=7.4971 kCE=4.2917 KD=5.6325 acc=0.042 state=13.6951 align=0.0000 latA=0.4980 latP=0.2441 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=16.66 | sec/step~4.43 | keep=0.81 | K=8 | first_w=6.01 | llama(L): tf=8.5076 first=7.2017 kCE=4.6563 KD=4.6036 acc=0.042 state=13.3106 align=0.0000 latA=0.5001 latP=0.2438 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=22.88 | sec/step~3.64 | keep=0.81 | K=8 | first_w=5.95 | llama(L): tf=9.0513 first=6.9687 kCE=5.3770 KD=7.1522 acc=0.042 state=11.9018 align=0.0000 latA=0.4958 latP=0.2438 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=15.11 | sec/step~3.68 | keep=0.81 | K=8 | first_w=5.90 | llama(L): tf=8.5831 first=7.1034 kCE=4.9048 KD=6.4130 acc=0.083 state=12.3344 align=0.0000 latA=0.4953 latP=0.2440 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=5.13 | sec/step~3.88 | keep=0.82 | K=8 | first_w=5.84 | llama(L): tf=8.8593 first=7.3763 kCE=5.1186 KD=5.9109 acc=0.042 state=13.1792 align=0.0000 latA=0.4984 latP=0.2440 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=20.24 | sec/step~3.81 | keep=0.82 | K=8 | first_w=5.79 | llama(L): tf=8.2435 first=7.1635 kCE=5.5265 KD=4.7133 acc=0.083 state=12.7895 align=0.0000 latA=0.4975 latP=0.2439 | scale_pen(llama)=6.9633e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=19.02 | sec/step~4.20 | keep=0.82 | K=8 | first_w=5.73 | llama(L): tf=8.9584 first=7.8213 kCE=5.9897 KD=6.1309 acc=0.042 state=13.0049 align=0.0000 latA=0.4976 latP=0.2436 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=11.43 | sec/step~4.19 | keep=0.82 | K=8 | first_w=5.68 | llama(L): tf=8.6589 first=7.7520 kCE=5.5862 KD=4.3625 acc=0.042 state=13.9079 align=0.0000 latA=0.4949 latP=0.2438 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=29.15 | sec/step~4.47 | keep=0.82 | K=8 | first_w=5.62 | llama(L): tf=8.6393 first=7.2901 kCE=5.3843 KD=5.8623 acc=0.042 state=13.2295 align=0.0000 latA=0.4967 latP=0.2438 | scale_pen(llama)=6.0041e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=12.80 | sec/step~4.21 | keep=0.82 | K=8 | first_w=5.56 | llama(L): tf=8.5288 first=7.5911 kCE=5.3592 KD=4.7000 acc=0.042 state=12.9107 align=0.0000 latA=0.4957 latP=0.2437 | scale_pen(llama)=6.0041e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=6.96 | sec/step~4.15 | keep=0.83 | K=8 | first_w=5.51 | llama(L): tf=8.8494 first=7.1462 kCE=5.3196 KD=3.9362 acc=0.083 state=13.2232 align=0.0000 latA=0.4946 latP=0.2435 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=5.74 | sec/step~4.04 | keep=0.83 | K=8 | first_w=5.45 | llama(L): tf=7.9692 first=6.8824 kCE=4.6325 KD=4.6453 acc=0.083 state=12.7086 align=0.0000 latA=0.4957 latP=0.2435 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=10.52 | sec/step~4.08 | keep=0.83 | K=8 | first_w=5.40 | llama(L): tf=8.8930 first=6.7205 kCE=5.5508 KD=5.8234 acc=0.042 state=12.5184 align=0.0000 latA=0.4952 latP=0.2437 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=18.31 | sec/step~3.87 | keep=0.83 | K=8 | first_w=5.34 | llama(L): tf=8.7900 first=7.6816 kCE=5.9312 KD=4.4106 acc=0.042 state=12.3253 align=0.0000 latA=0.4973 latP=0.2435 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=25.26 | sec/step~2.98 | keep=0.83 | K=8 | first_w=5.32 | llama(L): tf=9.3245 first=7.4086 kCE=6.1815 KD=5.2651 acc=0.000 state=12.0730 align=0.0000 latA=0.5003 latP=0.2437 | scale_pen(llama)=3.6380e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1336
Epoch 5/6
  step  10/334 | grad_norm=12.23 | sec/step~4.01 | keep=0.84 | K=8 | first_w=5.26 | llama(L): tf=9.3038 first=6.9223 kCE=5.4537 KD=5.4361 acc=0.042 state=12.6183 align=0.0000 latA=0.4965 latP=0.2433 | scale_pen(llama)=3.6380e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=7.01 | sec/step~4.73 | keep=0.84 | K=8 | first_w=5.21 | llama(L): tf=8.6491 first=6.6664 kCE=5.0749 KD=5.2307 acc=0.042 state=12.5145 align=0.0000 latA=0.4942 latP=0.2436 | scale_pen(llama)=7.9936e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=3.19 | sec/step~3.56 | keep=0.84 | K=8 | first_w=5.15 | llama(L): tf=8.8088 first=7.0365 kCE=5.3257 KD=4.7353 acc=0.042 state=11.5157 align=0.0000 latA=0.4949 latP=0.2434 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=9.89 | sec/step~4.22 | keep=0.84 | K=8 | first_w=5.10 | llama(L): tf=8.8972 first=8.0054 kCE=4.8478 KD=11.0647 acc=0.042 state=13.0514 align=0.0000 latA=0.4964 latP=0.2430 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=9.36 | sec/step~4.61 | keep=0.84 | K=8 | first_w=5.04 | llama(L): tf=8.6736 first=7.1135 kCE=4.5621 KD=4.1748 acc=0.042 state=13.8066 align=0.0000 latA=0.4930 latP=0.2429 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=6.58 | sec/step~3.74 | keep=0.85 | K=8 | first_w=4.99 | llama(L): tf=8.8838 first=7.8956 kCE=5.4636 KD=3.8300 acc=0.125 state=12.9666 align=0.0000 latA=0.4944 latP=0.2428 | scale_pen(llama)=6.0041e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/334 | grad_norm=21.22 | sec/step~4.00 | keep=0.85 | K=8 | first_w=4.93 | llama(L): tf=8.4302 first=6.5217 kCE=5.0989 KD=4.4157 acc=0.083 state=12.9919 align=0.0000 latA=0.4936 latP=0.2432 | scale_pen(llama)=1.5667e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/334 | grad_norm=16.60 | sec/step~4.13 | keep=0.85 | K=8 | first_w=4.88 | llama(L): tf=8.2485 first=6.8751 kCE=4.4538 KD=3.8614 acc=0.083 state=12.1652 align=0.0000 latA=0.4958 latP=0.2428 | scale_pen(llama)=1.5667e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  90/334 | grad_norm=6.64 | sec/step~3.81 | keep=0.85 | K=8 | first_w=4.82 | llama(L): tf=8.7893 first=6.4186 kCE=5.4298 KD=3.6115 acc=0.083 state=12.6704 align=0.0000 latA=0.4953 latP=0.2439 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=7.12 | sec/step~3.92 | keep=0.85 | K=8 | first_w=4.77 | llama(L): tf=8.2567 first=7.5794 kCE=5.3300 KD=4.7928 acc=0.042 state=11.8834 align=0.0000 latA=0.4950 latP=0.2439 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=35.46 | sec/step~3.80 | keep=0.86 | K=8 | first_w=4.72 | llama(L): tf=9.0649 first=7.3986 kCE=5.2282 KD=6.0607 acc=0.000 state=11.7242 align=0.0000 latA=0.4943 latP=0.2431 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=9.50 | sec/step~3.84 | keep=0.86 | K=8 | first_w=4.66 | llama(L): tf=8.8397 first=6.4168 kCE=4.4549 KD=4.2280 acc=0.125 state=12.6639 align=0.0000 latA=0.4938 latP=0.2428 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=7.20 | sec/step~4.00 | keep=0.86 | K=8 | first_w=4.61 | llama(L): tf=8.3403 first=6.8271 kCE=4.9917 KD=4.1092 acc=0.042 state=12.2577 align=0.0000 latA=0.4943 latP=0.2429 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=14.17 | sec/step~4.03 | keep=0.86 | K=8 | first_w=4.56 | llama(L): tf=8.8997 first=7.6963 kCE=4.5452 KD=3.8395 acc=0.000 state=12.0898 align=0.0000 latA=0.4963 latP=0.2430 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=8.96 | sec/step~3.75 | keep=0.86 | K=8 | first_w=4.51 | llama(L): tf=8.6042 first=6.8125 kCE=4.9815 KD=4.0294 acc=0.083 state=12.3470 align=0.0000 latA=0.4958 latP=0.2433 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=12.86 | sec/step~3.83 | keep=0.87 | K=8 | first_w=4.46 | llama(L): tf=8.6278 first=7.1207 kCE=5.2173 KD=7.2990 acc=0.042 state=12.3185 align=0.0000 latA=0.4988 latP=0.2428 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=4.24 | sec/step~3.74 | keep=0.87 | K=8 | first_w=4.40 | llama(L): tf=8.4071 first=7.6330 kCE=5.1746 KD=3.8479 acc=0.000 state=12.3366 align=0.0000 latA=0.4952 latP=0.2435 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=14.91 | sec/step~4.13 | keep=0.87 | K=8 | first_w=4.35 | llama(L): tf=8.3961 first=7.2470 kCE=5.7618 KD=4.1598 acc=0.125 state=13.2466 align=0.0000 latA=0.4970 latP=0.2429 | scale_pen(llama)=3.1974e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=7.56 | sec/step~3.90 | keep=0.87 | K=8 | first_w=4.31 | llama(L): tf=8.5705 first=7.0354 kCE=5.1643 KD=5.2093 acc=0.042 state=13.7026 align=0.0000 latA=0.4912 latP=0.2425 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=6.08 | sec/step~4.12 | keep=0.88 | K=8 | first_w=4.26 | llama(L): tf=8.1926 first=7.1326 kCE=5.1971 KD=3.7006 acc=0.083 state=13.0218 align=0.0000 latA=0.4959 latP=0.2428 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=18.55 | sec/step~3.79 | keep=0.88 | K=8 | first_w=4.21 | llama(L): tf=8.9875 first=7.1719 kCE=5.3091 KD=3.9334 acc=0.083 state=12.9773 align=0.0000 latA=0.4954 latP=0.2430 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=11.63 | sec/step~4.61 | keep=0.88 | K=8 | first_w=4.16 | llama(L): tf=9.3407 first=6.5925 kCE=6.2670 KD=4.4453 acc=0.125 state=13.1824 align=0.0000 latA=0.4924 latP=0.2433 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=8.86 | sec/step~4.24 | keep=0.88 | K=8 | first_w=4.11 | llama(L): tf=8.4599 first=6.9233 kCE=5.0072 KD=6.9248 acc=0.042 state=12.8479 align=0.0000 latA=0.4954 latP=0.2428 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=3.49 | sec/step~4.24 | keep=0.89 | K=8 | first_w=4.07 | llama(L): tf=8.9754 first=6.9906 kCE=5.1192 KD=3.7972 acc=0.083 state=12.8064 align=0.0000 latA=0.4942 latP=0.2420 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=9.87 | sec/step~4.29 | keep=0.89 | K=8 | first_w=4.02 | llama(L): tf=8.3683 first=6.3128 kCE=5.5797 KD=4.9398 acc=0.167 state=12.1307 align=0.0000 latA=0.4949 latP=0.2423 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=18.67 | sec/step~3.49 | keep=0.89 | K=8 | first_w=3.98 | llama(L): tf=8.3813 first=6.7100 kCE=4.7546 KD=4.1259 acc=0.000 state=12.1275 align=0.0000 latA=0.4945 latP=0.2424 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=8.78 | sec/step~5.13 | keep=0.89 | K=8 | first_w=3.93 | llama(L): tf=8.4643 first=7.0283 kCE=5.6200 KD=4.4025 acc=0.042 state=14.0251 align=0.0000 latA=0.4939 latP=0.2423 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=21.19 | sec/step~4.04 | keep=0.90 | K=8 | first_w=3.89 | llama(L): tf=8.2972 first=6.8291 kCE=5.1813 KD=3.8639 acc=0.042 state=13.0536 align=0.0000 latA=0.4965 latP=0.2419 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=10.09 | sec/step~4.60 | keep=0.90 | K=8 | first_w=3.85 | llama(L): tf=8.9342 first=6.7846 kCE=5.7056 KD=3.7321 acc=0.083 state=13.0914 align=0.0000 latA=0.4940 latP=0.2423 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=5.46 | sec/step~4.02 | keep=0.90 | K=8 | first_w=3.81 | llama(L): tf=8.6676 first=6.9550 kCE=5.1821 KD=4.4730 acc=0.125 state=12.8941 align=0.0000 latA=0.4921 latP=0.2419 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=5.47 | sec/step~3.87 | keep=0.90 | K=8 | first_w=3.77 | llama(L): tf=9.2754 first=7.0294 kCE=4.8768 KD=3.7008 acc=0.042 state=11.6797 align=0.0000 latA=0.4946 latP=0.2416 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=15.51 | sec/step~4.74 | keep=0.90 | K=8 | first_w=3.73 | llama(L): tf=8.7838 first=6.8632 kCE=5.0675 KD=4.7678 acc=0.042 state=12.1814 align=0.0000 latA=0.4926 latP=0.2415 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=14.47 | sec/step~3.82 | keep=0.91 | K=8 | first_w=3.69 | llama(L): tf=8.3924 first=6.1182 kCE=5.3752 KD=4.4127 acc=0.042 state=11.9225 align=0.0000 latA=0.4944 latP=0.2421 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=20.39 | sec/step~2.74 | keep=0.91 | K=8 | first_w=3.67 | llama(L): tf=8.5299 first=6.8581 kCE=4.5448 KD=11.7244 acc=0.000 state=12.1951 align=0.0000 latA=0.4994 latP=0.2411 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1670
Epoch 6/6
  step  10/334 | grad_norm=14.15 | sec/step~3.67 | keep=0.91 | K=8 | first_w=3.63 | llama(L): tf=8.6241 first=7.0832 kCE=5.4343 KD=4.8472 acc=0.083 state=11.6865 align=0.0000 latA=0.4940 latP=0.2417 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=7.68 | sec/step~3.97 | keep=0.91 | K=8 | first_w=3.60 | llama(L): tf=8.1912 first=7.2833 kCE=4.8840 KD=4.3052 acc=0.083 state=12.2534 align=0.0000 latA=0.4949 latP=0.2410 | scale_pen(llama)=9.0949e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=3.95 | sec/step~4.20 | keep=0.92 | K=8 | first_w=3.56 | llama(L): tf=8.6842 first=7.6696 kCE=5.5313 KD=4.1067 acc=0.042 state=12.7466 align=0.0000 latA=0.4955 latP=0.2414 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=12.98 | sec/step~4.25 | keep=0.92 | K=8 | first_w=3.53 | llama(L): tf=8.2937 first=6.8849 kCE=4.9497 KD=3.9231 acc=0.000 state=12.6599 align=0.0000 latA=0.4940 latP=0.2413 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=10.68 | sec/step~3.75 | keep=0.92 | K=8 | first_w=3.49 | llama(L): tf=7.8463 first=7.1807 kCE=5.0595 KD=3.7229 acc=0.000 state=11.8349 align=0.0000 latA=0.4955 latP=0.2412 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=6.54 | sec/step~3.85 | keep=0.92 | K=8 | first_w=3.46 | llama(L): tf=8.2372 first=6.7838 kCE=5.1230 KD=3.6037 acc=0.000 state=11.8210 align=0.0000 latA=0.4961 latP=0.2411 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/334 | grad_norm=20.03 | sec/step~4.48 | keep=0.93 | K=8 | first_w=3.43 | llama(L): tf=8.1145 first=6.6636 kCE=4.4102 KD=5.6492 acc=0.042 state=12.9347 align=0.0000 latA=0.4952 latP=0.2407 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/334 | grad_norm=11.74 | sec/step~3.79 | keep=0.93 | K=8 | first_w=3.40 | llama(L): tf=8.0105 first=6.3011 kCE=4.3585 KD=3.4107 acc=0.000 state=12.9205 align=0.0000 latA=0.4919 latP=0.2405 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  90/334 | grad_norm=6.90 | sec/step~4.10 | keep=0.93 | K=8 | first_w=3.37 | llama(L): tf=8.1221 first=6.5160 kCE=4.6698 KD=6.5180 acc=0.042 state=11.8574 align=0.0000 latA=0.4929 latP=0.2407 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  100/334 | grad_norm=3.50 | sec/step~3.92 | keep=0.93 | K=8 | first_w=3.34 | llama(L): tf=8.5249 first=6.5087 kCE=4.8410 KD=3.8149 acc=0.042 state=12.6130 align=0.0000 latA=0.4926 latP=0.2415 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  110/334 | grad_norm=9.77 | sec/step~3.52 | keep=0.94 | K=8 | first_w=3.31 | llama(L): tf=8.5132 first=6.4235 kCE=4.7403 KD=4.0373 acc=0.042 state=11.4401 align=0.0000 latA=0.4942 latP=0.2408 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  120/334 | grad_norm=9.45 | sec/step~3.59 | keep=0.94 | K=8 | first_w=3.28 | llama(L): tf=8.8763 first=6.8673 kCE=5.3294 KD=3.3415 acc=0.083 state=11.1775 align=0.0000 latA=0.4941 latP=0.2405 | scale_pen(llama)=3.8689e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  130/334 | grad_norm=6.88 | sec/step~3.79 | keep=0.94 | K=8 | first_w=3.26 | llama(L): tf=7.7256 first=6.8405 kCE=5.2075 KD=5.5410 acc=0.083 state=12.4341 align=0.0000 latA=0.4950 latP=0.2412 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  140/334 | grad_norm=17.56 | sec/step~4.78 | keep=0.94 | K=8 | first_w=3.23 | llama(L): tf=8.3209 first=6.1581 kCE=4.4662 KD=3.9789 acc=0.042 state=12.9303 align=0.0000 latA=0.4886 latP=0.2408 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  150/334 | grad_norm=13.05 | sec/step~4.41 | keep=0.95 | K=8 | first_w=3.21 | llama(L): tf=8.3784 first=7.1533 kCE=5.0533 KD=4.3495 acc=0.000 state=12.5274 align=0.0000 latA=0.4953 latP=0.2407 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  160/334 | grad_norm=6.04 | sec/step~3.79 | keep=0.95 | K=8 | first_w=3.19 | llama(L): tf=8.4564 first=6.8330 kCE=5.4804 KD=4.7953 acc=0.042 state=11.9303 align=0.0000 latA=0.4965 latP=0.2406 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  170/334 | grad_norm=4.66 | sec/step~3.93 | keep=0.95 | K=8 | first_w=3.17 | llama(L): tf=8.3747 first=6.4846 kCE=4.9110 KD=4.6488 acc=0.000 state=12.8844 align=0.0000 latA=0.4947 latP=0.2400 | scale_pen(llama)=8.1855e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=12.99 | sec/step~3.93 | keep=0.96 | K=8 | first_w=3.15 | llama(L): tf=7.6902 first=7.1324 kCE=4.7878 KD=3.6657 acc=0.083 state=11.1084 align=0.0000 latA=0.4910 latP=0.2410 | scale_pen(llama)=8.1855e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=9.36 | sec/step~3.90 | keep=0.96 | K=8 | first_w=3.13 | llama(L): tf=8.0528 first=6.8362 kCE=5.0163 KD=3.7481 acc=0.042 state=12.2632 align=0.0000 latA=0.4950 latP=0.2405 | scale_pen(llama)=5.1159e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=5.01 | sec/step~4.04 | keep=0.96 | K=8 | first_w=3.11 | llama(L): tf=8.3039 first=7.4381 kCE=4.8939 KD=4.0853 acc=0.042 state=12.5151 align=0.0000 latA=0.4971 latP=0.2401 | scale_pen(llama)=3.6380e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=14.53 | sec/step~4.12 | keep=0.96 | K=8 | first_w=3.10 | llama(L): tf=7.8127 first=6.7676 kCE=4.4743 KD=3.8702 acc=0.042 state=11.3144 align=0.0000 latA=0.4935 latP=0.2399 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=7.84 | sec/step~4.69 | keep=0.97 | K=8 | first_w=3.08 | llama(L): tf=8.2200 first=7.1658 kCE=5.1649 KD=4.7541 acc=0.042 state=13.0500 align=0.0000 latA=0.4946 latP=0.2397 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=8.58 | sec/step~4.36 | keep=0.97 | K=8 | first_w=3.07 | llama(L): tf=8.5247 first=7.0138 kCE=4.5827 KD=3.5498 acc=0.083 state=10.9362 align=0.0000 latA=0.4967 latP=0.2407 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=3.37 | sec/step~4.10 | keep=0.97 | K=8 | first_w=3.06 | llama(L): tf=7.6767 first=5.8599 kCE=4.1768 KD=3.7312 acc=0.083 state=12.2687 align=0.0000 latA=0.4931 latP=0.2403 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=11.67 | sec/step~4.23 | keep=0.98 | K=8 | first_w=3.05 | llama(L): tf=8.6323 first=6.8535 kCE=5.0160 KD=3.6563 acc=0.083 state=13.0688 align=0.0000 latA=0.4912 latP=0.2400 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=8.63 | sec/step~4.40 | keep=0.98 | K=8 | first_w=3.04 | llama(L): tf=8.2733 first=7.5820 kCE=4.3722 KD=4.2295 acc=0.000 state=12.2639 align=0.0000 latA=0.4939 latP=0.2401 | scale_pen(llama)=3.5527e-15 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=4.42 | sec/step~4.21 | keep=0.98 | K=8 | first_w=3.03 | llama(L): tf=8.1193 first=6.2597 kCE=4.9235 KD=4.4713 acc=0.083 state=11.6682 align=0.0000 latA=0.4954 latP=0.2408 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=10.95 | sec/step~4.29 | keep=0.98 | K=8 | first_w=3.02 | llama(L): tf=8.2208 first=7.3060 kCE=5.1865 KD=4.1900 acc=0.042 state=11.5057 align=0.0000 latA=0.4953 latP=0.2407 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=10.42 | sec/step~3.86 | keep=0.99 | K=8 | first_w=3.01 | llama(L): tf=8.5198 first=7.2475 kCE=4.7675 KD=4.0879 acc=0.042 state=12.0729 align=0.0000 latA=0.4925 latP=0.2394 | scale_pen(llama)=2.7853e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=5.94 | sec/step~4.17 | keep=0.99 | K=8 | first_w=3.01 | llama(L): tf=8.4243 first=6.9616 kCE=4.6511 KD=3.5777 acc=0.083 state=11.4837 align=0.0000 latA=0.4941 latP=0.2399 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=3.47 | sec/step~3.63 | keep=0.99 | K=8 | first_w=3.00 | llama(L): tf=8.0192 first=6.6433 kCE=4.7682 KD=6.0872 acc=0.083 state=12.1563 align=0.0000 latA=0.4920 latP=0.2399 | scale_pen(llama)=2.9878e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=11.06 | sec/step~3.90 | keep=1.00 | K=8 | first_w=3.00 | llama(L): tf=8.2724 first=7.3928 kCE=4.6233 KD=4.6127 acc=0.042 state=11.1026 align=0.0000 latA=0.4954 latP=0.2388 | scale_pen(llama)=2.9878e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=8.37 | sec/step~4.02 | keep=1.00 | K=8 | first_w=3.00 | llama(L): tf=7.9982 first=6.3818 kCE=4.2176 KD=3.7783 acc=0.042 state=12.1947 align=0.0000 latA=0.4924 latP=0.2391 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=10.46 | sec/step~3.37 | keep=1.00 | K=8 | first_w=3.00 | llama(L): tf=8.4981 first=7.4613 kCE=5.1003 KD=5.2053 acc=0.125 state=11.7954 align=0.0000 latA=0.4895 latP=0.2389 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2004
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/hero/ckpt/stageA
📝 Saved LoRA adapters for Llama
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0002182716143107, 'rms_mean_cal': 0.010571134756756072, 'embed_rms': 0.01057521253824234, 'count': 2004}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2855.21it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
trainable params: 272,732,160 || all params: 8,344,936,448 || trainable%: 3.2682
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⏪ Resuming from: runs/hero/ckpt/stageA/state.pt
   -> loaded encoder/adapters/deep_prefix/refiner FROM state.pt
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 222 steps
Epoch 1/10
[warmup] step=0 mode=text (warm-up)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/445 | (warm-up text) | align=0.0003 | text_tf=10.9343 | latent_scale=0.20
[warmup] step=1 mode=text (warm-up)
  step  2/445 | (warm-up text) | align=0.0003 | text_tf=10.6362 | latent_scale=0.20
[warmup] step=2 mode=text (warm-up)
  step  3/445 | (warm-up text) | align=0.0003 | text_tf=10.4490 | latent_scale=0.21
[warmup] step=3 mode=text (warm-up)
  step  4/445 | (warm-up text) | align=0.0003 | text_tf=11.0938 | latent_scale=0.21
[warmup] step=4 mode=text (warm-up)
  step  5/445 | (warm-up text) | align=0.0003 | text_tf=10.4133 | latent_scale=0.21
[warmup] step=5 mode=text (warm-up)
  step  6/445 | (warm-up text) | align=0.0003 | text_tf=10.2971 | latent_scale=0.22
[warmup] step=6 mode=text (warm-up)
  step  7/445 | (warm-up text) | align=0.0003 | text_tf=10.2945 | latent_scale=0.22
[warmup] step=7 mode=text (warm-up)
  step  8/445 | (warm-up text) | align=0.0003 | text_tf=11.0442 | latent_scale=0.23
[warmup] step=8 mode=text (warm-up)
  step  9/445 | (warm-up text) | align=0.0003 | text_tf=10.4320 | latent_scale=0.23
[warmup] step=9 mode=text (warm-up)
  step  10/445 | (warm-up text) | align=0.0003 | text_tf=10.6761 | latent_scale=0.23
  step  10/445 | grad_norm=0.00 | sec/step~8.03 | keep=0.50 | K=8 | llama(T): tf=2.9110 first=3.6754 kCE=2.8750 KD=0.0000 acc=0.000 state=10.0908 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  11/445 | (warm-up text) | align=0.0003 | text_tf=10.4526 | latent_scale=0.24
  step  12/445 | (warm-up text) | align=0.0003 | text_tf=10.3126 | latent_scale=0.24
  step  13/445 | (warm-up text) | align=0.0003 | text_tf=10.0785 | latent_scale=0.24
  step  14/445 | (warm-up text) | align=0.0003 | text_tf=9.6992 | latent_scale=0.25
  step  15/445 | (warm-up text) | align=0.0003 | text_tf=10.1665 | latent_scale=0.25
  step  16/445 | (warm-up text) | align=0.0003 | text_tf=10.1117 | latent_scale=0.25
  step  17/445 | (warm-up text) | align=0.0003 | text_tf=10.1998 | latent_scale=0.26
  step  18/445 | (warm-up text) | align=0.0003 | text_tf=10.4207 | latent_scale=0.26
  step  19/445 | (warm-up text) | align=0.0003 | text_tf=10.4745 | latent_scale=0.26
  step  20/445 | (warm-up text) | align=0.0003 | text_tf=9.5742 | latent_scale=0.27
  step  20/445 | grad_norm=0.00 | sec/step~8.01 | keep=0.50 | K=8 | llama(T): tf=2.8185 first=3.1087 kCE=3.2146 KD=0.0000 acc=0.000 state=7.4013 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=2.0629e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  21/445 | (warm-up text) | align=0.0003 | text_tf=9.8284 | latent_scale=0.27
  step  22/445 | (warm-up text) | align=0.0003 | text_tf=10.0499 | latent_scale=0.28
  step  23/445 | (warm-up text) | align=0.0003 | text_tf=10.5763 | latent_scale=0.28
  step  24/445 | (warm-up text) | align=0.0003 | text_tf=9.9808 | latent_scale=0.28
  step  25/445 | (warm-up text) | align=0.0003 | text_tf=9.8820 | latent_scale=0.29
  step  26/445 | (warm-up text) | align=0.0003 | text_tf=9.4299 | latent_scale=0.29
  step  27/445 | (warm-up text) | align=0.0003 | text_tf=9.8317 | latent_scale=0.29
  step  28/445 | (warm-up text) | align=0.0003 | text_tf=9.7188 | latent_scale=0.30
  step  29/445 | (warm-up text) | align=0.0003 | text_tf=9.4521 | latent_scale=0.30
  step  30/445 | (warm-up text) | align=0.0003 | text_tf=9.6277 | latent_scale=0.30
  step  30/445 | grad_norm=0.00 | sec/step~7.91 | keep=0.50 | K=8 | llama(T): tf=3.5532 first=3.6257 kCE=4.3132 KD=0.0000 acc=0.000 state=7.0670 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=9.4392e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  31/445 | (warm-up text) | align=0.0003 | text_tf=9.2460 | latent_scale=0.31
  step  32/445 | (warm-up text) | align=0.0003 | text_tf=9.9532 | latent_scale=0.31
  step  33/445 | (warm-up text) | align=0.0003 | text_tf=9.9265 | latent_scale=0.32
  step  34/445 | (warm-up text) | align=0.0003 | text_tf=9.5713 | latent_scale=0.32
  step  35/445 | (warm-up text) | align=0.0003 | text_tf=9.9832 | latent_scale=0.32
  step  36/445 | (warm-up text) | align=0.0003 | text_tf=10.1978 | latent_scale=0.33
  step  37/445 | (warm-up text) | align=0.0003 | text_tf=9.9941 | latent_scale=0.33
  step  38/445 | (warm-up text) | align=0.0003 | text_tf=9.9170 | latent_scale=0.33
  step  39/445 | (warm-up text) | align=0.0003 | text_tf=9.4510 | latent_scale=0.34
  step  40/445 | (warm-up text) | align=0.0003 | text_tf=9.7681 | latent_scale=0.34
  step  40/445 | grad_norm=0.00 | sec/step~7.79 | keep=0.50 | K=8 | llama(T): tf=3.7107 first=3.1914 kCE=4.4441 KD=0.0000 acc=0.000 state=6.7399 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.6843e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  41/445 | (warm-up text) | align=0.0003 | text_tf=9.6404 | latent_scale=0.34
  step  42/445 | (warm-up text) | align=0.0003 | text_tf=9.5836 | latent_scale=0.35
  step  43/445 | (warm-up text) | align=0.0003 | text_tf=9.4107 | latent_scale=0.35
  step  44/445 | (warm-up text) | align=0.0003 | text_tf=9.3445 | latent_scale=0.35
  step  45/445 | (warm-up text) | align=0.0003 | text_tf=10.2437 | latent_scale=0.36
  step  46/445 | (warm-up text) | align=0.0003 | text_tf=9.6467 | latent_scale=0.36
  step  47/445 | (warm-up text) | align=0.0003 | text_tf=9.6708 | latent_scale=0.37
  step  48/445 | (warm-up text) | align=0.0003 | text_tf=9.5564 | latent_scale=0.37
  step  49/445 | (warm-up text) | align=0.0003 | text_tf=9.3878 | latent_scale=0.37
  step  50/445 | (warm-up text) | align=0.0003 | text_tf=9.4493 | latent_scale=0.38
  step  50/445 | grad_norm=0.00 | sec/step~8.05 | keep=0.50 | K=8 | llama(T): tf=3.9123 first=3.3345 kCE=5.0707 KD=0.0000 acc=0.000 state=7.2779 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.2200e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  51/445 | (warm-up text) | align=0.0003 | text_tf=9.4159 | latent_scale=0.38
  step  52/445 | (warm-up text) | align=0.0003 | text_tf=9.3605 | latent_scale=0.38
  step  53/445 | (warm-up text) | align=0.0003 | text_tf=9.0448 | latent_scale=0.39
  step  54/445 | (warm-up text) | align=0.0003 | text_tf=9.8580 | latent_scale=0.39
  step  55/445 | (warm-up text) | align=0.0003 | text_tf=8.9540 | latent_scale=0.39
  step  56/445 | (warm-up text) | align=0.0003 | text_tf=9.7687 | latent_scale=0.40
  step  57/445 | (warm-up text) | align=0.0003 | text_tf=9.4485 | latent_scale=0.40
  step  58/445 | (warm-up text) | align=0.0003 | text_tf=9.5668 | latent_scale=0.41
  step  59/445 | (warm-up text) | align=0.0003 | text_tf=9.8446 | latent_scale=0.41
  step  60/445 | (warm-up text) | align=0.0003 | text_tf=9.5547 | latent_scale=0.41
  step  60/445 | grad_norm=0.00 | sec/step~8.06 | keep=0.50 | K=8 | llama(T): tf=4.4214 first=3.9846 kCE=5.5687 KD=0.0000 acc=0.000 state=7.7034 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=7.5175e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  61/445 | (warm-up text) | align=0.0003 | text_tf=9.4670 | latent_scale=0.42
  step  62/445 | (warm-up text) | align=0.0003 | text_tf=9.3185 | latent_scale=0.42
  step  63/445 | (warm-up text) | align=0.0003 | text_tf=9.4654 | latent_scale=0.42
  step  64/445 | (warm-up text) | align=0.0003 | text_tf=9.4868 | latent_scale=0.43
  step  65/445 | (warm-up text) | align=0.0003 | text_tf=9.6196 | latent_scale=0.43
  step  66/445 | (warm-up text) | align=0.0003 | text_tf=9.3709 | latent_scale=0.43
  step  67/445 | (warm-up text) | align=0.0003 | text_tf=9.4693 | latent_scale=0.44
  step  68/445 | (warm-up text) | align=0.0003 | text_tf=9.7745 | latent_scale=0.44
  step  69/445 | (warm-up text) | align=0.0003 | text_tf=9.8858 | latent_scale=0.45
  step  70/445 | (warm-up text) | align=0.0003 | text_tf=9.1922 | latent_scale=0.45
  step  70/445 | grad_norm=0.00 | sec/step~7.79 | keep=0.50 | K=8 | llama(T): tf=4.4931 first=4.0283 kCE=5.6510 KD=0.0000 acc=0.028 state=8.0418 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=7.5175e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  71/445 | (warm-up text) | align=0.0003 | text_tf=9.6175 | latent_scale=0.45
  step  72/445 | (warm-up text) | align=0.0003 | text_tf=9.5151 | latent_scale=0.46
  step  73/445 | (warm-up text) | align=0.0003 | text_tf=9.7794 | latent_scale=0.46
  step  74/445 | (warm-up text) | align=0.0003 | text_tf=9.0035 | latent_scale=0.46
  step  75/445 | (warm-up text) | align=0.0003 | text_tf=9.4209 | latent_scale=0.47
  step  76/445 | (warm-up text) | align=0.0003 | text_tf=9.2722 | latent_scale=0.47
  step  77/445 | (warm-up text) | align=0.0003 | text_tf=9.1598 | latent_scale=0.47
  step  78/445 | (warm-up text) | align=0.0003 | text_tf=9.7267 | latent_scale=0.48
  step  79/445 | (warm-up text) | align=0.0003 | text_tf=9.4681 | latent_scale=0.48
  step  80/445 | (warm-up text) | align=0.0003 | text_tf=9.2580 | latent_scale=0.48
  step  80/445 | grad_norm=0.00 | sec/step~8.76 | keep=0.50 | K=8 | llama(T): tf=4.6461 first=3.9137 kCE=6.4391 KD=0.0000 acc=0.056 state=9.1816 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.2028e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  81/445 | (warm-up text) | align=0.0003 | text_tf=9.3546 | latent_scale=0.49
  step  82/445 | (warm-up text) | align=0.0003 | text_tf=9.4877 | latent_scale=0.49
  step  83/445 | (warm-up text) | align=0.0003 | text_tf=9.8463 | latent_scale=0.50
  step  84/445 | (warm-up text) | align=0.0003 | text_tf=9.2092 | latent_scale=0.50
  step  85/445 | (warm-up text) | align=0.0003 | text_tf=9.5970 | latent_scale=0.50
  step  86/445 | (warm-up text) | align=0.0003 | text_tf=9.5859 | latent_scale=0.51
  step  87/445 | (warm-up text) | align=0.0003 | text_tf=9.3427 | latent_scale=0.51
  step  88/445 | (warm-up text) | align=0.0003 | text_tf=9.7659 | latent_scale=0.51
  step  89/445 | (warm-up text) | align=0.0003 | text_tf=9.0603 | latent_scale=0.52
  step  90/445 | (warm-up text) | align=0.0003 | text_tf=9.3344 | latent_scale=0.52
  step  90/445 | grad_norm=0.00 | sec/step~7.99 | keep=0.50 | K=8 | llama(T): tf=5.2515 first=4.3539 kCE=7.4154 KD=0.0000 acc=0.028 state=9.0343 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=5.2879e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  91/445 | (warm-up text) | align=0.0003 | text_tf=9.2620 | latent_scale=0.52
  step  92/445 | (warm-up text) | align=0.0003 | text_tf=9.6792 | latent_scale=0.53
  step  93/445 | (warm-up text) | align=0.0003 | text_tf=9.4200 | latent_scale=0.53
  step  94/445 | (warm-up text) | align=0.0003 | text_tf=9.8781 | latent_scale=0.54
  step  95/445 | (warm-up text) | align=0.0003 | text_tf=9.4834 | latent_scale=0.54
  step  96/445 | (warm-up text) | align=0.0003 | text_tf=9.2688 | latent_scale=0.54
  step  97/445 | (warm-up text) | align=0.0003 | text_tf=9.4936 | latent_scale=0.55
  step  98/445 | (warm-up text) | align=0.0003 | text_tf=8.9700 | latent_scale=0.55
  step  99/445 | (warm-up text) | align=0.0003 | text_tf=9.2521 | latent_scale=0.55
  step  100/445 | (warm-up text) | align=0.0003 | text_tf=9.3497 | latent_scale=0.56
  step  100/445 | grad_norm=0.00 | sec/step~7.88 | keep=0.50 | K=8 | llama(T): tf=5.5017 first=4.3701 kCE=7.9340 KD=0.0000 acc=0.028 state=9.7640 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=4.3025e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  101/445 | (warm-up text) | align=0.0003 | text_tf=9.3285 | latent_scale=0.56
  step  102/445 | (warm-up text) | align=0.0003 | text_tf=9.1203 | latent_scale=0.56
  step  103/445 | (warm-up text) | align=0.0003 | text_tf=9.5927 | latent_scale=0.57
  step  104/445 | (warm-up text) | align=0.0003 | text_tf=8.9621 | latent_scale=0.57
  step  105/445 | (warm-up text) | align=0.0003 | text_tf=9.2818 | latent_scale=0.57
  step  106/445 | (warm-up text) | align=0.0003 | text_tf=8.9010 | latent_scale=0.58
  step  107/445 | (warm-up text) | align=0.0003 | text_tf=9.6305 | latent_scale=0.58
  step  108/445 | (warm-up text) | align=0.0003 | text_tf=9.2493 | latent_scale=0.59
  step  109/445 | (warm-up text) | align=0.0003 | text_tf=9.0133 | latent_scale=0.59
  step  110/445 | (warm-up text) | align=0.0003 | text_tf=8.5108 | latent_scale=0.59
  step  110/445 | grad_norm=0.00 | sec/step~8.61 | keep=0.50 | K=8 | llama(T): tf=5.5543 first=4.1879 kCE=8.2699 KD=0.0000 acc=0.167 state=9.3401 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=6.4171e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  111/445 | (warm-up text) | align=0.0003 | text_tf=9.3658 | latent_scale=0.60
  step  112/445 | (warm-up text) | align=0.0003 | text_tf=9.0154 | latent_scale=0.60
  step  113/445 | (warm-up text) | align=0.0003 | text_tf=9.5314 | latent_scale=0.60
  step  114/445 | (warm-up text) | align=0.0003 | text_tf=9.1905 | latent_scale=0.61
  step  115/445 | (warm-up text) | align=0.0003 | text_tf=9.6481 | latent_scale=0.61
  step  116/445 | (warm-up text) | align=0.0003 | text_tf=8.9363 | latent_scale=0.61
  step  117/445 | (warm-up text) | align=0.0003 | text_tf=9.4506 | latent_scale=0.62
  step  118/445 | (warm-up text) | align=0.0003 | text_tf=9.0285 | latent_scale=0.62
  step  119/445 | (warm-up text) | align=0.0003 | text_tf=9.2370 | latent_scale=0.63
  step  120/445 | (warm-up text) | align=0.0003 | text_tf=9.4479 | latent_scale=0.63
  step  120/445 | grad_norm=0.00 | sec/step~7.82 | keep=0.50 | K=8 | llama(T): tf=6.2284 first=5.0311 kCE=8.6249 KD=0.0000 acc=0.056 state=10.6154 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=4.5279e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  121/445 | (warm-up text) | align=0.0003 | text_tf=9.3548 | latent_scale=0.63
  step  122/445 | (warm-up text) | align=0.0003 | text_tf=9.4433 | latent_scale=0.64
  step  123/445 | (warm-up text) | align=0.0003 | text_tf=8.9489 | latent_scale=0.64
  step  124/445 | (warm-up text) | align=0.0003 | text_tf=9.1358 | latent_scale=0.64
  step  125/445 | (warm-up text) | align=0.0003 | text_tf=8.8904 | latent_scale=0.65
  step  126/445 | (warm-up text) | align=0.0003 | text_tf=8.7735 | latent_scale=0.65
  step  127/445 | (warm-up text) | align=0.0003 | text_tf=9.1047 | latent_scale=0.65
  step  128/445 | (warm-up text) | align=0.0003 | text_tf=9.6618 | latent_scale=0.66
  step  129/445 | (warm-up text) | align=0.0003 | text_tf=9.0950 | latent_scale=0.66
  step  130/445 | (warm-up text) | align=0.0003 | text_tf=9.0553 | latent_scale=0.66
  step  130/445 | grad_norm=0.00 | sec/step~8.11 | keep=0.50 | K=8 | llama(T): tf=6.6007 first=5.1082 kCE=9.1736 KD=0.0000 acc=0.083 state=11.1616 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=4.5279e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  131/445 | (warm-up text) | align=0.0003 | text_tf=8.8873 | latent_scale=0.67
  step  132/445 | (warm-up text) | align=0.0003 | text_tf=8.8731 | latent_scale=0.67
  step  133/445 | (warm-up text) | align=0.0003 | text_tf=9.3294 | latent_scale=0.68
  step  134/445 | (warm-up text) | align=0.0003 | text_tf=8.6177 | latent_scale=0.68
  step  135/445 | (warm-up text) | align=0.0003 | text_tf=8.8978 | latent_scale=0.68
  step  136/445 | (warm-up text) | align=0.0003 | text_tf=9.0605 | latent_scale=0.69
  step  137/445 | (warm-up text) | align=0.0003 | text_tf=8.8771 | latent_scale=0.69
  step  138/445 | (warm-up text) | align=0.0003 | text_tf=9.5789 | latent_scale=0.69
  step  139/445 | (warm-up text) | align=0.0003 | text_tf=8.9208 | latent_scale=0.70
  step  140/445 | (warm-up text) | align=0.0003 | text_tf=9.4732 | latent_scale=0.70
  step  140/445 | grad_norm=0.00 | sec/step~8.33 | keep=0.50 | K=8 | llama(T): tf=7.1117 first=5.4834 kCE=9.5063 KD=0.0000 acc=0.056 state=11.8982 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.3097e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  141/445 | (warm-up text) | align=0.0003 | text_tf=8.5814 | latent_scale=0.70
  step  142/445 | (warm-up text) | align=0.0003 | text_tf=9.2340 | latent_scale=0.71
  step  143/445 | (warm-up text) | align=0.0003 | text_tf=8.9539 | latent_scale=0.71
  step  144/445 | (warm-up text) | align=0.0003 | text_tf=9.3272 | latent_scale=0.72
  step  145/445 | (warm-up text) | align=0.0003 | text_tf=9.0355 | latent_scale=0.72
  step  146/445 | (warm-up text) | align=0.0003 | text_tf=9.4974 | latent_scale=0.72
  step  147/445 | (warm-up text) | align=0.0003 | text_tf=9.2101 | latent_scale=0.73
  step  148/445 | (warm-up text) | align=0.0003 | text_tf=8.9780 | latent_scale=0.73
  step  149/445 | (warm-up text) | align=0.0003 | text_tf=9.2846 | latent_scale=0.73
  step  150/445 | (warm-up text) | align=0.0003 | text_tf=8.7274 | latent_scale=0.74
  step  150/445 | grad_norm=0.00 | sec/step~9.16 | keep=0.50 | K=8 | llama(T): tf=7.2274 first=5.7808 kCE=10.2452 KD=0.0000 acc=0.056 state=13.2015 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  151/445 | (warm-up text) | align=0.0003 | text_tf=8.9716 | latent_scale=0.74
  step  152/445 | (warm-up text) | align=0.0003 | text_tf=8.7885 | latent_scale=0.74
  step  153/445 | (warm-up text) | align=0.0003 | text_tf=8.7352 | latent_scale=0.75
  step  154/445 | (warm-up text) | align=0.0003 | text_tf=8.9717 | latent_scale=0.75
  step  155/445 | (warm-up text) | align=0.0003 | text_tf=8.9948 | latent_scale=0.75
  step  156/445 | (warm-up text) | align=0.0003 | text_tf=8.8980 | latent_scale=0.76
  step  157/445 | (warm-up text) | align=0.0003 | text_tf=9.1023 | latent_scale=0.76
  step  158/445 | (warm-up text) | align=0.0003 | text_tf=8.9938 | latent_scale=0.77
  step  159/445 | (warm-up text) | align=0.0003 | text_tf=9.3725 | latent_scale=0.77
  step  160/445 | (warm-up text) | align=0.0003 | text_tf=8.9443 | latent_scale=0.77
  step  160/445 | grad_norm=0.00 | sec/step~8.14 | keep=0.50 | K=8 | llama(T): tf=7.5641 first=6.3043 kCE=10.3993 KD=0.0000 acc=0.028 state=12.0349 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.2825e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  161/445 | (warm-up text) | align=0.0003 | text_tf=8.7629 | latent_scale=0.78
  step  162/445 | (warm-up text) | align=0.0003 | text_tf=8.9984 | latent_scale=0.78
  step  163/445 | (warm-up text) | align=0.0003 | text_tf=9.2009 | latent_scale=0.78
  step  164/445 | (warm-up text) | align=0.0003 | text_tf=9.1160 | latent_scale=0.79
  step  165/445 | (warm-up text) | align=0.0003 | text_tf=8.9510 | latent_scale=0.79
  step  166/445 | (warm-up text) | align=0.0003 | text_tf=9.1056 | latent_scale=0.79
  step  167/445 | (warm-up text) | align=0.0003 | text_tf=8.6736 | latent_scale=0.80
  step  168/445 | (warm-up text) | align=0.0003 | text_tf=9.2445 | latent_scale=0.80
  step  169/445 | (warm-up text) | align=0.0003 | text_tf=8.9962 | latent_scale=0.81
  step  170/445 | (warm-up text) | align=0.0003 | text_tf=8.9790 | latent_scale=0.81
  step  170/445 | grad_norm=0.00 | sec/step~9.05 | keep=0.50 | K=8 | llama(T): tf=7.6477 first=6.0764 kCE=10.4491 KD=0.0000 acc=0.056 state=14.4169 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.0708e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  171/445 | (warm-up text) | align=0.0003 | text_tf=8.9888 | latent_scale=0.81
  step  172/445 | (warm-up text) | align=0.0003 | text_tf=8.9726 | latent_scale=0.82
  step  173/445 | (warm-up text) | align=0.0003 | text_tf=9.1482 | latent_scale=0.82
  step  174/445 | (warm-up text) | align=0.0003 | text_tf=9.0073 | latent_scale=0.82
  step  175/445 | (warm-up text) | align=0.0003 | text_tf=8.8329 | latent_scale=0.83
  step  176/445 | (warm-up text) | align=0.0003 | text_tf=9.0484 | latent_scale=0.83
  step  177/445 | (warm-up text) | align=0.0003 | text_tf=8.6157 | latent_scale=0.83
  step  178/445 | (warm-up text) | align=0.0003 | text_tf=8.5353 | latent_scale=0.84
  step  179/445 | (warm-up text) | align=0.0003 | text_tf=8.9288 | latent_scale=0.84
  step  180/445 | (warm-up text) | align=0.0003 | text_tf=8.8109 | latent_scale=0.85
  step  180/445 | grad_norm=0.00 | sec/step~7.65 | keep=0.50 | K=8 | llama(T): tf=8.1422 first=6.6978 kCE=12.0286 KD=0.0000 acc=0.056 state=13.5535 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.1550e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  181/445 | (warm-up text) | align=0.0003 | text_tf=9.0652 | latent_scale=0.85
  step  182/445 | (warm-up text) | align=0.0003 | text_tf=8.7785 | latent_scale=0.85
  step  183/445 | (warm-up text) | align=0.0003 | text_tf=8.9377 | latent_scale=0.86
  step  184/445 | (warm-up text) | align=0.0003 | text_tf=8.8540 | latent_scale=0.86
  step  185/445 | (warm-up text) | align=0.0003 | text_tf=8.4813 | latent_scale=0.86
  step  186/445 | (warm-up text) | align=0.0003 | text_tf=9.0931 | latent_scale=0.87
  step  187/445 | (warm-up text) | align=0.0003 | text_tf=8.7856 | latent_scale=0.87
  step  188/445 | (warm-up text) | align=0.0003 | text_tf=8.7218 | latent_scale=0.87
  step  189/445 | (warm-up text) | align=0.0003 | text_tf=8.4274 | latent_scale=0.88
  step  190/445 | (warm-up text) | align=0.0003 | text_tf=8.4411 | latent_scale=0.88
  step  190/445 | grad_norm=0.00 | sec/step~9.13 | keep=0.50 | K=8 | llama(T): tf=8.1092 first=6.3232 kCE=11.4528 KD=0.0000 acc=0.056 state=16.7761 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.1550e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  191/445 | (warm-up text) | align=0.0003 | text_tf=8.7686 | latent_scale=0.88
  step  192/445 | (warm-up text) | align=0.0003 | text_tf=8.8226 | latent_scale=0.89
  step  193/445 | (warm-up text) | align=0.0003 | text_tf=8.9710 | latent_scale=0.89
  step  194/445 | (warm-up text) | align=0.0003 | text_tf=8.4184 | latent_scale=0.90
  step  195/445 | (warm-up text) | align=0.0003 | text_tf=8.6194 | latent_scale=0.90
  step  196/445 | (warm-up text) | align=0.0003 | text_tf=8.8092 | latent_scale=0.90
  step  197/445 | (warm-up text) | align=0.0003 | text_tf=9.0972 | latent_scale=0.91
  step  198/445 | (warm-up text) | align=0.0003 | text_tf=8.9919 | latent_scale=0.91
  step  199/445 | (warm-up text) | align=0.0003 | text_tf=8.7289 | latent_scale=0.91
  step  200/445 | (warm-up text) | align=0.0003 | text_tf=8.9924 | latent_scale=0.92
  step  200/445 | grad_norm=0.00 | sec/step~7.46 | keep=0.50 | K=8 | llama(T): tf=8.9986 first=7.4594 kCE=12.1553 KD=0.0000 acc=0.000 state=14.1080 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.6576e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  201/445 | (warm-up text) | align=0.0003 | text_tf=8.7466 | latent_scale=0.92
  step  202/445 | (warm-up text) | align=0.0003 | text_tf=8.7017 | latent_scale=0.92
  step  203/445 | (warm-up text) | align=0.0003 | text_tf=8.8913 | latent_scale=0.93
  step  204/445 | (warm-up text) | align=0.0003 | text_tf=8.7061 | latent_scale=0.93
  step  205/445 | (warm-up text) | align=0.0003 | text_tf=8.4997 | latent_scale=0.94
  step  206/445 | (warm-up text) | align=0.0003 | text_tf=8.5414 | latent_scale=0.94
  step  207/445 | (warm-up text) | align=0.0003 | text_tf=8.5594 | latent_scale=0.94
  step  208/445 | (warm-up text) | align=0.0003 | text_tf=8.6560 | latent_scale=0.95
  step  209/445 | (warm-up text) | align=0.0003 | text_tf=8.6621 | latent_scale=0.95
  step  210/445 | (warm-up text) | align=0.0003 | text_tf=8.6211 | latent_scale=0.95
  step  210/445 | grad_norm=0.00 | sec/step~7.82 | keep=0.50 | K=8 | llama(T): tf=8.6255 first=7.0637 kCE=11.9727 KD=0.0000 acc=0.083 state=15.1091 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=2.2737e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  211/445 | (warm-up text) | align=0.0003 | text_tf=8.4311 | latent_scale=0.96
  step  212/445 | (warm-up text) | align=0.0003 | text_tf=8.3364 | latent_scale=0.96
  step  213/445 | (warm-up text) | align=0.0003 | text_tf=9.1137 | latent_scale=0.96
  step  214/445 | (warm-up text) | align=0.0003 | text_tf=8.8205 | latent_scale=0.97
  step  215/445 | (warm-up text) | align=0.0003 | text_tf=8.6025 | latent_scale=0.97
  step  216/445 | (warm-up text) | align=0.0003 | text_tf=8.4820 | latent_scale=0.97
  step  217/445 | (warm-up text) | align=0.0003 | text_tf=8.6615 | latent_scale=0.98
  step  218/445 | (warm-up text) | align=0.0003 | text_tf=8.6761 | latent_scale=0.98
  step  219/445 | (warm-up text) | align=0.0003 | text_tf=8.8980 | latent_scale=0.99
  step  220/445 | (warm-up text) | align=0.0003 | text_tf=8.6112 | latent_scale=0.99
  step  220/445 | grad_norm=0.00 | sec/step~8.05 | keep=0.50 | K=8 | llama(T): tf=9.5412 first=7.9893 kCE=12.8189 KD=0.0000 acc=0.028 state=14.1964 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.5948e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  221/445 | (warm-up text) | align=0.0003 | text_tf=8.5956 | latent_scale=0.99
  step  222/445 | (warm-up text) | align=0.0003 | text_tf=8.4008 | latent_scale=1.00
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  step  230/445 | grad_norm=39.83 | sec/step~6.19 | keep=0.50 | K=8 | llama(L): tf=8.8853 first=7.2097 kCE=11.8067 KD=25.9166 acc=0.056 state=16.2273 align=0.0000 latA=0.9116 latP=0.4718 | scale_pen(llama)=1.4353e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  240/445 | grad_norm=244.54 | sec/step~5.63 | keep=0.50 | K=8 | llama(L): tf=9.6430 first=8.3269 kCE=12.4263 KD=27.1611 acc=0.000 state=14.1639 align=0.0000 latA=0.9262 latP=0.4733 | scale_pen(llama)=3.7295e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=241 mode=text (tail)
  step  242/445 | (tail text) | align=0.0003 | text_tf=8.7668 | latent_scale=1.00
  step  250/445 | grad_norm=63.76 | sec/step~5.87 | keep=0.50 | K=8 | llama(L): tf=9.3279 first=7.1642 kCE=12.2397 KD=6.7499 acc=0.083 state=16.0402 align=0.0000 latA=0.9164 latP=0.4729 | scale_pen(llama)=3.7295e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  260/445 | grad_norm=56.41 | sec/step~5.69 | keep=0.50 | K=8 | llama(L): tf=8.9366 first=7.5000 kCE=11.6984 KD=6.3653 acc=0.028 state=14.5395 align=0.0000 latA=0.9288 latP=0.4719 | scale_pen(llama)=6.7226e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  270/445 | grad_norm=22.11 | sec/step~6.49 | keep=0.50 | K=8 | llama(L): tf=9.1382 first=7.6515 kCE=11.7379 KD=5.6223 acc=0.083 state=16.2278 align=0.0000 latA=0.9039 latP=0.4721 | scale_pen(llama)=1.0169e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  280/445 | grad_norm=6.25 | sec/step~5.81 | keep=0.50 | K=8 | llama(L): tf=8.9526 first=7.6739 kCE=11.7200 KD=5.4457 acc=0.028 state=15.2898 align=0.0000 latA=0.9196 latP=0.4713 | scale_pen(llama)=1.3789e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  290/445 | grad_norm=2.85 | sec/step~6.40 | keep=0.50 | K=8 | llama(L): tf=8.6493 first=7.0659 kCE=11.2632 KD=4.0753 acc=0.028 state=16.2281 align=0.0000 latA=0.9208 latP=0.4724 | scale_pen(llama)=1.7062e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  300/445 | grad_norm=16.15 | sec/step~6.67 | keep=0.50 | K=8 | llama(L): tf=9.0995 first=8.1088 kCE=11.5560 KD=4.5935 acc=0.000 state=16.6030 align=0.0000 latA=0.9222 latP=0.4727 | scale_pen(llama)=1.9666e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  310/445 | grad_norm=12.33 | sec/step~5.46 | keep=0.50 | K=8 | llama(L): tf=8.5102 first=7.9401 kCE=11.2250 KD=4.4171 acc=0.000 state=12.4770 align=0.0000 latA=0.9265 latP=0.4728 | scale_pen(llama)=1.9666e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  320/445 | grad_norm=6.67 | sec/step~5.70 | keep=0.50 | K=8 | llama(L): tf=8.7369 first=7.3934 kCE=10.6701 KD=4.4042 acc=0.000 state=14.8211 align=0.0000 latA=0.9187 latP=0.4715 | scale_pen(llama)=2.1394e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  323/445 | (tail text) | align=0.0003 | text_tf=8.3767 | latent_scale=1.00
  step  330/445 | grad_norm=4.71 | sec/step~5.78 | keep=0.50 | K=8 | llama(L): tf=8.9902 first=7.5057 kCE=10.6239 KD=4.4955 acc=0.028 state=15.2900 align=0.0000 latA=0.9295 latP=0.4715 | scale_pen(llama)=2.1559e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  340/445 | grad_norm=3.18 | sec/step~5.25 | keep=0.50 | K=8 | llama(L): tf=8.4643 first=7.1934 kCE=10.0692 KD=4.1394 acc=0.083 state=12.6644 align=0.0000 latA=0.9156 latP=0.4711 | scale_pen(llama)=2.0091e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  347/445 | (tail text) | align=0.0003 | text_tf=8.6479 | latent_scale=1.00
  step  349/445 | (tail text) | align=0.0003 | text_tf=8.3958 | latent_scale=1.00
  step  350/445 | grad_norm=0.64 | sec/step~6.00 | keep=0.50 | K=8 | llama(L): tf=8.5468 first=7.6000 kCE=10.0538 KD=5.7462 acc=0.028 state=15.7596 align=0.0000 latA=0.9327 latP=0.4714 | scale_pen(llama)=1.7359e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  360/445 | grad_norm=5.42 | sec/step~5.42 | keep=0.50 | K=8 | llama(L): tf=8.6118 first=7.5375 kCE=9.8425 KD=4.0031 acc=0.083 state=13.6024 align=0.0000 latA=0.9213 latP=0.4712 | scale_pen(llama)=1.3176e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  370/445 | grad_norm=3.07 | sec/step~6.08 | keep=0.50 | K=8 | llama(L): tf=8.6381 first=7.1177 kCE=9.5385 KD=5.6201 acc=0.139 state=15.3844 align=0.0000 latA=0.9211 latP=0.4708 | scale_pen(llama)=1.3176e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  380/445 | grad_norm=5.75 | sec/step~5.66 | keep=0.50 | K=8 | llama(L): tf=9.0267 first=7.7682 kCE=9.4068 KD=4.1196 acc=0.056 state=14.2590 align=0.0000 latA=0.9317 latP=0.4706 | scale_pen(llama)=7.9149e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  390/445 | grad_norm=2.75 | sec/step~5.46 | keep=0.50 | K=8 | llama(L): tf=8.7683 first=7.6676 kCE=9.0453 KD=5.0683 acc=0.056 state=13.1334 align=0.0000 latA=0.9334 latP=0.4694 | scale_pen(llama)=3.8455e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  400/445 | grad_norm=1.16 | sec/step~5.32 | keep=0.50 | K=8 | llama(L): tf=9.1922 first=7.5909 kCE=8.9235 KD=4.0725 acc=0.083 state=12.2894 align=0.0000 latA=0.9368 latP=0.4692 | scale_pen(llama)=1.1383e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  410/445 | grad_norm=0.87 | sec/step~5.69 | keep=0.50 | K=8 | llama(L): tf=8.4526 first=7.5685 kCE=8.3870 KD=4.5782 acc=0.056 state=13.5089 align=0.0000 latA=0.9377 latP=0.4679 | scale_pen(llama)=2.5899e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  420/445 | grad_norm=5.37 | sec/step~5.31 | keep=0.50 | K=8 | llama(L): tf=8.6088 first=7.7846 kCE=8.6649 KD=4.0094 acc=0.028 state=12.4771 align=0.0000 latA=0.9378 latP=0.4681 | scale_pen(llama)=4.4565e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
  step  430/445 | grad_norm=3.87 | sec/step~5.61 | keep=0.50 | K=8 | llama(L): tf=8.4615 first=7.4331 kCE=8.1676 KD=4.0986 acc=0.056 state=13.7902 align=0.0000 latA=0.9254 latP=0.4672 | scale_pen(llama)=4.4565e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01057]
[WARN] KD teacher forward failed; retrying per-example: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[WARN] KD teacher per-example fallback failed on row; attempting CPU fallback: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[WARN] KD teacher CPU fallback failed; skipping KD for batch: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 130, in kd_first_k_prefix_vs_text
    _, _, logits_chunk = teacher_llm.loss_with_text_prompt(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/models.py", line 1120, in loss_with_text_prompt
    out = self.model(**model_kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/peft_model.py", line 1850, in forward
    return self.base_model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
    return self.model.forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2374, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1646, in main
    loss_kd_raw = kd_first_k_prefix_vs_text(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 177, in kd_first_k_prefix_vs_text
    teacher_llm.model.to(teacher_device)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 6 more times]
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

