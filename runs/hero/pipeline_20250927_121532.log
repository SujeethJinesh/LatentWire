
>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=hero

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3563.55it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⚠️  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 167 steps
Epoch 1/2
[warmup] step=0 mode=text (warm-up)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/334 | (warm-up text) | align=0.0002 | text_tf=14.1409 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/334 | (warm-up text) | align=0.0002 | text_tf=14.4763 | latent_scale=0.00
[warmup] step=2 mode=text (warm-up)
  step  3/334 | (warm-up text) | align=0.0002 | text_tf=15.6948 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/334 | (warm-up text) | align=0.0002 | text_tf=12.3696 | latent_scale=0.01
[warmup] step=4 mode=text (warm-up)
  step  5/334 | (warm-up text) | align=0.0002 | text_tf=14.9147 | latent_scale=0.01
[warmup] step=5 mode=text (warm-up)
  step  6/334 | (warm-up text) | align=0.0002 | text_tf=12.8197 | latent_scale=0.01
[warmup] step=6 mode=text (warm-up)
  step  7/334 | (warm-up text) | align=0.0002 | text_tf=12.6957 | latent_scale=0.02
[warmup] step=7 mode=text (warm-up)
  step  8/334 | (warm-up text) | align=0.0002 | text_tf=13.7441 | latent_scale=0.02
[warmup] step=8 mode=text (warm-up)
  step  9/334 | (warm-up text) | align=0.0002 | text_tf=13.3924 | latent_scale=0.02
[warmup] step=9 mode=text (warm-up)
  step  10/334 | (warm-up text) | align=0.0002 | text_tf=14.1952 | latent_scale=0.03
  step  10/334 | grad_norm=2.93 | sec/step~3.98 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=0.3666 first=0.3645 kCE=0.3581 KD=0.0000 acc=0.000 state=0.3883 align=0.0002 latA=0.0000 latP=0.0000 gist=1.0001 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  11/334 | (warm-up text) | align=0.0002 | text_tf=14.9200 | latent_scale=0.03
  step  12/334 | (warm-up text) | align=0.0002 | text_tf=13.7612 | latent_scale=0.03
  step  13/334 | (warm-up text) | align=0.0002 | text_tf=16.6193 | latent_scale=0.04
  step  14/334 | (warm-up text) | align=0.0002 | text_tf=14.2048 | latent_scale=0.04
  step  15/334 | (warm-up text) | align=0.0002 | text_tf=13.7806 | latent_scale=0.04
  step  16/334 | (warm-up text) | align=0.0002 | text_tf=12.6895 | latent_scale=0.04
  step  17/334 | (warm-up text) | align=0.0002 | text_tf=13.8399 | latent_scale=0.05
  step  18/334 | (warm-up text) | align=0.0002 | text_tf=13.4666 | latent_scale=0.05
  step  19/334 | (warm-up text) | align=0.0002 | text_tf=12.1886 | latent_scale=0.05
  step  20/334 | (warm-up text) | align=0.0002 | text_tf=12.4646 | latent_scale=0.06
  step  20/334 | grad_norm=12.80 | sec/step~3.48 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=1.8165 first=1.7069 kCE=2.4638 KD=0.0000 acc=0.000 state=0.8045 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9999 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  21/334 | (warm-up text) | align=0.0002 | text_tf=11.9592 | latent_scale=0.06
  step  22/334 | (warm-up text) | align=0.0002 | text_tf=12.5253 | latent_scale=0.06
  step  23/334 | (warm-up text) | align=0.0002 | text_tf=14.1611 | latent_scale=0.07
  step  24/334 | (warm-up text) | align=0.0002 | text_tf=12.4374 | latent_scale=0.07
  step  25/334 | (warm-up text) | align=0.0002 | text_tf=12.5409 | latent_scale=0.07
  step  26/334 | (warm-up text) | align=0.0002 | text_tf=14.0689 | latent_scale=0.07
  step  27/334 | (warm-up text) | align=0.0002 | text_tf=14.2531 | latent_scale=0.08
  step  28/334 | (warm-up text) | align=0.0002 | text_tf=13.2095 | latent_scale=0.08
  step  29/334 | (warm-up text) | align=0.0002 | text_tf=14.0715 | latent_scale=0.08
  step  30/334 | (warm-up text) | align=0.0002 | text_tf=12.5661 | latent_scale=0.09
  step  30/334 | grad_norm=57.22 | sec/step~3.60 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.9286 first=2.8852 kCE=3.7521 KD=0.0000 acc=0.000 state=1.1587 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9998 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  31/334 | (warm-up text) | align=0.0002 | text_tf=12.2363 | latent_scale=0.09
  step  32/334 | (warm-up text) | align=0.0002 | text_tf=13.3325 | latent_scale=0.09
  step  33/334 | (warm-up text) | align=0.0002 | text_tf=10.9251 | latent_scale=0.10
  step  34/334 | (warm-up text) | align=0.0002 | text_tf=11.4859 | latent_scale=0.10
  step  35/334 | (warm-up text) | align=0.0002 | text_tf=11.0689 | latent_scale=0.10
  step  36/334 | (warm-up text) | align=0.0002 | text_tf=11.8231 | latent_scale=0.10
  step  37/334 | (warm-up text) | align=0.0002 | text_tf=12.1400 | latent_scale=0.11
  step  38/334 | (warm-up text) | align=0.0002 | text_tf=11.1249 | latent_scale=0.11
  step  39/334 | (warm-up text) | align=0.0002 | text_tf=11.5625 | latent_scale=0.11
  step  40/334 | (warm-up text) | align=0.0002 | text_tf=11.2588 | latent_scale=0.12
  step  40/334 | grad_norm=17.44 | sec/step~4.02 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.3079 first=2.1272 kCE=3.0957 KD=0.0000 acc=0.000 state=1.7281 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9996 | scale_pen(llama)=6.5690e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  41/334 | (warm-up text) | align=0.0002 | text_tf=12.6453 | latent_scale=0.12
  step  42/334 | (warm-up text) | align=0.0002 | text_tf=11.6924 | latent_scale=0.12
  step  43/334 | (warm-up text) | align=0.0002 | text_tf=12.0438 | latent_scale=0.13
  step  44/334 | (warm-up text) | align=0.0002 | text_tf=11.1993 | latent_scale=0.13
  step  45/334 | (warm-up text) | align=0.0002 | text_tf=12.2062 | latent_scale=0.13
  step  46/334 | (warm-up text) | align=0.0002 | text_tf=11.6733 | latent_scale=0.13
  step  47/334 | (warm-up text) | align=0.0002 | text_tf=11.7481 | latent_scale=0.14
  step  48/334 | (warm-up text) | align=0.0002 | text_tf=10.6855 | latent_scale=0.14
  step  49/334 | (warm-up text) | align=0.0002 | text_tf=11.4108 | latent_scale=0.14
  step  50/334 | (warm-up text) | align=0.0002 | text_tf=10.1813 | latent_scale=0.15
  step  50/334 | grad_norm=4.33 | sec/step~4.37 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.1838 first=1.6280 kCE=2.3879 KD=0.0000 acc=0.000 state=2.3916 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9993 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  51/334 | (warm-up text) | align=0.0002 | text_tf=9.7747 | latent_scale=0.15
  step  52/334 | (warm-up text) | align=0.0002 | text_tf=10.0063 | latent_scale=0.15
  step  53/334 | (warm-up text) | align=0.0002 | text_tf=9.5665 | latent_scale=0.16
  step  54/334 | (warm-up text) | align=0.0002 | text_tf=10.5310 | latent_scale=0.16
  step  55/334 | (warm-up text) | align=0.0002 | text_tf=10.1082 | latent_scale=0.16
  step  56/334 | (warm-up text) | align=0.0002 | text_tf=11.4812 | latent_scale=0.16
  step  57/334 | (warm-up text) | align=0.0002 | text_tf=11.0530 | latent_scale=0.17
  step  58/334 | (warm-up text) | align=0.0002 | text_tf=11.4355 | latent_scale=0.17
  step  59/334 | (warm-up text) | align=0.0002 | text_tf=10.5427 | latent_scale=0.17
  step  60/334 | (warm-up text) | align=0.0002 | text_tf=10.8729 | latent_scale=0.18
  step  60/334 | grad_norm=28.75 | sec/step~3.44 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.5528 first=2.0835 kCE=2.8257 KD=0.0000 acc=0.000 state=2.6939 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9993 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  61/334 | (warm-up text) | align=0.0002 | text_tf=10.1040 | latent_scale=0.18
  step  62/334 | (warm-up text) | align=0.0002 | text_tf=10.4254 | latent_scale=0.18
  step  63/334 | (warm-up text) | align=0.0002 | text_tf=10.5198 | latent_scale=0.19
  step  64/334 | (warm-up text) | align=0.0002 | text_tf=9.4300 | latent_scale=0.19
  step  65/334 | (warm-up text) | align=0.0002 | text_tf=11.7268 | latent_scale=0.19
  step  66/334 | (warm-up text) | align=0.0002 | text_tf=10.2976 | latent_scale=0.19
  step  67/334 | (warm-up text) | align=0.0002 | text_tf=9.3995 | latent_scale=0.20
  step  68/334 | (warm-up text) | align=0.0002 | text_tf=10.4851 | latent_scale=0.20
  step  69/334 | (warm-up text) | align=0.0002 | text_tf=8.4589 | latent_scale=0.20
  step  70/334 | (warm-up text) | align=0.0002 | text_tf=11.1496 | latent_scale=0.21
  step  70/334 | grad_norm=5.14 | sec/step~3.30 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.7494 first=2.2067 kCE=2.4309 KD=0.0000 acc=0.000 state=3.3950 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9990 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  71/334 | (warm-up text) | align=0.0002 | text_tf=9.1393 | latent_scale=0.21
  step  72/334 | (warm-up text) | align=0.0002 | text_tf=10.1835 | latent_scale=0.21
  step  73/334 | (warm-up text) | align=0.0002 | text_tf=8.8879 | latent_scale=0.22
  step  74/334 | (warm-up text) | align=0.0002 | text_tf=10.0917 | latent_scale=0.22
  step  75/334 | (warm-up text) | align=0.0002 | text_tf=11.0413 | latent_scale=0.22
  step  76/334 | (warm-up text) | align=0.0002 | text_tf=0.0000 | latent_scale=0.22
  step  77/334 | (warm-up text) | align=0.0002 | text_tf=9.0898 | latent_scale=0.23
  step  78/334 | (warm-up text) | align=0.0002 | text_tf=9.2217 | latent_scale=0.23
  step  79/334 | (warm-up text) | align=0.0002 | text_tf=10.2888 | latent_scale=0.23
  step  80/334 | (warm-up text) | align=0.0002 | text_tf=9.5735 | latent_scale=0.24
  step  80/334 | grad_norm=13.97 | sec/step~3.67 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=3.1997 first=2.6080 kCE=2.8383 KD=0.0000 acc=0.000 state=4.1803 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9990 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  81/334 | (warm-up text) | align=0.0002 | text_tf=8.8246 | latent_scale=0.24
  step  82/334 | (warm-up text) | align=0.0002 | text_tf=10.1271 | latent_scale=0.24
  step  83/334 | (warm-up text) | align=0.0002 | text_tf=9.4843 | latent_scale=0.25
  step  84/334 | (warm-up text) | align=0.0002 | text_tf=9.7263 | latent_scale=0.25
  step  85/334 | (warm-up text) | align=0.0002 | text_tf=9.3589 | latent_scale=0.25
  step  86/334 | (warm-up text) | align=0.0002 | text_tf=9.1493 | latent_scale=0.25
  step  87/334 | (warm-up text) | align=0.0002 | text_tf=9.4051 | latent_scale=0.26
  step  88/334 | (warm-up text) | align=0.0002 | text_tf=9.2991 | latent_scale=0.26
  step  89/334 | (warm-up text) | align=0.0002 | text_tf=9.2915 | latent_scale=0.26
  step  90/334 | (warm-up text) | align=0.0002 | text_tf=9.6371 | latent_scale=0.27
  step  90/334 | grad_norm=8.50 | sec/step~3.35 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=3.6505 first=2.9302 kCE=3.0438 KD=0.0000 acc=0.000 state=4.7860 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9988 | scale_pen(llama)=1.4211e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  91/334 | (warm-up text) | align=0.0002 | text_tf=9.8403 | latent_scale=0.27
  step  92/334 | (warm-up text) | align=0.0002 | text_tf=9.4997 | latent_scale=0.27
  step  93/334 | (warm-up text) | align=0.0002 | text_tf=9.8266 | latent_scale=0.28
  step  94/334 | (warm-up text) | align=0.0002 | text_tf=10.2677 | latent_scale=0.28
  step  95/334 | (warm-up text) | align=0.0002 | text_tf=8.5296 | latent_scale=0.28
  step  96/334 | (warm-up text) | align=0.0002 | text_tf=9.7704 | latent_scale=0.28
  step  97/334 | (warm-up text) | align=0.0002 | text_tf=9.3580 | latent_scale=0.29
  step  98/334 | (warm-up text) | align=0.0002 | text_tf=9.1288 | latent_scale=0.29
  step  99/334 | (warm-up text) | align=0.0002 | text_tf=9.1645 | latent_scale=0.29
  step  100/334 | (warm-up text) | align=0.0002 | text_tf=10.5899 | latent_scale=0.30
  step  100/334 | grad_norm=3.22 | sec/step~3.59 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=3.7724 first=3.0603 kCE=3.3392 KD=0.0000 acc=0.000 state=5.6052 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9985 | scale_pen(llama)=5.1301e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  101/334 | (warm-up text) | align=0.0002 | text_tf=8.2790 | latent_scale=0.30
  step  102/334 | (warm-up text) | align=0.0002 | text_tf=9.5408 | latent_scale=0.30
  step  103/334 | (warm-up text) | align=0.0002 | text_tf=8.9623 | latent_scale=0.31
  step  104/334 | (warm-up text) | align=0.0002 | text_tf=9.4295 | latent_scale=0.31
  step  105/334 | (warm-up text) | align=0.0002 | text_tf=8.6064 | latent_scale=0.31
  step  106/334 | (warm-up text) | align=0.0002 | text_tf=8.5991 | latent_scale=0.31
  step  107/334 | (warm-up text) | align=0.0002 | text_tf=9.1207 | latent_scale=0.32
  step  108/334 | (warm-up text) | align=0.0002 | text_tf=9.1373 | latent_scale=0.32
  step  109/334 | (warm-up text) | align=0.0002 | text_tf=8.4017 | latent_scale=0.32
  step  110/334 | (warm-up text) | align=0.0002 | text_tf=8.5925 | latent_scale=0.33
  step  110/334 | grad_norm=11.30 | sec/step~3.66 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.2582 first=3.3642 kCE=3.6208 KD=0.0000 acc=0.000 state=5.9953 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9985 | scale_pen(llama)=5.1301e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  111/334 | (warm-up text) | align=0.0002 | text_tf=9.3714 | latent_scale=0.33
  step  112/334 | (warm-up text) | align=0.0002 | text_tf=8.2257 | latent_scale=0.33
  step  113/334 | (warm-up text) | align=0.0002 | text_tf=8.3288 | latent_scale=0.34
  step  114/334 | (warm-up text) | align=0.0002 | text_tf=8.3256 | latent_scale=0.34
  step  115/334 | (warm-up text) | align=0.0002 | text_tf=8.9165 | latent_scale=0.34
  step  116/334 | (warm-up text) | align=0.0002 | text_tf=8.9059 | latent_scale=0.34
  step  117/334 | (warm-up text) | align=0.0002 | text_tf=8.1196 | latent_scale=0.35
  step  118/334 | (warm-up text) | align=0.0002 | text_tf=8.0963 | latent_scale=0.35
  step  119/334 | (warm-up text) | align=0.0002 | text_tf=8.2833 | latent_scale=0.35
  step  120/334 | (warm-up text) | align=0.0002 | text_tf=8.6229 | latent_scale=0.36
  step  120/334 | grad_norm=6.15 | sec/step~4.04 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.6016 first=3.5030 kCE=4.1787 KD=0.0000 acc=0.000 state=6.5020 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9983 | scale_pen(llama)=1.4552e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  121/334 | (warm-up text) | align=0.0002 | text_tf=7.8380 | latent_scale=0.36
  step  122/334 | (warm-up text) | align=0.0002 | text_tf=7.7179 | latent_scale=0.36
  step  123/334 | (warm-up text) | align=0.0002 | text_tf=7.8805 | latent_scale=0.37
  step  124/334 | (warm-up text) | align=0.0002 | text_tf=8.7092 | latent_scale=0.37
  step  125/334 | (warm-up text) | align=0.0002 | text_tf=8.8037 | latent_scale=0.37
  step  126/334 | (warm-up text) | align=0.0002 | text_tf=8.1837 | latent_scale=0.37
  step  127/334 | (warm-up text) | align=0.0002 | text_tf=9.1938 | latent_scale=0.38
  step  128/334 | (warm-up text) | align=0.0002 | text_tf=8.2895 | latent_scale=0.38
  step  129/334 | (warm-up text) | align=0.0002 | text_tf=8.5785 | latent_scale=0.38
  step  130/334 | (warm-up text) | align=0.0002 | text_tf=8.3007 | latent_scale=0.39
  step  130/334 | grad_norm=1.09 | sec/step~3.74 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.5860 first=3.2042 kCE=4.4449 KD=0.0000 acc=0.000 state=6.4495 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9980 | scale_pen(llama)=1.3220e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  131/334 | (warm-up text) | align=0.0002 | text_tf=7.7666 | latent_scale=0.39
  step  132/334 | (warm-up text) | align=0.0002 | text_tf=7.6630 | latent_scale=0.39
  step  133/334 | (warm-up text) | align=0.0002 | text_tf=7.9724 | latent_scale=0.40
  step  134/334 | (warm-up text) | align=0.0002 | text_tf=8.0868 | latent_scale=0.40
  step  135/334 | (warm-up text) | align=0.0002 | text_tf=7.7660 | latent_scale=0.40
  step  136/334 | (warm-up text) | align=0.0002 | text_tf=7.5640 | latent_scale=0.40
  step  137/334 | (warm-up text) | align=0.0002 | text_tf=8.4041 | latent_scale=0.41
  step  138/334 | (warm-up text) | align=0.0002 | text_tf=7.9478 | latent_scale=0.41
  step  139/334 | (warm-up text) | align=0.0002 | text_tf=7.6637 | latent_scale=0.41
  step  140/334 | (warm-up text) | align=0.0002 | text_tf=8.4382 | latent_scale=0.42
  step  140/334 | grad_norm=5.46 | sec/step~3.55 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.7668 first=3.6726 kCE=4.6970 KD=0.0000 acc=0.000 state=6.7454 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9980 | scale_pen(llama)=1.3220e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  141/334 | (warm-up text) | align=0.0002 | text_tf=8.4149 | latent_scale=0.42
  step  142/334 | (warm-up text) | align=0.0002 | text_tf=8.2435 | latent_scale=0.42
  step  143/334 | (warm-up text) | align=0.0002 | text_tf=8.5311 | latent_scale=0.43
  step  144/334 | (warm-up text) | align=0.0002 | text_tf=8.4133 | latent_scale=0.43
  step  145/334 | (warm-up text) | align=0.0002 | text_tf=7.9738 | latent_scale=0.43
  step  146/334 | (warm-up text) | align=0.0002 | text_tf=7.4323 | latent_scale=0.43
  step  147/334 | (warm-up text) | align=0.0002 | text_tf=7.4151 | latent_scale=0.44
  step  148/334 | (warm-up text) | align=0.0002 | text_tf=6.9764 | latent_scale=0.44
  step  149/334 | (warm-up text) | align=0.0002 | text_tf=7.7270 | latent_scale=0.44
  step  150/334 | (warm-up text) | align=0.0002 | text_tf=7.3305 | latent_scale=0.45
  step  150/334 | grad_norm=6.67 | sec/step~3.55 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=5.2517 first=4.0658 kCE=5.4801 KD=0.0000 acc=0.042 state=6.6652 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9978 | scale_pen(llama)=2.2737e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  151/334 | (warm-up text) | align=0.0002 | text_tf=7.5540 | latent_scale=0.45
  step  152/334 | (warm-up text) | align=0.0002 | text_tf=8.0110 | latent_scale=0.45
  step  153/334 | (warm-up text) | align=0.0002 | text_tf=7.6191 | latent_scale=0.46
  step  154/334 | (warm-up text) | align=0.0002 | text_tf=7.1963 | latent_scale=0.46
  step  155/334 | (warm-up text) | align=0.0002 | text_tf=8.0737 | latent_scale=0.46
  step  156/334 | (warm-up text) | align=0.0002 | text_tf=6.5296 | latent_scale=0.46
  step  157/334 | (warm-up text) | align=0.0002 | text_tf=6.6789 | latent_scale=0.47
  step  158/334 | (warm-up text) | align=0.0002 | text_tf=7.0202 | latent_scale=0.47
  step  159/334 | (warm-up text) | align=0.0002 | text_tf=7.2508 | latent_scale=0.47
  step  160/334 | (warm-up text) | align=0.0002 | text_tf=6.9662 | latent_scale=0.48
  step  160/334 | grad_norm=17.37 | sec/step~4.34 | keep=0.72 | K=8 | first_w=4.00 | llama(T): tf=5.6136 first=4.5402 kCE=5.8221 KD=0.0000 acc=0.000 state=7.3405 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9978 | scale_pen(llama)=3.0070e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  161/334 | (warm-up text) | align=0.0002 | text_tf=7.9045 | latent_scale=0.48
  step  162/334 | (warm-up text) | align=0.0002 | text_tf=7.1199 | latent_scale=0.48
  step  163/334 | (warm-up text) | align=0.0002 | text_tf=6.4752 | latent_scale=0.49
  step  164/334 | (warm-up text) | align=0.0002 | text_tf=7.1706 | latent_scale=0.49
  step  165/334 | (warm-up text) | align=0.0002 | text_tf=7.4875 | latent_scale=0.49
  step  166/334 | (warm-up text) | align=0.0002 | text_tf=7.4453 | latent_scale=0.49
  step  167/334 | (warm-up text) | align=0.0002 | text_tf=7.1448 | latent_scale=0.50
  step  170/334 | grad_norm=11.78 | sec/step~3.51 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=11.4125 first=9.7477 kCE=11.8127 KD=22.4874 acc=0.083 state=13.3583 align=0.0000 latA=0.4998 latP=0.2490 gist=0.9976 | scale_pen(llama)=3.0070e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=4.36 | sec/step~3.68 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=10.0319 first=8.7664 kCE=10.3712 KD=23.0210 acc=0.000 state=13.7055 align=0.0000 latA=0.4971 latP=0.2490 gist=0.9974 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=14.06 | sec/step~3.38 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=9.6747 first=8.8526 kCE=10.2494 KD=27.6837 acc=0.000 state=13.9355 align=0.0000 latA=0.4958 latP=0.2488 gist=0.9974 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=9.65 | sec/step~3.69 | keep=0.73 | K=8 | first_w=4.00 | llama(L): tf=10.4091 first=8.2909 kCE=10.3728 KD=21.7727 acc=0.000 state=14.3688 align=0.0000 latA=0.5007 latP=0.2486 gist=0.9971 | scale_pen(llama)=2.3888e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=1.72 | sec/step~4.11 | keep=0.73 | K=8 | first_w=4.00 | llama(L): tf=10.5233 first=9.5909 kCE=10.8664 KD=20.8584 acc=0.042 state=14.0303 align=0.0000 latA=0.4980 latP=0.2486 gist=0.9969 | scale_pen(llama)=2.1064e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=6.87 | sec/step~3.65 | keep=0.73 | K=8 | first_w=3.99 | llama(L): tf=10.0421 first=9.2602 kCE=10.5322 KD=20.2868 acc=0.083 state=13.8462 align=0.0000 latA=0.4977 latP=0.2487 gist=0.9969 | scale_pen(llama)=2.1064e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=8.95 | sec/step~3.50 | keep=0.74 | K=8 | first_w=3.98 | llama(L): tf=10.3035 first=9.2880 kCE=10.7907 KD=20.0201 acc=0.083 state=14.1361 align=0.0000 latA=0.4998 latP=0.2482 gist=0.9966 | scale_pen(llama)=2.2737e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=23.78 | sec/step~3.57 | keep=0.74 | K=8 | first_w=3.96 | llama(L): tf=10.9269 first=9.4992 kCE=10.9723 KD=19.0247 acc=0.042 state=13.6424 align=0.0000 latA=0.4986 latP=0.2482 gist=0.9966 | scale_pen(llama)=8.8690e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=7.57 | sec/step~3.87 | keep=0.74 | K=8 | first_w=3.93 | llama(L): tf=10.0319 first=8.7161 kCE=10.0552 KD=19.1519 acc=0.000 state=14.1344 align=0.0000 latA=0.4968 latP=0.2481 gist=0.9964 | scale_pen(llama)=8.8690e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=4.69 | sec/step~3.62 | keep=0.75 | K=8 | first_w=3.90 | llama(L): tf=10.3276 first=9.4263 kCE=9.8125 KD=18.6436 acc=0.000 state=13.1604 align=0.0000 latA=0.5008 latP=0.2477 gist=0.9962 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=14.76 | sec/step~3.94 | keep=0.75 | K=8 | first_w=3.87 | llama(L): tf=10.2226 first=8.4630 kCE=9.9062 KD=17.6983 acc=0.250 state=14.2320 align=0.0000 latA=0.5037 latP=0.2480 gist=0.9961 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=7.28 | sec/step~3.87 | keep=0.75 | K=8 | first_w=3.83 | llama(L): tf=10.2494 first=7.9185 kCE=9.6491 KD=18.0965 acc=0.125 state=13.8872 align=0.0000 latA=0.5010 latP=0.2480 gist=0.9959 | scale_pen(llama)=5.2015e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=8.10 | sec/step~3.30 | keep=0.76 | K=8 | first_w=3.78 | llama(L): tf=10.8892 first=9.6985 kCE=10.3449 KD=14.5330 acc=0.000 state=13.6672 align=0.0000 latA=0.4991 latP=0.2478 gist=0.9957 | scale_pen(llama)=4.2210e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=47.47 | sec/step~3.73 | keep=0.76 | K=8 | first_w=3.73 | llama(L): tf=10.7661 first=8.8435 kCE=10.2770 KD=13.6132 acc=0.000 state=12.8593 align=0.0000 latA=0.4981 latP=0.2476 gist=0.9957 | scale_pen(llama)=4.2210e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=6.60 | sec/step~3.35 | keep=0.76 | K=8 | first_w=3.68 | llama(L): tf=10.1130 first=8.4099 kCE=9.8207 KD=17.5549 acc=0.000 state=12.9149 align=0.0000 latA=0.4979 latP=0.2475 gist=0.9954 | scale_pen(llama)=1.9455e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=15.14 | sec/step~3.46 | keep=0.77 | K=8 | first_w=3.62 | llama(L): tf=10.1250 first=8.2526 kCE=9.7668 KD=16.2807 acc=0.083 state=13.1045 align=0.0000 latA=0.4987 latP=0.2476 gist=0.9954 | scale_pen(llama)=2.4016e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=6.87 | sec/step~3.28 | keep=0.77 | K=8 | first_w=3.56 | llama(L): tf=9.8363 first=7.9889 kCE=9.4519 KD=16.3586 acc=0.083 state=13.5492 align=0.0000 latA=0.4995 latP=0.2476 gist=0.9952 | scale_pen(llama)=2.4016e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=9.54 | sec/step~4.90 | keep=0.77 | K=8 | first_w=3.53 | llama(L): tf=10.4519 first=8.4707 kCE=9.6333 KD=15.8017 acc=0.000 state=13.5565 align=0.0000 latA=0.4984 latP=0.2474 gist=0.9952 | scale_pen(llama)=2.3888e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 3.0KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 334
Epoch 2/2
  step  10/334 | grad_norm=12.30 | sec/step~3.64 | keep=0.78 | K=8 | first_w=3.47 | llama(L): tf=10.0318 first=7.8471 kCE=9.3801 KD=17.5074 acc=0.125 state=14.4858 align=0.0000 latA=0.5010 latP=0.2474 gist=0.9949 | scale_pen(llama)=2.3888e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=2.74 | sec/step~3.50 | keep=0.78 | K=8 | first_w=3.40 | llama(L): tf=9.6810 first=8.0976 kCE=9.0172 KD=17.9590 acc=0.000 state=12.8662 align=0.0000 latA=0.4974 latP=0.2471 gist=0.9947 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=9.09 | sec/step~3.30 | keep=0.79 | K=8 | first_w=3.32 | llama(L): tf=9.7711 first=7.4911 kCE=9.1545 KD=16.3427 acc=0.000 state=12.7945 align=0.0000 latA=0.4992 latP=0.2475 gist=0.9947 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=8.19 | sec/step~3.46 | keep=0.79 | K=8 | first_w=3.25 | llama(L): tf=10.0004 first=8.0170 kCE=9.4529 KD=16.6877 acc=0.000 state=13.4602 align=0.0000 latA=0.4988 latP=0.2473 gist=0.9944 | scale_pen(llama)=3.4142e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=2.06 | sec/step~3.78 | keep=0.80 | K=8 | first_w=3.17 | llama(L): tf=9.7521 first=7.0774 kCE=8.7791 KD=14.5726 acc=0.042 state=13.4840 align=0.0000 latA=0.4997 latP=0.2471 gist=0.9942 | scale_pen(llama)=3.0727e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=11.57 | sec/step~3.47 | keep=0.80 | K=8 | first_w=3.09 | llama(L): tf=9.4254 first=7.3308 kCE=8.6780 KD=16.2875 acc=0.000 state=11.6635 align=0.0000 latA=0.4977 latP=0.2470 gist=0.9942 | scale_pen(llama)=3.0727e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[WARN] KD teacher forward failed; skipping KD for this batch: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 118, in kd_first_k_prefix_vs_text
    _loss, _n_tok, teacher_logits_full = teacher_llm.loss_with_text_prompt(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/models.py", line 1120, in loss_with_text_prompt
    out = self.model(**model_kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/peft_model.py", line 1850, in forward
    return self.base_model(
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/tuners/tuners_utils.py", line 222, in forward
    return self.model.forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2354, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1625, in main
    loss_kd_raw = kd_first_k_prefix_vs_text(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 126, in kd_first_k_prefix_vs_text
    torch.cuda.synchronize()
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py", line 892, in synchronize
    return torch._C._cuda_synchronize()
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

