
>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=hero

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3352.09it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
⚠️  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 167 steps
Epoch 1/2
[warmup] step=0 mode=text (warm-up)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/334 | (warm-up text) | align=0.0002 | text_tf=14.1409 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/334 | (warm-up text) | align=0.0002 | text_tf=14.4763 | latent_scale=0.00
[warmup] step=2 mode=text (warm-up)
  step  3/334 | (warm-up text) | align=0.0002 | text_tf=15.6948 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/334 | (warm-up text) | align=0.0002 | text_tf=12.3696 | latent_scale=0.01
[warmup] step=4 mode=text (warm-up)
  step  5/334 | (warm-up text) | align=0.0002 | text_tf=14.9147 | latent_scale=0.01
[warmup] step=5 mode=text (warm-up)
  step  6/334 | (warm-up text) | align=0.0002 | text_tf=12.8197 | latent_scale=0.01
[warmup] step=6 mode=text (warm-up)
  step  7/334 | (warm-up text) | align=0.0002 | text_tf=12.6957 | latent_scale=0.02
[warmup] step=7 mode=text (warm-up)
  step  8/334 | (warm-up text) | align=0.0002 | text_tf=13.7441 | latent_scale=0.02
[warmup] step=8 mode=text (warm-up)
  step  9/334 | (warm-up text) | align=0.0002 | text_tf=13.3924 | latent_scale=0.02
[warmup] step=9 mode=text (warm-up)
  step  10/334 | (warm-up text) | align=0.0002 | text_tf=14.1952 | latent_scale=0.03
  step  10/334 | grad_norm=2.93 | sec/step~3.73 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=0.3666 first=0.3645 kCE=0.3581 KD=0.0000 acc=0.000 state=0.3883 align=0.0002 latA=0.0000 latP=0.0000 gist=1.0001 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  11/334 | (warm-up text) | align=0.0002 | text_tf=14.9200 | latent_scale=0.03
  step  12/334 | (warm-up text) | align=0.0002 | text_tf=13.7612 | latent_scale=0.03
  step  13/334 | (warm-up text) | align=0.0002 | text_tf=16.6193 | latent_scale=0.04
  step  14/334 | (warm-up text) | align=0.0002 | text_tf=14.2048 | latent_scale=0.04
  step  15/334 | (warm-up text) | align=0.0002 | text_tf=13.7806 | latent_scale=0.04
  step  16/334 | (warm-up text) | align=0.0002 | text_tf=12.6895 | latent_scale=0.04
  step  17/334 | (warm-up text) | align=0.0002 | text_tf=13.8391 | latent_scale=0.05
  step  18/334 | (warm-up text) | align=0.0002 | text_tf=13.4286 | latent_scale=0.05
  step  19/334 | (warm-up text) | align=0.0002 | text_tf=12.1901 | latent_scale=0.05
  step  20/334 | (warm-up text) | align=0.0002 | text_tf=12.4680 | latent_scale=0.06
  step  20/334 | grad_norm=12.80 | sec/step~3.75 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=1.8164 first=1.7066 kCE=2.4635 KD=0.0000 acc=0.000 state=0.8045 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9999 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  21/334 | (warm-up text) | align=0.0002 | text_tf=11.9517 | latent_scale=0.06
  step  22/334 | (warm-up text) | align=0.0002 | text_tf=12.5214 | latent_scale=0.06
  step  23/334 | (warm-up text) | align=0.0002 | text_tf=14.1759 | latent_scale=0.07
  step  24/334 | (warm-up text) | align=0.0002 | text_tf=12.4348 | latent_scale=0.07
  step  25/334 | (warm-up text) | align=0.0002 | text_tf=12.5419 | latent_scale=0.07
  step  26/334 | (warm-up text) | align=0.0002 | text_tf=14.0633 | latent_scale=0.07
  step  27/334 | (warm-up text) | align=0.0002 | text_tf=14.2221 | latent_scale=0.08
  step  28/334 | (warm-up text) | align=0.0002 | text_tf=13.2073 | latent_scale=0.08
  step  29/334 | (warm-up text) | align=0.0002 | text_tf=14.0834 | latent_scale=0.08
  step  30/334 | (warm-up text) | align=0.0002 | text_tf=12.5538 | latent_scale=0.09
  step  30/334 | grad_norm=57.22 | sec/step~3.45 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.9281 first=2.8848 kCE=3.7523 KD=0.0000 acc=0.000 state=1.1587 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9998 | scale_pen(llama)=1.1511e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  31/334 | (warm-up text) | align=0.0002 | text_tf=12.2320 | latent_scale=0.09
  step  32/334 | (warm-up text) | align=0.0002 | text_tf=13.3278 | latent_scale=0.09
  step  33/334 | (warm-up text) | align=0.0002 | text_tf=10.9252 | latent_scale=0.10
  step  34/334 | (warm-up text) | align=0.0002 | text_tf=11.5004 | latent_scale=0.10
  step  35/334 | (warm-up text) | align=0.0002 | text_tf=11.0662 | latent_scale=0.10
  step  36/334 | (warm-up text) | align=0.0002 | text_tf=11.8377 | latent_scale=0.10
  step  37/334 | (warm-up text) | align=0.0002 | text_tf=12.1438 | latent_scale=0.11
  step  38/334 | (warm-up text) | align=0.0002 | text_tf=11.1260 | latent_scale=0.11
  step  39/334 | (warm-up text) | align=0.0002 | text_tf=11.5839 | latent_scale=0.11
  step  40/334 | (warm-up text) | align=0.0002 | text_tf=11.2598 | latent_scale=0.12
  step  40/334 | grad_norm=17.43 | sec/step~4.08 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.3079 first=2.1267 kCE=3.0957 KD=0.0000 acc=0.000 state=1.7281 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9996 | scale_pen(llama)=5.6843e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  41/334 | (warm-up text) | align=0.0002 | text_tf=12.6461 | latent_scale=0.12
  step  42/334 | (warm-up text) | align=0.0002 | text_tf=11.6927 | latent_scale=0.12
  step  43/334 | (warm-up text) | align=0.0002 | text_tf=12.0570 | latent_scale=0.13
  step  44/334 | (warm-up text) | align=0.0002 | text_tf=11.2200 | latent_scale=0.13
  step  45/334 | (warm-up text) | align=0.0002 | text_tf=12.1987 | latent_scale=0.13
  step  46/334 | (warm-up text) | align=0.0002 | text_tf=11.6665 | latent_scale=0.13
  step  47/334 | (warm-up text) | align=0.0002 | text_tf=11.7376 | latent_scale=0.14
  step  48/334 | (warm-up text) | align=0.0002 | text_tf=10.6895 | latent_scale=0.14
  step  49/334 | (warm-up text) | align=0.0002 | text_tf=11.4241 | latent_scale=0.14
  step  50/334 | (warm-up text) | align=0.0002 | text_tf=10.1784 | latent_scale=0.15
  step  50/334 | grad_norm=4.33 | sec/step~4.09 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.1839 first=1.6287 kCE=2.3876 KD=0.0000 acc=0.000 state=2.3916 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9993 | scale_pen(llama)=1.4552e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  51/334 | (warm-up text) | align=0.0002 | text_tf=9.7775 | latent_scale=0.15
  step  52/334 | (warm-up text) | align=0.0002 | text_tf=10.0226 | latent_scale=0.15
  step  53/334 | (warm-up text) | align=0.0002 | text_tf=9.5788 | latent_scale=0.16
  step  54/334 | (warm-up text) | align=0.0002 | text_tf=10.5516 | latent_scale=0.16
  step  55/334 | (warm-up text) | align=0.0002 | text_tf=10.1188 | latent_scale=0.16
  step  56/334 | (warm-up text) | align=0.0002 | text_tf=11.4768 | latent_scale=0.16
  step  57/334 | (warm-up text) | align=0.0002 | text_tf=11.0425 | latent_scale=0.17
  step  58/334 | (warm-up text) | align=0.0002 | text_tf=11.4397 | latent_scale=0.17
  step  59/334 | (warm-up text) | align=0.0002 | text_tf=10.5543 | latent_scale=0.17
  step  60/334 | (warm-up text) | align=0.0002 | text_tf=10.8731 | latent_scale=0.18
  step  60/334 | grad_norm=28.76 | sec/step~3.51 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.5523 first=2.0838 kCE=2.8247 KD=0.0000 acc=0.000 state=2.6939 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9993 | scale_pen(llama)=1.4552e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  61/334 | (warm-up text) | align=0.0002 | text_tf=10.1022 | latent_scale=0.18
  step  62/334 | (warm-up text) | align=0.0002 | text_tf=10.4533 | latent_scale=0.18
  step  63/334 | (warm-up text) | align=0.0002 | text_tf=10.5221 | latent_scale=0.19
  step  64/334 | (warm-up text) | align=0.0002 | text_tf=9.4475 | latent_scale=0.19
  step  65/334 | (warm-up text) | align=0.0002 | text_tf=11.7196 | latent_scale=0.19
  step  66/334 | (warm-up text) | align=0.0002 | text_tf=10.3090 | latent_scale=0.19
  step  67/334 | (warm-up text) | align=0.0002 | text_tf=9.4033 | latent_scale=0.20
  step  68/334 | (warm-up text) | align=0.0002 | text_tf=10.4742 | latent_scale=0.20
  step  69/334 | (warm-up text) | align=0.0002 | text_tf=8.4590 | latent_scale=0.20
  step  70/334 | (warm-up text) | align=0.0002 | text_tf=11.1389 | latent_scale=0.21
  step  70/334 | grad_norm=5.14 | sec/step~3.37 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=2.7491 first=2.2071 kCE=2.4303 KD=0.0000 acc=0.000 state=3.3950 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9990 | scale_pen(llama)=1.8794e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  71/334 | (warm-up text) | align=0.0002 | text_tf=9.1295 | latent_scale=0.21
  step  72/334 | (warm-up text) | align=0.0002 | text_tf=10.1770 | latent_scale=0.21
  step  73/334 | (warm-up text) | align=0.0002 | text_tf=8.8941 | latent_scale=0.22
  step  74/334 | (warm-up text) | align=0.0002 | text_tf=10.0826 | latent_scale=0.22
  step  75/334 | (warm-up text) | align=0.0002 | text_tf=11.0313 | latent_scale=0.22
  step  76/334 | (warm-up text) | align=0.0002 | text_tf=0.0000 | latent_scale=0.22
  step  77/334 | (warm-up text) | align=0.0002 | text_tf=9.0736 | latent_scale=0.23
  step  78/334 | (warm-up text) | align=0.0002 | text_tf=9.2342 | latent_scale=0.23
  step  79/334 | (warm-up text) | align=0.0002 | text_tf=10.2903 | latent_scale=0.23
  step  80/334 | (warm-up text) | align=0.0002 | text_tf=9.5779 | latent_scale=0.24
  step  80/334 | grad_norm=13.96 | sec/step~4.14 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=3.1993 first=2.6076 kCE=2.8384 KD=0.0000 acc=0.000 state=4.1803 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9990 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  81/334 | (warm-up text) | align=0.0002 | text_tf=8.8311 | latent_scale=0.24
  step  82/334 | (warm-up text) | align=0.0002 | text_tf=10.1234 | latent_scale=0.24
  step  83/334 | (warm-up text) | align=0.0002 | text_tf=9.4837 | latent_scale=0.25
  step  84/334 | (warm-up text) | align=0.0002 | text_tf=9.7285 | latent_scale=0.25
  step  85/334 | (warm-up text) | align=0.0002 | text_tf=9.3804 | latent_scale=0.25
  step  86/334 | (warm-up text) | align=0.0002 | text_tf=9.1558 | latent_scale=0.25
  step  87/334 | (warm-up text) | align=0.0002 | text_tf=9.4105 | latent_scale=0.26
  step  88/334 | (warm-up text) | align=0.0002 | text_tf=9.2831 | latent_scale=0.26
  step  89/334 | (warm-up text) | align=0.0002 | text_tf=9.3038 | latent_scale=0.26
  step  90/334 | (warm-up text) | align=0.0002 | text_tf=9.6276 | latent_scale=0.27
  step  90/334 | grad_norm=8.49 | sec/step~3.34 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=3.6503 first=2.9308 kCE=3.0439 KD=0.0000 acc=0.000 state=4.7776 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9988 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  91/334 | (warm-up text) | align=0.0002 | text_tf=9.8349 | latent_scale=0.27
  step  92/334 | (warm-up text) | align=0.0002 | text_tf=9.5007 | latent_scale=0.27
  step  93/334 | (warm-up text) | align=0.0002 | text_tf=9.8240 | latent_scale=0.28
  step  94/334 | (warm-up text) | align=0.0002 | text_tf=10.2691 | latent_scale=0.28
  step  95/334 | (warm-up text) | align=0.0002 | text_tf=8.5313 | latent_scale=0.28
  step  96/334 | (warm-up text) | align=0.0002 | text_tf=9.7699 | latent_scale=0.28
  step  97/334 | (warm-up text) | align=0.0002 | text_tf=9.3398 | latent_scale=0.29
  step  98/334 | (warm-up text) | align=0.0002 | text_tf=9.1229 | latent_scale=0.29
  step  99/334 | (warm-up text) | align=0.0002 | text_tf=9.1646 | latent_scale=0.29
  step  100/334 | (warm-up text) | align=0.0002 | text_tf=10.5846 | latent_scale=0.30
  step  100/334 | grad_norm=3.21 | sec/step~4.11 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=3.7725 first=3.0607 kCE=3.3391 KD=0.0000 acc=0.000 state=5.6037 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9985 | scale_pen(llama)=1.5476e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  101/334 | (warm-up text) | align=0.0002 | text_tf=8.2755 | latent_scale=0.30
  step  102/334 | (warm-up text) | align=0.0002 | text_tf=9.5487 | latent_scale=0.30
  step  103/334 | (warm-up text) | align=0.0002 | text_tf=8.9626 | latent_scale=0.31
  step  104/334 | (warm-up text) | align=0.0002 | text_tf=9.4160 | latent_scale=0.31
  step  105/334 | (warm-up text) | align=0.0002 | text_tf=8.6176 | latent_scale=0.31
  step  106/334 | (warm-up text) | align=0.0002 | text_tf=8.6138 | latent_scale=0.31
  step  107/334 | (warm-up text) | align=0.0002 | text_tf=9.1299 | latent_scale=0.32
  step  108/334 | (warm-up text) | align=0.0002 | text_tf=9.1411 | latent_scale=0.32
  step  109/334 | (warm-up text) | align=0.0002 | text_tf=8.3977 | latent_scale=0.32
  step  110/334 | (warm-up text) | align=0.0002 | text_tf=8.5868 | latent_scale=0.33
  step  110/334 | grad_norm=11.30 | sec/step~3.71 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.2587 first=3.3643 kCE=3.6203 KD=0.0000 acc=0.000 state=5.9848 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9985 | scale_pen(llama)=1.5476e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  111/334 | (warm-up text) | align=0.0002 | text_tf=9.3725 | latent_scale=0.33
  step  112/334 | (warm-up text) | align=0.0002 | text_tf=8.2112 | latent_scale=0.33
  step  113/334 | (warm-up text) | align=0.0002 | text_tf=8.3355 | latent_scale=0.34
  step  114/334 | (warm-up text) | align=0.0002 | text_tf=8.3281 | latent_scale=0.34
  step  115/334 | (warm-up text) | align=0.0002 | text_tf=8.9164 | latent_scale=0.34
  step  116/334 | (warm-up text) | align=0.0002 | text_tf=8.9067 | latent_scale=0.34
  step  117/334 | (warm-up text) | align=0.0002 | text_tf=8.1217 | latent_scale=0.35
  step  118/334 | (warm-up text) | align=0.0002 | text_tf=8.1035 | latent_scale=0.35
  step  119/334 | (warm-up text) | align=0.0002 | text_tf=8.2675 | latent_scale=0.35
  step  120/334 | (warm-up text) | align=0.0002 | text_tf=8.6279 | latent_scale=0.36
  step  120/334 | grad_norm=6.15 | sec/step~4.06 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.6015 first=3.5034 kCE=4.1787 KD=0.0000 acc=0.000 state=6.4909 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9983 | scale_pen(llama)=1.9455e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  121/334 | (warm-up text) | align=0.0002 | text_tf=7.8442 | latent_scale=0.36
  step  122/334 | (warm-up text) | align=0.0002 | text_tf=7.7135 | latent_scale=0.36
  step  123/334 | (warm-up text) | align=0.0002 | text_tf=7.8960 | latent_scale=0.37
  step  124/334 | (warm-up text) | align=0.0002 | text_tf=8.7081 | latent_scale=0.37
  step  125/334 | (warm-up text) | align=0.0002 | text_tf=8.8081 | latent_scale=0.37
  step  126/334 | (warm-up text) | align=0.0002 | text_tf=8.1810 | latent_scale=0.37
  step  127/334 | (warm-up text) | align=0.0002 | text_tf=9.1928 | latent_scale=0.38
  step  128/334 | (warm-up text) | align=0.0002 | text_tf=8.2971 | latent_scale=0.38
  step  129/334 | (warm-up text) | align=0.0002 | text_tf=8.5738 | latent_scale=0.38
  step  130/334 | (warm-up text) | align=0.0002 | text_tf=8.2961 | latent_scale=0.39
  step  130/334 | grad_norm=1.09 | sec/step~3.60 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.5856 first=3.2050 kCE=4.4444 KD=0.0000 acc=0.000 state=6.4496 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9980 | scale_pen(llama)=7.9936e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  131/334 | (warm-up text) | align=0.0002 | text_tf=7.7569 | latent_scale=0.39
  step  132/334 | (warm-up text) | align=0.0002 | text_tf=7.6708 | latent_scale=0.39
  step  133/334 | (warm-up text) | align=0.0002 | text_tf=7.9727 | latent_scale=0.40
  step  134/334 | (warm-up text) | align=0.0002 | text_tf=8.0860 | latent_scale=0.40
  step  135/334 | (warm-up text) | align=0.0002 | text_tf=7.7613 | latent_scale=0.40
  step  136/334 | (warm-up text) | align=0.0002 | text_tf=7.5663 | latent_scale=0.40
  step  137/334 | (warm-up text) | align=0.0002 | text_tf=8.4056 | latent_scale=0.41
  step  138/334 | (warm-up text) | align=0.0002 | text_tf=7.9487 | latent_scale=0.41
  step  139/334 | (warm-up text) | align=0.0002 | text_tf=7.6685 | latent_scale=0.41
  step  140/334 | (warm-up text) | align=0.0002 | text_tf=8.4464 | latent_scale=0.42
  step  140/334 | grad_norm=5.46 | sec/step~3.41 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=4.7664 first=3.6716 kCE=4.6971 KD=0.0000 acc=0.000 state=6.7454 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9980 | scale_pen(llama)=7.9936e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  141/334 | (warm-up text) | align=0.0002 | text_tf=8.4149 | latent_scale=0.42
  step  142/334 | (warm-up text) | align=0.0002 | text_tf=8.2378 | latent_scale=0.42
  step  143/334 | (warm-up text) | align=0.0002 | text_tf=8.5315 | latent_scale=0.43
  step  144/334 | (warm-up text) | align=0.0002 | text_tf=8.4050 | latent_scale=0.43
  step  145/334 | (warm-up text) | align=0.0002 | text_tf=7.9755 | latent_scale=0.43
  step  146/334 | (warm-up text) | align=0.0002 | text_tf=7.4386 | latent_scale=0.43
  step  147/334 | (warm-up text) | align=0.0002 | text_tf=7.4076 | latent_scale=0.44
  step  148/334 | (warm-up text) | align=0.0002 | text_tf=6.9732 | latent_scale=0.44
  step  149/334 | (warm-up text) | align=0.0002 | text_tf=7.7237 | latent_scale=0.44
  step  150/334 | (warm-up text) | align=0.0002 | text_tf=7.3293 | latent_scale=0.45
  step  150/334 | grad_norm=6.66 | sec/step~3.79 | keep=0.71 | K=8 | first_w=4.00 | llama(T): tf=5.2510 first=4.0680 kCE=5.4806 KD=0.0000 acc=0.042 state=6.6652 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9978 | scale_pen(llama)=2.9878e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  151/334 | (warm-up text) | align=0.0002 | text_tf=7.5489 | latent_scale=0.45
  step  152/334 | (warm-up text) | align=0.0002 | text_tf=8.0078 | latent_scale=0.45
  step  153/334 | (warm-up text) | align=0.0002 | text_tf=7.6212 | latent_scale=0.46
  step  154/334 | (warm-up text) | align=0.0002 | text_tf=7.2026 | latent_scale=0.46
  step  155/334 | (warm-up text) | align=0.0002 | text_tf=8.0648 | latent_scale=0.46
  step  156/334 | (warm-up text) | align=0.0002 | text_tf=6.5233 | latent_scale=0.46
  step  157/334 | (warm-up text) | align=0.0002 | text_tf=6.6870 | latent_scale=0.47
  step  158/334 | (warm-up text) | align=0.0002 | text_tf=7.0163 | latent_scale=0.47
  step  159/334 | (warm-up text) | align=0.0002 | text_tf=7.2493 | latent_scale=0.47
  step  160/334 | (warm-up text) | align=0.0002 | text_tf=6.9677 | latent_scale=0.48
  step  160/334 | grad_norm=17.36 | sec/step~4.09 | keep=0.72 | K=8 | first_w=4.00 | llama(T): tf=5.6148 first=4.5415 kCE=5.8234 KD=0.0000 acc=0.000 state=7.3406 align=0.0002 latA=0.0000 latP=0.0000 gist=0.9978 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  161/334 | (warm-up text) | align=0.0002 | text_tf=7.9046 | latent_scale=0.48
  step  162/334 | (warm-up text) | align=0.0002 | text_tf=7.1181 | latent_scale=0.48
  step  163/334 | (warm-up text) | align=0.0002 | text_tf=6.4789 | latent_scale=0.49
  step  164/334 | (warm-up text) | align=0.0002 | text_tf=7.1742 | latent_scale=0.49
  step  165/334 | (warm-up text) | align=0.0002 | text_tf=7.4952 | latent_scale=0.49
  step  166/334 | (warm-up text) | align=0.0002 | text_tf=7.4448 | latent_scale=0.49
  step  167/334 | (warm-up text) | align=0.0002 | text_tf=7.1454 | latent_scale=0.50
  step  170/334 | grad_norm=11.79 | sec/step~3.32 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=11.4112 first=9.7521 kCE=11.8125 KD=22.4877 acc=0.083 state=13.3583 align=0.0000 latA=0.4998 latP=0.2490 gist=0.9976 | scale_pen(llama)=3.5527e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  180/334 | grad_norm=4.35 | sec/step~3.52 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=10.0324 first=8.7668 kCE=10.3714 KD=23.0204 acc=0.000 state=13.7374 align=0.0000 latA=0.4971 latP=0.2490 gist=0.9974 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  190/334 | grad_norm=14.01 | sec/step~3.77 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=9.6763 first=8.8562 kCE=10.2495 KD=27.6861 acc=0.000 state=13.9362 align=0.0000 latA=0.4958 latP=0.2488 gist=0.9974 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  200/334 | grad_norm=9.65 | sec/step~3.83 | keep=0.73 | K=8 | first_w=4.00 | llama(L): tf=10.4089 first=8.2908 kCE=10.3724 KD=21.7729 acc=0.000 state=14.3687 align=0.0000 latA=0.5007 latP=0.2486 gist=0.9971 | scale_pen(llama)=1.7408e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  210/334 | grad_norm=1.72 | sec/step~3.81 | keep=0.73 | K=8 | first_w=4.00 | llama(L): tf=10.5237 first=9.5894 kCE=10.8655 KD=20.8584 acc=0.042 state=14.0296 align=0.0000 latA=0.4980 latP=0.2486 gist=0.9969 | scale_pen(llama)=5.6843e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  220/334 | grad_norm=6.86 | sec/step~3.84 | keep=0.73 | K=8 | first_w=3.99 | llama(L): tf=10.0443 first=9.2612 kCE=10.5306 KD=20.2869 acc=0.083 state=13.8462 align=0.0000 latA=0.4977 latP=0.2487 gist=0.9969 | scale_pen(llama)=5.6843e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  230/334 | grad_norm=8.94 | sec/step~3.57 | keep=0.74 | K=8 | first_w=3.98 | llama(L): tf=10.3029 first=9.2853 kCE=10.7916 KD=20.0212 acc=0.083 state=14.1361 align=0.0000 latA=0.4998 latP=0.2482 gist=0.9966 | scale_pen(llama)=5.1301e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  240/334 | grad_norm=23.77 | sec/step~3.24 | keep=0.74 | K=8 | first_w=3.96 | llama(L): tf=10.9262 first=9.5025 kCE=10.9740 KD=19.0235 acc=0.042 state=13.6426 align=0.0000 latA=0.4986 latP=0.2482 gist=0.9966 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  250/334 | grad_norm=7.58 | sec/step~3.65 | keep=0.74 | K=8 | first_w=3.93 | llama(L): tf=10.0305 first=8.7166 kCE=10.0567 KD=19.1505 acc=0.000 state=14.1347 align=0.0000 latA=0.4968 latP=0.2481 gist=0.9964 | scale_pen(llama)=1.6428e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  260/334 | grad_norm=4.69 | sec/step~3.47 | keep=0.75 | K=8 | first_w=3.90 | llama(L): tf=10.3240 first=9.4269 kCE=9.8143 KD=18.6416 acc=0.000 state=13.1302 align=0.0000 latA=0.5008 latP=0.2477 gist=0.9962 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  270/334 | grad_norm=14.76 | sec/step~3.78 | keep=0.75 | K=8 | first_w=3.87 | llama(L): tf=10.2218 first=8.4624 kCE=9.9069 KD=17.6948 acc=0.250 state=14.2333 align=0.0000 latA=0.5037 latP=0.2480 gist=0.9961 | scale_pen(llama)=3.1974e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  280/334 | grad_norm=7.29 | sec/step~3.66 | keep=0.75 | K=8 | first_w=3.83 | llama(L): tf=10.2475 first=7.9119 kCE=9.6495 KD=18.1003 acc=0.125 state=13.8885 align=0.0000 latA=0.5010 latP=0.2480 gist=0.9959 | scale_pen(llama)=2.2172e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  290/334 | grad_norm=8.07 | sec/step~3.45 | keep=0.76 | K=8 | first_w=3.78 | llama(L): tf=10.8865 first=9.6965 kCE=10.3452 KD=14.5341 acc=0.000 state=13.6653 align=0.0000 latA=0.4991 latP=0.2478 gist=0.9957 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  300/334 | grad_norm=47.23 | sec/step~3.61 | keep=0.76 | K=8 | first_w=3.73 | llama(L): tf=10.7650 first=8.8389 kCE=10.2758 KD=13.6102 acc=0.000 state=12.8580 align=0.0000 latA=0.4981 latP=0.2476 gist=0.9957 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  310/334 | grad_norm=6.59 | sec/step~3.38 | keep=0.76 | K=8 | first_w=3.68 | llama(L): tf=10.1094 first=8.4099 kCE=9.8196 KD=17.5568 acc=0.000 state=12.9183 align=0.0000 latA=0.4979 latP=0.2475 gist=0.9954 | scale_pen(llama)=2.1615e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  320/334 | grad_norm=15.12 | sec/step~3.31 | keep=0.77 | K=8 | first_w=3.62 | llama(L): tf=10.1259 first=8.2516 kCE=9.7667 KD=16.2673 acc=0.083 state=13.1089 align=0.0000 latA=0.4987 latP=0.2476 gist=0.9954 | scale_pen(llama)=5.6843e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  330/334 | grad_norm=6.87 | sec/step~3.43 | keep=0.77 | K=8 | first_w=3.56 | llama(L): tf=9.8366 first=7.9890 kCE=9.4509 KD=16.3503 acc=0.083 state=13.5500 align=0.0000 latA=0.4995 latP=0.2476 gist=0.9952 | scale_pen(llama)=5.6843e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  334/334 | grad_norm=9.54 | sec/step~4.25 | keep=0.77 | K=8 | first_w=3.53 | llama(L): tf=10.4536 first=8.4649 kCE=9.6335 KD=15.7907 acc=0.000 state=13.5573 align=0.0000 latA=0.4984 latP=0.2474 gist=0.9952 | scale_pen(llama)=1.5948e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 3.0KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 334
Epoch 2/2
  step  10/334 | grad_norm=12.27 | sec/step~4.24 | keep=0.78 | K=8 | first_w=3.47 | llama(L): tf=10.0320 first=7.8399 kCE=9.3781 KD=17.4980 acc=0.125 state=14.4869 align=0.0000 latA=0.5010 latP=0.2474 gist=0.9949 | scale_pen(llama)=1.5948e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/334 | grad_norm=2.74 | sec/step~3.74 | keep=0.78 | K=8 | first_w=3.40 | llama(L): tf=9.6787 first=8.0962 kCE=9.0162 KD=17.9545 acc=0.000 state=12.8363 align=0.0000 latA=0.4974 latP=0.2471 gist=0.9947 | scale_pen(llama)=3.3427e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/334 | grad_norm=9.10 | sec/step~3.55 | keep=0.79 | K=8 | first_w=3.32 | llama(L): tf=9.7714 first=7.4872 kCE=9.1532 KD=16.3365 acc=0.000 state=12.7961 align=0.0000 latA=0.4992 latP=0.2475 gist=0.9947 | scale_pen(llama)=3.3427e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/334 | grad_norm=8.20 | sec/step~3.78 | keep=0.79 | K=8 | first_w=3.25 | llama(L): tf=9.9979 first=8.0135 kCE=9.4511 KD=16.6634 acc=0.000 state=13.4607 align=0.0000 latA=0.4988 latP=0.2473 gist=0.9944 | scale_pen(llama)=4.1069e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/334 | grad_norm=2.06 | sec/step~3.45 | keep=0.80 | K=8 | first_w=3.17 | llama(L): tf=9.7515 first=7.0738 kCE=8.7768 KD=14.5403 acc=0.042 state=13.4849 align=0.0000 latA=0.4997 latP=0.2471 gist=0.9942 | scale_pen(llama)=4.4565e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/334 | grad_norm=11.56 | sec/step~3.09 | keep=0.80 | K=8 | first_w=3.09 | llama(L): tf=9.4288 first=7.3331 kCE=8.6783 KD=16.2584 acc=0.000 state=11.6954 align=0.0000 latA=0.4977 latP=0.2470 gist=0.9942 | scale_pen(llama)=4.4565e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
[WARN] KD teacher forward failed; skipping KD for this batch: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2354, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1625, in main
    loss_kd_raw = kd_first_k_prefix_vs_text(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 129, in kd_first_k_prefix_vs_text
    return torch.zeros((), device=student_device)
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

