Starting training at Sat Oct 11 16:22:13 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

Configuration:
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  compress_dim: 1024
  compress_method: pca
  input_dim: 4096
  pca_samples: 10000
  adapter_hidden_mult: 4
  adapter_dropout: 0.1
  adapter_lr: 0.0005
  samples: 10000
  epochs: 3
  batch_size: 64
  eval_every: 1
  eval_samples: 500
  save_dir: runs/stage1_adapter_only
  diagnostic_log: runs/stage1_adapter_only/logs/diagnostics.jsonl
============================================================
STAGE 1 PHASE 1: PURE RECONSTRUCTION TRAINING
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Compression: 4096 → 1024 (4.0× compression)
Training samples: 10000
PCA samples: 10000
============================================================

GPU Information:
  CUDA available: Yes
  GPU count: 4
  GPU 0: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 1: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 2: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 3: NVIDIA H100 80GB HBM3 (85.0 GB)

Logging diagnostics to: runs/stage1_adapter_only/logs/diagnostics.jsonl

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3077.82it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.60s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]

Creating adapter...
Adapter parameters: 50,366,465

Loading dataset...
Training samples: 10000
Validation samples: 500

Fitting PCA compressor on 10,000 samples...

GPU memory status:
  GPU 0: 80.9 GB free / 85.0 GB total (allocated: 3.3 GB)
  GPU 1: 80.1 GB free / 85.0 GB total (allocated: 4.4 GB)
  GPU 2: 80.1 GB free / 85.0 GB total (allocated: 4.4 GB)
  GPU 3: 80.3 GB free / 85.0 GB total (allocated: 4.1 GB)

Using GPU 0 for PCA (80.9 GB free)
Collecting embeddings...
Collecting embeddings:   0%|          | 0/157 [00:00<?, ?it/s]Collecting embeddings:   1%|▏         | 2/157 [00:00<00:08, 18.94it/s]Collecting embeddings:   4%|▍         | 7/157 [00:00<00:04, 34.90it/s]Collecting embeddings:   8%|▊         | 13/157 [00:00<00:03, 42.57it/s]Collecting embeddings:  12%|█▏        | 19/157 [00:00<00:03, 45.33it/s]Collecting embeddings:  16%|█▌        | 25/157 [00:00<00:02, 48.22it/s]Collecting embeddings:  20%|█▉        | 31/157 [00:00<00:02, 48.12it/s]Collecting embeddings:  23%|██▎       | 36/157 [00:00<00:03, 35.17it/s]Collecting embeddings:  26%|██▌       | 41/157 [00:01<00:03, 33.50it/s]Collecting embeddings:  29%|██▊       | 45/157 [00:01<00:04, 27.22it/s]Collecting embeddings:  32%|███▏      | 51/157 [00:01<00:03, 32.54it/s]Collecting embeddings:  36%|███▋      | 57/157 [00:01<00:02, 37.08it/s]Collecting embeddings:  40%|████      | 63/157 [00:01<00:02, 40.77it/s]Collecting embeddings:  44%|████▍     | 69/157 [00:01<00:02, 43.94it/s]Collecting embeddings:  48%|████▊     | 75/157 [00:01<00:01, 46.24it/s]Collecting embeddings:  51%|█████     | 80/157 [00:01<00:01, 46.97it/s]Collecting embeddings:  54%|█████▍    | 85/157 [00:02<00:01, 46.79it/s]Collecting embeddings:  57%|█████▋    | 90/157 [00:02<00:01, 47.32it/s]Collecting embeddings:  61%|██████    | 96/157 [00:02<00:01, 48.92it/s]Collecting embeddings:  64%|██████▍   | 101/157 [00:02<00:01, 47.08it/s]Collecting embeddings:  68%|██████▊   | 106/157 [00:02<00:01, 36.04it/s]Collecting embeddings:  71%|███████   | 111/157 [00:02<00:01, 34.21it/s]Collecting embeddings:  73%|███████▎  | 115/157 [00:02<00:01, 32.18it/s]Collecting embeddings:  76%|███████▌  | 119/157 [00:03<00:01, 30.62it/s]Collecting embeddings:  79%|███████▉  | 124/157 [00:03<00:00, 34.38it/s]Collecting embeddings:  82%|████████▏ | 129/157 [00:03<00:00, 37.87it/s]Collecting embeddings:  86%|████████▌ | 135/157 [00:03<00:00, 41.95it/s]Collecting embeddings:  90%|████████▉ | 141/157 [00:03<00:00, 44.68it/s]Collecting embeddings:  93%|█████████▎| 146/157 [00:03<00:00, 45.47it/s]Collecting embeddings:  96%|█████████▌| 151/157 [00:03<00:00, 46.60it/s]Collecting embeddings: 100%|██████████| 157/157 [00:03<00:00, 49.93it/s]Collecting embeddings: 100%|██████████| 157/157 [00:03<00:00, 40.57it/s]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/train_adapter_only_phase1.py", line 624, in main
    best_f1 = train_adapter_phase1(args)
  File "/projects/m000066/sujinesh/LatentWire/train_adapter_only_phase1.py", line 295, in train_adapter_phase1
    compressor.fit(all_embeddings, device=pca_device)
  File "/projects/m000066/sujinesh/LatentWire/train_adapter_only_phase1.py", line 67, in fit
    U, S, Vt = torch.linalg.svd(centered, full_matrices=False)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 79.19 GiB of which 9.38 GiB is free. Including non-PyTorch memory, this process has 69.80 GiB memory in use. Of the allocated memory 69.07 GiB is allocated by PyTorch, and 135.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Concatenating 1,727,893 embedding vectors...
  Fitting PCA on CPU with 1,727,893 embedding vectors...

============================================================
ERROR: Training failed!
============================================================
Error: CUDA out of memory. Tried to allocate 26.37 GiB. GPU 0 has a total capacity of 79.19 GiB of which 9.38 GiB is free. Including non-PyTorch memory, this process has 69.80 GiB memory in use. Of the allocated memory 69.07 GiB is allocated by PyTorch, and 135.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
