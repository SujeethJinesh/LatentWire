Starting training at Sat Oct 11 15:38:04 PDT 2025

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

Configuration:
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  compress_dim: 1024
  compress_method: pca
  input_dim: 4096
  pca_samples: 10000
  adapter_hidden_mult: 4
  adapter_dropout: 0.1
  adapter_lr: 0.0005
  samples: 10000
  epochs: 3
  batch_size: 64
  eval_every: 1
  eval_samples: 500
  save_dir: runs/stage1_adapter_only
  diagnostic_log: runs/stage1_adapter_only/logs/diagnostics.jsonl
============================================================
STAGE 1 PHASE 1: PURE RECONSTRUCTION TRAINING
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Compression: 4096 → 1024 (4.0× compression)
Training samples: 10000
PCA samples: 10000
============================================================

GPU Information:
  CUDA available: Yes
  GPU count: 4
  GPU 0: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 1: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 2: NVIDIA H100 80GB HBM3 (85.0 GB)
  GPU 3: NVIDIA H100 80GB HBM3 (85.0 GB)

Logging diagnostics to: runs/stage1_adapter_only/logs/diagnostics.jsonl

Loading model...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2113.53it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.45s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  5.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.00s/it]

Creating adapter...
Adapter parameters: 50,366,465

Loading dataset...
Training samples: 10000
Validation samples: 500

Fitting PCA compressor on 10,000 samples...
Using memory-efficient chunked collection (5k samples per chunk)...
Collecting embeddings in chunks:   0%|          | 0/157 [00:00<?, ?it/s]Collecting embeddings in chunks:   1%|          | 1/157 [00:28<1:13:37, 28.32s/it]Collecting embeddings in chunks:   1%|▏         | 2/157 [01:26<1:58:55, 46.04s/it]Collecting embeddings in chunks:   2%|▏         | 3/157 [02:24<2:11:19, 51.16s/it]Collecting embeddings in chunks:   3%|▎         | 4/157 [03:21<2:17:09, 53.79s/it]Collecting embeddings in chunks:   3%|▎         | 5/157 [04:17<2:18:09, 54.53s/it]Collecting embeddings in chunks:   4%|▍         | 6/157 [05:14<2:19:18, 55.36s/it]Collecting embeddings in chunks:   4%|▍         | 7/157 [06:10<2:19:03, 55.63s/it]Collecting embeddings in chunks:   5%|▌         | 8/157 [07:07<2:19:19, 56.10s/it]Collecting embeddings in chunks:   6%|▌         | 9/157 [08:03<2:18:10, 56.02s/it]Collecting embeddings in chunks:   6%|▋         | 10/157 [08:59<2:16:55, 55.88s/it]Collecting embeddings in chunks:   7%|▋         | 11/157 [09:55<2:16:05, 55.92s/it]