
=========================================
Starting pipeline at Fri Sep 19 00:43:01 PDT 2025
=========================================

Preset: FAST_2H  |  GPUs: 0,1,2,3  |  Llama devices: 0,1  |  Qwen devices: 2,3
Run dir: runs/8B_hailmary_allknobs_corrected_fast_2h


=========================================
TRAIN + PER-EPOCH EVAL (All knobs enabled)
=========================================


=========================================
EPOCH 1/1
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_corrected_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode auto --latent_anchor_text '' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.15 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 84 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_corrected_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3262.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3204.20it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.585  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.655   F1: 0.836   |  NLL/token (gold): 25.81069942126198
Wall clock: 19.36s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.015  |  NLL/token (gold): 11.141284474980534
       First-token acc: top1=0.000  top5=0.000
Qwen   EM: 0.000   F1: 0.008  |  NLL/token (gold): 16.85340898755997
       First-token acc: top1=0.000  top5=0.000
Wall clock: 3.06s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.77s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.019

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.585,
      "f1": 0.7992829157767006,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.655,
      "f1": 0.8360093209598256,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 19.358525037765503
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.014529803609440537,
      "nll": 11.141284474980534,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 11.141284474980534
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.00756798226141794,
      "nll": 16.85340898755997,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 16.85340898755997
    },
    "wall_clock_sec": 3.060267210006714
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04115193480713192
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.7691702842712402
  },
  "joint": {
    "em": 0.0,
    "f1": 0.003888471074923906,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.01882941283337063
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "auto",
      "latent_anchor_text": "",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.01882941283337063
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3217.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3404.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.07it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_corrected_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=1, global_step=10
Epoch 2/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/10 | grad_norm=218.32 | sec/step~8.59 | keep=0.85 | K=8 | llama: tf=11.8643 first=11.4174 kCE=5.6473 KD=5.0212 state=3.8366 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=9.8307 first=13.8129 kCE=11.2674 KD=5.3015 state=0.1538 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
[checkpoint] Freed 2.1KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_corrected_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010572597291320563, 'rms_mean_cal': 0.010569874569773674, 'embed_rms': 0.010571543127298355, 'count': 10}, 'qwen': {'rms_mean_raw': 0.013659418653696775, 'rms_mean_cal': 0.013635240402072668, 'embed_rms': 0.013653330504894257, 'count': 10}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_corrected_fast_2h/epoch1/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1 --dataset squad --max_new_tokens 16 --latent_anchor_mode auto --latent_anchor_text '' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.15 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 84 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_corrected_fast_2h/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3183.53it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.34s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3555.25it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.585  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.655   F1: 0.836   |  NLL/token (gold): 25.81069942126198
Wall clock: 20.31s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.006  |  NLL/token (gold): 11.227892446950449
       First-token acc: top1=0.000  top5=0.005
Qwen   EM: 0.000   F1: 0.006  |  NLL/token (gold): 16.847822601202303
       First-token acc: top1=0.000  top5=0.000
Wall clock: 3.67s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.29s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.010

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.585,
      "f1": 0.7992829157767006,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.655,
      "f1": 0.8360093209598256,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 20.30587649345398
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.005843055576684474,
      "nll": 11.227892446950449,
      "first_token_top1": 0.0,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 11.227892446950449
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.005926587104980233,
      "nll": 16.847822601202303,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 16.847822601202303
    },
    "wall_clock_sec": 3.670410633087158
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04115193480713192
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.2934019565582275
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0036626982772496315,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.009763690369155881
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "auto",
      "latent_anchor_text": "",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.009763690369155881
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1/metrics.json
  Text F1:    Llama 0.799 | Qwen 0.836
  Latent F1:  Llama 0.006 | Qwen 0.006
  FirstTok@1: Llama - | Qwen -
  NLL/token:  Llama 11.228 | Qwen 16.848
Top 5 latent predictions from runs/8B_hailmary_allknobs_corrected_fast_2h/eval_epoch1/predictions.jsonl
  1. Llama: NNTP is a protocol used for sending and receiving news articles over the Internet | Qwen: It seems like you've entered a string of characters that don't form | Gold: linear
  2. Llama: NNTP is a protocol used for sending and receiving news articles over the internet | Qwen: It seems like you've entered a string of characters that don't form | Gold: Lampea
  3. Llama: NNTP (Network News Transfer Protocol) is a protocol used for transferring news articles | Qwen: Hello! It seems like there might have been a mix-up in | Gold: residents willing to pay higher market rate for housing
  4. Llama: NNTPD (Network News Transfer Protocol) is a protocol used for transferring news | Qwen: It seems like you've entered a string of "k" and | Gold: San Jose
  5. Llama: NNTP is a protocol used for sending and receiving news articles over the Internet | Qwen: Hello! It seems like there might have been a mix-up with | Gold: oxides

=========================================
FINAL FULL EVAL (best epoch by avg latent F1)
=========================================

Best epoch: 1 (avg latent F1: 0.005884821340832354)
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_corrected_fast_2h/epoch1/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 400 --out_dir runs/8B_hailmary_allknobs_corrected_fast_2h/eval_final_best --dataset squad --max_new_tokens 16 --latent_anchor_mode auto --latent_anchor_text '' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.15 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 84 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_corrected_fast_2h/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_final_best/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2935.65it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2744.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 400  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.4 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 8601600 bytes; fp32 reference: 17203200 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.530  F1: 0.779  |  NLL/token (gold): 12.783967164200934
Qwen   EM: 0.585   F1: 0.793   |  NLL/token (gold): 25.918280501998048
Wall clock: 35.78s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.007  |  NLL/token (gold): 11.139645238234378
       First-token acc: top1=0.000  top5=0.002
Qwen   EM: 0.000   F1: 0.006  |  NLL/token (gold): 16.735313057974402
       First-token acc: top1=0.000  top5=0.000
Wall clock: 6.23s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.007  F1: 0.040
Qwen   EM: 0.035   F1: 0.076
Wall clock: 6.81s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.011

==== METRICS_JSON ====
{
  "samples": 400,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.3875,
    "qwen": 231.6975
  },
  "compression": {
    "llama": 5.112239583333333,
    "qwen": 4.82703125
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 17203200,
    "fp16": 8601600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 501115,
      "qwen": 440315
    },
    "prompt_count": 400,
    "latent_shape": [
      400,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 17203200,
      "fp16": 8601600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.53,
      "f1": 0.7789608369278556,
      "nll_token": 12.783967164200934
    },
    "qwen": {
      "em": 0.585,
      "f1": 0.7931015706541418,
      "nll_token": 25.918280501998048
    },
    "wall_clock_sec": 35.784770011901855
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.007178867907655829,
      "nll": 11.139645238234378,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0024999999441206455,
      "nll_token": 11.139645238234378
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.005739216521887851,
      "nll": 16.735313057974402,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 16.735313057974402
    },
    "wall_clock_sec": 6.230425834655762
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0075,
      "f1": 0.040241602193382305
    },
    "qwen": {
      "em": 0.035,
      "f1": 0.07612336216088741
    },
    "wall_clock_sec": 6.810436248779297
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004479499153316909,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.011343212864361072
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "auto",
      "latent_anchor_text": "",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.011343212864361072
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_corrected_fast_2h/eval_final_best/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_corrected_fast_2h/eval_final_best/metrics.json
  Text F1:    Llama 0.779 | Qwen 0.793
  Latent F1:  Llama 0.007 | Qwen 0.006
  FirstTok@1: Llama - | Qwen -
  NLL/token:  Llama 11.140 | Qwen 16.735
Top 5 latent predictions from runs/8B_hailmary_allknobs_corrected_fast_2h/eval_final_best/predictions.jsonl
  1. Llama: NNTP is a protocol used for sending and receiving news articles over the Internet | Qwen: It seems like you've entered a string of characters that don't form | Gold: linear
  2. Llama: NNTP is a protocol used for sending and receiving news articles over the internet | Qwen: It seems like you've entered a string of characters that don't form | Gold: Lampea
  3. Llama: NNTP (Network News Transfer Protocol) is a protocol used for transferring news articles | Qwen: Hello! It seems like you might have sent a mix of text | Gold: residents willing to pay higher market rate for housing
  4. Llama: NNN = Not a Number | Qwen: It seems like you've entered a string of "k" and | Gold: San Jose
  5. Llama: NNTP is a protocol used for sending and receiving news articles over the Internet | Qwen: Hello! It seems like there might have been a mix-up with | Gold: oxides

=========================================
PIPELINE SUMMARY
=========================================

Run: 8B_hailmary_allknobs_corrected_fast_2h  |  Completed: Fri Sep 19 00:50:32 PDT 2025
All outputs under: runs/8B_hailmary_allknobs_corrected_fast_2h
