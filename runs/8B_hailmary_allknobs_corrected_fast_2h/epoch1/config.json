{
  "d_z": 256,
  "latent_len": 48,
  "latent_shared_len": 36,
  "latent_private_len": 6,
  "byte_max": 2048,
  "llama_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
  "qwen_id": "Qwen/Qwen2.5-7B-Instruct",
  "encoder_type": "stq",
  "encoder_use_chat_template": true,
  "hf_encoder_id": "sentence-transformers/all-MiniLM-L6-v2",
  "max_enc_tokens": 1024,
  "encoder_backbone": "",
  "warm_anchor_text": "",
  "train_append_bos_after_prefix": "yes",
  "first_token_ce_weight": 3.0,
  "adapter_hidden_mult": 2,
  "adapter_colorize": true,
  "adapter_enable_metadata": true,
  "llama_device_map": "auto",
  "qwen_device_map": "auto",
  "llama_devices": "0,1",
  "qwen_devices": "2,3",
  "gpu_mem_gib": 78.0,
  "manifold_stat_weight": 0.001,
  "state_kd_weight": 0.1,
  "state_kd_layers": "0,1,2",
  "K": 8,
  "k_ce_weight": 1.2,
  "kd_first_k_weight": 1.5,
  "kd_tau": 1.25,
  "max_answer_tokens": 24,
  "grad_accum_steps": 32,
  "seed": 42,
  "data_seed": 42
}