
=========================================
Starting pipeline at Fri Sep 19 01:16:11 PDT 2025
=========================================

Preset: FAST_2H  |  GPUs: 0,1,2,3  |  Llama devices: 0,1  |  Qwen devices: 2,3
Run dir: runs/8B_hailmary_allknobs_hero_fast_2h


=========================================
TRAIN + PER-EPOCH EVAL (All knobs enabled)
=========================================


=========================================
EPOCH 1/14
=========================================

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1951.07it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:23,  7.88s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:15<00:15,  7.99s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:24<00:08,  8.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  5.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.57s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1919.81it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  6.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.01s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=1111.55 | sec/step~3.22 | keep=1.00 | K=4 | llama: tf=11.1719 first=10.6840 kCE=8.6757 KD=5.8928 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.3442 first=16.2214 kCE=8.8810 KD=8.6530 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=2344.93 | sec/step~3.39 | keep=1.00 | K=4 | llama: tf=12.1014 first=11.3561 kCE=8.8041 KD=5.7531 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.6176 first=15.5038 kCE=8.1823 KD=7.6613 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=3182.57 | sec/step~3.06 | keep=1.00 | K=4 | llama: tf=11.4318 first=11.5575 kCE=9.0007 KD=5.8187 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.3264 first=18.1330 kCE=10.2675 KD=9.0880 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=155.81 | sec/step~3.45 | keep=1.00 | K=4 | llama: tf=12.5650 first=10.3817 kCE=9.2539 KD=6.3645 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=11.9002 first=15.7184 kCE=9.2000 KD=7.7758 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=321.79 | sec/step~4.37 | keep=1.00 | K=4 | llama: tf=11.5917 first=10.2117 kCE=8.9254 KD=5.6461 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=12.8990 first=18.7842 kCE=9.9919 KD=8.4719 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=490.37 | sec/step~3.34 | keep=1.00 | K=4 | llama: tf=13.0129 first=9.4240 kCE=8.6657 KD=6.2602 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=11.8118 first=17.4068 kCE=9.3223 KD=7.9207 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=65.93 | sec/step~2.82 | keep=0.99 | K=4 | llama: tf=13.9488 first=9.2288 kCE=9.2706 KD=6.9711 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=14.4464 first=17.2354 kCE=9.4511 KD=8.7641 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=169.08 | sec/step~3.78 | keep=0.99 | K=4 | llama: tf=13.6241 first=9.3104 kCE=9.6230 KD=6.6005 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=14.0441 first=17.7238 kCE=10.2717 KD=9.2429 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=284.68 | sec/step~3.26 | keep=0.99 | K=4 | llama: tf=12.5924 first=10.0189 kCE=9.1664 KD=6.4161 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=12.3757 first=17.4588 kCE=8.5769 KD=8.7188 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=38.17 | sec/step~3.37 | keep=0.99 | K=4 | llama: tf=11.0989 first=9.8132 kCE=9.8074 KD=5.8925 man=0.0001 | scale_pen(llama)=5.8208e-11 | qwen: tf=11.9907 first=16.7908 kCE=9.6038 KD=9.1049 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=167.57 | sec/step~3.29 | keep=0.99 | K=4 | llama: tf=11.2732 first=8.6239 kCE=9.3447 KD=6.8483 man=0.0001 | scale_pen(llama)=5.8208e-11 | qwen: tf=12.1853 first=13.7531 kCE=7.6882 KD=7.8540 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=314.45 | sec/step~3.61 | keep=0.98 | K=4 | llama: tf=12.3261 first=9.7888 kCE=10.4213 KD=6.1865 man=0.0001 | scale_pen(llama)=5.8208e-11 | qwen: tf=13.5634 first=16.8034 kCE=9.5782 KD=8.5720 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=21.78 | sec/step~4.29 | keep=0.98 | K=5 | llama: tf=10.9227 first=9.7897 kCE=9.9566 KD=6.3754 man=0.0001 | scale_pen(llama)=6.9633e-11 | qwen: tf=11.3762 first=15.7930 kCE=10.0293 KD=7.8633 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=142.84 | sec/step~3.56 | keep=0.98 | K=5 | llama: tf=10.8730 first=10.0001 kCE=8.8874 KD=6.0175 man=0.0001 | scale_pen(llama)=6.9633e-11 | qwen: tf=11.6418 first=17.0595 kCE=9.1788 KD=6.8894 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=231.57 | sec/step~3.96 | keep=0.97 | K=5 | llama: tf=10.8000 first=8.9908 kCE=8.9688 KD=5.9313 man=0.0001 | scale_pen(llama)=6.9633e-11 | qwen: tf=10.9188 first=12.6414 kCE=8.1086 KD=6.2090 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=377.44 | sec/step~3.37 | keep=0.97 | K=5 | llama: tf=11.0842 first=9.7971 kCE=9.1180 KD=6.4888 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=12.5493 first=15.1581 kCE=9.2666 KD=6.7137 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=307.77 | sec/step~4.00 | keep=0.96 | K=5 | llama: tf=10.2066 first=10.0264 kCE=9.6582 KD=6.0361 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=11.4649 first=15.6979 kCE=9.6887 KD=6.3300 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=580.72 | sec/step~4.04 | keep=0.96 | K=5 | llama: tf=11.1674 first=8.6343 kCE=9.0641 KD=6.2872 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=12.4021 first=16.4916 kCE=9.5352 KD=7.6871 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=871.56 | sec/step~3.66 | keep=0.96 | K=5 | llama: tf=11.3272 first=9.4845 kCE=9.0574 KD=6.6401 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=12.0053 first=16.5809 kCE=9.0401 KD=7.4940 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=98.96 | sec/step~3.77 | keep=0.95 | K=5 | llama: tf=10.7846 first=8.7721 kCE=9.1334 KD=6.5939 man=0.0001 | scale_pen(llama)=2.2172e-11 | qwen: tf=10.2178 first=15.2586 kCE=8.6179 KD=7.3412 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=255.01 | sec/step~4.15 | keep=0.95 | K=5 | llama: tf=11.0470 first=9.3216 kCE=9.4664 KD=6.0072 man=0.0001 | scale_pen(llama)=2.2172e-11 | qwen: tf=11.5953 first=13.5745 kCE=8.8397 KD=6.8352 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=5 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=405.28 | sec/step~4.63 | keep=0.94 | K=6 | llama: tf=11.3129 first=8.8797 kCE=8.2756 KD=5.9844 man=0.0001 | scale_pen(llama)=2.2172e-11 | qwen: tf=11.5859 first=16.0723 kCE=9.2370 KD=6.4942 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=102.88 | sec/step~4.22 | keep=0.94 | K=6 | llama: tf=10.8546 first=8.7362 kCE=8.7432 KD=5.7839 man=0.0001 | scale_pen(llama)=1.5010e-11 | qwen: tf=12.7868 first=14.8244 kCE=10.8978 KD=6.9133 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=255.65 | sec/step~4.81 | keep=0.93 | K=6 | llama: tf=10.1421 first=10.3849 kCE=8.1581 KD=5.2684 man=0.0001 | scale_pen(llama)=1.5010e-11 | qwen: tf=10.8764 first=17.5899 kCE=10.0304 KD=5.1268 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=422.48 | sec/step~5.22 | keep=0.92 | K=6 | llama: tf=9.7703 first=8.5729 kCE=8.3652 KD=5.6935 man=0.0001 | scale_pen(llama)=1.5010e-11 | qwen: tf=10.4828 first=14.9626 kCE=9.3720 KD=6.1332 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=62.50 | sec/step~4.85 | keep=0.92 | K=6 | llama: tf=10.5567 first=9.2610 kCE=8.0067 KD=5.3358 man=0.0001 | scale_pen(llama)=9.6065e-12 | qwen: tf=11.0406 first=14.9401 kCE=9.9818 KD=5.2538 man=0.0002 | scale_pen(qwen)=5.6843e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=228.67 | sec/step~4.70 | keep=0.91 | K=6 | llama: tf=10.7546 first=9.8284 kCE=8.6260 KD=5.6492 man=0.0001 | scale_pen(llama)=9.6065e-12 | qwen: tf=11.9763 first=15.5555 kCE=9.8286 KD=7.0443 man=0.0002 | scale_pen(qwen)=5.6843e-12 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=394.02 | sec/step~5.27 | keep=0.90 | K=7 | llama: tf=10.9461 first=9.9893 kCE=7.7264 KD=5.3605 man=0.0001 | scale_pen(llama)=9.6065e-12 | qwen: tf=11.4239 first=14.9745 kCE=12.1625 KD=4.7102 man=0.0002 | scale_pen(qwen)=5.6843e-12 | K=7 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=32.21 | sec/step~5.20 | keep=0.90 | K=7 | llama: tf=10.4946 first=8.3550 kCE=7.4554 KD=5.0250 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=12.6035 first=16.2676 kCE=11.4532 KD=5.7459 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=7 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=176.20 | sec/step~5.16 | keep=0.89 | K=7 | llama: tf=10.8964 first=8.1752 kCE=7.7045 KD=5.3228 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=12.1124 first=14.2512 kCE=12.3933 KD=5.0888 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=7 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=337.77 | sec/step~4.05 | keep=0.88 | K=7 | llama: tf=10.7320 first=8.1312 kCE=7.8127 KD=5.4403 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=11.1607 first=14.1919 kCE=9.9525 KD=5.2705 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=7 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=484.49 | sec/step~5.33 | keep=0.87 | K=7 | llama: tf=10.1067 first=8.2136 kCE=7.2991 KD=4.3385 man=0.0001 | scale_pen(llama)=5.1301e-12 | qwen: tf=10.3077 first=14.1784 kCE=11.0115 KD=4.5534 man=0.0002 | scale_pen(qwen)=2.0464e-12 | K=7 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=123.76 | sec/step~6.05 | keep=0.87 | K=8 | llama: tf=10.2110 first=8.5530 kCE=7.4412 KD=4.7041 man=0.0001 | scale_pen(llama)=5.1301e-12 | qwen: tf=11.4583 first=14.8817 kCE=12.4703 KD=5.4290 man=0.0002 | scale_pen(qwen)=2.0464e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=253.54 | sec/step~5.78 | keep=0.86 | K=8 | llama: tf=11.1055 first=8.7263 kCE=6.9863 KD=4.3192 man=0.0001 | scale_pen(llama)=5.1301e-12 | qwen: tf=13.0318 first=14.2494 kCE=12.6661 KD=4.2172 man=0.0002 | scale_pen(qwen)=2.0464e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=389.85 | sec/step~7.16 | keep=0.85 | K=8 | llama: tf=10.2144 first=9.1004 kCE=7.2944 KD=4.2503 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=10.3544 first=15.3281 kCE=11.7122 KD=4.6710 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010592156994555678, 'rms_mean_cal': 0.010571069629596813, 'embed_rms': 0.01057521253824234, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013629672732204199, 'rms_mean_cal': 0.013640625349112919, 'embed_rms': 0.013643525540828705, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch1/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch1 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch1/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3024.01it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 364.69it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.36s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.007  |  NLL/token (gold): 10.302908116187098
       First-token acc: top1=0.005  top5=0.015
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.848496042704456
       First-token acc: top1=0.050  top5=0.095
Wall clock: 3.71s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.03s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.010

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.360952615737915
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.007163169978285497,
      "nll": 10.302908116187098,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.014999999664723873,
      "nll_token": 10.302908116187098
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.848496042704456,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.0949999988079071,
      "nll_token": 10.848496042704456
    },
    "wall_clock_sec": 3.7131731510162354
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.030319929122925
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.009663169959535499
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.009663169959535499
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch1/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch1/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.007 | Qwen 0.002
  FirstTok@1: Llama 0.005 | Qwen 0.050
  NLL/token:  Llama 10.303 | Qwen 10.848
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch1/predictions.jsonl
  1. Llama: antibody | Qwen: 1. **(A)** | Gold: linear
  2. Llama: antibody-mediated immunity, 2) cell-mediated immunity, and 3 | Qwen: 1. **Answer:** 12 | Gold: Lampea
  3. Llama: antibody | Qwen: 1. **Answer:** 12 | Gold: residents willing to pay higher market rate for housing
  4. Llama: antibody | Qwen: 1. **Answer:** 12 | Gold: San Jose
  5. Llama: antibody-mediated immunity, 2: 1.1.1 | Qwen: 1. **Answer:** 12 | Gold: oxides

=========================================
EPOCH 2/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1235.80it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2658.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.16s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.007  |  NLL/token (gold): 10.302908116187098
       First-token acc: top1=0.005  top5=0.015
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.848496042704456
       First-token acc: top1=0.050  top5=0.095
Wall clock: 3.25s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.12s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.010

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.157877206802368
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.007163169978285497,
      "nll": 10.302908116187098,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.014999999664723873,
      "nll_token": 10.302908116187098
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.848496042704456,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.0949999988079071,
      "nll_token": 10.848496042704456
    },
    "wall_clock_sec": 3.2531464099884033
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.1239683628082275
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.009663169959535499
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.009663169959535499
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2690.38it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.82s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3401.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=1, global_step=350
Epoch 2/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=152.01 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=10.5352 first=9.0274 kCE=7.5172 KD=4.5787 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=11.5324 first=12.7857 kCE=10.6814 KD=5.0703 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=306.51 | sec/step~6.33 | keep=0.85 | K=8 | llama: tf=10.2457 first=8.7721 kCE=7.2735 KD=4.3799 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=10.2338 first=13.7730 kCE=10.4019 KD=4.4340 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=444.71 | sec/step~6.08 | keep=0.85 | K=8 | llama: tf=10.7897 first=8.5184 kCE=6.7167 KD=4.8376 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=11.3947 first=12.4889 kCE=10.8018 KD=4.5409 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=251.49 | sec/step~5.71 | keep=0.85 | K=8 | llama: tf=10.3690 first=8.6109 kCE=7.0440 KD=4.3826 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.3399 first=13.0286 kCE=9.8178 KD=4.5158 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=525.82 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=10.2731 first=8.4232 kCE=7.3029 KD=4.5018 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=11.4286 first=14.2628 kCE=12.4936 KD=4.7092 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=770.50 | sec/step~5.63 | keep=0.85 | K=8 | llama: tf=10.6494 first=8.9229 kCE=7.3901 KD=4.8441 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=12.0861 first=13.6571 kCE=11.3820 KD=4.7628 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=160.20 | sec/step~5.76 | keep=0.85 | K=8 | llama: tf=10.3791 first=8.5511 kCE=7.1540 KD=4.1655 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.9237 first=11.2600 kCE=10.4816 KD=4.6809 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=447.65 | sec/step~7.49 | keep=0.85 | K=8 | llama: tf=10.0565 first=8.1464 kCE=7.2287 KD=4.2506 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.3624 first=11.6877 kCE=8.9268 KD=5.3231 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=717.24 | sec/step~7.53 | keep=0.85 | K=8 | llama: tf=10.9534 first=9.4878 kCE=6.9041 KD=4.2293 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=11.5345 first=12.7017 kCE=10.3165 KD=4.4941 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=107.21 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=9.8636 first=8.5318 kCE=6.9642 KD=4.4687 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.1595 first=11.7138 kCE=9.1938 KD=5.3707 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=377.50 | sec/step~6.46 | keep=0.85 | K=8 | llama: tf=10.2647 first=8.7891 kCE=7.2750 KD=4.1962 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.6597 first=13.7900 kCE=11.7834 KD=4.9222 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=613.36 | sec/step~5.75 | keep=0.85 | K=8 | llama: tf=10.1152 first=8.6714 kCE=7.2196 KD=4.3100 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=9.3837 first=10.4224 kCE=9.8811 KD=5.0611 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=43.37 | sec/step~5.58 | keep=0.85 | K=8 | llama: tf=10.0466 first=8.2217 kCE=7.5860 KD=4.8251 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=10.2667 first=10.7897 kCE=10.8785 KD=5.0440 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=196.18 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=10.0337 first=8.3274 kCE=7.2816 KD=4.1399 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=10.6886 first=10.0833 kCE=11.0465 KD=5.0646 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=333.23 | sec/step~5.75 | keep=0.85 | K=8 | llama: tf=10.6639 first=9.0956 kCE=6.9802 KD=4.1534 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=11.2237 first=11.5988 kCE=10.9202 KD=4.9207 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=478.47 | sec/step~7.86 | keep=0.85 | K=8 | llama: tf=10.1571 first=8.8307 kCE=7.2140 KD=4.0710 man=0.0001 | scale_pen(llama)=1.1141e-11 | qwen: tf=9.9597 first=10.9499 kCE=10.2408 KD=5.2107 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=133.67 | sec/step~5.90 | keep=0.85 | K=8 | llama: tf=10.4355 first=9.0717 kCE=7.0943 KD=4.3250 man=0.0001 | scale_pen(llama)=1.1141e-11 | qwen: tf=10.1007 first=10.3741 kCE=11.3010 KD=5.1206 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=234.12 | sec/step~6.08 | keep=0.85 | K=8 | llama: tf=10.2172 first=8.8761 kCE=7.2114 KD=4.5354 man=0.0001 | scale_pen(llama)=1.1141e-11 | qwen: tf=10.6653 first=11.6303 kCE=10.4365 KD=4.9983 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=361.62 | sec/step~6.13 | keep=0.85 | K=8 | llama: tf=10.9225 first=8.5847 kCE=6.8245 KD=4.2270 man=0.0001 | scale_pen(llama)=1.1141e-11 | qwen: tf=10.8042 first=10.2496 kCE=9.2462 KD=4.9484 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=77.16 | sec/step~6.13 | keep=0.85 | K=8 | llama: tf=10.1698 first=9.0227 kCE=7.2625 KD=4.5273 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=9.9347 first=9.3973 kCE=10.5898 KD=4.9918 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=160.41 | sec/step~5.94 | keep=0.85 | K=8 | llama: tf=10.3916 first=8.1807 kCE=7.2579 KD=4.5044 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=11.1729 first=8.4469 kCE=10.9168 KD=4.8472 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=260.25 | sec/step~6.80 | keep=0.85 | K=8 | llama: tf=9.8869 first=9.4087 kCE=7.1101 KD=4.4305 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=9.6365 first=10.6954 kCE=10.2437 KD=5.0855 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=58.21 | sec/step~5.76 | keep=0.85 | K=8 | llama: tf=10.1697 first=9.1340 kCE=6.9989 KD=4.7235 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=10.8550 first=10.2629 kCE=10.3021 KD=5.1039 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=127.49 | sec/step~5.70 | keep=0.85 | K=8 | llama: tf=10.7660 first=8.6733 kCE=6.8769 KD=4.5747 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=12.5410 first=9.5866 kCE=11.3125 KD=4.9535 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=195.71 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=9.8194 first=9.5539 kCE=6.6438 KD=3.9837 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=10.7077 first=11.4954 kCE=8.6275 KD=5.1893 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=27.79 | sec/step~6.92 | keep=0.85 | K=8 | llama: tf=10.3465 first=9.3466 kCE=6.7289 KD=3.8644 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=10.1483 first=10.1054 kCE=9.7076 KD=5.4050 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=101.58 | sec/step~6.48 | keep=0.85 | K=8 | llama: tf=10.3367 first=8.4564 kCE=6.5172 KD=4.0813 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=10.4499 first=8.4160 kCE=9.7767 KD=4.8527 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=192.99 | sec/step~6.49 | keep=0.85 | K=8 | llama: tf=9.9008 first=8.4051 kCE=7.1410 KD=4.5570 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=9.4725 first=8.6129 kCE=10.2159 KD=5.6415 man=0.0002 | scale_pen(qwen)=4.3521e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=15.44 | sec/step~5.12 | keep=0.85 | K=8 | llama: tf=10.2821 first=9.4871 kCE=7.0127 KD=4.3002 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=8.9674 first=10.1315 kCE=11.8004 KD=4.6220 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=86.99 | sec/step~7.60 | keep=0.85 | K=8 | llama: tf=10.4855 first=8.2068 kCE=6.3811 KD=3.3772 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=9.5757 first=8.4963 kCE=8.9526 KD=4.1062 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=128.55 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=10.4226 first=8.6732 kCE=6.7708 KD=4.3458 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.8603 first=10.2767 kCE=11.7730 KD=4.4409 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=193.63 | sec/step~6.62 | keep=0.85 | K=8 | llama: tf=10.0250 first=8.5001 kCE=6.8138 KD=4.6188 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.5005 first=9.6176 kCE=9.4401 KD=5.7277 man=0.0002 | scale_pen(qwen)=1.5667e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=93.66 | sec/step~6.66 | keep=0.85 | K=8 | llama: tf=11.2170 first=8.0552 kCE=6.9762 KD=4.3047 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=8.5065 first=8.5811 kCE=8.8718 KD=4.9961 man=0.0002 | scale_pen(qwen)=1.5667e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=161.15 | sec/step~6.62 | keep=0.85 | K=8 | llama: tf=9.9226 first=8.5319 kCE=6.7364 KD=4.1501 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=9.7768 first=8.7826 kCE=11.6167 KD=4.5513 man=0.0002 | scale_pen(qwen)=1.5667e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=271.57 | sec/step~7.50 | keep=0.85 | K=8 | llama: tf=10.6681 first=9.1797 kCE=6.8360 KD=3.9325 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.5447 first=8.6871 kCE=11.1295 KD=4.6957 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010557852310261556, 'rms_mean_cal': 0.010571244175412825, 'embed_rms': 0.010568334721028805, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013628305744911943, 'rms_mean_cal': 0.013640910389700106, 'embed_rms': 0.013639703392982483, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch2/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch2/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2846.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3561.29it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.01s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.010  |  NLL/token (gold): 10.32281303459825
       First-token acc: top1=0.000  top5=0.030
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.774722891667533
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.65s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.84s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.012

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.014780044555664
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.009651879677926902,
      "nll": 10.32281303459825,
      "first_token_top1": 0.0,
      "first_token_top5": 0.029999999329447746,
      "nll_token": 10.32281303459825
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.774722891667533,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.774722891667533
    },
    "wall_clock_sec": 3.651963233947754
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.8386504650115967
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.012151879659176903
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.012151879659176903
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.010 | Qwen 0.002
  FirstTok@1: Llama - | Qwen 0.050
  NLL/token:  Llama 10.323 | Qwen 10.775
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch2/predictions.jsonl
  1. Llama: Question 2: What is the primary purpose of the 1998 Good Friday | Qwen: 1 | Gold: linear
  2. Llama: Question 2: What is the primary purpose of the 1998 Good Friday | Qwen: 1111111111111111 | Gold: Lampea
  3. Llama: Question 2: What is the primary purpose of the 1992 Convention on | Qwen: 1111111111111111 | Gold: residents willing to pay higher market rate for housing
  4. Llama: php | Qwen: 1111111111111111 | Gold: San Jose
  5. Llama: Question 2: What is the primary purpose of the 1998 Good Friday | Qwen: 1111111111111111 | Gold: oxides

=========================================
EPOCH 3/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2796.20it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2847.94it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.82s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.010  |  NLL/token (gold): 10.32281303459825
       First-token acc: top1=0.000  top5=0.030
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.774722891667533
       First-token acc: top1=0.050  top5=0.100
Wall clock: 4.01s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.81s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.012

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.821330547332764
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.009651879677926902,
      "nll": 10.32281303459825,
      "first_token_top1": 0.0,
      "first_token_top5": 0.029999999329447746,
      "nll_token": 10.32281303459825
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.774722891667533,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.774722891667533
    },
    "wall_clock_sec": 4.006964921951294
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.8065602779388428
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.012151879659176903
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.012151879659176903
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3039.35it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2677.93it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=2, global_step=700
Epoch 3/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=92.41 | sec/step~8.49 | keep=0.85 | K=8 | llama: tf=10.6246 first=9.1600 kCE=6.8215 KD=3.3927 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.5949 first=8.6683 kCE=10.4707 KD=4.5577 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=165.90 | sec/step~5.80 | keep=0.85 | K=8 | llama: tf=10.8244 first=8.1306 kCE=6.5855 KD=4.2472 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.6784 first=8.6059 kCE=10.9640 KD=4.9434 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=255.57 | sec/step~7.16 | keep=0.85 | K=8 | llama: tf=9.4895 first=8.8329 kCE=6.8538 KD=3.9130 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.4128 first=9.2756 kCE=10.4293 KD=4.7883 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=27.81 | sec/step~6.87 | keep=0.85 | K=8 | llama: tf=10.4226 first=8.2857 kCE=6.8319 KD=4.3149 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.9218 first=7.6836 kCE=10.9236 KD=5.3653 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=71.11 | sec/step~8.37 | keep=0.85 | K=8 | llama: tf=10.5220 first=8.2931 kCE=6.8171 KD=4.0440 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.2882 first=8.7807 kCE=10.5604 KD=4.8835 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=109.45 | sec/step~7.06 | keep=0.85 | K=8 | llama: tf=10.0024 first=7.8768 kCE=6.4414 KD=3.9447 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.9591 first=8.0068 kCE=9.4179 KD=4.7826 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=26.51 | sec/step~7.73 | keep=0.85 | K=8 | llama: tf=10.2084 first=8.0089 kCE=6.7062 KD=3.7432 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=9.7652 first=7.8272 kCE=9.3846 KD=4.9859 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=55.67 | sec/step~6.30 | keep=0.85 | K=8 | llama: tf=9.6725 first=8.9236 kCE=6.6217 KD=4.3131 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=8.4943 first=9.3513 kCE=9.5227 KD=5.6494 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=101.88 | sec/step~7.23 | keep=0.85 | K=8 | llama: tf=10.6241 first=7.8797 kCE=6.5123 KD=3.8417 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=10.1741 first=7.9690 kCE=11.1301 KD=4.8000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=22.94 | sec/step~5.51 | keep=0.85 | K=8 | llama: tf=10.4076 first=8.5111 kCE=6.7370 KD=4.0467 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=10.9546 first=8.0967 kCE=11.6778 KD=4.7459 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=107.81 | sec/step~5.84 | keep=0.85 | K=8 | llama: tf=9.9160 first=9.1060 kCE=6.4340 KD=4.0222 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=10.3271 first=9.9774 kCE=10.4516 KD=4.6018 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=170.59 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=10.2327 first=8.9229 kCE=6.7119 KD=4.1218 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=10.6823 first=9.3742 kCE=10.2128 KD=5.0190 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=12.97 | sec/step~6.26 | keep=0.85 | K=8 | llama: tf=10.5266 first=9.4594 kCE=6.6256 KD=4.1713 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.0128 first=10.3511 kCE=9.1562 KD=4.6572 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=73.41 | sec/step~6.93 | keep=0.85 | K=8 | llama: tf=10.2150 first=8.2626 kCE=6.6991 KD=3.8170 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=9.2516 first=8.1260 kCE=9.5687 KD=4.6438 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=128.41 | sec/step~5.56 | keep=0.85 | K=8 | llama: tf=9.7842 first=8.3296 kCE=6.7615 KD=4.5443 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.1706 first=8.4412 kCE=11.3399 KD=5.3043 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=190.53 | sec/step~6.27 | keep=0.85 | K=8 | llama: tf=9.9744 first=9.0331 kCE=6.3760 KD=4.3150 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=10.7665 first=9.2610 kCE=8.8698 KD=5.5681 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=39.57 | sec/step~5.82 | keep=0.85 | K=8 | llama: tf=10.4864 first=8.4400 kCE=6.7499 KD=4.0631 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=10.6802 first=8.7270 kCE=11.5404 KD=4.6226 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=57.91 | sec/step~6.61 | keep=0.85 | K=8 | llama: tf=10.3470 first=8.1599 kCE=6.4311 KD=4.0503 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=9.4392 first=6.9718 kCE=9.1267 KD=4.9428 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=74.83 | sec/step~6.35 | keep=0.85 | K=8 | llama: tf=11.2209 first=7.6909 kCE=6.3985 KD=4.0020 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=10.0082 first=7.4138 kCE=10.6469 KD=4.4164 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=36.18 | sec/step~5.38 | keep=0.85 | K=8 | llama: tf=10.1711 first=8.6721 kCE=6.2862 KD=4.1164 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=10.3188 first=8.6555 kCE=10.8422 KD=4.4072 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=94.10 | sec/step~5.65 | keep=0.85 | K=8 | llama: tf=9.3391 first=8.9054 kCE=6.3624 KD=3.7480 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=8.5563 first=9.1604 kCE=9.9919 KD=4.8146 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=160.50 | sec/step~6.73 | keep=0.85 | K=8 | llama: tf=10.4377 first=8.5246 kCE=6.2239 KD=3.5393 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=8.8755 first=7.8796 kCE=11.3612 KD=3.8257 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=22.97 | sec/step~6.07 | keep=0.85 | K=8 | llama: tf=10.0448 first=8.0946 kCE=6.5220 KD=4.0922 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=9.3409 first=8.2967 kCE=10.1925 KD=5.3434 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=47.85 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=9.4636 first=8.6678 kCE=6.5826 KD=4.5456 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=9.9457 first=8.3998 kCE=8.9892 KD=5.7611 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=76.55 | sec/step~6.48 | keep=0.85 | K=8 | llama: tf=9.8828 first=8.3490 kCE=6.1693 KD=3.7767 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=9.3677 first=7.1885 kCE=10.6056 KD=4.2646 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=30.20 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=10.9228 first=8.9730 kCE=6.7167 KD=3.9503 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.9322 first=8.9084 kCE=10.7270 KD=4.9993 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=94.55 | sec/step~6.17 | keep=0.85 | K=8 | llama: tf=10.3305 first=9.0894 kCE=6.5711 KD=4.3713 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.7589 first=9.3722 kCE=10.0959 KD=5.1229 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=158.90 | sec/step~5.62 | keep=0.85 | K=8 | llama: tf=10.7043 first=7.7168 kCE=6.4930 KD=4.1704 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.7084 first=7.8216 kCE=12.0838 KD=4.6647 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=10.03 | sec/step~6.23 | keep=0.85 | K=8 | llama: tf=9.6758 first=9.9902 kCE=6.7026 KD=4.1549 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=8.7079 first=9.3503 kCE=9.8080 KD=5.5588 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=53.26 | sec/step~6.23 | keep=0.85 | K=8 | llama: tf=10.5088 first=8.5633 kCE=6.7349 KD=4.2961 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.6037 first=8.3011 kCE=10.2105 KD=4.8659 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=95.84 | sec/step~6.34 | keep=0.85 | K=8 | llama: tf=10.2224 first=8.3231 kCE=6.5817 KD=3.7350 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.5658 first=8.3775 kCE=10.6878 KD=4.6495 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=139.73 | sec/step~6.74 | keep=0.85 | K=8 | llama: tf=10.0350 first=8.2335 kCE=6.1249 KD=3.9343 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.4683 first=7.2114 kCE=8.6062 KD=4.5241 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=40.09 | sec/step~6.15 | keep=0.85 | K=8 | llama: tf=10.7592 first=8.1767 kCE=6.5796 KD=3.8695 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=11.2379 first=8.4699 kCE=11.0322 KD=4.7547 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=74.89 | sec/step~6.23 | keep=0.85 | K=8 | llama: tf=10.4549 first=8.7946 kCE=6.3476 KD=3.7371 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=8.9293 first=8.7883 kCE=9.8833 KD=4.6469 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=105.62 | sec/step~5.79 | keep=0.85 | K=8 | llama: tf=10.2207 first=8.3005 kCE=6.5317 KD=3.9613 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.4681 first=7.5030 kCE=9.2412 KD=4.9164 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.01056872570887208, 'rms_mean_cal': 0.010571328611778362, 'embed_rms': 0.010570296086370945, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013618153639669929, 'rms_mean_cal': 0.013641307731824261, 'embed_rms': 0.01364416815340519, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch3/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch3/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3150.06it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.34s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3504.75it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.00it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.77s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.187370619806302
       First-token acc: top1=0.000  top5=0.015
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 10.265313100877894
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.76s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.96s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.773674964904785
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.187370619806302,
      "first_token_top1": 0.0,
      "first_token_top5": 0.014999999664723873,
      "nll_token": 10.187370619806302
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.265313100877894,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.265313100877894
    },
    "wall_clock_sec": 3.7605624198913574
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.963571786880493
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama - | Qwen -
  FirstTok@1: Llama - | Qwen 0.050
  NLL/token:  Llama 10.187 | Qwen 10.265
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch3/predictions.jsonl
  1. Llama: php | Qwen: 1111111111111111 | Gold: linear
  2. Llama: php | Qwen: 1111111111111111 | Gold: Lampea
  3. Llama: php | Qwen: 1111111111111111 | Gold: residents willing to pay higher market rate for housing
  4. Llama: php | Qwen: 1111111111111111 | Gold: San Jose
  5. Llama: php | Qwen: 1111111111111111 | Gold: oxides

=========================================
EPOCH 4/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3139.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3190.19it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.10s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.187370619806302
       First-token acc: top1=0.000  top5=0.015
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 10.265313100877894
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.40s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.64s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.103405952453613
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.187370619806302,
      "first_token_top1": 0.0,
      "first_token_top5": 0.014999999664723873,
      "nll_token": 10.187370619806302
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.265313100877894,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.265313100877894
    },
    "wall_clock_sec": 3.395918369293213
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.6421923637390137
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3021.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3629.86it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.01it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=3, global_step=1050
Epoch 4/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=34.38 | sec/step~6.56 | keep=0.85 | K=8 | llama: tf=10.1670 first=8.6376 kCE=6.3781 KD=3.7883 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.0323 first=7.7191 kCE=11.2122 KD=4.3584 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=51.52 | sec/step~7.43 | keep=0.85 | K=8 | llama: tf=10.4456 first=8.3054 kCE=6.8337 KD=3.8474 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.9932 first=7.6682 kCE=9.7187 KD=5.1154 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=73.27 | sec/step~6.25 | keep=0.85 | K=8 | llama: tf=9.7871 first=8.8352 kCE=6.4986 KD=4.1055 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.0547 first=7.9793 kCE=9.7116 KD=4.7196 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=31.75 | sec/step~6.75 | keep=0.85 | K=8 | llama: tf=10.2630 first=8.6026 kCE=6.7102 KD=4.0789 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.9146 first=8.0771 kCE=10.4374 KD=4.9346 man=0.0002 | scale_pen(qwen)=7.9936e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=85.22 | sec/step~6.12 | keep=0.85 | K=8 | llama: tf=9.9720 first=7.6427 kCE=6.3208 KD=3.8137 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=8.7342 first=7.3977 kCE=11.6262 KD=4.6814 man=0.0002 | scale_pen(qwen)=7.9936e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=138.56 | sec/step~6.33 | keep=0.85 | K=8 | llama: tf=10.5273 first=8.7316 kCE=6.7073 KD=4.2281 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=10.5374 first=7.6685 kCE=9.5853 KD=5.0630 man=0.0002 | scale_pen(qwen)=7.9936e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=24.87 | sec/step~6.42 | keep=0.85 | K=8 | llama: tf=10.5659 first=9.4606 kCE=6.4613 KD=3.9759 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.0604 first=8.5816 kCE=12.1143 KD=4.9472 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=340.91 | sec/step~5.31 | keep=0.85 | K=8 | llama: tf=10.2008 first=8.6470 kCE=6.4973 KD=4.1381 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=8.6574 first=7.0343 kCE=11.5333 KD=4.5073 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=391.07 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=9.4986 first=8.5127 kCE=6.0560 KD=3.4447 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.3085 first=7.1870 kCE=10.7722 KD=4.2768 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=14.72 | sec/step~5.96 | keep=0.85 | K=8 | llama: tf=10.8402 first=9.0914 kCE=6.6341 KD=3.9138 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.8623 first=8.1953 kCE=12.0255 KD=4.6324 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=51.23 | sec/step~6.59 | keep=0.85 | K=8 | llama: tf=10.5305 first=9.1008 kCE=6.7898 KD=4.2336 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=12.6095 first=8.4094 kCE=10.9180 KD=4.9941 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=84.31 | sec/step~7.14 | keep=0.85 | K=8 | llama: tf=10.5140 first=8.3144 kCE=6.7698 KD=4.1658 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.2620 first=7.6044 kCE=9.1066 KD=5.7177 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=9.54 | sec/step~6.88 | keep=0.85 | K=8 | llama: tf=10.5448 first=8.5536 kCE=6.1926 KD=3.8121 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=11.9642 first=7.7290 kCE=11.4437 KD=4.3291 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=31.60 | sec/step~6.51 | keep=0.85 | K=8 | llama: tf=9.2310 first=8.9193 kCE=6.2746 KD=3.9491 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.1412 first=7.7994 kCE=9.5539 KD=5.0734 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=62.25 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=9.8259 first=7.8536 kCE=6.5696 KD=4.1667 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.8770 first=7.0560 kCE=9.6650 KD=5.0631 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=80.51 | sec/step~5.71 | keep=0.85 | K=8 | llama: tf=10.0839 first=8.0956 kCE=6.5841 KD=4.3157 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.4083 first=7.4246 kCE=10.8830 KD=5.0191 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=30.80 | sec/step~6.85 | keep=0.85 | K=8 | llama: tf=10.1012 first=7.7565 kCE=6.5870 KD=3.8151 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.1379 first=6.6894 kCE=9.7701 KD=5.1476 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=76.36 | sec/step~6.44 | keep=0.85 | K=8 | llama: tf=9.9478 first=8.5367 kCE=6.4144 KD=3.8392 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.3699 first=7.9821 kCE=10.1167 KD=5.3625 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=95.67 | sec/step~6.33 | keep=0.85 | K=8 | llama: tf=9.8487 first=7.9591 kCE=6.2190 KD=3.7096 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=8.8735 first=6.8370 kCE=9.8080 KD=4.7382 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=27.95 | sec/step~5.72 | keep=0.85 | K=8 | llama: tf=10.0539 first=7.5322 kCE=6.5352 KD=4.1623 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=11.4227 first=6.9758 kCE=10.0343 KD=5.4312 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=49.37 | sec/step~6.64 | keep=0.85 | K=8 | llama: tf=9.8667 first=7.9646 kCE=6.4410 KD=3.9059 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.9674 first=7.5401 kCE=10.8162 KD=4.9736 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=79.23 | sec/step~7.57 | keep=0.85 | K=8 | llama: tf=10.4059 first=8.3875 kCE=6.4887 KD=3.9231 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=10.3191 first=7.1839 kCE=9.8336 KD=4.9098 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=51.25 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=10.2530 first=6.8514 kCE=6.2957 KD=3.7587 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=10.1109 first=5.9420 kCE=10.7781 KD=4.6561 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=122.41 | sec/step~7.70 | keep=0.85 | K=8 | llama: tf=10.3330 first=8.7857 kCE=6.2442 KD=3.2950 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.7939 first=7.7244 kCE=9.4849 KD=4.4862 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=184.81 | sec/step~6.10 | keep=0.85 | K=8 | llama: tf=11.2311 first=8.6080 kCE=6.7175 KD=3.8756 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=12.3423 first=7.6688 kCE=11.6554 KD=4.9987 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=12.73 | sec/step~6.64 | keep=0.85 | K=8 | llama: tf=9.6693 first=8.3937 kCE=6.3623 KD=3.7428 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.0112 first=7.8757 kCE=9.8124 KD=4.9177 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=43.91 | sec/step~6.35 | keep=0.85 | K=8 | llama: tf=9.4340 first=8.2765 kCE=6.5375 KD=3.6257 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.1824 first=7.5475 kCE=10.3818 KD=5.0906 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=71.73 | sec/step~5.61 | keep=0.85 | K=8 | llama: tf=9.7748 first=8.3663 kCE=6.3111 KD=3.8940 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.4247 first=7.0174 kCE=9.5299 KD=5.0593 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=18.59 | sec/step~6.35 | keep=0.85 | K=8 | llama: tf=10.1239 first=9.1002 kCE=6.7869 KD=3.9352 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.6015 first=8.0832 kCE=10.0489 KD=5.3304 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=97.26 | sec/step~6.02 | keep=0.85 | K=8 | llama: tf=10.3755 first=8.4237 kCE=6.5688 KD=4.0600 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.1424 first=7.9902 kCE=10.1588 KD=4.9881 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=149.12 | sec/step~7.44 | keep=0.85 | K=8 | llama: tf=10.1485 first=8.4408 kCE=6.5669 KD=3.8365 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.9124 first=7.5314 kCE=8.5633 KD=5.1587 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=193.67 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=9.4901 first=7.8530 kCE=6.5775 KD=4.1419 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.9536 first=7.0959 kCE=10.3323 KD=5.6688 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=77.76 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=10.8745 first=9.3397 kCE=6.8768 KD=3.9772 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.5404 first=8.7879 kCE=11.1799 KD=4.8650 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=148.05 | sec/step~6.59 | keep=0.85 | K=8 | llama: tf=9.6718 first=9.0822 kCE=6.8676 KD=3.9066 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.3790 first=8.6490 kCE=9.8021 KD=5.6025 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=227.83 | sec/step~6.44 | keep=0.85 | K=8 | llama: tf=10.9789 first=9.2706 kCE=6.6873 KD=3.9024 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.6711 first=7.9425 kCE=10.8883 KD=4.3141 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010570686611213855, 'rms_mean_cal': 0.01057113183130111, 'embed_rms': 0.010571061633527279, 'count': 350}, 'qwen': {'rms_mean_raw': 0.01362373617344669, 'rms_mean_cal': 0.013641403397279126, 'embed_rms': 0.013635721057653427, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch4/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch4/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3132.42it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3199.32it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.91s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.012  |  NLL/token (gold): 10.211560771038203
       First-token acc: top1=0.000  top5=0.035
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.101282567574234
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.22s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.98s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.014

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.914251804351807
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01183385508198984,
      "nll": 10.211560771038203,
      "first_token_top1": 0.0,
      "first_token_top5": 0.03500000014901161,
      "nll_token": 10.211560771038203
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.101282567574234,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.101282567574234
    },
    "wall_clock_sec": 3.216750144958496
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.9843971729278564
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.014333855063239838
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.014333855063239838
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.012 | Qwen 0.002
  FirstTok@1: Llama - | Qwen 0.050
  NLL/token:  Llama 10.212 | Qwen 10.101
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch4/predictions.jsonl
  1. Llama: 1993) and the _New York Times_ (1993) and the | Qwen: 1 | Gold: linear
  2. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1111111111111111 | Gold: Lampea
  3. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1111111111111111 | Gold: residents willing to pay higher market rate for housing
  4. Llama: php | Qwen: 1 | Gold: San Jose
  5. Llama: php | Qwen: 1 | Gold: oxides

=========================================
EPOCH 5/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2877.25it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 233.91it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.20s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.012  |  NLL/token (gold): 10.211560771038203
       First-token acc: top1=0.000  top5=0.035
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 10.101282567574234
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.90s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.02s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.014

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.195342779159546
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01183385508198984,
      "nll": 10.211560771038203,
      "first_token_top1": 0.0,
      "first_token_top5": 0.03500000014901161,
      "nll_token": 10.211560771038203
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 10.101282567574234,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 10.101282567574234
    },
    "wall_clock_sec": 3.900670289993286
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.0161144733428955
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.014333855063239838
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.014333855063239838
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3200.54it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.90s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.60s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3376.38it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=4, global_step=1400
Epoch 5/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=56.73 | sec/step~5.62 | keep=0.85 | K=8 | llama: tf=10.6397 first=8.4788 kCE=6.4076 KD=4.1613 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.1977 first=7.6977 kCE=9.7964 KD=5.4525 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  20/350 | grad_norm=91.70 | sec/step~6.37 | keep=0.85 | K=8 | llama: tf=9.8437 first=8.0657 kCE=6.3600 KD=4.1389 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.5781 first=7.4272 kCE=9.2373 KD=5.0714 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  30/350 | grad_norm=101.62 | sec/step~6.45 | keep=0.85 | K=8 | llama: tf=9.8160 first=6.7709 kCE=6.5528 KD=3.6805 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.2442 first=6.4486 kCE=8.6893 KD=4.9596 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  40/350 | grad_norm=77.45 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=10.3701 first=8.5320 kCE=6.7334 KD=4.4193 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=11.4159 first=7.5739 kCE=10.6353 KD=6.0485 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  50/350 | grad_norm=147.94 | sec/step~9.01 | keep=0.85 | K=8 | llama: tf=10.2436 first=9.6657 kCE=6.6685 KD=3.7694 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=11.1205 first=9.1709 kCE=10.3079 KD=5.9570 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  60/350 | grad_norm=254.59 | sec/step~6.01 | keep=0.85 | K=8 | llama: tf=10.6391 first=7.9276 kCE=6.3044 KD=4.0362 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.6234 first=6.7202 kCE=9.0397 KD=5.0285 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  70/350 | grad_norm=53.26 | sec/step~6.96 | keep=0.85 | K=8 | llama: tf=9.7818 first=7.9367 kCE=5.9243 KD=3.4021 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=9.1911 first=6.7571 kCE=9.5751 KD=4.6587 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  80/350 | grad_norm=145.71 | sec/step~5.71 | keep=0.85 | K=8 | llama: tf=10.0656 first=8.9826 kCE=6.2580 KD=4.0463 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=8.2863 first=7.5261 kCE=8.9994 KD=4.7543 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  90/350 | grad_norm=241.30 | sec/step~6.20 | keep=0.85 | K=8 | llama: tf=9.8229 first=8.0805 kCE=6.4752 KD=4.1817 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=10.2705 first=7.2352 kCE=9.3682 KD=5.2970 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  100/350 | grad_norm=21.12 | sec/step~6.71 | keep=0.85 | K=8 | llama: tf=9.6224 first=7.9825 kCE=6.4381 KD=3.9639 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=9.7440 first=6.2099 kCE=8.1127 KD=5.6863 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  110/350 | grad_norm=52.17 | sec/step~5.70 | keep=0.85 | K=8 | llama: tf=10.4351 first=8.7441 kCE=6.6438 KD=4.3839 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=11.0602 first=8.1591 kCE=9.4420 KD=5.1943 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  120/350 | grad_norm=116.24 | sec/step~8.19 | keep=0.85 | K=8 | llama: tf=9.9659 first=8.2984 kCE=6.1611 KD=3.3529 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.2415 first=6.9211 kCE=10.5156 KD=4.6338 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  130/350 | grad_norm=9.11 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=9.9003 first=8.5648 kCE=6.0435 KD=3.4226 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=8.5377 first=7.9126 kCE=11.4240 KD=4.1545 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  140/350 | grad_norm=36.97 | sec/step~6.14 | keep=0.85 | K=8 | llama: tf=10.4813 first=7.0973 kCE=6.4253 KD=4.2530 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.3378 first=5.8137 kCE=10.0387 KD=4.9264 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  150/350 | grad_norm=64.98 | sec/step~6.49 | keep=0.85 | K=8 | llama: tf=10.2928 first=9.0844 kCE=6.7468 KD=4.2736 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.8217 first=8.2110 kCE=10.3907 KD=5.7111 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  160/350 | grad_norm=96.54 | sec/step~6.46 | keep=0.85 | K=8 | llama: tf=10.0227 first=7.6904 kCE=6.3970 KD=4.2975 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=9.7870 first=6.3634 kCE=8.7457 KD=5.4181 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  170/350 | grad_norm=53.47 | sec/step~6.86 | keep=0.85 | K=8 | llama: tf=10.3379 first=8.6356 kCE=6.6274 KD=3.7774 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.9068 first=7.5928 kCE=10.1674 KD=4.8333 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  180/350 | grad_norm=99.97 | sec/step~6.26 | keep=0.85 | K=8 | llama: tf=9.8929 first=9.0062 kCE=6.4101 KD=4.3101 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=8.8364 first=6.8729 kCE=8.9395 KD=4.9794 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  190/350 | grad_norm=143.58 | sec/step~5.85 | keep=0.85 | K=8 | llama: tf=10.2409 first=8.4091 kCE=6.6916 KD=4.2363 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=11.3329 first=7.5119 kCE=10.1298 KD=5.3838 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  200/350 | grad_norm=18.69 | sec/step~5.76 | keep=0.85 | K=8 | llama: tf=10.5511 first=8.6036 kCE=6.2260 KD=3.7969 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.0043 first=7.7342 kCE=10.8254 KD=4.6758 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  210/350 | grad_norm=49.83 | sec/step~4.98 | keep=0.85 | K=8 | llama: tf=9.1213 first=8.2395 kCE=6.2582 KD=3.9708 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.3429 first=6.3437 kCE=9.8933 KD=4.6391 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  220/350 | grad_norm=89.24 | sec/step~5.80 | keep=0.85 | K=8 | llama: tf=9.5450 first=8.7941 kCE=6.2735 KD=4.1065 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.5688 first=7.5874 kCE=8.8136 KD=4.3201 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  230/350 | grad_norm=29.99 | sec/step~6.21 | keep=0.85 | K=8 | llama: tf=10.3664 first=8.8767 kCE=6.4089 KD=3.9182 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.7369 first=7.8056 kCE=9.4091 KD=5.5550 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  240/350 | grad_norm=96.27 | sec/step~6.16 | keep=0.85 | K=8 | llama: tf=10.0461 first=8.3899 kCE=6.6405 KD=4.0258 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.8126 first=7.3268 kCE=10.1783 KD=5.7439 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  250/350 | grad_norm=143.50 | sec/step~6.43 | keep=0.85 | K=8 | llama: tf=9.5250 first=8.0667 kCE=6.4656 KD=4.1104 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.3734 first=7.3029 kCE=9.7672 KD=4.8848 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  260/350 | grad_norm=30.10 | sec/step~6.29 | keep=0.85 | K=8 | llama: tf=10.1091 first=8.7791 kCE=6.0619 KD=3.5200 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.0148 first=6.9590 kCE=9.4194 KD=4.8204 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  270/350 | grad_norm=109.60 | sec/step~6.42 | keep=0.85 | K=8 | llama: tf=9.5045 first=8.7343 kCE=6.1999 KD=3.6165 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=9.6019 first=7.6957 kCE=8.7142 KD=5.8269 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  280/350 | grad_norm=165.81 | sec/step~5.62 | keep=0.85 | K=8 | llama: tf=10.5964 first=9.1956 kCE=6.3072 KD=4.1179 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.9921 first=7.8062 kCE=9.8762 KD=5.1036 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  290/350 | grad_norm=6.49 | sec/step~6.01 | keep=0.85 | K=8 | llama: tf=10.2309 first=8.9883 kCE=6.5465 KD=4.6374 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=10.9425 first=8.2282 kCE=9.0139 KD=6.0799 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  300/350 | grad_norm=38.20 | sec/step~6.15 | keep=0.85 | K=8 | llama: tf=10.1364 first=8.7690 kCE=6.1332 KD=3.6309 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=11.9045 first=7.4007 kCE=10.1336 KD=4.6071 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  310/350 | grad_norm=47.08 | sec/step~8.91 | keep=0.85 | K=8 | llama: tf=9.5417 first=8.8654 kCE=5.9543 KD=3.3980 man=0.0001 | scale_pen(llama)=6.0041e-13 | qwen: tf=8.7323 first=7.2452 kCE=9.7873 KD=4.3209 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  320/350 | grad_norm=70.16 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=10.5786 first=7.6931 kCE=6.1955 KD=3.5441 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.6426 first=6.6215 kCE=10.8226 KD=4.8671 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  330/350 | grad_norm=35.34 | sec/step~6.50 | keep=0.85 | K=8 | llama: tf=9.9107 first=7.7088 kCE=5.9746 KD=3.7747 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=9.0405 first=6.7685 kCE=8.9962 KD=4.6333 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  340/350 | grad_norm=63.04 | sec/step~5.38 | keep=0.85 | K=8 | llama: tf=10.5105 first=9.6796 kCE=6.2688 KD=4.2708 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=10.4675 first=8.0998 kCE=10.9380 KD=4.5697 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  350/350 | grad_norm=97.49 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=10.6736 first=8.7059 kCE=6.0636 KD=3.6097 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.8857 first=7.5509 kCE=11.3748 KD=3.9998 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.01056522953457066, 'rms_mean_cal': 0.010571293192250388, 'embed_rms': 0.01056915707886219, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013627264688589743, 'rms_mean_cal': 0.013641619102231094, 'embed_rms': 0.013653635047376156, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch5/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch5/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3209.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1693.64it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.76s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.013  |  NLL/token (gold): 10.188814681133175
       First-token acc: top1=0.005  top5=0.040
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 9.90768682562485
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.67s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.00s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.015

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.76211142539978
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01276083923457983,
      "nll": 10.188814681133175,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.03999999910593033,
      "nll_token": 10.188814681133175
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 9.90768682562485,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 9.90768682562485
    },
    "wall_clock_sec": 3.6677627563476562
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.000875949859619
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.015260839215829831
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.015260839215829831
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.013 | Qwen 0.002
  FirstTok@1: Llama 0.005 | Qwen 0.050
  NLL/token:  Llama 10.189 | Qwen 9.908
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch5/predictions.jsonl
  1. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1 | Gold: linear
  2. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1 | Gold: Lampea
  3. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1 | Gold: residents willing to pay higher market rate for housing
  4. Llama: Question 2: What is the primary purpose of the United Nations? | Qwen: 1 | Gold: San Jose
  5. Llama: 1990s | Qwen: 1 | Gold: oxides

=========================================
EPOCH 6/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3222.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2803.68it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.24s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.013  |  NLL/token (gold): 10.188814681133175
       First-token acc: top1=0.005  top5=0.040
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 9.90768682562485
       First-token acc: top1=0.050  top5=0.100
Wall clock: 3.80s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.93s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.015

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.237882137298584
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01276083923457983,
      "nll": 10.188814681133175,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.03999999910593033,
      "nll_token": 10.188814681133175
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0024999999812500002,
      "nll": 9.90768682562485,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.09999999403953552,
      "nll_token": 9.90768682562485
    },
    "wall_clock_sec": 3.79774808883667
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.9300692081451416
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999812500002,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.015260839215829831
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.015260839215829831
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3252.66it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.49s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3266.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=5, global_step=1750
Epoch 6/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=44.06 | sec/step~6.38 | keep=0.85 | K=8 | llama: tf=9.7962 first=7.0381 kCE=5.9098 KD=4.0049 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.3565 first=6.2047 kCE=8.1069 KD=4.4276 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  20/350 | grad_norm=72.94 | sec/step~6.53 | keep=0.85 | K=8 | llama: tf=10.5241 first=9.4462 kCE=6.2688 KD=3.7933 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.1874 first=7.1934 kCE=9.9712 KD=4.9074 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  30/350 | grad_norm=96.66 | sec/step~5.66 | keep=0.85 | K=8 | llama: tf=10.3014 first=8.7249 kCE=6.5063 KD=4.1742 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.5954 first=8.2093 kCE=10.1227 KD=5.3467 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  40/350 | grad_norm=15.26 | sec/step~5.58 | keep=0.85 | K=8 | llama: tf=9.6731 first=9.1173 kCE=6.1335 KD=3.7580 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=8.9384 first=7.9814 kCE=9.9040 KD=4.7334 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  50/350 | grad_norm=37.39 | sec/step~6.97 | keep=0.85 | K=8 | llama: tf=9.7509 first=9.0691 kCE=6.4653 KD=3.6076 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.0547 first=7.8262 kCE=9.0571 KD=5.2039 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  60/350 | grad_norm=56.25 | sec/step~6.33 | keep=0.85 | K=8 | llama: tf=9.8196 first=8.4018 kCE=6.2616 KD=3.9581 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.9264 first=7.0153 kCE=8.9321 KD=4.8282 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  70/350 | grad_norm=17.81 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=10.0636 first=7.8467 kCE=5.9637 KD=3.8044 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.7331 first=7.6186 kCE=8.8450 KD=5.0615 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  80/350 | grad_norm=46.74 | sec/step~5.91 | keep=0.85 | K=8 | llama: tf=10.1677 first=9.0238 kCE=6.4206 KD=4.1426 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.8243 first=8.1116 kCE=8.9394 KD=6.0225 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  90/350 | grad_norm=76.95 | sec/step~6.01 | keep=0.85 | K=8 | llama: tf=10.1100 first=8.3813 kCE=6.1623 KD=4.2721 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.1496 first=7.3596 kCE=9.6732 KD=5.5537 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  100/350 | grad_norm=16.66 | sec/step~6.06 | keep=0.85 | K=8 | llama: tf=10.4315 first=8.2565 kCE=6.1926 KD=3.9009 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=11.1953 first=7.2982 kCE=9.0992 KD=5.3804 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  110/350 | grad_norm=35.57 | sec/step~6.54 | keep=0.85 | K=8 | llama: tf=10.1581 first=8.7471 kCE=6.4090 KD=4.3335 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=11.3711 first=8.0221 kCE=9.5272 KD=5.3322 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  120/350 | grad_norm=64.63 | sec/step~5.78 | keep=0.85 | K=8 | llama: tf=10.1753 first=8.0286 kCE=5.7255 KD=3.9895 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.5505 first=6.9593 kCE=8.8759 KD=4.6172 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  130/350 | grad_norm=13.76 | sec/step~5.33 | keep=0.85 | K=8 | llama: tf=10.3790 first=8.5917 kCE=6.1813 KD=4.4657 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.6813 first=7.2261 kCE=9.2541 KD=5.0599 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  140/350 | grad_norm=99.13 | sec/step~6.00 | keep=0.85 | K=8 | llama: tf=9.6059 first=8.2070 kCE=5.9945 KD=4.0023 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.6399 first=7.2000 kCE=8.6238 KD=4.7765 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  150/350 | grad_norm=179.68 | sec/step~6.01 | keep=0.85 | K=8 | llama: tf=10.4257 first=8.0718 kCE=5.9959 KD=3.9790 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.6564 first=6.1757 kCE=9.5199 KD=4.7278 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  160/350 | grad_norm=259.96 | sec/step~6.85 | keep=0.85 | K=8 | llama: tf=10.3364 first=8.0521 kCE=5.9846 KD=3.4443 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=9.7161 first=6.5786 kCE=9.9060 KD=4.6596 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  170/350 | grad_norm=40.66 | sec/step~5.98 | keep=0.85 | K=8 | llama: tf=10.7012 first=7.8847 kCE=6.4493 KD=4.5334 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.5715 first=7.3195 kCE=9.0790 KD=6.0976 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  180/350 | grad_norm=81.89 | sec/step~5.61 | keep=0.85 | K=8 | llama: tf=10.2952 first=8.3535 kCE=5.9012 KD=3.9145 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=10.8949 first=6.7985 kCE=9.7594 KD=4.8863 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  190/350 | grad_norm=138.93 | sec/step~6.75 | keep=0.85 | K=8 | llama: tf=10.6386 first=9.3152 kCE=6.2882 KD=3.7929 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=10.8627 first=7.9224 kCE=10.3448 KD=4.3989 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  200/350 | grad_norm=23.62 | sec/step~5.56 | keep=0.85 | K=8 | llama: tf=9.4041 first=7.4542 kCE=5.5466 KD=4.0375 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=10.1556 first=6.4401 kCE=8.3619 KD=4.7557 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  210/350 | grad_norm=35.34 | sec/step~6.22 | keep=0.85 | K=8 | llama: tf=10.1828 first=7.9759 kCE=6.0356 KD=3.8055 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=10.6284 first=6.9166 kCE=8.4583 KD=4.8810 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  220/350 | grad_norm=58.46 | sec/step~7.15 | keep=0.85 | K=8 | llama: tf=10.0434 first=8.4050 kCE=6.0357 KD=3.7277 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=9.5465 first=6.9961 kCE=8.6534 KD=5.0695 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  230/350 | grad_norm=16.95 | sec/step~5.70 | keep=0.85 | K=8 | llama: tf=9.1473 first=8.5922 kCE=5.7229 KD=4.0378 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.8345 first=7.5575 kCE=9.1906 KD=4.9002 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  240/350 | grad_norm=51.05 | sec/step~6.29 | keep=0.85 | K=8 | llama: tf=10.3464 first=7.6510 kCE=6.1706 KD=4.3005 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.9637 first=6.9989 kCE=7.9335 KD=5.2774 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  250/350 | grad_norm=66.82 | sec/step~5.94 | keep=0.85 | K=8 | llama: tf=9.5333 first=8.8441 kCE=6.1958 KD=4.2778 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.0106 first=7.7785 kCE=9.0217 KD=5.8582 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  260/350 | grad_norm=11.02 | sec/step~7.08 | keep=0.85 | K=8 | llama: tf=10.3042 first=8.5709 kCE=5.9779 KD=3.7307 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=10.1335 first=7.2706 kCE=9.5022 KD=4.8616 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  270/350 | grad_norm=21.52 | sec/step~6.54 | keep=0.85 | K=8 | llama: tf=10.3265 first=9.1450 kCE=6.4462 KD=4.5494 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=11.1177 first=7.5339 kCE=8.7175 KD=5.8386 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  280/350 | grad_norm=39.56 | sec/step~7.36 | keep=0.85 | K=8 | llama: tf=9.5737 first=7.5185 kCE=6.5530 KD=4.1412 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=10.3493 first=6.9910 kCE=7.6185 KD=6.2836 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  290/350 | grad_norm=8.12 | sec/step~7.38 | keep=0.85 | K=8 | llama: tf=9.6878 first=7.5724 kCE=5.6665 KD=3.5772 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.4510 first=6.6185 kCE=8.4843 KD=4.7227 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  300/350 | grad_norm=52.08 | sec/step~6.04 | keep=0.85 | K=8 | llama: tf=9.8809 first=8.0686 kCE=5.6885 KD=3.9577 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=9.8816 first=6.7367 kCE=8.2112 KD=4.4951 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  310/350 | grad_norm=81.26 | sec/step~6.96 | keep=0.85 | K=8 | llama: tf=10.4566 first=9.2843 kCE=6.1764 KD=4.0798 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.6937 first=7.8380 kCE=9.5570 KD=5.4368 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  320/350 | grad_norm=128.70 | sec/step~6.11 | keep=0.85 | K=8 | llama: tf=10.0641 first=7.6587 kCE=5.9992 KD=4.1599 man=0.0001 | scale_pen(llama)=2.5899e-12 | qwen: tf=10.7441 first=6.3441 kCE=8.4353 KD=5.4080 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  330/350 | grad_norm=20.04 | sec/step~5.45 | keep=0.85 | K=8 | llama: tf=10.2075 first=7.6144 kCE=5.7658 KD=4.4412 man=0.0001 | scale_pen(llama)=2.5899e-12 | qwen: tf=10.4603 first=6.8365 kCE=6.1948 KD=5.1085 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  340/350 | grad_norm=38.75 | sec/step~6.24 | keep=0.85 | K=8 | llama: tf=9.8373 first=9.4623 kCE=6.2568 KD=4.1097 man=0.0001 | scale_pen(llama)=2.5899e-12 | qwen: tf=10.0893 first=7.8406 kCE=9.1753 KD=5.1571 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
  step  350/350 | grad_norm=52.35 | sec/step~7.19 | keep=0.85 | K=8 | llama: tf=10.1357 first=9.3543 kCE=6.0543 KD=3.7989 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=9.2488 first=7.6328 kCE=8.4521 KD=5.1003 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01366]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010561693443783693, 'rms_mean_cal': 0.010571013273937361, 'embed_rms': 0.010572514496743679, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013632256894239357, 'rms_mean_cal': 0.013640920128673316, 'embed_rms': 0.013656198047101498, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch6/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch6/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2847.94it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3212.80it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.33s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.40s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.069034764015216
       First-token acc: top1=0.000  top5=0.020
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.797829300480545
       First-token acc: top1=0.050  top5=0.105
Wall clock: 3.72s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.55s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.40246081352234
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.069034764015216,
      "first_token_top1": 0.0,
      "first_token_top5": 0.019999999552965164,
      "nll_token": 10.069034764015216
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.797829300480545,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.10499999672174454,
      "nll_token": 9.797829300480545
    },
    "wall_clock_sec": 3.7201449871063232
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.554553985595703
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama - | Qwen -
  FirstTok@1: Llama - | Qwen 0.050
  NLL/token:  Llama 10.069 | Qwen 9.798
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch6/predictions.jsonl
  1. Llama: php | Qwen: 1000000000000000 | Gold: linear
  2. Llama: php | Qwen: 1000000000000000 | Gold: Lampea
  3. Llama: php | Qwen: 1.0.0.0 | Gold: residents willing to pay higher market rate for housing
  4. Llama: php | Qwen: 1.0.0.0 | Gold: San Jose
  5. Llama: php | Qwen: 1000000000000000 | Gold: oxides

=========================================
EPOCH 7/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3201.76it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.37s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3235.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.45s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.85s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.069034764015216
       First-token acc: top1=0.000  top5=0.020
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.797829300480545
       First-token acc: top1=0.050  top5=0.105
Wall clock: 4.13s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.30s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.845425128936768
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.069034764015216,
      "first_token_top1": 0.0,
      "first_token_top5": 0.019999999552965164,
      "nll_token": 10.069034764015216
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.797829300480545,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.10499999672174454,
      "nll_token": 9.797829300480545
    },
    "wall_clock_sec": 4.133265495300293
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.2974300384521484
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1246.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3186.56it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.03it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=6, global_step=2100
Epoch 7/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=37.10 | sec/step~6.93 | keep=0.85 | K=8 | llama: tf=9.3718 first=9.3933 kCE=6.1512 KD=3.8495 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=9.7360 first=8.2932 kCE=8.9296 KD=5.2782 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  20/350 | grad_norm=76.80 | sec/step~5.72 | keep=0.85 | K=8 | llama: tf=10.1062 first=7.7546 kCE=5.3063 KD=3.6646 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=9.4721 first=6.0618 kCE=8.2931 KD=4.7382 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  30/350 | grad_norm=120.27 | sec/step~6.53 | keep=0.85 | K=8 | llama: tf=10.3195 first=9.5022 kCE=6.4578 KD=4.0451 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=10.1243 first=8.0383 kCE=8.7711 KD=5.9633 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  40/350 | grad_norm=51.36 | sec/step~5.78 | keep=0.85 | K=8 | llama: tf=9.6956 first=7.9817 kCE=5.7964 KD=3.8629 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.4723 first=7.3523 kCE=8.3713 KD=5.1102 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  50/350 | grad_norm=326.18 | sec/step~5.84 | keep=0.85 | K=8 | llama: tf=9.2770 first=8.5139 kCE=5.8791 KD=3.9100 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.2225 first=7.5429 kCE=8.4192 KD=5.1978 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  60/350 | grad_norm=322.00 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=9.8041 first=8.1893 kCE=5.7205 KD=3.8268 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.2426 first=6.5847 kCE=8.3616 KD=5.2151 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  70/350 | grad_norm=389.81 | sec/step~6.41 | keep=0.85 | K=8 | llama: tf=10.5699 first=8.3066 kCE=6.1746 KD=3.8871 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=11.3734 first=7.5043 kCE=9.9329 KD=5.2614 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  80/350 | grad_norm=370.10 | sec/step~6.39 | keep=0.85 | K=8 | llama: tf=10.2014 first=7.2976 kCE=6.2888 KD=4.3969 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=11.1202 first=6.5679 kCE=8.2606 KD=5.1431 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  90/350 | grad_norm=327.88 | sec/step~5.60 | keep=0.85 | K=8 | llama: tf=9.5566 first=8.5260 kCE=5.7746 KD=3.9718 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.9377 first=7.6734 kCE=9.0446 KD=4.9532 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  100/350 | grad_norm=71.36 | sec/step~6.06 | keep=0.85 | K=8 | llama: tf=9.5939 first=7.7838 kCE=5.6938 KD=3.9755 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.5992 first=6.9771 kCE=7.9381 KD=4.8608 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  110/350 | grad_norm=220.41 | sec/step~7.71 | keep=0.85 | K=8 | llama: tf=10.4697 first=8.0490 kCE=5.7071 KD=3.6004 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=10.3959 first=7.0420 kCE=8.9059 KD=4.6489 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  120/350 | grad_norm=243.44 | sec/step~5.99 | keep=0.85 | K=8 | llama: tf=10.3147 first=8.3951 kCE=5.8294 KD=3.6423 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=10.0417 first=6.9750 kCE=8.6747 KD=4.9166 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  130/350 | grad_norm=27.78 | sec/step~8.78 | keep=0.85 | K=8 | llama: tf=10.8206 first=8.7185 kCE=6.1105 KD=3.2899 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=9.0235 first=7.0799 kCE=9.0484 KD=4.9165 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  140/350 | grad_norm=74.54 | sec/step~5.80 | keep=0.85 | K=8 | llama: tf=10.2370 first=8.6813 kCE=5.7026 KD=3.9658 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=10.0134 first=8.2163 kCE=8.4549 KD=4.9991 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  150/350 | grad_norm=141.59 | sec/step~6.99 | keep=0.85 | K=8 | llama: tf=10.3915 first=8.1664 kCE=6.0252 KD=3.9257 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=10.6011 first=7.2250 kCE=9.3010 KD=4.8807 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  160/350 | grad_norm=275.16 | sec/step~7.03 | keep=0.85 | K=8 | llama: tf=9.9124 first=8.7318 kCE=6.0159 KD=3.7650 man=0.0001 | scale_pen(llama)=2.7853e-12 | qwen: tf=9.8245 first=7.9574 kCE=9.0171 KD=5.3702 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  170/350 | grad_norm=58.27 | sec/step~9.01 | keep=0.85 | K=8 | llama: tf=9.9020 first=8.4460 kCE=5.8853 KD=3.5933 man=0.0001 | scale_pen(llama)=2.7853e-12 | qwen: tf=9.9707 first=7.8224 kCE=8.6557 KD=5.6586 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  180/350 | grad_norm=206.03 | sec/step~6.26 | keep=0.85 | K=8 | llama: tf=10.7036 first=8.4292 kCE=5.6227 KD=3.7850 man=0.0001 | scale_pen(llama)=2.7853e-12 | qwen: tf=11.2806 first=7.6073 kCE=9.7487 KD=4.9667 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  190/350 | grad_norm=164.20 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=10.6743 first=8.3076 kCE=6.2574 KD=3.8838 man=0.0001 | scale_pen(llama)=2.7853e-12 | qwen: tf=9.4838 first=7.6590 kCE=9.3208 KD=5.7662 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  200/350 | grad_norm=100.25 | sec/step~6.19 | keep=0.85 | K=8 | llama: tf=9.4581 first=7.5264 kCE=5.7134 KD=3.8168 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.8837 first=6.7590 kCE=8.1085 KD=5.3604 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  210/350 | grad_norm=80.18 | sec/step~4.97 | keep=0.85 | K=8 | llama: tf=9.0850 first=8.7833 kCE=5.4657 KD=3.8624 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=8.9489 first=8.4286 kCE=9.4716 KD=5.1504 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  220/350 | grad_norm=132.03 | sec/step~6.28 | keep=0.85 | K=8 | llama: tf=9.8669 first=7.7867 kCE=5.6916 KD=3.9683 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.7920 first=6.5127 kCE=8.7485 KD=5.7243 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  230/350 | grad_norm=196.15 | sec/step~6.03 | keep=0.85 | K=8 | llama: tf=10.8579 first=9.0099 kCE=5.6030 KD=3.9244 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.1070 first=8.0374 kCE=9.5028 KD=5.1572 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  240/350 | grad_norm=195.68 | sec/step~6.03 | keep=0.85 | K=8 | llama: tf=10.4339 first=7.7325 kCE=6.0733 KD=4.3630 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.9269 first=6.8139 kCE=7.9744 KD=5.9823 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  250/350 | grad_norm=717.93 | sec/step~7.76 | keep=0.85 | K=8 | llama: tf=9.9849 first=8.1180 kCE=5.6671 KD=3.4058 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.2516 first=6.8983 kCE=9.4599 KD=4.8203 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  260/350 | grad_norm=86.99 | sec/step~6.81 | keep=0.85 | K=8 | llama: tf=9.7498 first=7.7837 kCE=5.8457 KD=3.9487 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=9.9413 first=6.6858 kCE=8.1330 KD=5.6281 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  270/350 | grad_norm=72.60 | sec/step~5.69 | keep=0.85 | K=8 | llama: tf=10.0578 first=8.2848 kCE=6.1178 KD=4.5863 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=10.6140 first=7.1780 kCE=8.4899 KD=5.9333 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  280/350 | grad_norm=93.38 | sec/step~7.68 | keep=0.85 | K=8 | llama: tf=9.6509 first=8.7823 kCE=5.9170 KD=4.0338 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=9.8370 first=7.9271 kCE=7.5964 KD=5.5318 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  290/350 | grad_norm=12.67 | sec/step~6.21 | keep=0.85 | K=8 | llama: tf=9.9706 first=7.9030 kCE=5.6365 KD=3.6699 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=9.3919 first=7.0171 kCE=8.1826 KD=4.8533 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  300/350 | grad_norm=81.23 | sec/step~7.04 | keep=0.85 | K=8 | llama: tf=8.9395 first=8.6784 kCE=5.4050 KD=3.7380 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=9.5447 first=7.7515 kCE=9.0142 KD=4.8474 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  310/350 | grad_norm=403.52 | sec/step~8.08 | keep=0.85 | K=8 | llama: tf=9.0456 first=8.7558 kCE=5.5391 KD=3.6625 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=9.2657 first=7.2802 kCE=9.1788 KD=5.0158 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  320/350 | grad_norm=388.34 | sec/step~6.18 | keep=0.85 | K=8 | llama: tf=10.0103 first=8.0935 kCE=5.3292 KD=3.6462 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=9.2047 first=6.5539 kCE=7.8322 KD=4.9245 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  330/350 | grad_norm=44.43 | sec/step~6.20 | keep=0.85 | K=8 | llama: tf=9.5748 first=8.1789 kCE=5.7798 KD=4.1010 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=10.2227 first=7.8399 kCE=8.8088 KD=5.1448 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  340/350 | grad_norm=78.98 | sec/step~5.96 | keep=0.85 | K=8 | llama: tf=10.7558 first=8.1572 kCE=5.7961 KD=4.1587 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=11.2478 first=7.3190 kCE=9.9397 KD=4.8691 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  350/350 | grad_norm=119.60 | sec/step~6.56 | keep=0.85 | K=8 | llama: tf=9.5050 first=7.9970 kCE=5.7729 KD=3.8564 man=0.0001 | scale_pen(llama)=2.2204e-12 | qwen: tf=9.8930 first=6.4728 kCE=7.4293 KD=5.1087 man=0.0002 | scale_pen(qwen)=8.5301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010560999480741365, 'rms_mean_cal': 0.010571532297347273, 'embed_rms': 0.010570552200078964, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013638051607246909, 'rms_mean_cal': 0.013640417187873807, 'embed_rms': 0.013647305779159069, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch7/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch7/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3141.80it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2951.14it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.24s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.016  |  NLL/token (gold): 9.986388282170372
       First-token acc: top1=0.020  top5=0.025
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.847207774560918
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.27s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.81s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.016

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.244165420532227
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01594816261030471,
      "nll": 9.986388282170372,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.02499999850988388,
      "nll_token": 9.986388282170372
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.847207774560918,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.847207774560918
    },
    "wall_clock_sec": 3.270726203918457
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.806439161300659
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.01594816261030471
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.01594816261030471
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.016 | Qwen -
  FirstTok@1: Llama 0.020 | Qwen 0.050
  NLL/token:  Llama 9.986 | Qwen 9.847
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch7/predictions.jsonl
  1. Llama: 1989 | Qwen: 1000000000000000 | Gold: linear
  2. Llama: the United States of America | Qwen: 1000000000000000 | Gold: Lampea
  3. Llama: the United States of America | Qwen: 1.25 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1990s | Qwen: 1000000000000000 | Gold: San Jose
  5. Llama: php | Qwen: 1000000000000000 | Gold: oxides

=========================================
EPOCH 8/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2026.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3058.19it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.21s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.016  |  NLL/token (gold): 9.986388282170372
       First-token acc: top1=0.020  top5=0.025
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.847207774560918
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.78s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.85s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.016

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.21207547187805
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.01594816261030471,
      "nll": 9.986388282170372,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.02499999850988388,
      "nll_token": 9.986388282170372
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.847207774560918,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.847207774560918
    },
    "wall_clock_sec": 3.775752305984497
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.853037118911743
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.01594816261030471
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.01594816261030471
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2806.49it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2765.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=7, global_step=2450
Epoch 8/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=38.67 | sec/step~8.19 | keep=0.85 | K=8 | llama: tf=9.9265 first=8.2195 kCE=5.7055 KD=3.4413 man=0.0001 | scale_pen(llama)=2.2204e-12 | qwen: tf=10.8736 first=6.9834 kCE=8.7769 KD=5.4884 man=0.0002 | scale_pen(qwen)=8.5301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=90.04 | sec/step~7.12 | keep=0.85 | K=8 | llama: tf=10.9743 first=8.0825 kCE=5.5760 KD=3.6511 man=0.0001 | scale_pen(llama)=2.2204e-12 | qwen: tf=10.5939 first=7.0435 kCE=10.3878 KD=4.6940 man=0.0002 | scale_pen(qwen)=8.5301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=122.95 | sec/step~6.41 | keep=0.85 | K=8 | llama: tf=10.1466 first=7.6073 kCE=5.6883 KD=4.1921 man=0.0001 | scale_pen(llama)=2.2204e-12 | qwen: tf=10.1529 first=7.1270 kCE=8.2574 KD=5.0558 man=0.0002 | scale_pen(qwen)=8.5301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=68.93 | sec/step~5.93 | keep=0.85 | K=8 | llama: tf=9.8182 first=8.4419 kCE=5.5559 KD=3.5500 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=10.6408 first=7.1490 kCE=9.1883 KD=4.7627 man=0.0002 | scale_pen(qwen)=1.2367e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=123.70 | sec/step~6.00 | keep=0.85 | K=8 | llama: tf=9.1692 first=8.0596 kCE=5.6125 KD=4.2427 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=9.1581 first=6.9819 kCE=8.6069 KD=5.3288 man=0.0002 | scale_pen(qwen)=1.2367e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=168.75 | sec/step~6.97 | keep=0.85 | K=8 | llama: tf=9.4695 first=7.9410 kCE=5.3122 KD=3.4431 man=0.0001 | scale_pen(llama)=2.9878e-12 | qwen: tf=8.0927 first=6.7446 kCE=8.3854 KD=4.1985 man=0.0002 | scale_pen(qwen)=1.2367e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=38.18 | sec/step~6.67 | keep=0.85 | K=8 | llama: tf=10.1832 first=7.8465 kCE=5.5012 KD=3.8030 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=10.0405 first=6.9724 kCE=9.2903 KD=4.4686 man=0.0002 | scale_pen(qwen)=1.2790e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=91.71 | sec/step~6.80 | keep=0.85 | K=8 | llama: tf=10.2333 first=8.7623 kCE=5.8882 KD=4.1905 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.9677 first=7.7620 kCE=8.8304 KD=5.3695 man=0.0002 | scale_pen(qwen)=1.2790e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=139.38 | sec/step~6.01 | keep=0.85 | K=8 | llama: tf=9.5899 first=8.3769 kCE=5.4863 KD=3.9237 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.5904 first=7.2458 kCE=7.9883 KD=4.9424 man=0.0002 | scale_pen(qwen)=1.2790e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=15.67 | sec/step~5.55 | keep=0.85 | K=8 | llama: tf=10.3268 first=8.0367 kCE=6.0586 KD=4.5258 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.9984 first=7.0415 kCE=9.2214 KD=5.9072 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=42.36 | sec/step~6.09 | keep=0.85 | K=8 | llama: tf=9.9305 first=7.7400 kCE=5.4255 KD=3.8400 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.1260 first=6.6376 kCE=9.3924 KD=4.5768 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=81.00 | sec/step~6.33 | keep=0.85 | K=8 | llama: tf=9.7049 first=6.9735 kCE=5.9667 KD=3.7985 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.3072 first=6.1327 kCE=9.5336 KD=4.9146 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=8.93 | sec/step~6.73 | keep=0.85 | K=8 | llama: tf=10.0721 first=8.8234 kCE=5.7228 KD=3.8821 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=10.5474 first=7.6815 kCE=9.3398 KD=5.2311 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=26.24 | sec/step~6.20 | keep=0.85 | K=8 | llama: tf=10.4569 first=7.7706 kCE=5.8236 KD=4.1349 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=11.4773 first=7.3738 kCE=9.4529 KD=5.6342 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=54.97 | sec/step~5.71 | keep=0.85 | K=8 | llama: tf=10.6328 first=8.5139 kCE=5.5369 KD=4.1770 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=10.7891 first=7.0363 kCE=8.9275 KD=4.9807 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=74.29 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=9.2038 first=7.2351 kCE=5.7401 KD=4.3045 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=9.7195 first=6.6968 kCE=8.4669 KD=5.6975 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=29.08 | sec/step~5.80 | keep=0.85 | K=8 | llama: tf=9.9760 first=7.1429 kCE=5.8513 KD=4.4063 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.4183 first=6.7366 kCE=8.0122 KD=5.7282 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=63.33 | sec/step~5.92 | keep=0.85 | K=8 | llama: tf=9.6292 first=8.2441 kCE=5.7240 KD=4.1780 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=9.5269 first=7.2586 kCE=8.8688 KD=5.4980 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=97.50 | sec/step~6.72 | keep=0.85 | K=8 | llama: tf=9.7902 first=8.1723 kCE=5.2396 KD=3.8001 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=10.4196 first=7.1566 kCE=7.5209 KD=4.9838 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=26.29 | sec/step~6.02 | keep=0.85 | K=8 | llama: tf=9.0150 first=6.7702 kCE=5.7433 KD=4.2280 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.0030 first=6.1130 kCE=7.4792 KD=5.6382 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=48.06 | sec/step~6.00 | keep=0.85 | K=8 | llama: tf=9.3504 first=7.8081 kCE=5.4934 KD=4.1227 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.8555 first=7.0374 kCE=8.0560 KD=5.3965 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=71.23 | sec/step~6.09 | keep=0.85 | K=8 | llama: tf=10.0543 first=8.0914 kCE=5.7177 KD=3.9570 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.2414 first=6.9049 kCE=8.1086 KD=5.3735 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=19.65 | sec/step~5.91 | keep=0.85 | K=8 | llama: tf=9.8797 first=8.4057 kCE=5.3829 KD=4.1455 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.5123 first=7.0708 kCE=8.1995 KD=5.0412 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=41.76 | sec/step~7.74 | keep=0.85 | K=8 | llama: tf=9.8425 first=7.4491 kCE=5.3319 KD=3.3534 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=10.5928 first=6.5549 kCE=8.0779 KD=4.9413 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=69.94 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=9.8914 first=7.6881 kCE=5.4189 KD=4.2251 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=10.4624 first=6.6404 kCE=8.6934 KD=4.9439 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=21.09 | sec/step~6.52 | keep=0.85 | K=8 | llama: tf=9.7253 first=6.6773 kCE=5.3488 KD=3.8278 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.3704 first=5.9297 kCE=8.0983 KD=4.7800 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=68.43 | sec/step~5.93 | keep=0.85 | K=8 | llama: tf=9.8328 first=8.4210 kCE=5.4271 KD=3.9875 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.3846 first=7.0394 kCE=9.1131 KD=4.6242 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=111.02 | sec/step~5.76 | keep=0.85 | K=8 | llama: tf=9.3979 first=8.0002 kCE=5.3822 KD=3.8118 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.3509 first=7.4576 kCE=7.6801 KD=4.8599 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=11.87 | sec/step~5.41 | keep=0.85 | K=8 | llama: tf=9.8130 first=8.2551 kCE=5.3773 KD=4.7262 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.4071 first=7.2572 kCE=8.0643 KD=5.5162 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=29.74 | sec/step~5.33 | keep=0.85 | K=8 | llama: tf=9.6221 first=7.3505 kCE=5.4837 KD=4.1187 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.2819 first=6.1647 kCE=9.3436 KD=5.0895 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=45.29 | sec/step~5.92 | keep=0.85 | K=8 | llama: tf=10.8649 first=8.3758 kCE=5.4153 KD=4.3376 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.6031 first=7.3797 kCE=9.0413 KD=4.9022 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=63.82 | sec/step~6.37 | keep=0.85 | K=8 | llama: tf=9.6838 first=8.3308 kCE=5.3536 KD=3.8067 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.0044 first=7.2361 kCE=9.0179 KD=5.0831 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=35.43 | sec/step~5.82 | keep=0.85 | K=8 | llama: tf=8.9296 first=7.8105 kCE=4.9115 KD=3.7463 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=9.7547 first=6.2865 kCE=9.1229 KD=4.7542 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=80.09 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=11.5844 first=8.9516 kCE=5.2743 KD=3.6308 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=12.0227 first=7.9932 kCE=10.0802 KD=4.4293 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=127.55 | sec/step~7.40 | keep=0.85 | K=8 | llama: tf=10.0122 first=8.4278 kCE=5.3497 KD=3.6285 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=9.5362 first=7.3156 kCE=7.9355 KD=4.9125 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010556275621056557, 'rms_mean_cal': 0.010571326254201787, 'embed_rms': 0.010566910728812218, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013641102146357298, 'rms_mean_cal': 0.013640998285263778, 'embed_rms': 0.013637831434607506, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch8/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch8/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3315.66it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3449.26it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.07s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.009  |  NLL/token (gold): 9.92245334292215
       First-token acc: top1=0.015  top5=0.025
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 9.77806029401759
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.85s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.52s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.011

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.06862735748291
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.008944960153532728,
      "nll": 9.92245334292215,
      "first_token_top1": 0.014999999664723873,
      "first_token_top5": 0.02499999850988388,
      "nll_token": 9.92245334292215
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.001999999976,
      "nll": 9.77806029401759,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.77806029401759
    },
    "wall_clock_sec": 3.8534185886383057
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.515306234359741
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0048026314332108785,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.010944960129532728
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.010944960129532728
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.009 | Qwen 0.002
  FirstTok@1: Llama 0.015 | Qwen 0.050
  NLL/token:  Llama 9.922 | Qwen 9.778
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch8/predictions.jsonl
  1. Llama: php | Qwen: 1. 1. 1. 1. 1. 1 | Gold: linear
  2. Llama: php | Qwen: 1. 1. 1. 1. 1. 1 | Gold: Lampea
  3. Llama: the first permanent settlement of the United States was Jamestown, Virginia, in | Qwen: 1. 1999年10月1日，中国国家 | Gold: residents willing to pay higher market rate for housing
  4. Llama: php | Qwen: 1. 1999 | Gold: San Jose
  5. Llama: php | Qwen: 1. 1. 1. 1. 1. 1 | Gold: oxides

=========================================
EPOCH 9/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2795.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.51s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.42s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.17s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2662.63it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.11s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.99s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.009  |  NLL/token (gold): 9.92245334292215
       First-token acc: top1=0.015  top5=0.025
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 9.77806029401759
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.69s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.21s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.011

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.992955923080444
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.008944960153532728,
      "nll": 9.92245334292215,
      "first_token_top1": 0.014999999664723873,
      "first_token_top5": 0.02499999850988388,
      "nll_token": 9.92245334292215
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.001999999976,
      "nll": 9.77806029401759,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.77806029401759
    },
    "wall_clock_sec": 3.6861660480499268
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.210220098495483
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0048026314332108785,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.010944960129532728
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.010944960129532728
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2902.63it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.70s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3577.99it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.01it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=8, global_step=2800
Epoch 9/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=52.37 | sec/step~5.89 | keep=0.85 | K=8 | llama: tf=10.4525 first=8.5260 kCE=5.3907 KD=4.1357 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=9.2631 first=6.9376 kCE=8.8409 KD=5.1171 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=91.84 | sec/step~6.25 | keep=0.85 | K=8 | llama: tf=8.9410 first=8.0730 kCE=4.7233 KD=3.5549 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=8.4352 first=6.4360 kCE=8.5621 KD=4.3882 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=118.17 | sec/step~6.52 | keep=0.85 | K=8 | llama: tf=9.5848 first=6.9823 kCE=4.8163 KD=3.7137 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=7.9833 first=5.3323 kCE=8.0664 KD=4.1784 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=25.58 | sec/step~6.06 | keep=0.85 | K=8 | llama: tf=9.8130 first=7.1554 kCE=5.5063 KD=3.8993 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=10.1525 first=6.6873 kCE=7.6944 KD=5.3931 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=51.63 | sec/step~5.74 | keep=0.85 | K=8 | llama: tf=10.0375 first=7.7598 kCE=5.2745 KD=4.0853 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=11.3169 first=6.5617 kCE=8.5744 KD=5.2515 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=82.50 | sec/step~7.63 | keep=0.85 | K=8 | llama: tf=9.7268 first=8.1584 kCE=5.1741 KD=3.4179 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=9.1730 first=6.5258 kCE=9.3444 KD=4.7666 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=33.82 | sec/step~5.68 | keep=0.85 | K=8 | llama: tf=9.8887 first=8.0564 kCE=5.8647 KD=4.6183 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=10.5915 first=7.2828 kCE=8.0247 KD=5.6633 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=51.61 | sec/step~8.71 | keep=0.85 | K=8 | llama: tf=10.0023 first=9.1552 kCE=5.4993 KD=3.6867 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=11.6126 first=8.0956 kCE=9.2753 KD=4.5683 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=85.30 | sec/step~7.17 | keep=0.85 | K=8 | llama: tf=9.8936 first=7.9599 kCE=5.4319 KD=3.9493 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.5288 first=7.0078 kCE=8.8196 KD=5.4080 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=31.01 | sec/step~6.31 | keep=0.85 | K=8 | llama: tf=9.9596 first=8.0156 kCE=5.2821 KD=4.0528 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.4249 first=7.0508 kCE=9.2123 KD=4.7949 man=0.0002 | scale_pen(qwen)=9.9796e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=65.79 | sec/step~7.01 | keep=0.85 | K=8 | llama: tf=11.0177 first=9.6839 kCE=5.3534 KD=3.5587 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.8366 first=8.4685 kCE=10.0708 KD=4.7556 man=0.0002 | scale_pen(qwen)=9.9796e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=125.90 | sec/step~6.69 | keep=0.85 | K=8 | llama: tf=9.1229 first=7.2485 kCE=5.1224 KD=4.0171 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.2672 first=6.2283 kCE=8.5016 KD=4.8997 man=0.0002 | scale_pen(qwen)=9.9796e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=8.25 | sec/step~6.51 | keep=0.85 | K=8 | llama: tf=9.9334 first=8.7090 kCE=5.0578 KD=3.7209 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.8513 first=7.1316 kCE=9.0467 KD=4.7606 man=0.0002 | scale_pen(qwen)=7.8479e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=34.50 | sec/step~7.03 | keep=0.85 | K=8 | llama: tf=9.4157 first=7.0757 kCE=5.1420 KD=3.7751 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.5335 first=6.2519 kCE=8.3212 KD=4.9652 man=0.0002 | scale_pen(qwen)=7.8479e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=67.77 | sec/step~6.81 | keep=0.85 | K=8 | llama: tf=10.4416 first=8.2847 kCE=5.0062 KD=3.7483 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.6627 first=7.2322 kCE=9.6474 KD=4.8839 man=0.0002 | scale_pen(qwen)=7.8479e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=94.41 | sec/step~6.63 | keep=0.85 | K=8 | llama: tf=9.7173 first=7.9253 kCE=5.7246 KD=4.2345 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.9039 first=6.9375 kCE=8.0372 KD=5.6940 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=23.61 | sec/step~6.79 | keep=0.85 | K=8 | llama: tf=9.9797 first=9.0582 kCE=5.1434 KD=3.4753 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=11.6455 first=8.2713 kCE=8.6840 KD=5.1119 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=60.51 | sec/step~6.28 | keep=0.85 | K=8 | llama: tf=9.0697 first=7.6118 kCE=5.2213 KD=4.2686 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.3228 first=6.6074 kCE=7.4713 KD=5.0132 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=92.18 | sec/step~5.82 | keep=0.85 | K=8 | llama: tf=9.5446 first=8.6682 kCE=5.4733 KD=4.4428 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.4780 first=7.4913 kCE=8.2026 KD=5.0454 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=46.36 | sec/step~6.38 | keep=0.85 | K=8 | llama: tf=9.5957 first=8.4870 kCE=5.3135 KD=4.0464 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=10.8830 first=7.2598 kCE=8.6246 KD=5.1363 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=88.11 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=9.7179 first=8.7267 kCE=5.1995 KD=3.9897 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=10.4880 first=7.6017 kCE=9.3279 KD=4.7928 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=143.59 | sec/step~6.24 | keep=0.85 | K=8 | llama: tf=10.5220 first=8.8179 kCE=5.2831 KD=3.9216 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=10.7870 first=7.3004 kCE=9.7054 KD=4.6686 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=48.52 | sec/step~5.99 | keep=0.85 | K=8 | llama: tf=9.6143 first=7.7351 kCE=5.1433 KD=3.8463 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.4390 first=6.9851 kCE=8.8187 KD=5.1535 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=90.00 | sec/step~6.42 | keep=0.85 | K=8 | llama: tf=9.9230 first=7.6178 kCE=5.7151 KD=4.3602 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.4382 first=6.4099 kCE=9.2120 KD=5.2665 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=135.53 | sec/step~6.16 | keep=0.85 | K=8 | llama: tf=9.8557 first=8.4411 kCE=5.5224 KD=3.9641 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.0398 first=7.2724 kCE=8.5442 KD=5.2074 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=17.28 | sec/step~5.35 | keep=0.85 | K=8 | llama: tf=8.6300 first=7.5239 kCE=5.2076 KD=4.2124 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.6621 first=5.6237 kCE=7.1472 KD=4.8896 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=99.43 | sec/step~6.61 | keep=0.85 | K=8 | llama: tf=9.6775 first=7.6344 kCE=5.4257 KD=4.2989 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=11.7265 first=6.9726 kCE=8.2210 KD=5.4128 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=105.24 | sec/step~6.86 | keep=0.85 | K=8 | llama: tf=9.6109 first=9.1268 kCE=6.0328 KD=4.1359 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.5122 first=8.6630 kCE=8.2293 KD=5.9264 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=16.53 | sec/step~6.70 | keep=0.85 | K=8 | llama: tf=9.9364 first=7.7355 kCE=5.5716 KD=4.1789 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.1085 first=7.0179 kCE=7.6884 KD=5.5557 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=36.52 | sec/step~6.11 | keep=0.85 | K=8 | llama: tf=9.3292 first=6.6099 kCE=5.3462 KD=4.6287 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.6530 first=5.4791 kCE=8.0026 KD=5.5036 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=66.06 | sec/step~6.30 | keep=0.85 | K=8 | llama: tf=9.8204 first=7.6222 kCE=5.1653 KD=4.0117 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.4539 first=6.6332 kCE=7.6475 KD=4.9471 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=102.04 | sec/step~5.64 | keep=0.85 | K=8 | llama: tf=10.3684 first=8.5624 kCE=5.5228 KD=4.1865 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.9493 first=6.6511 kCE=8.6690 KD=5.3679 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=90.42 | sec/step~5.82 | keep=0.85 | K=8 | llama: tf=9.5056 first=7.9333 kCE=5.2203 KD=4.3695 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.9393 first=6.9229 kCE=8.5885 KD=5.1750 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=184.49 | sec/step~5.92 | keep=0.85 | K=8 | llama: tf=9.6623 first=7.7112 kCE=4.9046 KD=3.9336 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.7117 first=6.6575 kCE=9.2914 KD=4.3200 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=296.02 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=10.0888 first=8.2229 kCE=5.5547 KD=4.1059 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.8965 first=7.1970 kCE=8.2114 KD=5.4411 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010555511069084917, 'rms_mean_cal': 0.010570976127471243, 'embed_rms': 0.010562099516391754, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013642840345523186, 'rms_mean_cal': 0.013640508111566306, 'embed_rms': 0.013636281713843346, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch9/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch9/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2163.41it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.47s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3167.31it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 18.17s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.009  |  NLL/token (gold): 9.80288500418198
       First-token acc: top1=0.010  top5=0.035
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.788916819310062
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.80s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.64s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.009

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 18.1662917137146
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.00921654198860109,
      "nll": 9.80288500418198,
      "first_token_top1": 0.009999999776482582,
      "first_token_top5": 0.03500000014901161,
      "nll_token": 9.80288500418198
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.788916819310062,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.788916819310062
    },
    "wall_clock_sec": 3.801619052886963
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.635537624359131
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.00921654198860109
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.00921654198860109
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.009 | Qwen -
  FirstTok@1: Llama 0.010 | Qwen 0.050
  NLL/token:  Llama 9.803 | Qwen 9.789
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch9/predictions.jsonl
  1. Llama: 1980 | Qwen: 1. 1922 | Gold: linear
  2. Llama: the 1960s and 1970s, when the US government was | Qwen: 1. 1922 | Gold: Lampea
  3. Llama: the 1960s and 1970s, the 1960s | Qwen: 1. 1999 2. 1999 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1.5 million | Qwen: 1. 1922 | Gold: San Jose
  5. Llama: 1980 | Qwen: 1. 1922 | Gold: oxides

=========================================
EPOCH 10/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2907.16it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3126.58it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.53s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.009  |  NLL/token (gold): 9.80288500418198
       First-token acc: top1=0.010  top5=0.035
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.788916819310062
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.41s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.85s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.009

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.5325825214386
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.00921654198860109,
      "nll": 9.80288500418198,
      "first_token_top1": 0.009999999776482582,
      "first_token_top5": 0.03500000014901161,
      "nll_token": 9.80288500418198
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.788916819310062,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.788916819310062
    },
    "wall_clock_sec": 3.4071218967437744
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.8489737510681152
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.00921654198860109
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.00921654198860109
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3009.37it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.65s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.25s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 218.70it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=9, global_step=3150
Epoch 10/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=39.13 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=9.9243 first=7.4186 kCE=5.2661 KD=4.2624 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.7912 first=6.5811 kCE=8.0984 KD=5.0791 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=64.93 | sec/step~6.43 | keep=0.85 | K=8 | llama: tf=10.0633 first=8.8391 kCE=4.9418 KD=3.9468 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.7248 first=7.3861 kCE=9.5666 KD=4.8211 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=80.66 | sec/step~6.44 | keep=0.85 | K=8 | llama: tf=10.1069 first=7.9254 kCE=5.5008 KD=4.3805 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.4731 first=6.7614 kCE=8.5126 KD=5.6531 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=77.29 | sec/step~6.07 | keep=0.85 | K=8 | llama: tf=9.5720 first=8.4946 kCE=4.6292 KD=3.6932 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.1726 first=7.5041 kCE=9.9296 KD=4.6203 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=171.32 | sec/step~7.01 | keep=0.85 | K=8 | llama: tf=9.4846 first=8.7595 kCE=4.9667 KD=3.9626 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=8.6984 first=6.9652 kCE=9.2470 KD=4.7062 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=274.73 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=10.0318 first=9.0140 kCE=5.2347 KD=4.1990 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.0585 first=7.6178 kCE=9.0452 KD=5.3115 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=50.19 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=9.7194 first=7.9955 kCE=5.1561 KD=3.9094 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.5106 first=7.1124 kCE=9.0548 KD=5.2118 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=128.01 | sec/step~6.36 | keep=0.85 | K=8 | llama: tf=9.5934 first=8.1938 kCE=5.1696 KD=4.0239 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.7284 first=7.4375 kCE=8.3128 KD=5.2747 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=178.77 | sec/step~5.48 | keep=0.85 | K=8 | llama: tf=8.9014 first=8.0493 kCE=5.2079 KD=4.4399 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.0855 first=6.7509 kCE=8.4268 KD=4.8639 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=21.56 | sec/step~6.52 | keep=0.85 | K=8 | llama: tf=10.2827 first=8.8657 kCE=5.6106 KD=4.2654 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=11.3159 first=8.1254 kCE=7.4500 KD=5.7954 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=67.32 | sec/step~6.55 | keep=0.85 | K=8 | llama: tf=9.4361 first=8.9757 kCE=5.8779 KD=4.3372 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.8704 first=7.3534 kCE=7.6021 KD=6.3612 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=130.64 | sec/step~6.17 | keep=0.85 | K=8 | llama: tf=9.0975 first=7.2813 kCE=5.0943 KD=4.0685 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.0354 first=6.1423 kCE=9.1621 KD=5.0425 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=12.03 | sec/step~5.72 | keep=0.85 | K=8 | llama: tf=9.2784 first=7.6557 kCE=5.0813 KD=4.1160 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.8954 first=6.1405 kCE=7.9432 KD=4.9483 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=77.78 | sec/step~5.27 | keep=0.85 | K=8 | llama: tf=10.2905 first=7.7368 kCE=5.3650 KD=4.6169 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.7121 first=6.8697 kCE=8.6791 KD=5.3922 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=151.00 | sec/step~8.30 | keep=0.85 | K=8 | llama: tf=10.8628 first=8.1781 kCE=5.4870 KD=3.9075 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=11.1627 first=6.7415 kCE=8.2078 KD=5.3814 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=222.92 | sec/step~5.67 | keep=0.85 | K=8 | llama: tf=9.6874 first=8.5685 kCE=5.2674 KD=4.2425 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.5067 first=8.0807 kCE=8.9957 KD=5.2369 man=0.0002 | scale_pen(qwen)=1.8794e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=102.71 | sec/step~5.44 | keep=0.85 | K=8 | llama: tf=10.1849 first=8.1064 kCE=5.0895 KD=4.0342 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.7892 first=7.0486 kCE=9.6760 KD=5.1609 man=0.0002 | scale_pen(qwen)=1.8794e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=194.02 | sec/step~6.08 | keep=0.85 | K=8 | llama: tf=9.5830 first=7.8872 kCE=5.1149 KD=4.0581 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.2717 first=6.6032 kCE=8.6173 KD=5.0347 man=0.0002 | scale_pen(qwen)=1.8794e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=287.90 | sec/step~6.47 | keep=0.85 | K=8 | llama: tf=9.7122 first=8.8536 kCE=5.0960 KD=4.0420 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.8498 first=8.1948 kCE=7.3766 KD=5.3743 man=0.0002 | scale_pen(qwen)=1.8794e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=77.26 | sec/step~6.17 | keep=0.85 | K=8 | llama: tf=10.2071 first=7.9470 kCE=5.0503 KD=4.1424 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.0742 first=6.6902 kCE=6.9917 KD=4.9603 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=184.15 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=9.9786 first=7.9635 kCE=5.5452 KD=4.5901 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=10.1453 first=6.7359 kCE=8.5771 KD=5.6860 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=253.04 | sec/step~6.45 | keep=0.85 | K=8 | llama: tf=10.0043 first=9.1155 kCE=4.9270 KD=3.5591 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=10.3891 first=7.8603 kCE=10.3836 KD=4.7985 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=39.40 | sec/step~6.57 | keep=0.85 | K=8 | llama: tf=9.6731 first=7.6927 kCE=5.0288 KD=3.7975 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.6201 first=6.7210 kCE=7.8071 KD=4.8504 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=85.83 | sec/step~6.31 | keep=0.85 | K=8 | llama: tf=9.6973 first=8.2556 kCE=4.8378 KD=3.9565 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=10.6202 first=7.4934 kCE=9.0853 KD=4.6989 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=117.04 | sec/step~6.32 | keep=0.85 | K=8 | llama: tf=10.5280 first=8.2210 kCE=5.6498 KD=4.5749 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.9645 first=7.4699 kCE=7.8489 KD=5.5860 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=44.92 | sec/step~9.04 | keep=0.85 | K=8 | llama: tf=9.9670 first=8.7944 kCE=5.3531 KD=3.6261 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.0453 first=7.4697 kCE=9.6654 KD=5.1335 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=85.62 | sec/step~6.66 | keep=0.85 | K=8 | llama: tf=9.7358 first=8.5349 kCE=5.2181 KD=4.0824 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.6892 first=7.4936 kCE=7.9705 KD=5.0550 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=154.60 | sec/step~5.92 | keep=0.85 | K=8 | llama: tf=9.7891 first=7.9455 kCE=5.5017 KD=4.6388 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.2482 first=7.4582 kCE=7.7101 KD=6.0168 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=14.48 | sec/step~6.37 | keep=0.85 | K=8 | llama: tf=9.2447 first=8.7338 kCE=5.3917 KD=4.1169 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.5137 first=7.5570 kCE=8.5487 KD=5.5866 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=51.94 | sec/step~6.34 | keep=0.85 | K=8 | llama: tf=10.3331 first=9.4107 kCE=5.5043 KD=3.9432 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.4783 first=7.3727 kCE=8.6597 KD=5.1595 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=84.57 | sec/step~5.87 | keep=0.85 | K=8 | llama: tf=9.7800 first=9.1300 kCE=4.9917 KD=3.8237 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.3755 first=8.1615 kCE=9.0366 KD=4.7853 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=135.10 | sec/step~5.31 | keep=0.85 | K=8 | llama: tf=9.3286 first=8.3077 kCE=4.7028 KD=4.1238 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.0882 first=7.0889 kCE=9.3969 KD=4.7742 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=70.34 | sec/step~6.96 | keep=0.85 | K=8 | llama: tf=9.8212 first=8.5671 kCE=4.9969 KD=3.6272 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.4919 first=7.3247 kCE=9.2700 KD=5.2182 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=135.07 | sec/step~6.58 | keep=0.85 | K=8 | llama: tf=9.5432 first=7.7900 kCE=5.0495 KD=3.8336 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.3753 first=6.7704 kCE=8.4470 KD=4.9624 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=204.45 | sec/step~6.60 | keep=0.85 | K=8 | llama: tf=8.8889 first=7.1991 kCE=5.0185 KD=3.6929 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=8.9168 first=5.9254 kCE=8.1486 KD=5.0736 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.01055328293038266, 'rms_mean_cal': 0.010570971162191458, 'embed_rms': 0.010568301193416119, 'count': 350}, 'qwen': {'rms_mean_raw': 0.01364112368917891, 'rms_mean_cal': 0.013641325224723135, 'embed_rms': 0.013638017699122429, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch10/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch10/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3043.21it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.35s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2968.89it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.82s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.012  |  NLL/token (gold): 9.836644840889237
       First-token acc: top1=0.020  top5=0.050
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.752024693463845
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.92s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.66s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.012

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.820016860961914
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0121829001412715,
      "nll": 9.836644840889237,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.04999999701976776,
      "nll_token": 9.836644840889237
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.752024693463845,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.752024693463845
    },
    "wall_clock_sec": 3.919208288192749
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.655395984649658
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0121829001412715
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0121829001412715
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.012 | Qwen -
  FirstTok@1: Llama 0.020 | Qwen 0.050
  NLL/token:  Llama 9.837 | Qwen 9.752
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch10/predictions.jsonl
  1. Llama: 1980 | Qwen: 1. 1. 1. 1. 1. 1 | Gold: linear
  2. Llama: the "Great Depression" (1929-1941) and the "Great | Qwen: 1. 1999年10月1日，中国国家 | Gold: Lampea
  3. Llama: the Great Depression | Qwen: 1. 1999 2. 1999 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1.5 million | Qwen: 1. 19222. 19223 | Gold: San Jose
  5. Llama: 1980 | Qwen: 1. 19992. 19993 | Gold: oxides

=========================================
EPOCH 11/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3207.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3142.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.09it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.60s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.012  |  NLL/token (gold): 9.836644840889237
       First-token acc: top1=0.020  top5=0.050
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.752024693463845
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.66s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.90s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.012

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.59849977493286
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0121829001412715,
      "nll": 9.836644840889237,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.04999999701976776,
      "nll_token": 9.836644840889237
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.752024693463845,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.752024693463845
    },
    "wall_clock_sec": 3.6577188968658447
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.901087522506714
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0121829001412715
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0121829001412715
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2983.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.53s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3264.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=10, global_step=3500
Epoch 11/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=41.27 | sec/step~6.47 | keep=0.85 | K=8 | llama: tf=9.1135 first=7.5328 kCE=5.2099 KD=4.3163 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=8.6288 first=6.2562 kCE=7.0973 KD=5.2864 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  20/350 | grad_norm=83.46 | sec/step~6.30 | keep=0.85 | K=8 | llama: tf=9.6125 first=7.7206 kCE=5.3490 KD=4.2309 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.0300 first=6.8106 kCE=8.6534 KD=5.3579 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  30/350 | grad_norm=114.93 | sec/step~6.82 | keep=0.85 | K=8 | llama: tf=9.5409 first=7.9577 kCE=5.1036 KD=3.8704 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.2561 first=7.1621 kCE=9.0606 KD=4.7465 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  40/350 | grad_norm=49.16 | sec/step~6.39 | keep=0.85 | K=8 | llama: tf=9.7927 first=9.5215 kCE=5.4893 KD=4.2862 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.4886 first=8.5526 kCE=8.7001 KD=5.5714 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  50/350 | grad_norm=91.31 | sec/step~8.72 | keep=0.85 | K=8 | llama: tf=9.7291 first=7.7112 kCE=5.0159 KD=3.4071 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.7360 first=7.2534 kCE=9.0903 KD=4.6964 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  60/350 | grad_norm=126.75 | sec/step~6.44 | keep=0.85 | K=8 | llama: tf=9.9525 first=7.5056 kCE=4.9318 KD=4.0346 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.4174 first=6.2869 kCE=8.8341 KD=4.7059 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  70/350 | grad_norm=25.59 | sec/step~5.66 | keep=0.85 | K=8 | llama: tf=10.3350 first=8.3016 kCE=5.2925 KD=4.1050 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=11.4734 first=7.3158 kCE=8.7799 KD=4.7548 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  80/350 | grad_norm=106.44 | sec/step~7.84 | keep=0.85 | K=8 | llama: tf=9.4072 first=8.4255 kCE=5.4614 KD=3.8235 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.1155 first=7.7633 kCE=8.1857 KD=4.9986 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  90/350 | grad_norm=184.83 | sec/step~7.17 | keep=0.85 | K=8 | llama: tf=10.5205 first=8.6069 kCE=4.8380 KD=3.4472 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.9705 first=7.7586 kCE=9.3796 KD=4.6033 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  100/350 | grad_norm=24.75 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=10.8319 first=8.3932 kCE=5.2813 KD=4.2866 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=11.2359 first=7.6385 kCE=8.7946 KD=4.9727 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  110/350 | grad_norm=114.86 | sec/step~5.70 | keep=0.85 | K=8 | llama: tf=9.3914 first=6.9605 kCE=5.4137 KD=4.7944 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.1822 first=6.1284 kCE=6.7134 KD=5.8270 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  120/350 | grad_norm=166.06 | sec/step~6.75 | keep=0.85 | K=8 | llama: tf=10.0578 first=8.1282 kCE=5.3241 KD=4.0541 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.7252 first=7.6851 kCE=8.7841 KD=5.3109 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  130/350 | grad_norm=10.67 | sec/step~6.66 | keep=0.85 | K=8 | llama: tf=9.2139 first=6.7183 kCE=5.0677 KD=3.9302 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.2340 first=6.3140 kCE=8.6766 KD=5.2717 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  140/350 | grad_norm=54.98 | sec/step~6.35 | keep=0.85 | K=8 | llama: tf=9.9130 first=7.9766 kCE=5.3357 KD=4.1876 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.8691 first=7.3683 kCE=8.2571 KD=5.4957 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  150/350 | grad_norm=88.45 | sec/step~6.60 | keep=0.85 | K=8 | llama: tf=9.9378 first=7.6009 kCE=5.3036 KD=4.4551 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.2474 first=6.2012 kCE=7.8023 KD=5.5636 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  160/350 | grad_norm=141.55 | sec/step~6.44 | keep=0.85 | K=8 | llama: tf=9.7860 first=8.4033 kCE=5.2400 KD=3.9156 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.5385 first=7.5336 kCE=8.9565 KD=5.6199 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  170/350 | grad_norm=52.80 | sec/step~5.55 | keep=0.85 | K=8 | llama: tf=9.6602 first=6.8524 kCE=5.2333 KD=4.3680 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.0343 first=6.1565 kCE=8.5357 KD=5.1901 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  180/350 | grad_norm=106.85 | sec/step~6.18 | keep=0.85 | K=8 | llama: tf=10.2851 first=8.2935 kCE=4.9657 KD=3.9387 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.2080 first=6.8244 kCE=7.6315 KD=4.7166 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  190/350 | grad_norm=148.17 | sec/step~6.21 | keep=0.85 | K=8 | llama: tf=9.8734 first=7.1657 kCE=5.4636 KD=4.1052 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.5155 first=6.6926 kCE=8.5813 KD=5.4137 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  200/350 | grad_norm=21.85 | sec/step~6.81 | keep=0.85 | K=8 | llama: tf=10.3695 first=8.1833 kCE=4.5635 KD=3.6630 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=11.9884 first=7.1535 kCE=9.9909 KD=4.6038 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  210/350 | grad_norm=40.25 | sec/step~6.11 | keep=0.85 | K=8 | llama: tf=9.2782 first=7.6135 kCE=5.4491 KD=4.7970 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.0580 first=6.4195 kCE=7.4654 KD=5.7055 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  220/350 | grad_norm=52.70 | sec/step~7.37 | keep=0.85 | K=8 | llama: tf=10.6134 first=8.2072 kCE=5.1006 KD=3.7673 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.6376 first=7.6342 kCE=8.4561 KD=4.9053 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  230/350 | grad_norm=51.80 | sec/step~7.71 | keep=0.85 | K=8 | llama: tf=9.4753 first=7.8771 kCE=4.7557 KD=3.7063 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.8364 first=7.4540 kCE=8.9075 KD=5.1719 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  240/350 | grad_norm=107.59 | sec/step~6.37 | keep=0.85 | K=8 | llama: tf=9.2826 first=7.5590 kCE=5.3188 KD=4.3008 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.4834 first=6.6828 kCE=8.3575 KD=5.3570 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  250/350 | grad_norm=152.98 | sec/step~6.41 | keep=0.85 | K=8 | llama: tf=9.5202 first=8.2902 kCE=5.0940 KD=3.8761 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=9.9118 first=6.7607 kCE=8.1318 KD=4.9778 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  260/350 | grad_norm=23.10 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=9.6269 first=8.0581 kCE=5.4100 KD=4.4716 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.8865 first=7.3068 kCE=8.7466 KD=5.3308 man=0.0002 | scale_pen(qwen)=6.0041e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  270/350 | grad_norm=59.70 | sec/step~8.01 | keep=0.85 | K=8 | llama: tf=9.9145 first=8.0874 kCE=4.8459 KD=3.6441 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.5712 first=7.6774 kCE=8.7762 KD=4.9387 man=0.0002 | scale_pen(qwen)=6.0041e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  280/350 | grad_norm=88.52 | sec/step~7.48 | keep=0.85 | K=8 | llama: tf=9.4570 first=8.2072 kCE=4.9315 KD=3.6937 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=8.8255 first=6.3497 kCE=8.1186 KD=4.8236 man=0.0002 | scale_pen(qwen)=6.0041e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  290/350 | grad_norm=15.74 | sec/step~7.18 | keep=0.85 | K=8 | llama: tf=8.9716 first=8.3222 kCE=4.6483 KD=3.3924 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=8.2568 first=6.6359 kCE=7.7233 KD=4.3315 man=0.0002 | scale_pen(qwen)=1.0267e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  300/350 | grad_norm=48.14 | sec/step~6.39 | keep=0.85 | K=8 | llama: tf=9.1813 first=7.2439 kCE=5.3313 KD=4.3007 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.1342 first=6.6209 kCE=7.7639 KD=5.4427 man=0.0002 | scale_pen(qwen)=1.0267e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  310/350 | grad_norm=75.57 | sec/step~6.17 | keep=0.85 | K=8 | llama: tf=9.5223 first=7.2631 kCE=4.8935 KD=4.1066 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.8795 first=6.2461 kCE=7.5687 KD=4.8103 man=0.0002 | scale_pen(qwen)=1.0267e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  320/350 | grad_norm=104.14 | sec/step~6.58 | keep=0.85 | K=8 | llama: tf=9.8718 first=8.7349 kCE=4.9632 KD=4.1715 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.3251 first=7.7074 kCE=6.7459 KD=4.7865 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  330/350 | grad_norm=56.32 | sec/step~6.19 | keep=0.85 | K=8 | llama: tf=10.2789 first=8.3321 kCE=5.0885 KD=4.1835 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.6356 first=7.7742 kCE=8.6911 KD=5.0452 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  340/350 | grad_norm=91.92 | sec/step~6.29 | keep=0.85 | K=8 | llama: tf=9.3711 first=7.4804 kCE=5.2688 KD=4.1381 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.6544 first=6.2036 kCE=8.0454 KD=4.8611 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
  step  350/350 | grad_norm=134.10 | sec/step~5.88 | keep=0.85 | K=8 | llama: tf=9.6167 first=7.8041 kCE=5.1975 KD=4.0958 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.4285 first=7.3075 kCE=8.3747 KD=4.9813 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01365]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010552105733326503, 'rms_mean_cal': 0.010570983261402165, 'embed_rms': 0.010566353797912598, 'count': 350}, 'qwen': {'rms_mean_raw': 0.013643024739410196, 'rms_mean_cal': 0.013640797268599271, 'embed_rms': 0.013653620146214962, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch11/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch11/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1653.91it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3596.40it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.85s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.018  |  NLL/token (gold): 9.787569851561738
       First-token acc: top1=0.025  top5=0.055
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.782400765116252
       First-token acc: top1=0.050  top5=0.140
Wall clock: 4.43s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.09s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.018

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 16.849254846572876
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.017639822214053767,
      "nll": 9.787569851561738,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.054999999701976776,
      "nll_token": 9.787569851561738
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.782400765116252,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.782400765116252
    },
    "wall_clock_sec": 4.427805662155151
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.089768171310425
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999750000006,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.017639822214053767
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.017639822214053767
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.018 | Qwen -
  FirstTok@1: Llama 0.025 | Qwen 0.050
  NLL/token:  Llama 9.788 | Qwen 9.782
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch11/predictions.jsonl
  1. Llama: php | Qwen: 1. 19222. 19223 | Gold: linear
  2. Llama: the 1960s and 1970s, when the US government was | Qwen: 1. 19222. 19223 | Gold: Lampea
  3. Llama: the 1960s and 1970s. The 1960s | Qwen: 1.55251189344068 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1.5 million | Qwen: 1985-1986 | Gold: San Jose
  5. Llama: php | Qwen: 1. 1922 | Gold: oxides

=========================================
EPOCH 12/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2979.44it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3459.93it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.97s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.018  |  NLL/token (gold): 9.787569851561738
       First-token acc: top1=0.025  top5=0.055
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.782400765116252
       First-token acc: top1=0.050  top5=0.140
Wall clock: 3.93s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.15s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.018

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.974430799484253
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.017639822214053767,
      "nll": 9.787569851561738,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.054999999701976776,
      "nll_token": 9.787569851561738
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.782400765116252,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14000000059604645,
      "nll_token": 9.782400765116252
    },
    "wall_clock_sec": 3.9302334785461426
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.146301507949829
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0024999999750000006,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.017639822214053767
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.017639822214053767
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2990.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3333.44it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=11, global_step=3850
Epoch 12/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=47.91 | sec/step~6.57 | keep=0.85 | K=8 | llama: tf=9.1304 first=7.7826 kCE=4.9799 KD=3.6642 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.4026 first=7.0700 kCE=9.1640 KD=4.8687 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=106.90 | sec/step~6.15 | keep=0.85 | K=8 | llama: tf=8.8621 first=6.8837 kCE=5.2870 KD=4.0338 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.6796 first=6.3640 kCE=8.1627 KD=5.2762 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=156.63 | sec/step~5.56 | keep=0.85 | K=8 | llama: tf=10.0146 first=8.0883 kCE=4.9377 KD=3.6998 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.7350 first=7.1017 kCE=9.5587 KD=4.8423 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=24.88 | sec/step~6.48 | keep=0.85 | K=8 | llama: tf=9.8345 first=8.4726 kCE=5.2986 KD=4.1245 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.5661 first=6.9545 kCE=8.0833 KD=4.8694 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=59.72 | sec/step~8.21 | keep=0.85 | K=8 | llama: tf=9.9858 first=8.7352 kCE=5.3132 KD=3.6828 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.4824 first=7.4377 kCE=8.8167 KD=5.1866 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=86.28 | sec/step~6.25 | keep=0.85 | K=8 | llama: tf=9.3330 first=7.5454 kCE=5.2689 KD=4.4318 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=8.6711 first=6.8031 kCE=6.9715 KD=4.8872 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=41.14 | sec/step~7.39 | keep=0.85 | K=8 | llama: tf=9.5072 first=7.8549 kCE=5.6304 KD=3.8425 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.1879 first=7.2499 kCE=6.9649 KD=5.8057 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=104.23 | sec/step~5.82 | keep=0.85 | K=8 | llama: tf=9.9167 first=7.2408 kCE=4.9156 KD=4.1285 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.5601 first=5.8150 kCE=7.1707 KD=5.2380 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=167.10 | sec/step~6.06 | keep=0.85 | K=8 | llama: tf=9.1419 first=7.7915 kCE=5.2413 KD=4.3598 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=8.9745 first=6.6391 kCE=7.7474 KD=4.8920 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=61.49 | sec/step~6.08 | keep=0.85 | K=8 | llama: tf=9.7209 first=8.0785 kCE=5.2497 KD=4.1433 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.2393 first=7.2558 kCE=7.7101 KD=5.3077 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=122.93 | sec/step~6.60 | keep=0.85 | K=8 | llama: tf=9.7306 first=7.9665 kCE=4.8202 KD=3.7846 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.7450 first=6.9410 kCE=8.3066 KD=4.4627 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=188.87 | sec/step~5.35 | keep=0.85 | K=8 | llama: tf=10.5157 first=8.1625 kCE=5.1945 KD=4.4543 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.9704 first=7.3940 kCE=9.1477 KD=5.4382 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=18.80 | sec/step~7.36 | keep=0.85 | K=8 | llama: tf=9.7454 first=7.8583 kCE=5.1789 KD=3.8500 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.9183 first=6.5775 kCE=7.6122 KD=4.6744 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=63.25 | sec/step~5.99 | keep=0.85 | K=8 | llama: tf=10.1795 first=8.0585 kCE=5.2543 KD=4.4121 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.5283 first=6.4224 kCE=8.3739 KD=5.2958 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=85.74 | sec/step~5.96 | keep=0.85 | K=8 | llama: tf=10.2248 first=7.9668 kCE=4.8433 KD=4.1685 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.4292 first=6.5927 kCE=8.2735 KD=4.5627 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=119.05 | sec/step~6.88 | keep=0.85 | K=8 | llama: tf=9.7158 first=7.8671 kCE=5.2599 KD=3.9466 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.9948 first=6.5948 kCE=8.2867 KD=5.1562 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=58.33 | sec/step~7.23 | keep=0.85 | K=8 | llama: tf=9.6977 first=8.4782 kCE=4.9253 KD=3.8324 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.3471 first=7.5405 kCE=8.6468 KD=5.1922 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=108.03 | sec/step~6.61 | keep=0.85 | K=8 | llama: tf=9.9928 first=9.1228 kCE=4.9532 KD=3.9797 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.9640 first=7.1265 kCE=8.8492 KD=4.8551 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=162.24 | sec/step~6.16 | keep=0.85 | K=8 | llama: tf=10.1757 first=7.1044 kCE=4.9397 KD=4.0855 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=10.7546 first=5.8948 kCE=7.8774 KD=5.2855 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=42.51 | sec/step~5.57 | keep=0.85 | K=8 | llama: tf=10.0444 first=8.6925 kCE=5.0141 KD=4.1148 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.8620 first=7.3165 kCE=8.7826 KD=4.8553 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=74.61 | sec/step~7.77 | keep=0.85 | K=8 | llama: tf=9.6880 first=8.0157 kCE=4.8386 KD=3.3016 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.5204 first=6.7966 kCE=7.9911 KD=4.6927 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=95.95 | sec/step~6.94 | keep=0.85 | K=8 | llama: tf=9.6851 first=7.2653 kCE=5.0260 KD=3.9480 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.6767 first=6.6483 kCE=7.6779 KD=5.4309 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=61.90 | sec/step~5.95 | keep=0.85 | K=8 | llama: tf=9.1971 first=8.3219 kCE=4.5544 KD=3.7584 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.6457 first=7.1121 kCE=8.5300 KD=4.8984 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=122.50 | sec/step~7.71 | keep=0.85 | K=8 | llama: tf=9.8995 first=6.5337 kCE=5.2515 KD=3.8514 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.8035 first=6.0175 kCE=8.4278 KD=5.2027 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=165.70 | sec/step~6.70 | keep=0.85 | K=8 | llama: tf=10.5714 first=7.3903 kCE=4.9384 KD=3.7240 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.3020 first=6.7742 kCE=8.0146 KD=4.7176 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=28.58 | sec/step~6.51 | keep=0.85 | K=8 | llama: tf=9.5209 first=7.4759 kCE=5.2497 KD=4.3743 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=9.6028 first=7.1337 kCE=7.7219 KD=5.3394 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=65.84 | sec/step~7.09 | keep=0.85 | K=8 | llama: tf=9.3198 first=8.4165 kCE=4.6351 KD=3.7772 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=9.8340 first=7.0351 kCE=8.8880 KD=4.9478 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=99.96 | sec/step~6.59 | keep=0.85 | K=8 | llama: tf=10.1266 first=7.9053 kCE=5.2988 KD=4.1951 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=11.1609 first=7.1683 kCE=8.5177 KD=6.0284 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=17.59 | sec/step~6.28 | keep=0.85 | K=8 | llama: tf=9.1578 first=8.7365 kCE=5.1222 KD=4.0376 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=9.6631 first=7.2122 kCE=7.7030 KD=5.3824 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=39.09 | sec/step~6.94 | keep=0.85 | K=8 | llama: tf=9.9754 first=10.1746 kCE=5.3658 KD=4.0433 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=10.7820 first=8.3115 kCE=7.7460 KD=5.3162 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=60.83 | sec/step~5.64 | keep=0.85 | K=8 | llama: tf=9.6554 first=8.1390 kCE=5.0823 KD=4.2188 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=10.6395 first=7.4573 kCE=9.2335 KD=5.1624 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/350 | grad_norm=88.90 | sec/step~5.70 | keep=0.85 | K=8 | llama: tf=9.1667 first=7.1014 kCE=4.9011 KD=4.0704 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=8.6172 first=5.9234 kCE=7.5191 KD=5.1452 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/350 | grad_norm=64.59 | sec/step~5.84 | keep=0.85 | K=8 | llama: tf=9.2856 first=7.7644 kCE=4.8273 KD=3.7502 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=9.3126 first=6.4804 kCE=8.3883 KD=4.6666 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/350 | grad_norm=133.38 | sec/step~7.70 | keep=0.85 | K=8 | llama: tf=9.8119 first=8.2292 kCE=4.6792 KD=3.6400 man=0.0001 | scale_pen(llama)=5.1159e-13 | qwen: tf=10.9260 first=7.2863 kCE=9.3786 KD=4.3247 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/350 | grad_norm=212.24 | sec/step~6.62 | keep=0.85 | K=8 | llama: tf=9.4848 first=8.1542 kCE=5.0429 KD=4.2375 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.5214 first=6.8981 kCE=8.1720 KD=5.2234 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hailmary_allknobs_hero_fast_2h/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010551542325743606, 'rms_mean_cal': 0.010571158988667386, 'embed_rms': 0.010575145483016968, 'count': 350}, 'qwen': {'rms_mean_raw': 0.01364258725196123, 'rms_mean_cal': 0.013640940535281386, 'embed_rms': 0.01363915205001831, 'count': 350}}
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/epoch12/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12 --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/epoch12/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3101.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.43s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2842.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.50s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.006  |  NLL/token (gold): 9.804483931621457
       First-token acc: top1=0.020  top5=0.060
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.756985196676204
       First-token acc: top1=0.050  top5=0.150
Wall clock: 3.73s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 4.14s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.006

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.501624584197998
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.006483598534155402,
      "nll": 9.804483931621457,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.05999999865889549,
      "nll_token": 9.804483931621457
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.756985196676204,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14999999105930328,
      "nll_token": 9.756985196676204
    },
    "wall_clock_sec": 3.731293201446533
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 4.135280609130859
  },
  "joint": {
    "em": 0.0,
    "f1": 0.003921568580315265,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.006483598534155402
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.006483598534155402
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12/predictions.jsonl
+ set +x
✓ Metrics from: runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12/metrics.json
  Text F1:    Llama 0.801 | Qwen 0.838
  Latent F1:  Llama 0.006 | Qwen -
  FirstTok@1: Llama 0.020 | Qwen 0.050
  NLL/token:  Llama 9.804 | Qwen 9.757
Top 5 latent predictions from runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch12/predictions.jsonl
  1. Llama: php | Qwen: 1. 19222. 19223 | Gold: linear
  2. Llama: the 1960s.** | Qwen: 1. 19992. 20003 | Gold: Lampea
  3. Llama: the 1960s.** | Qwen: 1.55252233499894 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 2000 | Qwen: 1985-1986 | Gold: San Jose
  5. Llama: the 1960s.** | Qwen: 1.55251950283289 | Gold: oxides

=========================================
EPOCH 13/14
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch13_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode text --latent_anchor_text 'Answer: ' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.25 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 64 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch13_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3098.29it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2727.12it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 48
Compression ratio (Llama): 5.1x | (Qwen): 4.8x
Approx interlingua payload per example: 43008 bytes (fp32); fp16 reference: 4300800 bytes; fp32 reference: 8601600 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.801  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.665   F1: 0.838   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.46s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.006  |  NLL/token (gold): 9.804483931621457
       First-token acc: top1=0.020  top5=0.060
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 9.756985196676204
       First-token acc: top1=0.050  top5=0.150
Wall clock: 4.38s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.87s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.006

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 48,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 5.105520833333333,
    "qwen": 4.826875
  },
  "payload_bytes": 43008,
  "payload_bytes_detail": {
    "fp32": 8601600,
    "fp16": 4300800,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      42,
      256
    ],
    "latent_bytes": {
      "fp32": 8601600,
      "fp16": 4300800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 43008,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.8007829157729505,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.665,
      "f1": 0.8383902733374898,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 17.4599769115448
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.006483598534155402,
      "nll": 9.804483931621457,
      "first_token_top1": 0.019999999552965164,
      "first_token_top5": 0.05999999865889549,
      "nll_token": 9.804483931621457
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.756985196676204,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.14999999105930328,
      "nll_token": 9.756985196676204
    },
    "wall_clock_sec": 4.381702423095703
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.8728532791137695
  },
  "joint": {
    "em": 0.0,
    "f1": 0.003921568580315265,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.006483598534155402
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.25,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.006483598534155402
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_hero_fast_2h/eval_epoch13_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3140.04it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.62s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.53s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.24s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3369.60it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_hero_fast_2h/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=12, global_step=4200
Epoch 13/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/350 | grad_norm=49.27 | sec/step~8.05 | keep=0.85 | K=8 | llama: tf=9.9102 first=7.7025 kCE=4.6045 KD=3.2522 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.7864 first=6.2947 kCE=8.6740 KD=4.2194 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/350 | grad_norm=98.24 | sec/step~6.02 | keep=0.85 | K=8 | llama: tf=9.9591 first=7.9608 kCE=5.0522 KD=4.0392 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.7799 first=6.5170 kCE=8.8580 KD=5.2710 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/350 | grad_norm=137.79 | sec/step~6.72 | keep=0.85 | K=8 | llama: tf=9.6982 first=7.6900 kCE=5.2524 KD=4.2901 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.2554 first=6.8946 kCE=7.1913 KD=5.3663 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/350 | grad_norm=93.40 | sec/step~6.69 | keep=0.85 | K=8 | llama: tf=9.8960 first=8.4716 kCE=5.4755 KD=4.1240 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.9336 first=7.1221 kCE=8.4807 KD=5.3808 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/350 | grad_norm=187.71 | sec/step~6.21 | keep=0.85 | K=8 | llama: tf=9.9468 first=8.6493 kCE=5.4191 KD=4.3523 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.6929 first=8.0383 kCE=8.0368 KD=5.3179 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/350 | grad_norm=306.95 | sec/step~6.66 | keep=0.85 | K=8 | llama: tf=9.7810 first=7.8651 kCE=4.7757 KD=3.7661 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=9.4101 first=7.4291 kCE=8.7698 KD=4.8048 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/350 | grad_norm=45.67 | sec/step~5.71 | keep=0.85 | K=8 | llama: tf=9.4482 first=8.1193 kCE=5.3619 KD=4.1544 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=9.2448 first=6.4984 kCE=8.8384 KD=5.2760 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/350 | grad_norm=83.92 | sec/step~5.09 | keep=0.85 | K=8 | llama: tf=8.6272 first=8.5060 kCE=5.0658 KD=4.0946 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=8.3246 first=7.3692 kCE=8.6847 KD=5.0901 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/350 | grad_norm=112.69 | sec/step~5.56 | keep=0.85 | K=8 | llama: tf=9.5034 first=7.8088 kCE=5.3055 KD=4.2676 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=8.9759 first=6.3248 kCE=7.8769 KD=5.2842 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/350 | grad_norm=30.66 | sec/step~5.25 | keep=0.85 | K=8 | llama: tf=8.7103 first=7.5326 kCE=5.1380 KD=4.3266 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.2918 first=6.7652 kCE=7.5984 KD=5.2021 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/350 | grad_norm=136.24 | sec/step~6.77 | keep=0.85 | K=8 | llama: tf=10.3927 first=7.1627 kCE=4.6147 KD=3.5601 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.8195 first=6.5557 kCE=8.4895 KD=4.6957 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/350 | grad_norm=257.17 | sec/step~7.07 | keep=0.85 | K=8 | llama: tf=9.5544 first=8.1274 kCE=4.8947 KD=3.7030 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.3725 first=7.1345 kCE=9.2182 KD=5.3627 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/350 | grad_norm=21.81 | sec/step~5.85 | keep=0.85 | K=8 | llama: tf=10.0678 first=8.3559 kCE=5.0175 KD=4.1816 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.4620 first=6.8457 kCE=8.4720 KD=5.4154 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/350 | grad_norm=74.21 | sec/step~6.31 | keep=0.85 | K=8 | llama: tf=9.5320 first=8.0905 kCE=5.0322 KD=4.0251 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.8074 first=7.1024 kCE=7.8108 KD=5.0776 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/350 | grad_norm=155.16 | sec/step~6.46 | keep=0.85 | K=8 | llama: tf=9.9307 first=8.4133 kCE=4.7895 KD=3.8654 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.1490 first=7.2959 kCE=9.0054 KD=4.7126 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/350 | grad_norm=238.71 | sec/step~6.50 | keep=0.85 | K=8 | llama: tf=9.9982 first=7.3593 kCE=4.9190 KD=3.7086 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.3749 first=6.2838 kCE=7.8019 KD=4.8732 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/350 | grad_norm=89.72 | sec/step~6.17 | keep=0.85 | K=8 | llama: tf=9.7496 first=8.3534 kCE=5.0413 KD=4.0236 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.1608 first=6.6990 kCE=7.7791 KD=5.1333 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/350 | grad_norm=171.27 | sec/step~5.66 | keep=0.85 | K=8 | llama: tf=10.0049 first=7.7255 kCE=5.0880 KD=4.4372 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=11.1340 first=7.1299 kCE=8.4248 KD=5.6001 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/350 | grad_norm=259.43 | sec/step~6.69 | keep=0.85 | K=8 | llama: tf=9.3271 first=6.6875 kCE=5.0966 KD=4.2932 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.5705 first=5.8386 kCE=7.4380 KD=5.3882 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/350 | grad_norm=102.32 | sec/step~6.70 | keep=0.85 | K=8 | llama: tf=9.9905 first=7.8965 kCE=5.0447 KD=4.0687 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.2328 first=6.4553 kCE=8.2386 KD=5.4300 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/350 | grad_norm=210.62 | sec/step~6.25 | keep=0.85 | K=8 | llama: tf=10.0977 first=8.6645 kCE=4.7864 KD=3.9644 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=11.1252 first=7.7916 kCE=7.5408 KD=4.7455 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/350 | grad_norm=307.33 | sec/step~6.54 | keep=0.85 | K=8 | llama: tf=9.7607 first=8.0711 kCE=5.1590 KD=4.2885 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.4878 first=7.0833 kCE=8.8940 KD=5.0315 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/350 | grad_norm=34.68 | sec/step~6.42 | keep=0.85 | K=8 | llama: tf=9.6215 first=8.4491 kCE=5.1135 KD=4.5094 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.2235 first=8.0319 kCE=9.0573 KD=5.3638 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/350 | grad_norm=75.08 | sec/step~5.98 | keep=0.85 | K=8 | llama: tf=9.8030 first=8.9011 kCE=4.8467 KD=3.8551 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.8347 first=7.7333 kCE=8.4177 KD=5.3740 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/350 | grad_norm=133.92 | sec/step~6.03 | keep=0.85 | K=8 | llama: tf=9.8176 first=7.5407 kCE=4.7723 KD=4.1933 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=9.8883 first=7.0358 kCE=8.4130 KD=5.1654 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/350 | grad_norm=27.01 | sec/step~6.14 | keep=0.85 | K=8 | llama: tf=9.7162 first=8.6891 kCE=5.3089 KD=4.4524 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.4605 first=7.0590 kCE=7.4489 KD=5.4974 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/350 | grad_norm=80.16 | sec/step~6.76 | keep=0.85 | K=8 | llama: tf=9.7185 first=7.9429 kCE=4.4800 KD=3.4481 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.5213 first=6.5519 kCE=8.3052 KD=4.4006 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/350 | grad_norm=123.24 | sec/step~6.98 | keep=0.85 | K=8 | llama: tf=8.8535 first=7.5376 kCE=5.0504 KD=3.9887 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.1802 first=6.3988 kCE=7.8646 KD=5.1720 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/350 | grad_norm=21.75 | sec/step~6.99 | keep=0.85 | K=8 | llama: tf=10.2221 first=8.8406 kCE=4.8820 KD=3.9458 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.7439 first=7.4802 kCE=7.2726 KD=4.7438 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/350 | grad_norm=94.64 | sec/step~7.23 | keep=0.85 | K=8 | llama: tf=9.3627 first=7.5612 kCE=4.7145 KD=3.5297 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=9.8813 first=6.7873 kCE=8.5648 KD=4.5592 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/350 | grad_norm=114.97 | sec/step~6.20 | keep=0.85 | K=8 | llama: tf=10.2246 first=8.3976 kCE=5.2891 KD=4.5280 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=10.6035 first=7.9975 kCE=8.6445 KD=5.2834 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0105 rms_cal~0.0106 embed_rms~0.01056; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
