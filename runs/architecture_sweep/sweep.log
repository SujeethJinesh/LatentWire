/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
ARCHITECTURE SWEEP: Diagnose and Fix Mode Collapse
================================================================================

Device: cuda
Samples: 1000, Steps: 300, M: 32

[1/3] Loading frozen LLM...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3303.90it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.27s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
  ✓ Model loaded

[2/3] Loading data (SQuAD, n=1000)...
  ✓ Loaded 1000 examples

[3/3] Testing architectures...

================================================================================
Configuration: Direct Sequence Compression (PROPOSED)
================================================================================

Training...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  Step 1/300: loss=7.9054
  Step 51/300: loss=3.5881
  Step 101/300: loss=3.1680
  Step 151/300: loss=5.7071
  Step 201/300: loss=3.5993
  Step 251/300: loss=4.8519
  Step 300/300: loss=3.0091

Evaluating...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

  Results:
    Diversity: 1/10 (10.0%)
    Avg cosine similarity: 0.999 (GOOD if < 0.5, BAD if > 0.8)
    PCA variance explained: 1.000 (GOOD if > 0.7, BAD if < 0.3)
    NN diversity score: 0.000 (GOOD if > 0.7, BAD if < 0.3)

  Sample predictions:
    [1] Gold: Dane                           → Pred: the 19th century, and the 20th
    [2] Gold: Muslims                        → Pred: the 19th century, and the 20th
    [3] Gold: orientalism and tropicality    → Pred: the 19th century, and the 20th
    [4] Gold: numeracy                       → Pred: the 19th century, and the 20th
    [5] Gold: Mental Health (Care and Treatm → Pred: the 19th century, and the 20th

================================================================================
Configuration: Mean Pool Bottleneck (BROKEN)
================================================================================

Training...
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/scripts/sweep_architectures.py", line 521, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/scripts/sweep_architectures.py", line 456, in main
    result = train_and_evaluate(
  File "/projects/m000066/sujinesh/LatentWire/scripts/sweep_architectures.py", line 324, in train_and_evaluate
    loss = k_token_ce(llama_model, llama_tokenizer, compressed, ex['answer'], K=K)
  File "/projects/m000066/sujinesh/LatentWire/scripts/sweep_architectures.py", line 175, in k_token_ce
    inputs_embeds = torch.cat([prefix_embeds, model.get_input_embeddings()(anchor_ids)], dim=1)
RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 512 but got size 4096 for tensor number 1 in the list.
