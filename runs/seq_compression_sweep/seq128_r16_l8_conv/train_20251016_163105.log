Running: python train_sequence_compression.py --model_id meta-llama/Meta-Llama-3.1-8B-Instruct --dataset squad --samples 10000 --epochs 3 --batch_size 48 --max_length 512 --eval_samples 100 --lr 5e-4 --target_sequence_length 128 --pooling_method convolutional --save_dir runs/seq_compression_sweep/seq128_r16_l8_conv --diagnostic_log runs/seq_compression_sweep/seq128_r16_l8_conv/diagnostics.jsonl --use_lora --lora_r 16 --lora_alpha 32 --lora_layers 8
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using device: cuda:0
GPUs available: 4
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3

Loading meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3291.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.19s/it]

Applying LoRA: r=16, alpha=32, layers=8
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

Model loaded!

Creating sequence compressor: 300 → 128
Pooling method: convolutional
Compression ratio: 2.34×

Compressor parameters: 20,480

Loading 10000 training examples from squad...
Loading 100 eval examples...
Loaded 10000 train, 100 eval

================================================================================
TRAINING
================================================================================


================================================================================
Epoch 1/3
================================================================================

Epoch 0:   0%|          | 0/209 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/209 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/train_sequence_compression.py", line 727, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/train_sequence_compression.py", line 660, in main
    global_step = train_epoch(
  File "/projects/m000066/sujinesh/LatentWire/train_sequence_compression.py", line 337, in train_epoch
    compressed = compressor(source_embeds, positions)  # [batch, target_len, dim]
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/train_sequence_compression.py", line 205, in forward
    return self.pooler(embeddings, positions)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/train_sequence_compression.py", line 169, in forward
    pooled = pooled + pos_emb
RuntimeError: The size of tensor a (173) must match the size of tensor b (128) at non-singleton dimension 1
