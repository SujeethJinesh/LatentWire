/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
GIST TOKENS EVALUATION
================================================================================
Checkpoint: runs/gist_validate
Test samples: 200
Max new tokens: 128
Device: cuda:0
================================================================================

Loading gist model from runs/gist_validate...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:21<01:05, 21.89s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:52<00:54, 27.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:20<00:27, 27.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:25<00:00, 18.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:25<00:00, 21.25s/it]
✓ Loaded GistLlama wrapper with 1 gist tokens
Loading Alpaca test data...
✓ Loaded 200 test samples

Evaluating samples (batch_size=16)...
Preparing prompts...
CRITICAL: Gist prompts have INSTRUCTION REMOVED for true compression!
Generating outputs...
Batches:   0%|          | 0/13 [00:00<?, ?it/s]/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Batches:   0%|          | 0/13 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/compressions/eval_gist.py", line 328, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/compressions/eval_gist.py", line 215, in main
    gist_batch = generate_batch(
  File "/projects/m000066/sujinesh/LatentWire/compressions/eval_gist.py", line 92, in generate_batch
    outputs = model.generate(**gen_kwargs)
  File "/projects/m000066/sujinesh/LatentWire/compressions/train_gist_faithful.py", line 264, in generate
    inputs_embeds[gist_mask] = self.gist_embedding
RuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and BFloat16 for the source.
