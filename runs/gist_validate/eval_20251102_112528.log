/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
GIST TOKENS EVALUATION
================================================================================
Checkpoint: runs/gist_validate
Test samples: 200
Max new tokens: 128
Device: cuda:0
================================================================================

Loading gist model from runs/gist_validate...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:16<00:48, 16.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:33<00:33, 16.85s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:51<00:17, 17.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:55<00:00, 11.99s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:55<00:00, 13.79s/it]
✓ Loaded GistLlama wrapper with 1 gist tokens
Loading Alpaca test data...
✓ Loaded 200 test samples

Evaluating samples (batch_size=16)...
Preparing prompts...
CRITICAL: Gist prompts have INSTRUCTION REMOVED for true compression!
Generating outputs...
Batches:   0%|          | 0/13 [00:00<?, ?it/s]/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Batches:   8%|▊         | 1/13 [00:13<02:45, 13.82s/it]Batches:  15%|█▌        | 2/13 [00:26<02:26, 13.28s/it]Batches:  23%|██▎       | 3/13 [00:40<02:13, 13.38s/it]Batches:  31%|███       | 4/13 [00:54<02:02, 13.67s/it]Batches:  38%|███▊      | 5/13 [01:07<01:47, 13.45s/it]Batches:  46%|████▌     | 6/13 [01:21<01:35, 13.60s/it]Batches:  54%|█████▍    | 7/13 [01:30<01:12, 12.15s/it]Batches:  62%|██████▏   | 8/13 [01:43<01:02, 12.52s/it]Batches:  69%|██████▉   | 9/13 [01:52<00:45, 11.43s/it]Batches:  77%|███████▋  | 10/13 [02:02<00:32, 10.92s/it]Batches:  85%|████████▍ | 11/13 [02:17<00:24, 12.29s/it]Batches:  92%|█████████▏| 12/13 [02:29<00:12, 12.19s/it]Batches: 100%|██████████| 13/13 [02:50<00:00, 14.74s/it]Batches: 100%|██████████| 13/13 [02:50<00:00, 13.12s/it]

Computing ROUGE scores...

================================================================================
RESULTS
================================================================================

Compression:
  Full text avg tokens: 33.3
  Gist avg tokens: 12.4
  Compression ratio: 2.69×

ROUGE Scores:

  Full Text (positive control):
    ROUGE-1: 0.1241
    ROUGE-2: 0.0352
    ROUGE-L: 0.0988

  Gist Tokens (our method):
    ROUGE-1: 0.0520
    ROUGE-2: 0.0114
    ROUGE-L: 0.0443

  Truncated Text (negative control):
    ROUGE-1: 0.1331
    ROUGE-2: 0.0386
    ROUGE-L: 0.1059

  Relative Performance (vs Full Text):
    Gist ROUGE-L: 44.8% of full text
    Truncated ROUGE-L: 107.2% of full text

  Evaluation time: 170.6s (0.85s per sample)
================================================================================

✓ Saved results to runs/gist_validate/eval_results.json
✓ Saved sample outputs to runs/gist_validate/sample_outputs.json
