W1102 10:56:56.774000 1079972 torch/distributed/run.py:793] 
W1102 10:56:56.774000 1079972 torch/distributed/run.py:793] *****************************************
W1102 10:56:56.774000 1079972 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1102 10:56:56.774000 1079972 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
FAITHFUL GIST TOKENS REPRODUCTION
================================================================================
Model: meta-llama/Meta-Llama-3.1-8B
Gist tokens: 1
Samples: 2,000 (QUICK TEST)
GPUs: 4
Batch size per GPU: 12
Gradient accumulation steps: 2
Effective batch size: 96 (12 × 4 GPUs × 2 accum)
Learning rate: 2e-05
Epochs: 2
Output: runs/gist_validate
================================================================================

Loading tokenizer...
✓ Added <GIST> token with ID: 128256
Loading model: meta-llama/Meta-Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2095.84it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.64it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6817.24it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:00,  2.02it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3465.65it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8244.33it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.35it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.40it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.43it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.52it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.30it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.94it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.98it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.02it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Initializing <GIST> embedding...
✓ Initialized <GIST> to vocab average
Freezing base model (only training gist token embedding)...
✓ Trainable params: 4,096 / 8,030,273,536 (0.0001%)
✓ Gist embedding: 4,096 parameters (trainable)

Loading Alpaca+ dataset...
✓ Loaded 2,000 samples
LR Scheduler: cosine
Warmup steps: 1 (3.0% of 42 total steps)

================================================================================
STARTING TRAINING
================================================================================

Epoch 1/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/42 [00:02<?, ?it/s, loss=0.2540, avg_loss=0.2540]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:25,  2.08s/it, loss=0.2540, avg_loss=0.2540]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:25,  2.08s/it, loss=0.2496, avg_loss=0.2518]Epoch 1/2:   5%|▍         | 2/42 [00:02<00:53,  1.35s/it, loss=0.2496, avg_loss=0.2518]Epoch 1/2:   5%|▍         | 2/42 [00:03<00:53,  1.35s/it, loss=0.2293, avg_loss=0.2443]Epoch 1/2:   7%|▋         | 3/42 [00:03<00:42,  1.09s/it, loss=0.2293, avg_loss=0.2443]Epoch 1/2:   7%|▋         | 3/42 [00:04<00:42,  1.09s/it, loss=0.2818, avg_loss=0.2537]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:35,  1.06it/s, loss=0.2818, avg_loss=0.2537]Epoch 1/2:  10%|▉         | 4/42 [00:05<00:35,  1.06it/s, loss=0.1992, avg_loss=0.2428]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.13it/s, loss=0.1992, avg_loss=0.2428]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.13it/s, loss=0.3371, avg_loss=0.2585]Epoch 1/2:  14%|█▍        | 6/42 [00:05<00:28,  1.27it/s, loss=0.3371, avg_loss=0.2585]Epoch 1/2:  14%|█▍        | 6/42 [00:06<00:28,  1.27it/s, loss=0.2523, avg_loss=0.2576]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.30it/s, loss=0.2523, avg_loss=0.2576]Epoch 1/2:  17%|█▋        | 7/42 [00:07<00:26,  1.30it/s, loss=0.4230, avg_loss=0.2783]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:24,  1.39it/s, loss=0.4230, avg_loss=0.2783]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:24,  1.39it/s, loss=0.1886, avg_loss=0.2683]Epoch 1/2:  21%|██▏       | 9/42 [00:07<00:23,  1.43it/s, loss=0.1886, avg_loss=0.2683]Epoch 1/2:  21%|██▏       | 9/42 [00:08<00:23,  1.43it/s, loss=0.2515, avg_loss=0.2667]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.41it/s, loss=0.2515, avg_loss=0.2667]Epoch 1/2:  24%|██▍       | 10/42 [00:09<00:22,  1.41it/s, loss=0.2152, avg_loss=0.2620]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:22,  1.39it/s, loss=0.2152, avg_loss=0.2620]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:22,  1.39it/s, loss=0.2004, avg_loss=0.2568]Epoch 1/2:  29%|██▊       | 12/42 [00:09<00:20,  1.45it/s, loss=0.2004, avg_loss=0.2568]Epoch 1/2:  29%|██▊       | 12/42 [00:10<00:20,  1.45it/s, loss=0.2778, avg_loss=0.2585]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.39it/s, loss=0.2778, avg_loss=0.2585]Epoch 1/2:  31%|███       | 13/42 [00:11<00:20,  1.39it/s, loss=0.2801, avg_loss=0.2600]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:18,  1.50it/s, loss=0.2801, avg_loss=0.2600]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:18,  1.50it/s, loss=0.2507, avg_loss=0.2594]Epoch 1/2:  36%|███▌      | 15/42 [00:11<00:18,  1.47it/s, loss=0.2507, avg_loss=0.2594]Epoch 1/2:  36%|███▌      | 15/42 [00:12<00:18,  1.47it/s, loss=0.1780, avg_loss=0.2543]Epoch 1/2:  38%|███▊      | 16/42 [00:12<00:18,  1.44it/s, loss=0.1780, avg_loss=0.2543]Epoch 1/2:  38%|███▊      | 16/42 [00:13<00:18,  1.44it/s, loss=0.2423, avg_loss=0.2536]Epoch 1/2:  40%|████      | 17/42 [00:13<00:18,  1.38it/s, loss=0.2423, avg_loss=0.2536]Epoch 1/2:  40%|████      | 17/42 [00:14<00:18,  1.38it/s, loss=0.1624, avg_loss=0.2485]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:17,  1.40it/s, loss=0.1624, avg_loss=0.2485]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:17,  1.40it/s, loss=0.2558, avg_loss=0.2489]Epoch 1/2:  45%|████▌     | 19/42 [00:14<00:16,  1.39it/s, loss=0.2558, avg_loss=0.2489]Epoch 1/2:  45%|████▌     | 19/42 [00:15<00:16,  1.39it/s, loss=0.1555, avg_loss=0.2442]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:15,  1.42it/s, loss=0.1555, avg_loss=0.2442]Epoch 1/2:  48%|████▊     | 20/42 [00:16<00:15,  1.42it/s, loss=0.2100, avg_loss=0.2426]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.45it/s, loss=0.2100, avg_loss=0.2426]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.45it/s, loss=0.1466, avg_loss=0.2382]Epoch 1/2:  52%|█████▏    | 22/42 [00:16<00:13,  1.45it/s, loss=0.1466, avg_loss=0.2382]Epoch 1/2:  52%|█████▏    | 22/42 [00:17<00:13,  1.45it/s, loss=0.2181, avg_loss=0.2374]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:13,  1.45it/s, loss=0.2181, avg_loss=0.2374]Epoch 1/2:  55%|█████▍    | 23/42 [00:18<00:13,  1.45it/s, loss=0.1969, avg_loss=0.2357]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.38it/s, loss=0.1969, avg_loss=0.2357]Epoch 1/2:  57%|█████▋    | 24/42 [00:19<00:12,  1.38it/s, loss=0.1658, avg_loss=0.2329]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:12,  1.37it/s, loss=0.1658, avg_loss=0.2329]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:12,  1.37it/s, loss=0.2013, avg_loss=0.2317]Epoch 1/2:  62%|██████▏   | 26/42 [00:19<00:11,  1.38it/s, loss=0.2013, avg_loss=0.2317]Epoch 1/2:  62%|██████▏   | 26/42 [00:20<00:11,  1.38it/s, loss=0.1420, avg_loss=0.2283]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.42it/s, loss=0.1420, avg_loss=0.2283]Epoch 1/2:  64%|██████▍   | 27/42 [00:21<00:10,  1.42it/s, loss=0.1967, avg_loss=0.2272]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:09,  1.41it/s, loss=0.1967, avg_loss=0.2272]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:09,  1.41it/s, loss=0.1880, avg_loss=0.2259]Epoch 1/2:  69%|██████▉   | 29/42 [00:21<00:08,  1.46it/s, loss=0.1880, avg_loss=0.2259]Epoch 1/2:  69%|██████▉   | 29/42 [00:22<00:08,  1.46it/s, loss=0.2613, avg_loss=0.2270]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.41it/s, loss=0.2613, avg_loss=0.2270]Epoch 1/2:  71%|███████▏  | 30/42 [00:23<00:08,  1.41it/s, loss=0.1753, avg_loss=0.2254]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.45it/s, loss=0.1753, avg_loss=0.2254]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.45it/s, loss=0.1458, avg_loss=0.2229]Epoch 1/2:  76%|███████▌  | 32/42 [00:23<00:06,  1.47it/s, loss=0.1458, avg_loss=0.2229]Epoch 1/2:  76%|███████▌  | 32/42 [00:24<00:06,  1.47it/s, loss=0.2044, avg_loss=0.2223]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:05,  1.50it/s, loss=0.2044, avg_loss=0.2223]Epoch 1/2:  79%|███████▊  | 33/42 [00:25<00:05,  1.50it/s, loss=0.1610, avg_loss=0.2205]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.50it/s, loss=0.1610, avg_loss=0.2205]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.50it/s, loss=0.2030, avg_loss=0.2200]Epoch 1/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.52it/s, loss=0.2030, avg_loss=0.2200]Epoch 1/2:  83%|████████▎ | 35/42 [00:26<00:04,  1.52it/s, loss=0.1777, avg_loss=0.2189]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:03,  1.54it/s, loss=0.1777, avg_loss=0.2189]Epoch 1/2:  86%|████████▌ | 36/42 [00:27<00:03,  1.54it/s, loss=0.1832, avg_loss=0.2179]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.53it/s, loss=0.1832, avg_loss=0.2179]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.53it/s, loss=0.1887, avg_loss=0.2171]Epoch 1/2:  90%|█████████ | 38/42 [00:27<00:02,  1.48it/s, loss=0.1887, avg_loss=0.2171]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.48it/s, loss=0.1486, avg_loss=0.2154]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:01,  1.51it/s, loss=0.1486, avg_loss=0.2154]Epoch 1/2:  93%|█████████▎| 39/42 [00:29<00:01,  1.51it/s, loss=0.2952, avg_loss=0.2174]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.48it/s, loss=0.2952, avg_loss=0.2174]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.48it/s, loss=0.2225, avg_loss=0.2175]Epoch 1/2:  98%|█████████▊| 41/42 [00:29<00:00,  1.49it/s, loss=0.2225, avg_loss=0.2175]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.49it/s, loss=0.1960, avg_loss=0.2170]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.58it/s, loss=0.1960, avg_loss=0.2170]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.38it/s, loss=0.1960, avg_loss=0.2170]

Epoch 1/2 complete - Avg loss: 0.2170
Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s, loss=0.1312, avg_loss=0.1312]Epoch 2/2:   2%|▏         | 1/42 [00:00<00:29,  1.39it/s, loss=0.1312, avg_loss=0.1312]Epoch 2/2:   2%|▏         | 1/42 [00:01<00:29,  1.39it/s, loss=0.1675, avg_loss=0.1493]Epoch 2/2:   5%|▍         | 2/42 [00:01<00:28,  1.38it/s, loss=0.1675, avg_loss=0.1493]Epoch 2/2:   5%|▍         | 2/42 [00:02<00:28,  1.38it/s, loss=0.2008, avg_loss=0.1665]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:27,  1.40it/s, loss=0.2008, avg_loss=0.1665]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:27,  1.40it/s, loss=0.1733, avg_loss=0.1682]Epoch 2/2:  10%|▉         | 4/42 [00:02<00:26,  1.44it/s, loss=0.1733, avg_loss=0.1682]Epoch 2/2:  10%|▉         | 4/42 [00:03<00:26,  1.44it/s, loss=0.1252, avg_loss=0.1596]Epoch 2/2:  12%|█▏        | 5/42 [00:03<00:25,  1.45it/s, loss=0.1252, avg_loss=0.1596]Epoch 2/2:  12%|█▏        | 5/42 [00:04<00:25,  1.45it/s, loss=0.1687, avg_loss=0.1611]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.48it/s, loss=0.1687, avg_loss=0.1611]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.48it/s, loss=0.1244, avg_loss=0.1559]Epoch 2/2:  17%|█▋        | 7/42 [00:04<00:22,  1.53it/s, loss=0.1244, avg_loss=0.1559]Epoch 2/2:  17%|█▋        | 7/42 [00:05<00:22,  1.53it/s, loss=0.1238, avg_loss=0.1519]Epoch 2/2:  19%|█▉        | 8/42 [00:05<00:22,  1.50it/s, loss=0.1238, avg_loss=0.1519]Epoch 2/2:  19%|█▉        | 8/42 [00:06<00:22,  1.50it/s, loss=0.1333, avg_loss=0.1498]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:22,  1.50it/s, loss=0.1333, avg_loss=0.1498]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:22,  1.50it/s, loss=0.1359, avg_loss=0.1484]Epoch 2/2:  24%|██▍       | 10/42 [00:06<00:20,  1.56it/s, loss=0.1359, avg_loss=0.1484]Epoch 2/2:  24%|██▍       | 10/42 [00:07<00:20,  1.56it/s, loss=0.1663, avg_loss=0.1500]Epoch 2/2:  26%|██▌       | 11/42 [00:07<00:20,  1.54it/s, loss=0.1663, avg_loss=0.1500]Epoch 2/2:  26%|██▌       | 11/42 [00:08<00:20,  1.54it/s, loss=0.1457, avg_loss=0.1497]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.50it/s, loss=0.1457, avg_loss=0.1497]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.50it/s, loss=0.1602, avg_loss=0.1505]Epoch 2/2:  31%|███       | 13/42 [00:08<00:20,  1.42it/s, loss=0.1602, avg_loss=0.1505]Epoch 2/2:  31%|███       | 13/42 [00:09<00:20,  1.42it/s, loss=0.1935, avg_loss=0.1536]Epoch 2/2:  33%|███▎      | 14/42 [00:09<00:19,  1.46it/s, loss=0.1935, avg_loss=0.1536]Epoch 2/2:  33%|███▎      | 14/42 [00:10<00:19,  1.46it/s, loss=0.1428, avg_loss=0.1528]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:18,  1.43it/s, loss=0.1428, avg_loss=0.1528]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:18,  1.43it/s, loss=0.2119, avg_loss=0.1565]Epoch 2/2:  38%|███▊      | 16/42 [00:10<00:18,  1.42it/s, loss=0.2119, avg_loss=0.1565]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.42it/s, loss=0.1730, avg_loss=0.1575]Epoch 2/2:  40%|████      | 17/42 [00:11<00:17,  1.47it/s, loss=0.1730, avg_loss=0.1575]Epoch 2/2:  40%|████      | 17/42 [00:12<00:17,  1.47it/s, loss=0.1623, avg_loss=0.1578]Epoch 2/2:  43%|████▎     | 18/42 [00:12<00:16,  1.43it/s, loss=0.1623, avg_loss=0.1578]Epoch 2/2:  43%|████▎     | 18/42 [00:13<00:16,  1.43it/s, loss=0.1374, avg_loss=0.1567]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.40it/s, loss=0.1374, avg_loss=0.1567]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.40it/s, loss=0.1319, avg_loss=0.1555]Epoch 2/2:  48%|████▊     | 20/42 [00:13<00:15,  1.42it/s, loss=0.1319, avg_loss=0.1555]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.42it/s, loss=0.1537, avg_loss=0.1554]Epoch 2/2:  50%|█████     | 21/42 [00:14<00:15,  1.37it/s, loss=0.1537, avg_loss=0.1554]Epoch 2/2:  50%|█████     | 21/42 [00:15<00:15,  1.37it/s, loss=0.1713, avg_loss=0.1561]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.38it/s, loss=0.1713, avg_loss=0.1561]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.38it/s, loss=0.1349, avg_loss=0.1552]Epoch 2/2:  55%|█████▍    | 23/42 [00:15<00:13,  1.40it/s, loss=0.1349, avg_loss=0.1552]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.40it/s, loss=0.1904, avg_loss=0.1566]Epoch 2/2:  57%|█████▋    | 24/42 [00:16<00:12,  1.47it/s, loss=0.1904, avg_loss=0.1566]Epoch 2/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.47it/s, loss=0.1184, avg_loss=0.1551]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.44it/s, loss=0.1184, avg_loss=0.1551]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.44it/s, loss=0.1810, avg_loss=0.1561]Epoch 2/2:  62%|██████▏   | 26/42 [00:17<00:11,  1.44it/s, loss=0.1810, avg_loss=0.1561]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.44it/s, loss=0.1986, avg_loss=0.1577]Epoch 2/2:  64%|██████▍   | 27/42 [00:18<00:10,  1.45it/s, loss=0.1986, avg_loss=0.1577]Epoch 2/2:  64%|██████▍   | 27/42 [00:19<00:10,  1.45it/s, loss=0.1379, avg_loss=0.1570]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.45it/s, loss=0.1379, avg_loss=0.1570]Epoch 2/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.45it/s, loss=0.1628, avg_loss=0.1572]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.47it/s, loss=0.1628, avg_loss=0.1572]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.47it/s, loss=0.1379, avg_loss=0.1565]Epoch 2/2:  71%|███████▏  | 30/42 [00:20<00:08,  1.44it/s, loss=0.1379, avg_loss=0.1565]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.44it/s, loss=0.1416, avg_loss=0.1561]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.51it/s, loss=0.1416, avg_loss=0.1561]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.51it/s, loss=0.1859, avg_loss=0.1570]Epoch 2/2:  76%|███████▌  | 32/42 [00:21<00:06,  1.56it/s, loss=0.1859, avg_loss=0.1570]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.56it/s, loss=0.1398, avg_loss=0.1565]Epoch 2/2:  79%|███████▊  | 33/42 [00:22<00:05,  1.52it/s, loss=0.1398, avg_loss=0.1565]Epoch 2/2:  79%|███████▊  | 33/42 [00:23<00:05,  1.52it/s, loss=0.1906, avg_loss=0.1575]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.48it/s, loss=0.1906, avg_loss=0.1575]Epoch 2/2:  81%|████████  | 34/42 [00:24<00:05,  1.48it/s, loss=0.1214, avg_loss=0.1564]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.45it/s, loss=0.1214, avg_loss=0.1564]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.45it/s, loss=0.1342, avg_loss=0.1558]Epoch 2/2:  86%|████████▌ | 36/42 [00:24<00:04,  1.45it/s, loss=0.1342, avg_loss=0.1558]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.45it/s, loss=0.1544, avg_loss=0.1558]Epoch 2/2:  88%|████████▊ | 37/42 [00:25<00:03,  1.44it/s, loss=0.1544, avg_loss=0.1558]Epoch 2/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.44it/s, loss=0.1195, avg_loss=0.1548]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.43it/s, loss=0.1195, avg_loss=0.1548]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.43it/s, loss=0.1594, avg_loss=0.1549]Epoch 2/2:  93%|█████████▎| 39/42 [00:26<00:02,  1.39it/s, loss=0.1594, avg_loss=0.1549]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.39it/s, loss=0.2258, avg_loss=0.1567]Epoch 2/2:  95%|█████████▌| 40/42 [00:27<00:01,  1.51it/s, loss=0.2258, avg_loss=0.1567]Epoch 2/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.51it/s, loss=0.1388, avg_loss=0.1563]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.52it/s, loss=0.1388, avg_loss=0.1563]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.52it/s, loss=0.1880, avg_loss=0.1570]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.68it/s, loss=0.1880, avg_loss=0.1570]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.47it/s, loss=0.1880, avg_loss=0.1570]

Epoch 2/2 complete - Avg loss: 0.1570

================================================================================
TRAINING COMPLETE
================================================================================
Total time: 0.98 minutes
Total steps: 42
Final avg loss: 0.1870

Saving checkpoint to runs/gist_validate...
✓ Saved base model to runs/gist_validate
✓ Saved gist embeddings to runs/gist_validate/gist_embedding.pt
✓ Saved to runs/gist_validate

Done!
