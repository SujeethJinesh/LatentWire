W1102 11:25:31.027000 1112714 torch/distributed/run.py:793] 
W1102 11:25:31.027000 1112714 torch/distributed/run.py:793] *****************************************
W1102 11:25:31.027000 1112714 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1102 11:25:31.027000 1112714 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
FAITHFUL GIST TOKENS REPRODUCTION
================================================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Gist tokens: 1
Samples: 2,000 (QUICK TEST)
GPUs: 4
Batch size per GPU: 12
Gradient accumulation steps: 2
Effective batch size: 96 (12 × 4 GPUs × 2 accum)
Learning rate: 2e-05
Epochs: 2
Output: runs/gist_validate
================================================================================

Loading tokenizer...
✓ Added <GIST> token with ID: 128256
Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2180.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.43it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.55it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.58it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.72it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5104.11it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1444.69it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7574.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.80it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.67it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.97it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.19it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.93it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.90it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.97it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.87it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  2.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.98it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.07it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.02it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Initializing <GIST> embedding...
✓ Initialized <GIST> to vocab average
Freezing base model (only training gist token embedding)...
✓ Trainable params: 4,096 / 8,030,273,536 (0.0001%)
✓ Gist embedding: 4,096 parameters (trainable)

Loading Alpaca+ dataset...
✓ Loaded 2,000 samples
LR Scheduler: cosine
Warmup steps: 1 (3.0% of 42 total steps)

================================================================================
STARTING TRAINING
================================================================================

Epoch 1/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/42 [00:01<?, ?it/s, loss=0.3377, avg_loss=0.3377]Epoch 1/2:   2%|▏         | 1/42 [00:01<01:18,  1.91s/it, loss=0.3377, avg_loss=0.3377]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:18,  1.91s/it, loss=0.4683, avg_loss=0.4030]Epoch 1/2:   5%|▍         | 2/42 [00:02<00:50,  1.27s/it, loss=0.4683, avg_loss=0.4030]Epoch 1/2:   5%|▍         | 2/42 [00:03<00:50,  1.27s/it, loss=0.4567, avg_loss=0.4209]Epoch 1/2:   7%|▋         | 3/42 [00:03<00:39,  1.01s/it, loss=0.4567, avg_loss=0.4209]Epoch 1/2:   7%|▋         | 3/42 [00:04<00:39,  1.01s/it, loss=0.4384, avg_loss=0.4253]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:34,  1.11it/s, loss=0.4384, avg_loss=0.4253]Epoch 1/2:  10%|▉         | 4/42 [00:05<00:34,  1.11it/s, loss=0.3272, avg_loss=0.4057]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.12it/s, loss=0.3272, avg_loss=0.4057]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.12it/s, loss=0.4088, avg_loss=0.4062]Epoch 1/2:  14%|█▍        | 6/42 [00:05<00:27,  1.32it/s, loss=0.4088, avg_loss=0.4062]Epoch 1/2:  14%|█▍        | 6/42 [00:06<00:27,  1.32it/s, loss=0.4094, avg_loss=0.4067]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=0.4094, avg_loss=0.4067]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=0.3895, avg_loss=0.4045]Epoch 1/2:  19%|█▉        | 8/42 [00:06<00:24,  1.41it/s, loss=0.3895, avg_loss=0.4045]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:24,  1.41it/s, loss=0.2835, avg_loss=0.3911]Epoch 1/2:  21%|██▏       | 9/42 [00:07<00:22,  1.45it/s, loss=0.2835, avg_loss=0.3911]Epoch 1/2:  21%|██▏       | 9/42 [00:08<00:22,  1.45it/s, loss=0.4114, avg_loss=0.3931]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.41it/s, loss=0.4114, avg_loss=0.3931]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.41it/s, loss=0.4830, avg_loss=0.4013]Epoch 1/2:  26%|██▌       | 11/42 [00:08<00:21,  1.44it/s, loss=0.4830, avg_loss=0.4013]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:21,  1.44it/s, loss=0.3686, avg_loss=0.3985]Epoch 1/2:  29%|██▊       | 12/42 [00:09<00:21,  1.43it/s, loss=0.3686, avg_loss=0.3985]Epoch 1/2:  29%|██▊       | 12/42 [00:10<00:21,  1.43it/s, loss=0.3639, avg_loss=0.3959]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.43it/s, loss=0.3639, avg_loss=0.3959]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.43it/s, loss=0.2488, avg_loss=0.3854]Epoch 1/2:  33%|███▎      | 14/42 [00:10<00:18,  1.51it/s, loss=0.2488, avg_loss=0.3854]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:18,  1.51it/s, loss=0.4267, avg_loss=0.3881]Epoch 1/2:  36%|███▌      | 15/42 [00:11<00:17,  1.50it/s, loss=0.4267, avg_loss=0.3881]Epoch 1/2:  36%|███▌      | 15/42 [00:12<00:17,  1.50it/s, loss=0.3173, avg_loss=0.3837]Epoch 1/2:  38%|███▊      | 16/42 [00:12<00:17,  1.46it/s, loss=0.3173, avg_loss=0.3837]Epoch 1/2:  38%|███▊      | 16/42 [00:13<00:17,  1.46it/s, loss=0.3115, avg_loss=0.3795]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.42it/s, loss=0.3115, avg_loss=0.3795]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.42it/s, loss=0.3385, avg_loss=0.3772]Epoch 1/2:  43%|████▎     | 18/42 [00:13<00:16,  1.43it/s, loss=0.3385, avg_loss=0.3772]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:16,  1.43it/s, loss=0.3939, avg_loss=0.3781]Epoch 1/2:  45%|████▌     | 19/42 [00:14<00:16,  1.39it/s, loss=0.3939, avg_loss=0.3781]Epoch 1/2:  45%|████▌     | 19/42 [00:15<00:16,  1.39it/s, loss=0.3381, avg_loss=0.3761]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:15,  1.42it/s, loss=0.3381, avg_loss=0.3761]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:15,  1.42it/s, loss=0.3303, avg_loss=0.3739]Epoch 1/2:  50%|█████     | 21/42 [00:15<00:14,  1.46it/s, loss=0.3303, avg_loss=0.3739]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.46it/s, loss=0.3040, avg_loss=0.3707]Epoch 1/2:  52%|█████▏    | 22/42 [00:16<00:13,  1.47it/s, loss=0.3040, avg_loss=0.3707]Epoch 1/2:  52%|█████▏    | 22/42 [00:17<00:13,  1.47it/s, loss=0.2995, avg_loss=0.3676]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:12,  1.46it/s, loss=0.2995, avg_loss=0.3676]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:12,  1.46it/s, loss=0.3505, avg_loss=0.3669]Epoch 1/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.40it/s, loss=0.3505, avg_loss=0.3669]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.40it/s, loss=0.2802, avg_loss=0.3634]Epoch 1/2:  60%|█████▉    | 25/42 [00:18<00:12,  1.41it/s, loss=0.2802, avg_loss=0.3634]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:12,  1.41it/s, loss=0.3111, avg_loss=0.3614]Epoch 1/2:  62%|██████▏   | 26/42 [00:19<00:11,  1.42it/s, loss=0.3111, avg_loss=0.3614]Epoch 1/2:  62%|██████▏   | 26/42 [00:20<00:11,  1.42it/s, loss=0.2446, avg_loss=0.3571]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.45it/s, loss=0.2446, avg_loss=0.3571]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.45it/s, loss=0.3218, avg_loss=0.3558]Epoch 1/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.44it/s, loss=0.3218, avg_loss=0.3558]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:09,  1.44it/s, loss=0.2949, avg_loss=0.3537]Epoch 1/2:  69%|██████▉   | 29/42 [00:21<00:08,  1.49it/s, loss=0.2949, avg_loss=0.3537]Epoch 1/2:  69%|██████▉   | 29/42 [00:22<00:08,  1.49it/s, loss=0.3508, avg_loss=0.3536]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.44it/s, loss=0.3508, avg_loss=0.3536]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.44it/s, loss=0.3256, avg_loss=0.3527]Epoch 1/2:  74%|███████▍  | 31/42 [00:22<00:07,  1.53it/s, loss=0.3256, avg_loss=0.3527]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.53it/s, loss=0.2530, avg_loss=0.3496]Epoch 1/2:  76%|███████▌  | 32/42 [00:23<00:06,  1.46it/s, loss=0.2530, avg_loss=0.3496]Epoch 1/2:  76%|███████▌  | 32/42 [00:24<00:06,  1.46it/s, loss=0.3922, avg_loss=0.3509]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:06,  1.49it/s, loss=0.3922, avg_loss=0.3509]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:06,  1.49it/s, loss=0.2721, avg_loss=0.3486]Epoch 1/2:  81%|████████  | 34/42 [00:24<00:05,  1.47it/s, loss=0.2721, avg_loss=0.3486]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.47it/s, loss=0.3543, avg_loss=0.3487]Epoch 1/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.51it/s, loss=0.3543, avg_loss=0.3487]Epoch 1/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.51it/s, loss=0.3304, avg_loss=0.3482]Epoch 1/2:  86%|████████▌ | 36/42 [00:25<00:03,  1.56it/s, loss=0.3304, avg_loss=0.3482]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:03,  1.56it/s, loss=0.2837, avg_loss=0.3465]Epoch 1/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.57it/s, loss=0.2837, avg_loss=0.3465]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.57it/s, loss=0.2468, avg_loss=0.3439]Epoch 1/2:  90%|█████████ | 38/42 [00:27<00:02,  1.44it/s, loss=0.2468, avg_loss=0.3439]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.44it/s, loss=0.2374, avg_loss=0.3411]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.48it/s, loss=0.2374, avg_loss=0.3411]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.48it/s, loss=0.3655, avg_loss=0.3417]Epoch 1/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.47it/s, loss=0.3655, avg_loss=0.3417]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.47it/s, loss=0.3557, avg_loss=0.3421]Epoch 1/2:  98%|█████████▊| 41/42 [00:29<00:00,  1.48it/s, loss=0.3557, avg_loss=0.3421]Epoch 1/2:  98%|█████████▊| 41/42 [00:29<00:00,  1.48it/s, loss=0.3330, avg_loss=0.3419]Epoch 1/2: 100%|██████████| 42/42 [00:29<00:00,  1.60it/s, loss=0.3330, avg_loss=0.3419]Epoch 1/2: 100%|██████████| 42/42 [00:29<00:00,  1.40it/s, loss=0.3330, avg_loss=0.3419]

Epoch 1/2 complete - Avg loss: 0.3419
Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s, loss=0.2527, avg_loss=0.2527]Epoch 2/2:   2%|▏         | 1/42 [00:00<00:29,  1.37it/s, loss=0.2527, avg_loss=0.2527]Epoch 2/2:   2%|▏         | 1/42 [00:01<00:29,  1.37it/s, loss=0.3157, avg_loss=0.2842]Epoch 2/2:   5%|▍         | 2/42 [00:01<00:29,  1.34it/s, loss=0.3157, avg_loss=0.2842]Epoch 2/2:   5%|▍         | 2/42 [00:02<00:29,  1.34it/s, loss=0.4137, avg_loss=0.3274]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:28,  1.38it/s, loss=0.4137, avg_loss=0.3274]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:28,  1.38it/s, loss=0.4051, avg_loss=0.3468]Epoch 2/2:  10%|▉         | 4/42 [00:02<00:26,  1.42it/s, loss=0.4051, avg_loss=0.3468]Epoch 2/2:  10%|▉         | 4/42 [00:03<00:26,  1.42it/s, loss=0.2764, avg_loss=0.3327]Epoch 2/2:  12%|█▏        | 5/42 [00:03<00:25,  1.47it/s, loss=0.2764, avg_loss=0.3327]Epoch 2/2:  12%|█▏        | 5/42 [00:04<00:25,  1.47it/s, loss=0.2622, avg_loss=0.3210]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:23,  1.50it/s, loss=0.2622, avg_loss=0.3210]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:23,  1.50it/s, loss=0.2599, avg_loss=0.3123]Epoch 2/2:  17%|█▋        | 7/42 [00:04<00:22,  1.54it/s, loss=0.2599, avg_loss=0.3123]Epoch 2/2:  17%|█▋        | 7/42 [00:05<00:22,  1.54it/s, loss=0.2611, avg_loss=0.3059]Epoch 2/2:  19%|█▉        | 8/42 [00:05<00:21,  1.59it/s, loss=0.2611, avg_loss=0.3059]Epoch 2/2:  19%|█▉        | 8/42 [00:06<00:21,  1.59it/s, loss=0.2533, avg_loss=0.3000]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:23,  1.42it/s, loss=0.2533, avg_loss=0.3000]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:23,  1.42it/s, loss=0.2446, avg_loss=0.2945]Epoch 2/2:  24%|██▍       | 10/42 [00:06<00:21,  1.48it/s, loss=0.2446, avg_loss=0.2945]Epoch 2/2:  24%|██▍       | 10/42 [00:07<00:21,  1.48it/s, loss=0.2768, avg_loss=0.2929]Epoch 2/2:  26%|██▌       | 11/42 [00:07<00:20,  1.51it/s, loss=0.2768, avg_loss=0.2929]Epoch 2/2:  26%|██▌       | 11/42 [00:08<00:20,  1.51it/s, loss=0.2402, avg_loss=0.2885]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.50it/s, loss=0.2402, avg_loss=0.2885]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.50it/s, loss=0.2236, avg_loss=0.2835]Epoch 2/2:  31%|███       | 13/42 [00:08<00:20,  1.45it/s, loss=0.2236, avg_loss=0.2835]Epoch 2/2:  31%|███       | 13/42 [00:09<00:20,  1.45it/s, loss=0.3395, avg_loss=0.2875]Epoch 2/2:  33%|███▎      | 14/42 [00:09<00:18,  1.47it/s, loss=0.3395, avg_loss=0.2875]Epoch 2/2:  33%|███▎      | 14/42 [00:10<00:18,  1.47it/s, loss=0.2608, avg_loss=0.2857]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:18,  1.43it/s, loss=0.2608, avg_loss=0.2857]Epoch 2/2:  36%|███▌      | 15/42 [00:11<00:18,  1.43it/s, loss=0.3534, avg_loss=0.2899]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.3534, avg_loss=0.2899]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.3262, avg_loss=0.2921]Epoch 2/2:  40%|████      | 17/42 [00:11<00:17,  1.46it/s, loss=0.3262, avg_loss=0.2921]Epoch 2/2:  40%|████      | 17/42 [00:12<00:17,  1.46it/s, loss=0.2383, avg_loss=0.2891]Epoch 2/2:  43%|████▎     | 18/42 [00:12<00:16,  1.45it/s, loss=0.2383, avg_loss=0.2891]Epoch 2/2:  43%|████▎     | 18/42 [00:13<00:16,  1.45it/s, loss=0.3239, avg_loss=0.2909]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.41it/s, loss=0.3239, avg_loss=0.2909]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.41it/s, loss=0.2302, avg_loss=0.2879]Epoch 2/2:  48%|████▊     | 20/42 [00:13<00:15,  1.43it/s, loss=0.2302, avg_loss=0.2879]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.43it/s, loss=0.2902, avg_loss=0.2880]Epoch 2/2:  50%|█████     | 21/42 [00:14<00:15,  1.37it/s, loss=0.2902, avg_loss=0.2880]Epoch 2/2:  50%|█████     | 21/42 [00:15<00:15,  1.37it/s, loss=0.1951, avg_loss=0.2838]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.34it/s, loss=0.1951, avg_loss=0.2838]Epoch 2/2:  52%|█████▏    | 22/42 [00:16<00:14,  1.34it/s, loss=0.2448, avg_loss=0.2821]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.37it/s, loss=0.2448, avg_loss=0.2821]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.37it/s, loss=0.3236, avg_loss=0.2838]Epoch 2/2:  57%|█████▋    | 24/42 [00:16<00:12,  1.43it/s, loss=0.3236, avg_loss=0.2838]Epoch 2/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.43it/s, loss=0.2994, avg_loss=0.2844]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.43it/s, loss=0.2994, avg_loss=0.2844]Epoch 2/2:  60%|█████▉    | 25/42 [00:18<00:11,  1.43it/s, loss=0.2670, avg_loss=0.2838]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.44it/s, loss=0.2670, avg_loss=0.2838]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.44it/s, loss=0.2500, avg_loss=0.2825]Epoch 2/2:  64%|██████▍   | 27/42 [00:18<00:10,  1.48it/s, loss=0.2500, avg_loss=0.2825]Epoch 2/2:  64%|██████▍   | 27/42 [00:19<00:10,  1.48it/s, loss=0.2639, avg_loss=0.2819]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.46it/s, loss=0.2639, avg_loss=0.2819]Epoch 2/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.46it/s, loss=0.2635, avg_loss=0.2812]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.47it/s, loss=0.2635, avg_loss=0.2812]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.47it/s, loss=0.3695, avg_loss=0.2842]Epoch 2/2:  71%|███████▏  | 30/42 [00:20<00:08,  1.45it/s, loss=0.3695, avg_loss=0.2842]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.45it/s, loss=0.2377, avg_loss=0.2827]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.53it/s, loss=0.2377, avg_loss=0.2827]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.53it/s, loss=0.2500, avg_loss=0.2816]Epoch 2/2:  76%|███████▌  | 32/42 [00:21<00:06,  1.63it/s, loss=0.2500, avg_loss=0.2816]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.63it/s, loss=0.2605, avg_loss=0.2810]Epoch 2/2:  79%|███████▊  | 33/42 [00:22<00:05,  1.52it/s, loss=0.2605, avg_loss=0.2810]Epoch 2/2:  79%|███████▊  | 33/42 [00:23<00:05,  1.52it/s, loss=0.2973, avg_loss=0.2815]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.44it/s, loss=0.2973, avg_loss=0.2815]Epoch 2/2:  81%|████████  | 34/42 [00:24<00:05,  1.44it/s, loss=0.2850, avg_loss=0.2816]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.45it/s, loss=0.2850, avg_loss=0.2816]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.45it/s, loss=0.2968, avg_loss=0.2820]Epoch 2/2:  86%|████████▌ | 36/42 [00:24<00:04,  1.45it/s, loss=0.2968, avg_loss=0.2820]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.45it/s, loss=0.2470, avg_loss=0.2811]Epoch 2/2:  88%|████████▊ | 37/42 [00:25<00:03,  1.42it/s, loss=0.2470, avg_loss=0.2811]Epoch 2/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.42it/s, loss=0.2195, avg_loss=0.2794]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.45it/s, loss=0.2195, avg_loss=0.2794]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.45it/s, loss=0.2690, avg_loss=0.2792]Epoch 2/2:  93%|█████████▎| 39/42 [00:26<00:02,  1.39it/s, loss=0.2690, avg_loss=0.2792]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.39it/s, loss=0.2767, avg_loss=0.2791]Epoch 2/2:  95%|█████████▌| 40/42 [00:27<00:01,  1.49it/s, loss=0.2767, avg_loss=0.2791]Epoch 2/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.49it/s, loss=0.2602, avg_loss=0.2786]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.52it/s, loss=0.2602, avg_loss=0.2786]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.52it/s, loss=0.3444, avg_loss=0.2802]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.69it/s, loss=0.3444, avg_loss=0.2802]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.47it/s, loss=0.3444, avg_loss=0.2802]

Epoch 2/2 complete - Avg loss: 0.2802

================================================================================
TRAINING COMPLETE
================================================================================
Total time: 0.98 minutes
Total steps: 42
Final avg loss: 0.3110

Saving checkpoint to runs/gist_validate...
✓ Saved base model to runs/gist_validate
✓ Saved gist embeddings to runs/gist_validate/gist_embedding.pt
✓ Saved to runs/gist_validate

Done!
