W1102 10:21:03.041000 1039073 torch/distributed/run.py:793] 
W1102 10:21:03.041000 1039073 torch/distributed/run.py:793] *****************************************
W1102 10:21:03.041000 1039073 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1102 10:21:03.041000 1039073 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
FAITHFUL GIST TOKENS REPRODUCTION
================================================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Gist tokens: 1
Samples: 2,000 (QUICK TEST)
GPUs: 4
Batch size per GPU: 12
Gradient accumulation steps: 2
Effective batch size: 96 (12 × 4 GPUs × 2 accum)
Learning rate: 0.0001
Epochs: 2
Output: runs/gist_validate
================================================================================

Loading tokenizer...
✓ Added <GIST> token with ID: 128256
Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2072.28it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.89it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7231.56it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7479.81it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7792.48it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.75it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.59it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.64it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.62it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.48it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.84it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.87it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.82it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.96it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.97it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.98it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.11it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.08it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.10it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Initializing <GIST> embedding...
✓ Initialized <GIST> to vocab average
Freezing base model (only training gist token embedding)...
✓ Trainable params: 4,096 / 8,030,273,536 (0.0001%)
✓ Gist embedding: 4,096 parameters (trainable)

Loading Alpaca+ dataset...
✓ Loaded 2,000 samples

================================================================================
STARTING TRAINING
================================================================================

Epoch 1/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/42 [00:01<?, ?it/s, loss=0.3377, avg_loss=0.3377]Epoch 1/2:   2%|▏         | 1/42 [00:01<01:21,  1.98s/it, loss=0.3377, avg_loss=0.3377]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:21,  1.98s/it, loss=0.4683, avg_loss=0.4030]Epoch 1/2:   5%|▍         | 2/42 [00:02<00:52,  1.31s/it, loss=0.4683, avg_loss=0.4030]Epoch 1/2:   5%|▍         | 2/42 [00:03<00:52,  1.31s/it, loss=0.3344, avg_loss=0.3801]Epoch 1/2:   7%|▋         | 3/42 [00:03<00:40,  1.03s/it, loss=0.3344, avg_loss=0.3801]Epoch 1/2:   7%|▋         | 3/42 [00:04<00:40,  1.03s/it, loss=0.3646, avg_loss=0.3762]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:35,  1.09it/s, loss=0.3646, avg_loss=0.3762]Epoch 1/2:  10%|▉         | 4/42 [00:05<00:35,  1.09it/s, loss=0.2638, avg_loss=0.3538]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.15it/s, loss=0.2638, avg_loss=0.3538]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:32,  1.15it/s, loss=0.3318, avg_loss=0.3501]Epoch 1/2:  14%|█▍        | 6/42 [00:05<00:27,  1.31it/s, loss=0.3318, avg_loss=0.3501]Epoch 1/2:  14%|█▍        | 6/42 [00:06<00:27,  1.31it/s, loss=0.2804, avg_loss=0.3401]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=0.2804, avg_loss=0.3401]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=0.3046, avg_loss=0.3357]Epoch 1/2:  19%|█▉        | 8/42 [00:06<00:24,  1.40it/s, loss=0.3046, avg_loss=0.3357]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:24,  1.40it/s, loss=0.2317, avg_loss=0.3242]Epoch 1/2:  21%|██▏       | 9/42 [00:07<00:22,  1.44it/s, loss=0.2317, avg_loss=0.3242]Epoch 1/2:  21%|██▏       | 9/42 [00:08<00:22,  1.44it/s, loss=0.3229, avg_loss=0.3240]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.42it/s, loss=0.3229, avg_loss=0.3240]Epoch 1/2:  24%|██▍       | 10/42 [00:09<00:22,  1.42it/s, loss=0.3496, avg_loss=0.3264]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:22,  1.38it/s, loss=0.3496, avg_loss=0.3264]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:22,  1.38it/s, loss=0.2464, avg_loss=0.3197]Epoch 1/2:  29%|██▊       | 12/42 [00:09<00:21,  1.40it/s, loss=0.2464, avg_loss=0.3197]Epoch 1/2:  29%|██▊       | 12/42 [00:10<00:21,  1.40it/s, loss=0.2561, avg_loss=0.3148]Epoch 1/2:  31%|███       | 13/42 [00:10<00:21,  1.38it/s, loss=0.2561, avg_loss=0.3148]Epoch 1/2:  31%|███       | 13/42 [00:11<00:21,  1.38it/s, loss=0.1900, avg_loss=0.3059]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:18,  1.51it/s, loss=0.1900, avg_loss=0.3059]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:18,  1.51it/s, loss=0.2171, avg_loss=0.3000]Epoch 1/2:  36%|███▌      | 15/42 [00:11<00:18,  1.47it/s, loss=0.2171, avg_loss=0.3000]Epoch 1/2:  36%|███▌      | 15/42 [00:12<00:18,  1.47it/s, loss=0.2190, avg_loss=0.2949]Epoch 1/2:  38%|███▊      | 16/42 [00:12<00:17,  1.46it/s, loss=0.2190, avg_loss=0.2949]Epoch 1/2:  38%|███▊      | 16/42 [00:13<00:17,  1.46it/s, loss=0.2117, avg_loss=0.2900]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.45it/s, loss=0.2117, avg_loss=0.2900]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.45it/s, loss=0.1995, avg_loss=0.2850]Epoch 1/2:  43%|████▎     | 18/42 [00:13<00:16,  1.45it/s, loss=0.1995, avg_loss=0.2850]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:16,  1.45it/s, loss=0.2062, avg_loss=0.2808]Epoch 1/2:  45%|████▌     | 19/42 [00:14<00:16,  1.42it/s, loss=0.2062, avg_loss=0.2808]Epoch 1/2:  45%|████▌     | 19/42 [00:15<00:16,  1.42it/s, loss=0.1952, avg_loss=0.2766]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:15,  1.44it/s, loss=0.1952, avg_loss=0.2766]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:15,  1.44it/s, loss=0.1525, avg_loss=0.2707]Epoch 1/2:  50%|█████     | 21/42 [00:15<00:14,  1.49it/s, loss=0.1525, avg_loss=0.2707]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.49it/s, loss=0.1652, avg_loss=0.2659]Epoch 1/2:  52%|█████▏    | 22/42 [00:16<00:13,  1.48it/s, loss=0.1652, avg_loss=0.2659]Epoch 1/2:  52%|█████▏    | 22/42 [00:17<00:13,  1.48it/s, loss=0.1469, avg_loss=0.2607]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:13,  1.45it/s, loss=0.1469, avg_loss=0.2607]Epoch 1/2:  55%|█████▍    | 23/42 [00:18<00:13,  1.45it/s, loss=0.1740, avg_loss=0.2571]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.42it/s, loss=0.1740, avg_loss=0.2571]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.42it/s, loss=0.1685, avg_loss=0.2535]Epoch 1/2:  60%|█████▉    | 25/42 [00:18<00:11,  1.43it/s, loss=0.1685, avg_loss=0.2535]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:11,  1.43it/s, loss=0.1642, avg_loss=0.2501]Epoch 1/2:  62%|██████▏   | 26/42 [00:19<00:11,  1.42it/s, loss=0.1642, avg_loss=0.2501]Epoch 1/2:  62%|██████▏   | 26/42 [00:20<00:11,  1.42it/s, loss=0.1028, avg_loss=0.2446]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.43it/s, loss=0.1028, avg_loss=0.2446]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.43it/s, loss=0.1634, avg_loss=0.2417]Epoch 1/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.41it/s, loss=0.1634, avg_loss=0.2417]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:09,  1.41it/s, loss=0.1647, avg_loss=0.2391]Epoch 1/2:  69%|██████▉   | 29/42 [00:21<00:09,  1.42it/s, loss=0.1647, avg_loss=0.2391]Epoch 1/2:  69%|██████▉   | 29/42 [00:22<00:09,  1.42it/s, loss=0.1769, avg_loss=0.2370]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.38it/s, loss=0.1769, avg_loss=0.2370]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.38it/s, loss=0.1471, avg_loss=0.2341]Epoch 1/2:  74%|███████▍  | 31/42 [00:22<00:07,  1.48it/s, loss=0.1471, avg_loss=0.2341]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.48it/s, loss=0.1318, avg_loss=0.2309]Epoch 1/2:  76%|███████▌  | 32/42 [00:23<00:06,  1.49it/s, loss=0.1318, avg_loss=0.2309]Epoch 1/2:  76%|███████▌  | 32/42 [00:24<00:06,  1.49it/s, loss=0.1528, avg_loss=0.2285]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:05,  1.51it/s, loss=0.1528, avg_loss=0.2285]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:05,  1.51it/s, loss=0.1167, avg_loss=0.2253]Epoch 1/2:  81%|████████  | 34/42 [00:24<00:05,  1.50it/s, loss=0.1167, avg_loss=0.2253]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.50it/s, loss=0.1415, avg_loss=0.2229]Epoch 1/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.50it/s, loss=0.1415, avg_loss=0.2229]Epoch 1/2:  83%|████████▎ | 35/42 [00:26<00:04,  1.50it/s, loss=0.1115, avg_loss=0.2198]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:03,  1.55it/s, loss=0.1115, avg_loss=0.2198]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:03,  1.55it/s, loss=0.1092, avg_loss=0.2168]Epoch 1/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.54it/s, loss=0.1092, avg_loss=0.2168]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.54it/s, loss=0.0989, avg_loss=0.2137]Epoch 1/2:  90%|█████████ | 38/42 [00:27<00:02,  1.47it/s, loss=0.0989, avg_loss=0.2137]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.47it/s, loss=0.0730, avg_loss=0.2101]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.48it/s, loss=0.0730, avg_loss=0.2101]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.48it/s, loss=0.1536, avg_loss=0.2087]Epoch 1/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.49it/s, loss=0.1536, avg_loss=0.2087]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.49it/s, loss=0.1191, avg_loss=0.2065]Epoch 1/2:  98%|█████████▊| 41/42 [00:29<00:00,  1.48it/s, loss=0.1191, avg_loss=0.2065]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.48it/s, loss=0.1078, avg_loss=0.2041]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.60it/s, loss=0.1078, avg_loss=0.2041]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.40it/s, loss=0.1078, avg_loss=0.2041]

Epoch 1/2 complete - Avg loss: 0.2041
Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s, loss=0.0851, avg_loss=0.0851]Epoch 2/2:   2%|▏         | 1/42 [00:00<00:31,  1.32it/s, loss=0.0851, avg_loss=0.0851]Epoch 2/2:   2%|▏         | 1/42 [00:01<00:31,  1.32it/s, loss=0.1058, avg_loss=0.0954]Epoch 2/2:   5%|▍         | 2/42 [00:01<00:28,  1.40it/s, loss=0.1058, avg_loss=0.0954]Epoch 2/2:   5%|▍         | 2/42 [00:02<00:28,  1.40it/s, loss=0.2141, avg_loss=0.1350]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:27,  1.40it/s, loss=0.2141, avg_loss=0.1350]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:27,  1.40it/s, loss=0.1262, avg_loss=0.1328]Epoch 2/2:  10%|▉         | 4/42 [00:02<00:26,  1.43it/s, loss=0.1262, avg_loss=0.1328]Epoch 2/2:  10%|▉         | 4/42 [00:03<00:26,  1.43it/s, loss=0.0777, avg_loss=0.1218]Epoch 2/2:  12%|█▏        | 5/42 [00:03<00:25,  1.47it/s, loss=0.0777, avg_loss=0.1218]Epoch 2/2:  12%|█▏        | 5/42 [00:04<00:25,  1.47it/s, loss=0.0817, avg_loss=0.1151]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.50it/s, loss=0.0817, avg_loss=0.1151]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.50it/s, loss=0.0729, avg_loss=0.1091]Epoch 2/2:  17%|█▋        | 7/42 [00:04<00:23,  1.52it/s, loss=0.0729, avg_loss=0.1091]Epoch 2/2:  17%|█▋        | 7/42 [00:05<00:23,  1.52it/s, loss=0.0607, avg_loss=0.1030]Epoch 2/2:  19%|█▉        | 8/42 [00:05<00:22,  1.50it/s, loss=0.0607, avg_loss=0.1030]Epoch 2/2:  19%|█▉        | 8/42 [00:06<00:22,  1.50it/s, loss=0.0627, avg_loss=0.0985]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:21,  1.51it/s, loss=0.0627, avg_loss=0.0985]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:21,  1.51it/s, loss=0.0921, avg_loss=0.0979]Epoch 2/2:  24%|██▍       | 10/42 [00:06<00:20,  1.53it/s, loss=0.0921, avg_loss=0.0979]Epoch 2/2:  24%|██▍       | 10/42 [00:07<00:20,  1.53it/s, loss=0.0754, avg_loss=0.0959]Epoch 2/2:  26%|██▌       | 11/42 [00:07<00:20,  1.54it/s, loss=0.0754, avg_loss=0.0959]Epoch 2/2:  26%|██▌       | 11/42 [00:08<00:20,  1.54it/s, loss=0.2728, avg_loss=0.1106]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.49it/s, loss=0.2728, avg_loss=0.1106]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.49it/s, loss=0.0666, avg_loss=0.1072]Epoch 2/2:  31%|███       | 13/42 [00:08<00:20,  1.43it/s, loss=0.0666, avg_loss=0.1072]Epoch 2/2:  31%|███       | 13/42 [00:09<00:20,  1.43it/s, loss=0.1084, avg_loss=0.1073]Epoch 2/2:  33%|███▎      | 14/42 [00:09<00:18,  1.48it/s, loss=0.1084, avg_loss=0.1073]Epoch 2/2:  33%|███▎      | 14/42 [00:10<00:18,  1.48it/s, loss=0.0599, avg_loss=0.1041]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:19,  1.36it/s, loss=0.0599, avg_loss=0.1041]Epoch 2/2:  36%|███▌      | 15/42 [00:11<00:19,  1.36it/s, loss=0.1638, avg_loss=0.1079]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.1638, avg_loss=0.1079]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.0609, avg_loss=0.1051]Epoch 2/2:  40%|████      | 17/42 [00:11<00:17,  1.43it/s, loss=0.0609, avg_loss=0.1051]Epoch 2/2:  40%|████      | 17/42 [00:12<00:17,  1.43it/s, loss=0.0570, avg_loss=0.1024]Epoch 2/2:  43%|████▎     | 18/42 [00:12<00:16,  1.43it/s, loss=0.0570, avg_loss=0.1024]Epoch 2/2:  43%|████▎     | 18/42 [00:13<00:16,  1.43it/s, loss=0.0649, avg_loss=0.1005]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.43it/s, loss=0.0649, avg_loss=0.1005]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.43it/s, loss=0.0588, avg_loss=0.0984]Epoch 2/2:  48%|████▊     | 20/42 [00:13<00:15,  1.41it/s, loss=0.0588, avg_loss=0.0984]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.41it/s, loss=0.0619, avg_loss=0.0966]Epoch 2/2:  50%|█████     | 21/42 [00:14<00:15,  1.35it/s, loss=0.0619, avg_loss=0.0966]Epoch 2/2:  50%|█████     | 21/42 [00:15<00:15,  1.35it/s, loss=0.0498, avg_loss=0.0945]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.38it/s, loss=0.0498, avg_loss=0.0945]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.38it/s, loss=0.0406, avg_loss=0.0922]Epoch 2/2:  55%|█████▍    | 23/42 [00:15<00:13,  1.42it/s, loss=0.0406, avg_loss=0.0922]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.42it/s, loss=0.0618, avg_loss=0.0909]Epoch 2/2:  57%|█████▋    | 24/42 [00:16<00:12,  1.48it/s, loss=0.0618, avg_loss=0.0909]Epoch 2/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.48it/s, loss=0.0550, avg_loss=0.0895]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.47it/s, loss=0.0550, avg_loss=0.0895]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.47it/s, loss=0.0547, avg_loss=0.0881]Epoch 2/2:  62%|██████▏   | 26/42 [00:17<00:10,  1.49it/s, loss=0.0547, avg_loss=0.0881]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:10,  1.49it/s, loss=0.0456, avg_loss=0.0866]Epoch 2/2:  64%|██████▍   | 27/42 [00:18<00:09,  1.52it/s, loss=0.0456, avg_loss=0.0866]Epoch 2/2:  64%|██████▍   | 27/42 [00:19<00:09,  1.52it/s, loss=0.0449, avg_loss=0.0851]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.50it/s, loss=0.0449, avg_loss=0.0851]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.50it/s, loss=0.0632, avg_loss=0.0843]Epoch 2/2:  69%|██████▉   | 29/42 [00:19<00:08,  1.48it/s, loss=0.0632, avg_loss=0.0843]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.48it/s, loss=0.0657, avg_loss=0.0837]Epoch 2/2:  71%|███████▏  | 30/42 [00:20<00:08,  1.48it/s, loss=0.0657, avg_loss=0.0837]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.48it/s, loss=0.0343, avg_loss=0.0821]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.55it/s, loss=0.0343, avg_loss=0.0821]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.55it/s, loss=0.0463, avg_loss=0.0810]Epoch 2/2:  76%|███████▌  | 32/42 [00:21<00:06,  1.65it/s, loss=0.0463, avg_loss=0.0810]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.65it/s, loss=0.0382, avg_loss=0.0797]Epoch 2/2:  79%|███████▊  | 33/42 [00:22<00:05,  1.57it/s, loss=0.0382, avg_loss=0.0797]Epoch 2/2:  79%|███████▊  | 33/42 [00:23<00:05,  1.57it/s, loss=0.0365, avg_loss=0.0784]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.47it/s, loss=0.0365, avg_loss=0.0784]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.47it/s, loss=0.0478, avg_loss=0.0775]Epoch 2/2:  83%|████████▎ | 35/42 [00:23<00:05,  1.39it/s, loss=0.0478, avg_loss=0.0775]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:05,  1.39it/s, loss=0.0427, avg_loss=0.0766]Epoch 2/2:  86%|████████▌ | 36/42 [00:24<00:04,  1.41it/s, loss=0.0427, avg_loss=0.0766]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.41it/s, loss=0.0476, avg_loss=0.0758]Epoch 2/2:  88%|████████▊ | 37/42 [00:25<00:03,  1.41it/s, loss=0.0476, avg_loss=0.0758]Epoch 2/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.41it/s, loss=0.0390, avg_loss=0.0748]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.44it/s, loss=0.0390, avg_loss=0.0748]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.44it/s, loss=0.0380, avg_loss=0.0739]Epoch 2/2:  93%|█████████▎| 39/42 [00:26<00:02,  1.40it/s, loss=0.0380, avg_loss=0.0739]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.40it/s, loss=0.0462, avg_loss=0.0732]Epoch 2/2:  95%|█████████▌| 40/42 [00:27<00:01,  1.50it/s, loss=0.0462, avg_loss=0.0732]Epoch 2/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.50it/s, loss=0.0324, avg_loss=0.0722]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.51it/s, loss=0.0324, avg_loss=0.0722]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.51it/s, loss=0.1239, avg_loss=0.0734]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.66it/s, loss=0.1239, avg_loss=0.0734]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.47it/s, loss=0.1239, avg_loss=0.0734]

Epoch 2/2 complete - Avg loss: 0.0734

================================================================================
TRAINING COMPLETE
================================================================================
Total time: 0.98 minutes
Total steps: 42
Final avg loss: 0.1388

Saving checkpoint to runs/gist_validate...
✓ Saved base model to runs/gist_validate
✓ Saved gist embeddings to runs/gist_validate/gist_embedding.pt
✓ Saved to runs/gist_validate

Done!
