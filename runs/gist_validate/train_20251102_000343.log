W1102 00:03:44.982000 2831383 torch/distributed/run.py:793] 
W1102 00:03:44.982000 2831383 torch/distributed/run.py:793] *****************************************
W1102 00:03:44.982000 2831383 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1102 00:03:44.982000 2831383 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
FAITHFUL GIST TOKENS REPRODUCTION
================================================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Gist tokens: 1
Samples: 2,000 (QUICK TEST)
GPUs: 4
Batch size per GPU: 12
Gradient accumulation steps: 2
Effective batch size: 96 (12 × 4 GPUs × 2 accum)
Learning rate: 0.0001
Epochs: 2
Output: runs/gist_validate
================================================================================

Loading tokenizer...
✓ Added <GIST> token with ID: 128256
Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1046.22it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.54it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.72it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.76it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1540.18it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2065.40it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6533.18it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.69it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.63it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.78it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.72it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.71it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.78it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.69it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.78it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.81it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.74it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Initializing <GIST> embedding...
✓ Initialized <GIST> to vocab average
Freezing base model (only training gist token embedding)...
✓ Trainable params: 4,096 / 8,030,273,536 (0.0001%)
✓ Gist embedding: 4,096 parameters (trainable)

Loading Alpaca+ dataset...
✓ Loaded 2,000 samples

================================================================================
STARTING TRAINING
================================================================================

Epoch 1/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/42 [00:01<?, ?it/s, loss=0.9781, avg_loss=0.9781]Epoch 1/2:   2%|▏         | 1/42 [00:01<01:18,  1.92s/it, loss=0.9781, avg_loss=0.9781]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:18,  1.92s/it, loss=1.1467, avg_loss=1.0624]Epoch 1/2:   5%|▍         | 2/42 [00:02<00:51,  1.28s/it, loss=1.1467, avg_loss=1.0624]Epoch 1/2:   5%|▍         | 2/42 [00:03<00:51,  1.28s/it, loss=1.1695, avg_loss=1.0981]Epoch 1/2:   7%|▋         | 3/42 [00:03<00:40,  1.03s/it, loss=1.1695, avg_loss=1.0981]Epoch 1/2:   7%|▋         | 3/42 [00:04<00:40,  1.03s/it, loss=1.1154, avg_loss=1.1024]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:34,  1.10it/s, loss=1.1154, avg_loss=1.1024]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:34,  1.10it/s, loss=1.0262, avg_loss=1.0872]Epoch 1/2:  12%|█▏        | 5/42 [00:04<00:31,  1.17it/s, loss=1.0262, avg_loss=1.0872]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:31,  1.17it/s, loss=0.9461, avg_loss=1.0637]Epoch 1/2:  14%|█▍        | 6/42 [00:05<00:26,  1.34it/s, loss=0.9461, avg_loss=1.0637]Epoch 1/2:  14%|█▍        | 6/42 [00:06<00:26,  1.34it/s, loss=1.2660, avg_loss=1.0926]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=1.2660, avg_loss=1.0926]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.34it/s, loss=1.0990, avg_loss=1.0934]Epoch 1/2:  19%|█▉        | 8/42 [00:06<00:23,  1.42it/s, loss=1.0990, avg_loss=1.0934]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:23,  1.42it/s, loss=1.0402, avg_loss=1.0875]Epoch 1/2:  21%|██▏       | 9/42 [00:07<00:22,  1.45it/s, loss=1.0402, avg_loss=1.0875]Epoch 1/2:  21%|██▏       | 9/42 [00:08<00:22,  1.45it/s, loss=1.3336, avg_loss=1.1121]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.40it/s, loss=1.3336, avg_loss=1.1121]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.40it/s, loss=1.3061, avg_loss=1.1297]Epoch 1/2:  26%|██▌       | 11/42 [00:08<00:22,  1.41it/s, loss=1.3061, avg_loss=1.1297]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:22,  1.41it/s, loss=1.0491, avg_loss=1.1230]Epoch 1/2:  29%|██▊       | 12/42 [00:09<00:20,  1.45it/s, loss=1.0491, avg_loss=1.1230]Epoch 1/2:  29%|██▊       | 12/42 [00:10<00:20,  1.45it/s, loss=1.0556, avg_loss=1.1178]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.41it/s, loss=1.0556, avg_loss=1.1178]Epoch 1/2:  31%|███       | 13/42 [00:11<00:20,  1.41it/s, loss=0.9136, avg_loss=1.1032]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:19,  1.45it/s, loss=0.9136, avg_loss=1.1032]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:19,  1.45it/s, loss=0.9551, avg_loss=1.0934]Epoch 1/2:  36%|███▌      | 15/42 [00:11<00:18,  1.45it/s, loss=0.9551, avg_loss=1.0934]Epoch 1/2:  36%|███▌      | 15/42 [00:12<00:18,  1.45it/s, loss=1.3633, avg_loss=1.1102]Epoch 1/2:  38%|███▊      | 16/42 [00:12<00:18,  1.43it/s, loss=1.3633, avg_loss=1.1102]Epoch 1/2:  38%|███▊      | 16/42 [00:13<00:18,  1.43it/s, loss=1.0495, avg_loss=1.1067]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.42it/s, loss=1.0495, avg_loss=1.1067]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.42it/s, loss=1.1032, avg_loss=1.1065]Epoch 1/2:  43%|████▎     | 18/42 [00:13<00:17,  1.41it/s, loss=1.1032, avg_loss=1.1065]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:17,  1.41it/s, loss=0.9462, avg_loss=1.0980]Epoch 1/2:  45%|████▌     | 19/42 [00:14<00:16,  1.36it/s, loss=0.9462, avg_loss=1.0980]Epoch 1/2:  45%|████▌     | 19/42 [00:15<00:16,  1.36it/s, loss=1.0339, avg_loss=1.0948]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:16,  1.35it/s, loss=1.0339, avg_loss=1.0948]Epoch 1/2:  48%|████▊     | 20/42 [00:16<00:16,  1.35it/s, loss=1.1717, avg_loss=1.0985]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.41it/s, loss=1.1717, avg_loss=1.0985]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:14,  1.41it/s, loss=0.9449, avg_loss=1.0915]Epoch 1/2:  52%|█████▏    | 22/42 [00:16<00:14,  1.43it/s, loss=0.9449, avg_loss=1.0915]Epoch 1/2:  52%|█████▏    | 22/42 [00:17<00:14,  1.43it/s, loss=1.2685, avg_loss=1.0992]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:13,  1.43it/s, loss=1.2685, avg_loss=1.0992]Epoch 1/2:  55%|█████▍    | 23/42 [00:18<00:13,  1.43it/s, loss=1.1735, avg_loss=1.1023]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:13,  1.35it/s, loss=1.1735, avg_loss=1.1023]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:13,  1.35it/s, loss=1.0273, avg_loss=1.0993]Epoch 1/2:  60%|█████▉    | 25/42 [00:18<00:12,  1.38it/s, loss=1.0273, avg_loss=1.0993]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:12,  1.38it/s, loss=1.1265, avg_loss=1.1003]Epoch 1/2:  62%|██████▏   | 26/42 [00:19<00:11,  1.35it/s, loss=1.1265, avg_loss=1.1003]Epoch 1/2:  62%|██████▏   | 26/42 [00:20<00:11,  1.35it/s, loss=1.2465, avg_loss=1.1058]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:10,  1.38it/s, loss=1.2465, avg_loss=1.1058]Epoch 1/2:  64%|██████▍   | 27/42 [00:21<00:10,  1.38it/s, loss=1.2873, avg_loss=1.1122]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:10,  1.37it/s, loss=1.2873, avg_loss=1.1122]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:10,  1.37it/s, loss=1.2087, avg_loss=1.1156]Epoch 1/2:  69%|██████▉   | 29/42 [00:21<00:09,  1.43it/s, loss=1.2087, avg_loss=1.1156]Epoch 1/2:  69%|██████▉   | 29/42 [00:22<00:09,  1.43it/s, loss=1.1029, avg_loss=1.1151]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.41it/s, loss=1.1029, avg_loss=1.1151]Epoch 1/2:  71%|███████▏  | 30/42 [00:23<00:08,  1.41it/s, loss=1.2224, avg_loss=1.1186]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.44it/s, loss=1.2224, avg_loss=1.1186]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.44it/s, loss=1.1512, avg_loss=1.1196]Epoch 1/2:  76%|███████▌  | 32/42 [00:23<00:07,  1.41it/s, loss=1.1512, avg_loss=1.1196]Epoch 1/2:  76%|███████▌  | 32/42 [00:24<00:07,  1.41it/s, loss=1.0559, avg_loss=1.1177]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:06,  1.43it/s, loss=1.0559, avg_loss=1.1177]Epoch 1/2:  79%|███████▊  | 33/42 [00:25<00:06,  1.43it/s, loss=1.0687, avg_loss=1.1162]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.41it/s, loss=1.0687, avg_loss=1.1162]Epoch 1/2:  81%|████████  | 34/42 [00:26<00:05,  1.41it/s, loss=1.0177, avg_loss=1.1134]Epoch 1/2:  83%|████████▎ | 35/42 [00:26<00:04,  1.41it/s, loss=1.0177, avg_loss=1.1134]Epoch 1/2:  83%|████████▎ | 35/42 [00:26<00:04,  1.41it/s, loss=1.4306, avg_loss=1.1222]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:04,  1.49it/s, loss=1.4306, avg_loss=1.1222]Epoch 1/2:  86%|████████▌ | 36/42 [00:27<00:04,  1.49it/s, loss=1.1932, avg_loss=1.1242]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.51it/s, loss=1.1932, avg_loss=1.1242]Epoch 1/2:  88%|████████▊ | 37/42 [00:28<00:03,  1.51it/s, loss=0.9689, avg_loss=1.1201]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.40it/s, loss=0.9689, avg_loss=1.1201]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.40it/s, loss=1.3435, avg_loss=1.1258]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.45it/s, loss=1.3435, avg_loss=1.1258]Epoch 1/2:  93%|█████████▎| 39/42 [00:29<00:02,  1.45it/s, loss=1.0178, avg_loss=1.1231]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.44it/s, loss=1.0178, avg_loss=1.1231]Epoch 1/2:  95%|█████████▌| 40/42 [00:30<00:01,  1.44it/s, loss=1.0718, avg_loss=1.1219]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.44it/s, loss=1.0718, avg_loss=1.1219]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.44it/s, loss=1.0524, avg_loss=1.1202]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.56it/s, loss=1.0524, avg_loss=1.1202]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.37it/s, loss=1.0524, avg_loss=1.1202]

Epoch 1/2 complete - Avg loss: 1.1202
Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s, loss=1.1309, avg_loss=1.1309]Epoch 2/2:   2%|▏         | 1/42 [00:00<00:30,  1.33it/s, loss=1.1309, avg_loss=1.1309]Epoch 2/2:   2%|▏         | 1/42 [00:01<00:30,  1.33it/s, loss=1.1585, avg_loss=1.1447]Epoch 2/2:   5%|▍         | 2/42 [00:01<00:29,  1.35it/s, loss=1.1585, avg_loss=1.1447]Epoch 2/2:   5%|▍         | 2/42 [00:02<00:29,  1.35it/s, loss=0.9945, avg_loss=1.0947]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:29,  1.34it/s, loss=0.9945, avg_loss=1.0947]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:29,  1.34it/s, loss=0.8879, avg_loss=1.0430]Epoch 2/2:  10%|▉         | 4/42 [00:02<00:27,  1.39it/s, loss=0.8879, avg_loss=1.0430]Epoch 2/2:  10%|▉         | 4/42 [00:03<00:27,  1.39it/s, loss=1.0449, avg_loss=1.0433]Epoch 2/2:  12%|█▏        | 5/42 [00:03<00:25,  1.42it/s, loss=1.0449, avg_loss=1.0433]Epoch 2/2:  12%|█▏        | 5/42 [00:04<00:25,  1.42it/s, loss=0.9925, avg_loss=1.0349]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.46it/s, loss=0.9925, avg_loss=1.0349]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:24,  1.46it/s, loss=1.2967, avg_loss=1.0723]Epoch 2/2:  17%|█▋        | 7/42 [00:04<00:23,  1.51it/s, loss=1.2967, avg_loss=1.0723]Epoch 2/2:  17%|█▋        | 7/42 [00:05<00:23,  1.51it/s, loss=1.1399, avg_loss=1.0807]Epoch 2/2:  19%|█▉        | 8/42 [00:05<00:22,  1.52it/s, loss=1.1399, avg_loss=1.0807]Epoch 2/2:  19%|█▉        | 8/42 [00:06<00:22,  1.52it/s, loss=1.1408, avg_loss=1.0874]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:21,  1.52it/s, loss=1.1408, avg_loss=1.0874]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:21,  1.52it/s, loss=1.0447, avg_loss=1.0831]Epoch 2/2:  24%|██▍       | 10/42 [00:06<00:20,  1.56it/s, loss=1.0447, avg_loss=1.0831]Epoch 2/2:  24%|██▍       | 10/42 [00:07<00:20,  1.56it/s, loss=1.1161, avg_loss=1.0861]Epoch 2/2:  26%|██▌       | 11/42 [00:07<00:20,  1.53it/s, loss=1.1161, avg_loss=1.0861]Epoch 2/2:  26%|██▌       | 11/42 [00:08<00:20,  1.53it/s, loss=0.9077, avg_loss=1.0713]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:19,  1.51it/s, loss=0.9077, avg_loss=1.0713]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:19,  1.51it/s, loss=1.0387, avg_loss=1.0688]Epoch 2/2:  31%|███       | 13/42 [00:08<00:19,  1.47it/s, loss=1.0387, avg_loss=1.0688]Epoch 2/2:  31%|███       | 13/42 [00:09<00:19,  1.47it/s, loss=1.1784, avg_loss=1.0766]Epoch 2/2:  33%|███▎      | 14/42 [00:09<00:18,  1.50it/s, loss=1.1784, avg_loss=1.0766]Epoch 2/2:  33%|███▎      | 14/42 [00:10<00:18,  1.50it/s, loss=1.1935, avg_loss=1.0844]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:18,  1.42it/s, loss=1.1935, avg_loss=1.0844]Epoch 2/2:  36%|███▌      | 15/42 [00:11<00:18,  1.42it/s, loss=1.0210, avg_loss=1.0804]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:19,  1.36it/s, loss=1.0210, avg_loss=1.0804]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:19,  1.36it/s, loss=1.2357, avg_loss=1.0895]Epoch 2/2:  40%|████      | 17/42 [00:11<00:17,  1.41it/s, loss=1.2357, avg_loss=1.0895]Epoch 2/2:  40%|████      | 17/42 [00:12<00:17,  1.41it/s, loss=1.2069, avg_loss=1.0961]Epoch 2/2:  43%|████▎     | 18/42 [00:12<00:17,  1.38it/s, loss=1.2069, avg_loss=1.0961]Epoch 2/2:  43%|████▎     | 18/42 [00:13<00:17,  1.38it/s, loss=1.0959, avg_loss=1.0961]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.40it/s, loss=1.0959, avg_loss=1.0961]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.40it/s, loss=1.0981, avg_loss=1.0962]Epoch 2/2:  48%|████▊     | 20/42 [00:13<00:15,  1.39it/s, loss=1.0981, avg_loss=1.0962]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.39it/s, loss=1.0938, avg_loss=1.0960]Epoch 2/2:  50%|█████     | 21/42 [00:14<00:15,  1.34it/s, loss=1.0938, avg_loss=1.0960]Epoch 2/2:  50%|█████     | 21/42 [00:15<00:15,  1.34it/s, loss=1.3249, avg_loss=1.1064]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.35it/s, loss=1.3249, avg_loss=1.1064]Epoch 2/2:  52%|█████▏    | 22/42 [00:16<00:14,  1.35it/s, loss=1.0531, avg_loss=1.1041]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.38it/s, loss=1.0531, avg_loss=1.1041]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.38it/s, loss=1.1128, avg_loss=1.1045]Epoch 2/2:  57%|█████▋    | 24/42 [00:16<00:12,  1.46it/s, loss=1.1128, avg_loss=1.1045]Epoch 2/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.46it/s, loss=1.0671, avg_loss=1.1030]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.45it/s, loss=1.0671, avg_loss=1.1030]Epoch 2/2:  60%|█████▉    | 25/42 [00:18<00:11,  1.45it/s, loss=0.7423, avg_loss=1.0891]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.41it/s, loss=0.7423, avg_loss=1.0891]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.41it/s, loss=1.2597, avg_loss=1.0954]Epoch 2/2:  64%|██████▍   | 27/42 [00:18<00:09,  1.50it/s, loss=1.2597, avg_loss=1.0954]Epoch 2/2:  64%|██████▍   | 27/42 [00:19<00:09,  1.50it/s, loss=1.0977, avg_loss=1.0955]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.47it/s, loss=1.0977, avg_loss=1.0955]Epoch 2/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.47it/s, loss=1.0065, avg_loss=1.0925]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.46it/s, loss=1.0065, avg_loss=1.0925]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:08,  1.46it/s, loss=0.9732, avg_loss=1.0885]Epoch 2/2:  71%|███████▏  | 30/42 [00:20<00:08,  1.44it/s, loss=0.9732, avg_loss=1.0885]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.44it/s, loss=1.1110, avg_loss=1.0892]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.51it/s, loss=1.1110, avg_loss=1.0892]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.51it/s, loss=1.2280, avg_loss=1.0935]Epoch 2/2:  76%|███████▌  | 32/42 [00:21<00:06,  1.61it/s, loss=1.2280, avg_loss=1.0935]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.61it/s, loss=1.4352, avg_loss=1.1039]Epoch 2/2:  79%|███████▊  | 33/42 [00:22<00:05,  1.52it/s, loss=1.4352, avg_loss=1.1039]Epoch 2/2:  79%|███████▊  | 33/42 [00:23<00:05,  1.52it/s, loss=1.0160, avg_loss=1.1013]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.46it/s, loss=1.0160, avg_loss=1.1013]Epoch 2/2:  81%|████████  | 34/42 [00:24<00:05,  1.46it/s, loss=1.0761, avg_loss=1.1006]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.41it/s, loss=1.0761, avg_loss=1.1006]Epoch 2/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.41it/s, loss=1.0346, avg_loss=1.0988]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.38it/s, loss=1.0346, avg_loss=1.0988]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.38it/s, loss=0.9658, avg_loss=1.0952]Epoch 2/2:  88%|████████▊ | 37/42 [00:25<00:03,  1.37it/s, loss=0.9658, avg_loss=1.0952]Epoch 2/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.37it/s, loss=1.0598, avg_loss=1.0942]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.39it/s, loss=1.0598, avg_loss=1.0942]Epoch 2/2:  90%|█████████ | 38/42 [00:27<00:02,  1.39it/s, loss=1.1428, avg_loss=1.0955]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.37it/s, loss=1.1428, avg_loss=1.0955]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.37it/s, loss=0.9499, avg_loss=1.0918]Epoch 2/2:  95%|█████████▌| 40/42 [00:27<00:01,  1.46it/s, loss=0.9499, avg_loss=1.0918]Epoch 2/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.46it/s, loss=0.9707, avg_loss=1.0889]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.46it/s, loss=0.9707, avg_loss=1.0889]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.46it/s, loss=0.8502, avg_loss=1.0832]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.64it/s, loss=0.8502, avg_loss=1.0832]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.45it/s, loss=0.8502, avg_loss=1.0832]

Epoch 2/2 complete - Avg loss: 1.0832

================================================================================
TRAINING COMPLETE
================================================================================
Total time: 0.99 minutes
Total steps: 42
Final avg loss: 1.1017

Saving checkpoint to runs/gist_validate...
✓ Saved base model to runs/gist_validate
✓ Saved gist embeddings to runs/gist_validate/gist_embedding.pt
✓ Saved to runs/gist_validate

Done!
