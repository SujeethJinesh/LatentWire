W1102 11:40:06.824000 1129323 torch/distributed/run.py:793] 
W1102 11:40:06.824000 1129323 torch/distributed/run.py:793] *****************************************
W1102 11:40:06.824000 1129323 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1102 11:40:06.824000 1129323 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(

================================================================================
FAITHFUL GIST TOKENS REPRODUCTION
================================================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Gist tokens: 1
Samples: 2,000 (QUICK TEST)
GPUs: 4
Batch size per GPU: 12
Gradient accumulation steps: 2
Effective batch size: 96 (12 × 4 GPUs × 2 accum)
Learning rate: 2e-05
Epochs: 2
Output: runs/gist_validate
================================================================================

Loading tokenizer...
✓ Added <GIST> token with ID: 128256
Loading model: meta-llama/Meta-Llama-3.1-8B-Instruct...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2362.99it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.27it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.28it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.62it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.77it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2403.61it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2974.16it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8551.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.41it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.39it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.38it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.76it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.74it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.73it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.87it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.78it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.75it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.71it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.70it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.66it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.67it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.64it/s]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Initializing <GIST> embedding...
✓ Initialized <GIST> to vocab average
Freezing base model (only training gist token embedding)...
✓ Trainable params: 4,096 / 8,030,273,536 (0.0001%)
✓ Gist embedding: 4,096 parameters (trainable)

Loading Alpaca+ dataset...
✓ Loaded 2,000 samples
LR Scheduler: cosine
Warmup steps: 1 (3.0% of 42 total steps)

================================================================================
STARTING TRAINING
================================================================================

Epoch 1/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 1/2:   0%|          | 0/42 [00:01<?, ?it/s, loss=0.4099, avg_loss=0.4099]Epoch 1/2:   2%|▏         | 1/42 [00:01<01:09,  1.69s/it, loss=0.4099, avg_loss=0.4099]Epoch 1/2:   2%|▏         | 1/42 [00:02<01:09,  1.69s/it, loss=0.6411, avg_loss=0.5255]Epoch 1/2:   5%|▍         | 2/42 [00:02<00:49,  1.24s/it, loss=0.6411, avg_loss=0.5255]Epoch 1/2:   5%|▍         | 2/42 [00:03<00:49,  1.24s/it, loss=0.6245, avg_loss=0.5585]Epoch 1/2:   7%|▋         | 3/42 [00:03<00:39,  1.00s/it, loss=0.6245, avg_loss=0.5585]Epoch 1/2:   7%|▋         | 3/42 [00:04<00:39,  1.00s/it, loss=0.5479, avg_loss=0.5558]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:33,  1.14it/s, loss=0.5479, avg_loss=0.5558]Epoch 1/2:  10%|▉         | 4/42 [00:04<00:33,  1.14it/s, loss=0.4465, avg_loss=0.5340]Epoch 1/2:  12%|█▏        | 5/42 [00:04<00:30,  1.19it/s, loss=0.4465, avg_loss=0.5340]Epoch 1/2:  12%|█▏        | 5/42 [00:05<00:30,  1.19it/s, loss=0.5070, avg_loss=0.5295]Epoch 1/2:  14%|█▍        | 6/42 [00:05<00:27,  1.33it/s, loss=0.5070, avg_loss=0.5295]Epoch 1/2:  14%|█▍        | 6/42 [00:06<00:27,  1.33it/s, loss=0.6357, avg_loss=0.5447]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.32it/s, loss=0.6357, avg_loss=0.5447]Epoch 1/2:  17%|█▋        | 7/42 [00:06<00:26,  1.32it/s, loss=0.4532, avg_loss=0.5332]Epoch 1/2:  19%|█▉        | 8/42 [00:06<00:24,  1.39it/s, loss=0.4532, avg_loss=0.5332]Epoch 1/2:  19%|█▉        | 8/42 [00:07<00:24,  1.39it/s, loss=0.3604, avg_loss=0.5140]Epoch 1/2:  21%|██▏       | 9/42 [00:07<00:23,  1.41it/s, loss=0.3604, avg_loss=0.5140]Epoch 1/2:  21%|██▏       | 9/42 [00:08<00:23,  1.41it/s, loss=0.4869, avg_loss=0.5113]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.41it/s, loss=0.4869, avg_loss=0.5113]Epoch 1/2:  24%|██▍       | 10/42 [00:08<00:22,  1.41it/s, loss=0.5828, avg_loss=0.5178]Epoch 1/2:  26%|██▌       | 11/42 [00:08<00:21,  1.41it/s, loss=0.5828, avg_loss=0.5178]Epoch 1/2:  26%|██▌       | 11/42 [00:09<00:21,  1.41it/s, loss=0.4610, avg_loss=0.5131]Epoch 1/2:  29%|██▊       | 12/42 [00:09<00:20,  1.45it/s, loss=0.4610, avg_loss=0.5131]Epoch 1/2:  29%|██▊       | 12/42 [00:10<00:20,  1.45it/s, loss=0.4220, avg_loss=0.5061]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.42it/s, loss=0.4220, avg_loss=0.5061]Epoch 1/2:  31%|███       | 13/42 [00:10<00:20,  1.42it/s, loss=0.3288, avg_loss=0.4934]Epoch 1/2:  33%|███▎      | 14/42 [00:10<00:19,  1.43it/s, loss=0.3288, avg_loss=0.4934]Epoch 1/2:  33%|███▎      | 14/42 [00:11<00:19,  1.43it/s, loss=0.5065, avg_loss=0.4943]Epoch 1/2:  36%|███▌      | 15/42 [00:11<00:19,  1.41it/s, loss=0.5065, avg_loss=0.4943]Epoch 1/2:  36%|███▌      | 15/42 [00:12<00:19,  1.41it/s, loss=0.4119, avg_loss=0.4891]Epoch 1/2:  38%|███▊      | 16/42 [00:12<00:18,  1.41it/s, loss=0.4119, avg_loss=0.4891]Epoch 1/2:  38%|███▊      | 16/42 [00:13<00:18,  1.41it/s, loss=0.3906, avg_loss=0.4833]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.41it/s, loss=0.3906, avg_loss=0.4833]Epoch 1/2:  40%|████      | 17/42 [00:13<00:17,  1.41it/s, loss=0.4317, avg_loss=0.4805]Epoch 1/2:  43%|████▎     | 18/42 [00:13<00:16,  1.41it/s, loss=0.4317, avg_loss=0.4805]Epoch 1/2:  43%|████▎     | 18/42 [00:14<00:16,  1.41it/s, loss=0.4917, avg_loss=0.4810]Epoch 1/2:  45%|████▌     | 19/42 [00:14<00:16,  1.37it/s, loss=0.4917, avg_loss=0.4810]Epoch 1/2:  45%|████▌     | 19/42 [00:15<00:16,  1.37it/s, loss=0.4486, avg_loss=0.4794]Epoch 1/2:  48%|████▊     | 20/42 [00:15<00:16,  1.34it/s, loss=0.4486, avg_loss=0.4794]Epoch 1/2:  48%|████▊     | 20/42 [00:16<00:16,  1.34it/s, loss=0.3795, avg_loss=0.4747]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:15,  1.40it/s, loss=0.3795, avg_loss=0.4747]Epoch 1/2:  50%|█████     | 21/42 [00:16<00:15,  1.40it/s, loss=0.3594, avg_loss=0.4694]Epoch 1/2:  52%|█████▏    | 22/42 [00:16<00:14,  1.40it/s, loss=0.3594, avg_loss=0.4694]Epoch 1/2:  52%|█████▏    | 22/42 [00:17<00:14,  1.40it/s, loss=0.3508, avg_loss=0.4643]Epoch 1/2:  55%|█████▍    | 23/42 [00:17<00:13,  1.43it/s, loss=0.3508, avg_loss=0.4643]Epoch 1/2:  55%|█████▍    | 23/42 [00:18<00:13,  1.43it/s, loss=0.4574, avg_loss=0.4640]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.40it/s, loss=0.4574, avg_loss=0.4640]Epoch 1/2:  57%|█████▋    | 24/42 [00:18<00:12,  1.40it/s, loss=0.3190, avg_loss=0.4582]Epoch 1/2:  60%|█████▉    | 25/42 [00:18<00:12,  1.36it/s, loss=0.3190, avg_loss=0.4582]Epoch 1/2:  60%|█████▉    | 25/42 [00:19<00:12,  1.36it/s, loss=0.3452, avg_loss=0.4538]Epoch 1/2:  62%|██████▏   | 26/42 [00:19<00:11,  1.33it/s, loss=0.3452, avg_loss=0.4538]Epoch 1/2:  62%|██████▏   | 26/42 [00:20<00:11,  1.33it/s, loss=0.2955, avg_loss=0.4480]Epoch 1/2:  64%|██████▍   | 27/42 [00:20<00:11,  1.35it/s, loss=0.2955, avg_loss=0.4480]Epoch 1/2:  64%|██████▍   | 27/42 [00:21<00:11,  1.35it/s, loss=0.3861, avg_loss=0.4458]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:10,  1.37it/s, loss=0.3861, avg_loss=0.4458]Epoch 1/2:  67%|██████▋   | 28/42 [00:21<00:10,  1.37it/s, loss=0.4097, avg_loss=0.4445]Epoch 1/2:  69%|██████▉   | 29/42 [00:21<00:09,  1.42it/s, loss=0.4097, avg_loss=0.4445]Epoch 1/2:  69%|██████▉   | 29/42 [00:22<00:09,  1.42it/s, loss=0.4344, avg_loss=0.4442]Epoch 1/2:  71%|███████▏  | 30/42 [00:22<00:08,  1.36it/s, loss=0.4344, avg_loss=0.4442]Epoch 1/2:  71%|███████▏  | 30/42 [00:23<00:08,  1.36it/s, loss=0.3590, avg_loss=0.4414]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.45it/s, loss=0.3590, avg_loss=0.4414]Epoch 1/2:  74%|███████▍  | 31/42 [00:23<00:07,  1.45it/s, loss=0.2920, avg_loss=0.4368]Epoch 1/2:  76%|███████▌  | 32/42 [00:23<00:06,  1.43it/s, loss=0.2920, avg_loss=0.4368]Epoch 1/2:  76%|███████▌  | 32/42 [00:24<00:06,  1.43it/s, loss=0.4732, avg_loss=0.4379]Epoch 1/2:  79%|███████▊  | 33/42 [00:24<00:06,  1.47it/s, loss=0.4732, avg_loss=0.4379]Epoch 1/2:  79%|███████▊  | 33/42 [00:25<00:06,  1.47it/s, loss=0.3039, avg_loss=0.4339]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.40it/s, loss=0.3039, avg_loss=0.4339]Epoch 1/2:  81%|████████  | 34/42 [00:25<00:05,  1.40it/s, loss=0.4473, avg_loss=0.4343]Epoch 1/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.42it/s, loss=0.4473, avg_loss=0.4343]Epoch 1/2:  83%|████████▎ | 35/42 [00:26<00:04,  1.42it/s, loss=0.3260, avg_loss=0.4313]Epoch 1/2:  86%|████████▌ | 36/42 [00:26<00:04,  1.49it/s, loss=0.3260, avg_loss=0.4313]Epoch 1/2:  86%|████████▌ | 36/42 [00:27<00:04,  1.49it/s, loss=0.3711, avg_loss=0.4297]Epoch 1/2:  88%|████████▊ | 37/42 [00:27<00:03,  1.46it/s, loss=0.3711, avg_loss=0.4297]Epoch 1/2:  88%|████████▊ | 37/42 [00:28<00:03,  1.46it/s, loss=0.2839, avg_loss=0.4258]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.40it/s, loss=0.2839, avg_loss=0.4258]Epoch 1/2:  90%|█████████ | 38/42 [00:28<00:02,  1.40it/s, loss=0.2497, avg_loss=0.4213]Epoch 1/2:  93%|█████████▎| 39/42 [00:28<00:02,  1.43it/s, loss=0.2497, avg_loss=0.4213]Epoch 1/2:  93%|█████████▎| 39/42 [00:29<00:02,  1.43it/s, loss=0.4570, avg_loss=0.4222]Epoch 1/2:  95%|█████████▌| 40/42 [00:29<00:01,  1.43it/s, loss=0.4570, avg_loss=0.4222]Epoch 1/2:  95%|█████████▌| 40/42 [00:30<00:01,  1.43it/s, loss=0.4344, avg_loss=0.4225]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.45it/s, loss=0.4344, avg_loss=0.4225]Epoch 1/2:  98%|█████████▊| 41/42 [00:30<00:00,  1.45it/s, loss=0.4194, avg_loss=0.4224]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.55it/s, loss=0.4194, avg_loss=0.4224]Epoch 1/2: 100%|██████████| 42/42 [00:30<00:00,  1.37it/s, loss=0.4194, avg_loss=0.4224]

Epoch 1/2 complete - Avg loss: 0.4224
Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s]Epoch 2/2:   0%|          | 0/42 [00:00<?, ?it/s, loss=0.3065, avg_loss=0.3065]Epoch 2/2:   2%|▏         | 1/42 [00:00<00:28,  1.42it/s, loss=0.3065, avg_loss=0.3065]Epoch 2/2:   2%|▏         | 1/42 [00:01<00:28,  1.42it/s, loss=0.3462, avg_loss=0.3263]Epoch 2/2:   5%|▍         | 2/42 [00:01<00:28,  1.39it/s, loss=0.3462, avg_loss=0.3263]Epoch 2/2:   5%|▍         | 2/42 [00:02<00:28,  1.39it/s, loss=0.4944, avg_loss=0.3823]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:28,  1.38it/s, loss=0.4944, avg_loss=0.3823]Epoch 2/2:   7%|▋         | 3/42 [00:02<00:28,  1.38it/s, loss=0.4291, avg_loss=0.3940]Epoch 2/2:  10%|▉         | 4/42 [00:02<00:27,  1.40it/s, loss=0.4291, avg_loss=0.3940]Epoch 2/2:  10%|▉         | 4/42 [00:03<00:27,  1.40it/s, loss=0.3181, avg_loss=0.3788]Epoch 2/2:  12%|█▏        | 5/42 [00:03<00:25,  1.45it/s, loss=0.3181, avg_loss=0.3788]Epoch 2/2:  12%|█▏        | 5/42 [00:04<00:25,  1.45it/s, loss=0.3281, avg_loss=0.3704]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:25,  1.44it/s, loss=0.3281, avg_loss=0.3704]Epoch 2/2:  14%|█▍        | 6/42 [00:04<00:25,  1.44it/s, loss=0.2903, avg_loss=0.3590]Epoch 2/2:  17%|█▋        | 7/42 [00:04<00:23,  1.47it/s, loss=0.2903, avg_loss=0.3590]Epoch 2/2:  17%|█▋        | 7/42 [00:05<00:23,  1.47it/s, loss=0.2873, avg_loss=0.3500]Epoch 2/2:  19%|█▉        | 8/42 [00:05<00:23,  1.48it/s, loss=0.2873, avg_loss=0.3500]Epoch 2/2:  19%|█▉        | 8/42 [00:06<00:23,  1.48it/s, loss=0.2915, avg_loss=0.3435]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:22,  1.44it/s, loss=0.2915, avg_loss=0.3435]Epoch 2/2:  21%|██▏       | 9/42 [00:06<00:22,  1.44it/s, loss=0.2979, avg_loss=0.3389]Epoch 2/2:  24%|██▍       | 10/42 [00:06<00:21,  1.46it/s, loss=0.2979, avg_loss=0.3389]Epoch 2/2:  24%|██▍       | 10/42 [00:07<00:21,  1.46it/s, loss=0.3191, avg_loss=0.3371]Epoch 2/2:  26%|██▌       | 11/42 [00:07<00:21,  1.46it/s, loss=0.3191, avg_loss=0.3371]Epoch 2/2:  26%|██▌       | 11/42 [00:08<00:21,  1.46it/s, loss=0.2677, avg_loss=0.3314]Epoch 2/2:  29%|██▊       | 12/42 [00:08<00:20,  1.47it/s, loss=0.2677, avg_loss=0.3314]Epoch 2/2:  29%|██▊       | 12/42 [00:09<00:20,  1.47it/s, loss=0.2665, avg_loss=0.3264]Epoch 2/2:  31%|███       | 13/42 [00:09<00:20,  1.42it/s, loss=0.2665, avg_loss=0.3264]Epoch 2/2:  31%|███       | 13/42 [00:09<00:20,  1.42it/s, loss=0.3973, avg_loss=0.3314]Epoch 2/2:  33%|███▎      | 14/42 [00:09<00:20,  1.40it/s, loss=0.3973, avg_loss=0.3314]Epoch 2/2:  33%|███▎      | 14/42 [00:10<00:20,  1.40it/s, loss=0.2978, avg_loss=0.3292]Epoch 2/2:  36%|███▌      | 15/42 [00:10<00:18,  1.43it/s, loss=0.2978, avg_loss=0.3292]Epoch 2/2:  36%|███▌      | 15/42 [00:11<00:18,  1.43it/s, loss=0.3605, avg_loss=0.3311]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.3605, avg_loss=0.3311]Epoch 2/2:  38%|███▊      | 16/42 [00:11<00:18,  1.40it/s, loss=0.3888, avg_loss=0.3345]Epoch 2/2:  40%|████      | 17/42 [00:11<00:17,  1.43it/s, loss=0.3888, avg_loss=0.3345]Epoch 2/2:  40%|████      | 17/42 [00:12<00:17,  1.43it/s, loss=0.2826, avg_loss=0.3316]Epoch 2/2:  43%|████▎     | 18/42 [00:12<00:16,  1.44it/s, loss=0.2826, avg_loss=0.3316]Epoch 2/2:  43%|████▎     | 18/42 [00:13<00:16,  1.44it/s, loss=0.3735, avg_loss=0.3338]Epoch 2/2:  45%|████▌     | 19/42 [00:13<00:16,  1.43it/s, loss=0.3735, avg_loss=0.3338]Epoch 2/2:  45%|████▌     | 19/42 [00:14<00:16,  1.43it/s, loss=0.2611, avg_loss=0.3302]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.39it/s, loss=0.2611, avg_loss=0.3302]Epoch 2/2:  48%|████▊     | 20/42 [00:14<00:15,  1.39it/s, loss=0.3277, avg_loss=0.3301]Epoch 2/2:  50%|█████     | 21/42 [00:14<00:15,  1.36it/s, loss=0.3277, avg_loss=0.3301]Epoch 2/2:  50%|█████     | 21/42 [00:15<00:15,  1.36it/s, loss=0.2208, avg_loss=0.3251]Epoch 2/2:  52%|█████▏    | 22/42 [00:15<00:14,  1.37it/s, loss=0.2208, avg_loss=0.3251]Epoch 2/2:  52%|█████▏    | 22/42 [00:16<00:14,  1.37it/s, loss=0.2818, avg_loss=0.3232]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.39it/s, loss=0.2818, avg_loss=0.3232]Epoch 2/2:  55%|█████▍    | 23/42 [00:16<00:13,  1.39it/s, loss=0.3348, avg_loss=0.3237]Epoch 2/2:  57%|█████▋    | 24/42 [00:16<00:12,  1.44it/s, loss=0.3348, avg_loss=0.3237]Epoch 2/2:  57%|█████▋    | 24/42 [00:17<00:12,  1.44it/s, loss=0.3441, avg_loss=0.3245]Epoch 2/2:  60%|█████▉    | 25/42 [00:17<00:11,  1.45it/s, loss=0.3441, avg_loss=0.3245]Epoch 2/2:  60%|█████▉    | 25/42 [00:18<00:11,  1.45it/s, loss=0.3248, avg_loss=0.3245]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.42it/s, loss=0.3248, avg_loss=0.3245]Epoch 2/2:  62%|██████▏   | 26/42 [00:18<00:11,  1.42it/s, loss=0.3369, avg_loss=0.3250]Epoch 2/2:  64%|██████▍   | 27/42 [00:18<00:10,  1.46it/s, loss=0.3369, avg_loss=0.3250]Epoch 2/2:  64%|██████▍   | 27/42 [00:19<00:10,  1.46it/s, loss=0.2969, avg_loss=0.3240]Epoch 2/2:  67%|██████▋   | 28/42 [00:19<00:09,  1.44it/s, loss=0.2969, avg_loss=0.3240]Epoch 2/2:  67%|██████▋   | 28/42 [00:20<00:09,  1.44it/s, loss=0.3047, avg_loss=0.3233]Epoch 2/2:  69%|██████▉   | 29/42 [00:20<00:09,  1.43it/s, loss=0.3047, avg_loss=0.3233]Epoch 2/2:  69%|██████▉   | 29/42 [00:21<00:09,  1.43it/s, loss=0.4516, avg_loss=0.3276]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.42it/s, loss=0.4516, avg_loss=0.3276]Epoch 2/2:  71%|███████▏  | 30/42 [00:21<00:08,  1.42it/s, loss=0.2618, avg_loss=0.3255]Epoch 2/2:  74%|███████▍  | 31/42 [00:21<00:07,  1.51it/s, loss=0.2618, avg_loss=0.3255]Epoch 2/2:  74%|███████▍  | 31/42 [00:22<00:07,  1.51it/s, loss=0.3002, avg_loss=0.3247]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.56it/s, loss=0.3002, avg_loss=0.3247]Epoch 2/2:  76%|███████▌  | 32/42 [00:22<00:06,  1.56it/s, loss=0.3092, avg_loss=0.3242]Epoch 2/2:  79%|███████▊  | 33/42 [00:22<00:05,  1.52it/s, loss=0.3092, avg_loss=0.3242]Epoch 2/2:  79%|███████▊  | 33/42 [00:23<00:05,  1.52it/s, loss=0.3287, avg_loss=0.3244]Epoch 2/2:  81%|████████  | 34/42 [00:23<00:05,  1.46it/s, loss=0.3287, avg_loss=0.3244]Epoch 2/2:  81%|████████  | 34/42 [00:24<00:05,  1.46it/s, loss=0.3239, avg_loss=0.3243]Epoch 2/2:  83%|████████▎ | 35/42 [00:24<00:04,  1.45it/s, loss=0.3239, avg_loss=0.3243]Epoch 2/2:  83%|████████▎ | 35/42 [00:25<00:04,  1.45it/s, loss=0.3272, avg_loss=0.3244]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.42it/s, loss=0.3272, avg_loss=0.3244]Epoch 2/2:  86%|████████▌ | 36/42 [00:25<00:04,  1.42it/s, loss=0.2933, avg_loss=0.3236]Epoch 2/2:  88%|████████▊ | 37/42 [00:25<00:03,  1.43it/s, loss=0.2933, avg_loss=0.3236]Epoch 2/2:  88%|████████▊ | 37/42 [00:26<00:03,  1.43it/s, loss=0.2440, avg_loss=0.3215]Epoch 2/2:  90%|█████████ | 38/42 [00:26<00:02,  1.43it/s, loss=0.2440, avg_loss=0.3215]Epoch 2/2:  90%|█████████ | 38/42 [00:27<00:02,  1.43it/s, loss=0.2800, avg_loss=0.3204]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.40it/s, loss=0.2800, avg_loss=0.3204]Epoch 2/2:  93%|█████████▎| 39/42 [00:27<00:02,  1.40it/s, loss=0.3657, avg_loss=0.3216]Epoch 2/2:  95%|█████████▌| 40/42 [00:27<00:01,  1.48it/s, loss=0.3657, avg_loss=0.3216]Epoch 2/2:  95%|█████████▌| 40/42 [00:28<00:01,  1.48it/s, loss=0.2703, avg_loss=0.3203]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.46it/s, loss=0.2703, avg_loss=0.3203]Epoch 2/2:  98%|█████████▊| 41/42 [00:28<00:00,  1.46it/s, loss=0.3730, avg_loss=0.3216]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.64it/s, loss=0.3730, avg_loss=0.3216]Epoch 2/2: 100%|██████████| 42/42 [00:28<00:00,  1.45it/s, loss=0.3730, avg_loss=0.3216]

Epoch 2/2 complete - Avg loss: 0.3216

================================================================================
TRAINING COMPLETE
================================================================================
Total time: 0.99 minutes
Total steps: 42
Final avg loss: 0.3720

Saving checkpoint to runs/gist_validate...
✓ Saved base model to runs/gist_validate
✓ Saved gist embeddings to runs/gist_validate/gist_embedding.pt
✓ Saved to runs/gist_validate

Done!
