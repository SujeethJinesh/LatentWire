
=== Preflight: CUDA / SLURM / bitsandbytes ===

Sun Sep 21 14:55:11 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          Off |   00000000:1B:00.0 Off |                    0 |
| N/A   32C    P0             65W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          Off |   00000000:43:00.0 Off |                    0 |
| N/A   33C    P0             67W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          Off |   00000000:52:00.0 Off |                    0 |
| N/A   35C    P0             67W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          Off |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0             66W /  700W |       4MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
torch: 2.4.0+cu121 cuda: 12.1 is_available: True count: 4
CUDA_VISIBLE_DEVICES: 0,1,2,3
bitsandbytes: 0.47.0

=== Stage A: LoRA (tiny) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2613.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.20s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3236.35it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
[Qwen/Qwen2.5-7B-Instruct] hf_device_map: {'model.embed_tokens': 2, 'model.layers.0': 2, 'model.layers.1': 2, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 2, 'model.layers.5': 2, 'model.layers.6': 2, 'model.layers.7': 2, 'model.layers.8': 2, 'model.layers.9': 2, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 3, 'model.layers.16': 3, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Llama hidden size: 4096, Qwen hidden size: 3584
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
[DeviceMap] Qwen : {'model.embed_tokens': 2, 'model.layers.0': 2, 'model.layers.1': 2, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 2, 'model.layers.5': 2, 'model.layers.6': 2, 'model.layers.7': 2, 'model.layers.8': 2, 'model.layers.9': 2, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 3, 'model.layers.16': 3, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
  step  10/250 | grad_norm=581.68 | sec/step~14.51 | keep=1.00 | K=4 | llama: tf=10.0804 first=9.1643 kCE=10.0299 KD=5.5138 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.0594 first=15.5941 kCE=9.3305 KD=6.8254 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  20/250 | grad_norm=100.51 | sec/step~13.79 | keep=1.00 | K=4 | llama: tf=10.9182 first=10.1750 kCE=10.2999 KD=5.7568 man=0.0001 | scale_pen(llama)=8.8915e-09 | qwen: tf=10.8802 first=11.4527 kCE=8.9796 KD=5.1411 man=0.0002 | scale_pen(qwen)=6.0875e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  30/250 | grad_norm=332.38 | sec/step~12.76 | keep=1.00 | K=4 | llama: tf=10.7653 first=10.0757 kCE=10.1623 KD=6.4105 man=0.0001 | scale_pen(llama)=8.8915e-09 | qwen: tf=11.7289 first=11.4207 kCE=9.1879 KD=5.8104 man=0.0002 | scale_pen(qwen)=6.0875e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  40/250 | grad_norm=128.82 | sec/step~11.47 | keep=1.00 | K=4 | llama: tf=10.4868 first=9.9661 kCE=10.8790 KD=6.2070 man=0.0001 | scale_pen(llama)=4.1554e-10 | qwen: tf=9.4903 first=10.2636 kCE=8.5396 KD=5.6215 man=0.0002 | scale_pen(qwen)=2.5668e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  50/250 | grad_norm=27.68 | sec/step~14.35 | keep=1.00 | K=4 | llama: tf=10.5653 first=9.3943 kCE=10.1641 KD=4.4590 man=0.0001 | scale_pen(llama)=2.4239e-09 | qwen: tf=10.6329 first=9.5262 kCE=7.4764 KD=5.9016 man=0.0002 | scale_pen(qwen)=3.1125e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  60/250 | grad_norm=170.95 | sec/step~11.68 | keep=1.00 | K=4 | llama: tf=10.4594 first=9.0479 kCE=9.8259 KD=4.9082 man=0.0001 | scale_pen(llama)=2.4239e-09 | qwen: tf=9.6484 first=9.0276 kCE=7.0654 KD=5.9826 man=0.0002 | scale_pen(qwen)=3.1125e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  70/250 | grad_norm=47.35 | sec/step~11.70 | keep=1.00 | K=4 | llama: tf=9.9760 first=9.0069 kCE=9.1757 KD=4.3012 man=0.0001 | scale_pen(llama)=5.4891e-09 | qwen: tf=9.1022 first=8.3959 kCE=6.6755 KD=5.2507 man=0.0002 | scale_pen(qwen)=4.0069e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  80/250 | grad_norm=116.31 | sec/step~12.86 | keep=1.00 | K=4 | llama: tf=10.5425 first=8.7045 kCE=9.3590 KD=4.0028 man=0.0001 | scale_pen(llama)=3.7108e-09 | qwen: tf=10.0912 first=8.4059 kCE=7.0748 KD=5.2484 man=0.0002 | scale_pen(qwen)=1.4918e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  90/250 | grad_norm=106.77 | sec/step~12.81 | keep=1.00 | K=4 | llama: tf=9.9068 first=8.7664 kCE=9.4386 KD=4.3796 man=0.0001 | scale_pen(llama)=3.7108e-09 | qwen: tf=9.9547 first=8.6801 kCE=7.2107 KD=5.6185 man=0.0002 | scale_pen(qwen)=1.4918e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  100/250 | grad_norm=16.21 | sec/step~11.60 | keep=1.00 | K=4 | llama: tf=10.0085 first=8.7516 kCE=9.5620 KD=5.5929 man=0.0001 | scale_pen(llama)=8.3569e-10 | qwen: tf=10.4590 first=8.0864 kCE=7.0460 KD=6.2503 man=0.0002 | scale_pen(qwen)=6.8781e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  110/250 | grad_norm=56.71 | sec/step~13.03 | keep=1.00 | K=4 | llama: tf=10.0092 first=8.2105 kCE=9.3193 KD=4.3538 man=0.0001 | scale_pen(llama)=8.3569e-10 | qwen: tf=9.2572 first=7.7768 kCE=6.6085 KD=5.0272 man=0.0002 | scale_pen(qwen)=6.8781e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  120/250 | grad_norm=23.33 | sec/step~12.79 | keep=1.00 | K=4 | llama: tf=9.9807 first=8.2357 kCE=9.9432 KD=4.4311 man=0.0001 | scale_pen(llama)=6.7658e-11 | qwen: tf=8.9510 first=7.2599 kCE=7.1169 KD=5.5903 man=0.0002 | scale_pen(qwen)=8.9887e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  130/250 | grad_norm=7.57 | sec/step~13.99 | keep=1.00 | K=4 | llama: tf=10.0636 first=8.2600 kCE=9.1294 KD=3.6184 man=0.0001 | scale_pen(llama)=1.4371e-09 | qwen: tf=8.7279 first=7.5638 kCE=6.0573 KD=4.4143 man=0.0002 | scale_pen(qwen)=2.1228e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  140/250 | grad_norm=26.15 | sec/step~11.05 | keep=1.00 | K=4 | llama: tf=9.3987 first=8.2581 kCE=8.8671 KD=4.7633 man=0.0001 | scale_pen(llama)=1.4371e-09 | qwen: tf=9.1710 first=7.6130 kCE=6.6036 KD=5.3786 man=0.0002 | scale_pen(qwen)=2.1228e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  150/250 | grad_norm=13.94 | sec/step~13.59 | keep=1.00 | K=4 | llama: tf=9.4632 first=8.3827 kCE=9.8026 KD=3.9975 man=0.0001 | scale_pen(llama)=2.6154e-09 | qwen: tf=9.4439 first=6.8482 kCE=7.2309 KD=4.9007 man=0.0002 | scale_pen(qwen)=1.8162e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  160/250 | grad_norm=34.22 | sec/step~12.51 | keep=1.00 | K=4 | llama: tf=9.6465 first=8.8663 kCE=9.5507 KD=3.9597 man=0.0001 | scale_pen(llama)=2.1837e-09 | qwen: tf=9.1095 first=7.6385 kCE=7.3397 KD=4.8956 man=0.0002 | scale_pen(qwen)=6.2670e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  170/250 | grad_norm=20.59 | sec/step~11.67 | keep=1.00 | K=4 | llama: tf=10.0515 first=8.5056 kCE=9.5139 KD=4.1839 man=0.0001 | scale_pen(llama)=2.1837e-09 | qwen: tf=9.9333 first=7.2509 kCE=7.2015 KD=5.1778 man=0.0002 | scale_pen(qwen)=6.2670e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  180/250 | grad_norm=7.83 | sec/step~11.86 | keep=1.00 | K=4 | llama: tf=10.3265 first=9.2399 kCE=9.7481 KD=4.1847 man=0.0001 | scale_pen(llama)=8.5998e-10 | qwen: tf=10.3978 first=7.9540 kCE=7.5803 KD=5.2495 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  190/250 | grad_norm=19.85 | sec/step~13.11 | keep=1.00 | K=4 | llama: tf=9.6344 first=8.6859 kCE=9.7112 KD=3.6347 man=0.0001 | scale_pen(llama)=8.5998e-10 | qwen: tf=8.1459 first=7.0979 kCE=6.7362 KD=4.7932 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  200/250 | grad_norm=7.89 | sec/step~12.66 | keep=1.00 | K=4 | llama: tf=9.6306 first=8.9549 kCE=10.2521 KD=4.0913 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=9.6891 first=7.3452 kCE=8.3895 KD=4.7938 man=0.0002 | scale_pen(qwen)=4.1554e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  210/250 | grad_norm=1.91 | sec/step~13.42 | keep=1.00 | K=4 | llama: tf=9.3554 first=7.9763 kCE=9.2589 KD=3.9569 man=0.0001 | scale_pen(llama)=3.1127e-10 | qwen: tf=9.9735 first=7.0729 kCE=6.9912 KD=5.0666 man=0.0002 | scale_pen(qwen)=1.0669e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  220/250 | grad_norm=8.75 | sec/step~12.80 | keep=1.00 | K=4 | llama: tf=9.8800 first=7.7810 kCE=9.0606 KD=4.0131 man=0.0001 | scale_pen(llama)=3.1127e-10 | qwen: tf=9.5423 first=6.7784 kCE=6.4636 KD=4.9279 man=0.0002 | scale_pen(qwen)=1.0669e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  230/250 | grad_norm=4.56 | sec/step~10.61 | keep=1.00 | K=4 | llama: tf=9.8605 first=8.4452 kCE=9.2877 KD=4.4250 man=0.0001 | scale_pen(llama)=1.1181e-09 | qwen: tf=9.1745 first=7.0585 kCE=6.6758 KD=4.8076 man=0.0002 | scale_pen(qwen)=1.0591e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  240/250 | grad_norm=10.41 | sec/step~11.97 | keep=1.00 | K=4 | llama: tf=9.2965 first=8.2058 kCE=8.6382 KD=4.2554 man=0.0001 | scale_pen(llama)=1.4597e-09 | qwen: tf=9.5271 first=6.8895 kCE=5.8323 KD=5.0744 man=0.0002 | scale_pen(qwen)=4.7072e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  250/250 | grad_norm=6.52 | sec/step~13.10 | keep=1.00 | K=4 | llama: tf=9.4915 first=8.7335 kCE=9.1577 KD=3.4717 man=0.0001 | scale_pen(llama)=1.2918e-09 | qwen: tf=9.2479 first=7.2201 kCE=6.5896 KD=5.2546 man=0.0002 | scale_pen(qwen)=5.4627e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 1.6KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/scoped_softprompt_20250921_145505/ckpt
📝 Saved LoRA adapters for Llama
📝 Saved LoRA adapters for Qwen
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0000389878749847, 'rms_mean_cal': 0.010571178175508976, 'embed_rms': 0.01057521253824234, 'count': 250}, 'qwen': {'rms_mean_raw': 1.0000751571655273, 'rms_mean_cal': 0.013641561437398196, 'embed_rms': 0.013643525540828705, 'count': 250}}

=== Stage A -> merge LoRA ===


=== Stage B: Deep Prefix ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5821.38it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]
[runs/scoped_softprompt_20250921_145505/ckpt/merged_llama] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7142.28it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]
[runs/scoped_softprompt_20250921_145505/ckpt/merged_qwen] hf_device_map: {'model.embed_tokens': 2, 'model.layers.0': 2, 'model.layers.1': 2, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 2, 'model.layers.5': 2, 'model.layers.6': 2, 'model.layers.7': 2, 'model.layers.8': 2, 'model.layers.9': 2, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 3, 'model.layers.16': 3, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
trainable params: 272,723,968 || all params: 8,323,956,736 || trainable%: 3.2764
trainable params: 104,640,000 || all params: 7,740,441,600 || trainable%: 1.3519
Llama hidden size: 4096, Qwen hidden size: 3584
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
[DeviceMap] Qwen : {'model.embed_tokens': 2, 'model.layers.0': 2, 'model.layers.1': 2, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 2, 'model.layers.5': 2, 'model.layers.6': 2, 'model.layers.7': 2, 'model.layers.8': 2, 'model.layers.9': 2, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 3, 'model.layers.16': 3, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[OK] t=0 alignment for runs/scoped_softprompt_20250921_145505/ckpt/merged_llama
[OK] t=0 alignment for runs/scoped_softprompt_20250921_145505/ckpt/merged_qwen
[INFO] Encoder frozen (--freeze_encoder)
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/8
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/334 | grad_norm=947.86 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=11.7686 first=10.4694 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=14.5606 first=21.3868 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=64.34 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=11.6357 first=12.2870 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.6032e-09 | qwen: tf=11.3476 first=11.0323 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0123e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=214.83 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=11.3380 first=12.3461 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.6032e-09 | qwen: tf=11.2444 first=12.2831 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0123e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=63.74 | sec/step~1.11 | keep=1.00 | K=4 | llama: tf=12.2696 first=9.8234 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.8637e-10 | qwen: tf=12.8564 first=12.9097 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.9666e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=5.74 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=11.4246 first=8.4381 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2624e-09 | qwen: tf=8.5571 first=7.9617 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.1554e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=28.33 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=12.3038 first=9.1681 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2624e-09 | qwen: tf=11.1103 first=8.9064 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.1554e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=15.10 | sec/step~1.16 | keep=1.00 | K=4 | llama: tf=11.5727 first=8.9117 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.5690e-10 | qwen: tf=11.5068 first=7.8619 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8279e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.9999 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=22.69 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=11.1354 first=8.4176 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.7899e-11 | qwen: tf=9.9303 first=8.3257 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.4247e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=13.71 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=10.6822 first=8.8074 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.7899e-11 | qwen: tf=9.7979 first=7.0518 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.4247e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.9999 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=4.01 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=11.1030 first=7.9578 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.1462e-09 | qwen: tf=10.1748 first=6.6485 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.1174e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=17.38 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=11.0652 first=8.7444 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.1462e-09 | qwen: tf=10.5297 first=7.2939 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.1174e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=9.23 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.9561 first=8.5583 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.1102e-09 | qwen: tf=10.0000 first=7.4805 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.8468e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=2.71 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=11.0112 first=8.1083 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2741e-10 | qwen: tf=10.9809 first=6.9233 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.7526e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=13.05 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=10.9340 first=8.7369 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2741e-10 | qwen: tf=10.0379 first=7.4172 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.7526e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=7.18 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=10.3161 first=8.6004 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.3669e-11 | qwen: tf=8.2117 first=7.2392 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7960e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=15.70 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=10.2594 first=8.6967 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.2670e-10 | qwen: tf=9.1619 first=7.6107 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.4534e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=8.73 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=10.2686 first=9.0758 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.2670e-10 | qwen: tf=9.2977 first=7.8084 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.4534e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=2.45 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=9.5566 first=7.8205 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.1304e-10 | qwen: tf=8.4299 first=6.3265 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7259e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=9.10 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.6118 first=8.0496 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.1304e-10 | qwen: tf=7.9015 first=6.6679 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7259e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=3.82 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=9.9638 first=7.6203 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.3283e-10 | qwen: tf=8.8849 first=6.3105 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.3484e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=1.44 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=9.9991 first=8.4169 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.4037e-12 | qwen: tf=8.9824 first=7.1705 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=6.74 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=9.4896 first=8.5837 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.4037e-12 | qwen: tf=8.1545 first=7.3272 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=4.29 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=9.5970 first=8.4341 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.9263e-10 | qwen: tf=8.9012 first=7.7299 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.0588e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=8.64 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=10.2282 first=8.6494 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.9960e-10 | qwen: tf=9.7066 first=7.8956 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0825e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=8.99 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.4074 first=8.2748 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.9960e-10 | qwen: tf=8.0290 first=6.8043 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0825e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=2.20 | sec/step~1.23 | keep=1.00 | K=4 | llama: tf=9.5421 first=9.1203 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8053e-10 | qwen: tf=8.2294 first=7.5316 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2283e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=9.18 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.5294 first=7.7758 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8053e-10 | qwen: tf=9.3243 first=7.0721 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2283e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=2.69 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.4582 first=7.5154 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=9.3620 first=7.1468 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.5175e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0000 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=0.56 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=9.6922 first=8.8004 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.4256e-11 | qwen: tf=9.0287 first=7.6987 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.6884e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=3.84 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=9.9337 first=8.7323 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.4256e-11 | qwen: tf=9.7096 first=7.6027 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.6884e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=2.33 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.4375 first=8.3825 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8253e-10 | qwen: tf=7.5567 first=7.0331 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=4.86 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=9.6189 first=8.0249 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4387e-10 | qwen: tf=8.4826 first=6.6177 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=3.39 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.0221 first=7.3042 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4387e-10 | qwen: tf=7.8233 first=6.7556 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=4.77 | sec/step~0.43 | keep=1.00 | K=4 | llama: tf=10.2411 first=8.4779 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.5711e-11 | qwen: tf=9.3447 first=7.3671 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.0353e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
Epoch 2/8
  step  10/334 | grad_norm=2.66 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=9.6686 first=7.6241 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.5711e-11 | qwen: tf=8.2855 first=6.5672 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.0353e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=0.90 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=9.3083 first=7.8457 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.1942e-12 | qwen: tf=8.2107 first=6.7659 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.3227e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=2.11 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.4180 first=7.3619 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.1942e-12 | qwen: tf=8.8345 first=6.7110 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.3227e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=2.03 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=9.5094 first=7.8951 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2691e-10 | qwen: tf=8.5477 first=6.7401 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.0348e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=0.21 | sec/step~1.00 | keep=1.00 | K=4 | llama: tf=9.6541 first=7.2441 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.8794e-10 | qwen: tf=8.1714 first=6.5480 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.8642e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=1.91 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.0431 first=7.7870 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.8794e-10 | qwen: tf=7.1170 first=6.7263 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.8642e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=0.96 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=10.0689 first=7.4779 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8690e-11 | qwen: tf=8.5068 first=6.6842 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=2.34 | sec/step~1.18 | keep=1.00 | K=4 | llama: tf=9.6360 first=7.6901 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.0267e-12 | qwen: tf=8.3571 first=6.7501 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2921e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=1.93 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.2093 first=7.2382 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.0267e-12 | qwen: tf=7.2488 first=6.4227 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2921e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=0.77 | sec/step~0.98 | keep=1.00 | K=4 | llama: tf=9.3323 first=7.5227 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.9468e-11 | qwen: tf=8.2792 first=7.2480 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.0109e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=1.92 | sec/step~1.11 | keep=1.00 | K=4 | llama: tf=9.4570 first=8.6122 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.9468e-11 | qwen: tf=7.9223 first=7.5050 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.0109e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=0.96 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.5775 first=8.8147 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.1511e-10 | qwen: tf=8.3631 first=8.0264 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.4584e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=0.21 | sec/step~0.94 | keep=1.00 | K=4 | llama: tf=9.3860 first=8.0031 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.3669e-11 | qwen: tf=7.3300 first=6.4056 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3648e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=0.91 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=9.3637 first=9.0777 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.3669e-11 | qwen: tf=7.8766 first=7.9696 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3648e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=1.02 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.8885 first=7.3869 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.1301e-12 | qwen: tf=7.8313 first=6.6215 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=1.45 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=9.3751 first=7.7059 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-11 | qwen: tf=7.8084 first=6.3492 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.9121e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=0.81 | sec/step~1.20 | keep=1.00 | K=4 | llama: tf=10.0375 first=7.4255 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-11 | qwen: tf=9.2145 first=6.7624 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.9121e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=0.43 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.2715 first=7.5076 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.6771e-11 | qwen: tf=8.6551 first=7.1735 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.9955e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=2.44 | sec/step~0.99 | keep=1.00 | K=4 | llama: tf=9.2436 first=7.0916 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.6771e-11 | qwen: tf=7.1559 first=6.1435 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.9955e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=0.51 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=9.4079 first=8.1211 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.1902e-11 | qwen: tf=6.9697 first=7.1065 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.4761e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=0.21 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.2814 first=7.3307 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.2406e-12 | qwen: tf=8.9646 first=6.9933 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.5519e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=1.73 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.9246 first=8.5206 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.2406e-12 | qwen: tf=8.3535 first=7.3364 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.5519e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=0.52 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=9.4394 first=8.0246 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-12 | qwen: tf=7.9403 first=7.2296 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.2742e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=1.95 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.6737 first=8.1509 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.4565e-11 | qwen: tf=7.0722 first=7.3060 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=1.07 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=9.0487 first=8.2001 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.4565e-11 | qwen: tf=7.0581 first=7.5196 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=0.50 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.0881 first=7.5254 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=7.2146 first=6.7037 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.1637e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=4.16 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=8.7714 first=7.5341 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.9918e-11 | qwen: tf=7.7048 first=6.5541 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.1637e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=2.40 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=8.8990 first=6.8643 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=7.6049 first=5.6921 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3097e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=0.87 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.5658 first=8.0193 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=7.5380 first=6.8170 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0510e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=5.43 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.7291 first=7.4255 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=7.2845 first=6.5191 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0510e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=2.68 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.0306 first=8.8758 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.3427e-11 | qwen: tf=8.0756 first=8.2140 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.2742e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=7.54 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.0711 first=7.6687 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=7.8648 first=7.4457 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=0.75 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=9.2238 first=8.1256 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=7.6592 first=6.7020 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=0.96 | sec/step~0.40 | keep=1.00 | K=4 | llama: tf=8.6804 first=8.6435 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.5690e-12 | qwen: tf=7.3284 first=7.5540 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.4120e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
Epoch 3/8
  step  10/334 | grad_norm=3.55 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8905 first=7.6459 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.5690e-12 | qwen: tf=8.0248 first=7.1618 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.4120e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=2.79 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.7621 first=6.9688 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=8.3519 first=6.3717 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.9936e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=5.34 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=9.1011 first=8.2014 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=7.4804 first=7.6173 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.9936e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=0.97 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=9.3244 first=7.1142 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.8417e-11 | qwen: tf=8.1215 first=6.6875 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=7.6771e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=1.02 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8234 first=7.8173 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=7.6825 first=7.1962 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.2063e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0001 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=7.58 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8151 first=7.7078 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=6.6532 first=6.2285 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.2063e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=5.68 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=8.7021 first=7.9518 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=7.6026 first=7.1617 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.0267e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=4.47 | sec/step~1.00 | keep=1.00 | K=4 | llama: tf=8.8984 first=8.3480 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.3521e-12 | qwen: tf=6.6924 first=7.5240 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=2.93 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=9.0122 first=7.4421 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.3521e-12 | qwen: tf=6.9366 first=6.2610 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=1.69 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.1645 first=7.6961 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.5010e-11 | qwen: tf=7.3096 first=6.9675 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8426e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=0.98 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=9.3404 first=7.9932 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.5010e-11 | qwen: tf=7.8663 first=7.2871 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8426e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=0.48 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5317 first=8.1177 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.9796e-12 | qwen: tf=6.7773 first=7.4683 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.9918e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=0.89 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.9411 first=7.7285 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=7.4333 first=6.5859 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.6428e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=2.50 | sec/step~1.02 | keep=1.00 | K=4 | llama: tf=8.6764 first=7.6962 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.2988e-13 | qwen: tf=7.4847 first=7.4542 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.6428e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=2.84 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.6781 first=8.1511 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=7.2679 first=7.5287 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=9.45 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=9.0291 first=7.6188 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.1855e-12 | qwen: tf=7.6475 first=6.8774 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=9.2406e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=11.44 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=9.4433 first=8.0884 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.1855e-12 | qwen: tf=7.7500 first=7.3052 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=9.2406e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=3.70 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.9211 first=7.1078 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=7.3638 first=6.4603 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.6890e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=5.39 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.0615 first=7.6635 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=8.4575 first=7.3473 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.6890e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=18.37 | sec/step~1.12 | keep=1.00 | K=4 | llama: tf=8.7602 first=7.0109 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=7.1245 first=6.5652 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.7512e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=1.39 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.2962 first=7.8062 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=7.9229 first=7.3335 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=11.71 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.3045 first=7.9154 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=8.0750 first=7.4103 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=3.29 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.6344 first=8.1647 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.8479e-12 | qwen: tf=7.1877 first=7.1063 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=10.46 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.7958 first=7.4598 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=6.8158 first=6.7017 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=4.73 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.8140 first=7.3008 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.5667e-12 | qwen: tf=6.2574 first=6.2441 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=1.91 | sec/step~1.12 | keep=1.00 | K=4 | llama: tf=9.3625 first=7.9264 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=7.8770 first=7.6337 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=7.89 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.1002 first=7.6846 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=7.8640 first=7.5321 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=3.65 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=8.7172 first=8.0330 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=7.5503 first=7.1344 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=0.34 | sec/step~1.17 | keep=1.00 | K=4 | llama: tf=9.1271 first=7.4773 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=7.5622 first=6.8000 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=1.18 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.5500 first=8.0268 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=7.2143 first=7.5649 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=2.29 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=9.1681 first=8.1388 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.4968 first=7.9735 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=5.13 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.7053 first=6.9742 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.5899e-12 | qwen: tf=7.1465 first=6.4439 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=2.87 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.5409 first=7.8826 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.5899e-12 | qwen: tf=6.5076 first=6.9912 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=4.18 | sec/step~0.46 | keep=1.00 | K=4 | llama: tf=9.5889 first=8.2358 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=8.5170 first=8.0133 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1543e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
Epoch 4/8
  step  10/334 | grad_norm=2.37 | sec/step~1.02 | keep=1.00 | K=4 | llama: tf=8.4657 first=7.6635 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.8689e-12 | qwen: tf=7.0206 first=6.9936 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.1543e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=0.72 | sec/step~1.00 | keep=1.00 | K=4 | llama: tf=9.0611 first=7.0342 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.0267e-12 | qwen: tf=7.7719 first=6.2082 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=2.01 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.8217 first=7.1721 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.0267e-12 | qwen: tf=6.8410 first=6.0745 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=1.04 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4049 first=6.8090 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.3492 first=6.6332 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2204e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=0.56 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=9.0006 first=6.7291 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=7.8740 first=6.6478 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=3.70 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=9.2651 first=7.3799 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=8.3571 first=6.9404 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=1.51 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.6862 first=6.9044 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=7.5179 first=6.7906 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=3.54 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=8.4059 first=7.8183 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.9832 first=7.0616 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=0.75 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.3415 first=7.8666 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.7764 first=7.3051 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=0.91 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.8855 first=7.7805 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=8.0702 first=7.3791 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=1.77 | sec/step~1.19 | keep=1.00 | K=4 | llama: tf=9.0338 first=7.1454 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=7.9124 first=6.4098 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=0.99 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=9.1085 first=7.7094 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=7.5097 first=7.3607 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=0.39 | sec/step~1.09 | keep=1.00 | K=4 | llama: tf=8.4763 first=7.7755 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=6.6788 first=6.9572 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=2.11 | sec/step~1.10 | keep=1.00 | K=4 | llama: tf=8.7068 first=8.2711 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=6.7022 first=7.4998 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=1.25 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.8977 first=7.9638 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.5966 first=7.3245 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=3.48 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3274 first=7.3613 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=6.4356 first=6.5774 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=3.43 | sec/step~1.15 | keep=1.00 | K=4 | llama: tf=7.9871 first=7.0471 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=6.4442 first=6.3208 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.8689e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=1.11 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=8.2208 first=6.9437 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=6.6510 first=6.0203 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=6.22 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8399 first=7.7215 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=7.4350 first=7.5899 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=2.16 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.7332 first=7.8236 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.3759 first=7.1246 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=0.53 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5218 first=7.3709 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=6.9233 first=6.8364 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=4.65 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8927 first=7.2539 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=9.0949e-13 | qwen: tf=8.0873 first=6.4938 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=1.44 | sec/step~1.02 | keep=1.00 | K=4 | llama: tf=8.6372 first=7.3455 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=7.9936e-13 | qwen: tf=7.3176 first=6.6813 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=3.57 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5416 first=7.6238 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.7725 first=6.8299 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=1.79 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.3189 first=7.1197 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.1006 first=6.6602 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=2.43 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=8.8422 first=8.1105 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.4982 first=7.7182 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=4.59 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.7562 first=7.9042 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.2636 first=7.3843 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=5.39 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.6297 first=7.6902 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=7.0013 first=6.5928 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=0.31 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=8.5002 first=7.6632 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.3675 first=6.9464 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=2.95 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5425 first=7.3445 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.2306 first=6.4467 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7195e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=2.00 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.0126 first=7.0994 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=5.8991 first=6.1521 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=5.73 | sec/step~1.19 | keep=1.00 | K=4 | llama: tf=8.7196 first=6.9932 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=7.5092 first=6.2961 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=3.96 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.6884 first=7.9866 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=7.4346 first=7.6319 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=4.2988e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=6.20 | sec/step~0.41 | keep=1.00 | K=4 | llama: tf=9.1291 first=7.7962 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=8.6277 first=7.6397 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
Epoch 5/8
  step  10/334 | grad_norm=2.95 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=9.0217 first=7.3981 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=7.8634 first=6.5784 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=1.45 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.5963 first=7.0622 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.8521 first=6.2754 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=1.31 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8702 first=7.5584 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.2367 first=7.3394 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=6.93 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.0741 first=8.4828 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=6.8740 first=7.5935 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=0.55 | sec/step~1.14 | keep=1.00 | K=4 | llama: tf=8.7554 first=7.3808 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4329 first=6.9345 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=4.14 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.8779 first=8.3534 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.9261 first=7.8121 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=2.46 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.3458 first=6.7746 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.1749 first=5.7406 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=4.56 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=8.3709 first=7.1266 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=6.5570 first=6.1600 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=3.23 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.7957 first=6.9724 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-13 | qwen: tf=7.3239 first=6.2127 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=2.45 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=8.1211 first=7.7883 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=6.7124 first=7.2009 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=6.22 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.8479 first=7.4896 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.3453 first=7.1550 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=1.29 | sec/step~1.40 | keep=1.00 | K=4 | llama: tf=8.8381 first=6.7610 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=8.2474 first=6.3092 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=0.43 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3079 first=7.1471 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.9052 first=6.8706 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=3.45 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.9977 first=7.9978 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=8.2912 first=7.4638 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=1.51 | sec/step~1.12 | keep=1.00 | K=4 | llama: tf=8.4750 first=7.5235 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4582 first=7.2934 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=5.94 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.6536 first=7.3732 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.3314 first=6.6015 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=0.89 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4413 first=8.0999 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.4288 first=6.5725 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=0.46 | sec/step~1.12 | keep=1.00 | K=4 | llama: tf=8.2720 first=7.5102 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=6.9009 first=7.1339 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=2.44 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.5184 first=7.4624 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=7.2209 first=7.1239 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=3.05 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3638 first=7.2449 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=6.5693 first=6.6369 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=0.58 | sec/step~1.21 | keep=1.00 | K=4 | llama: tf=8.6964 first=7.3407 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=7.6999 first=7.4281 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=1.84 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=9.1381 first=6.8295 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=8.0101 first=6.4707 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=1.66 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.3012 first=7.3056 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.2094 first=7.0444 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=2.13 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=9.2354 first=7.2809 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=8.0539 first=6.7560 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=6.27 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.2244 first=6.4120 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.1616 first=5.6665 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=2.32 | sec/step~0.96 | keep=1.00 | K=4 | llama: tf=8.3837 first=6.8960 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.8556 first=5.7778 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=8.58 | sec/step~1.10 | keep=1.00 | K=4 | llama: tf=8.5727 first=7.3375 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.9696 first=6.9210 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=2.39 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.1896 first=7.2345 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.8777e-13 | qwen: tf=6.5723 first=6.6706 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=2.18 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.6843 first=7.0466 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.6166 first=7.0189 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=6.99 | sec/step~1.10 | keep=1.00 | K=4 | llama: tf=8.5739 first=7.3112 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.6155 first=6.5094 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=2.67 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=9.1559 first=7.5138 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=8.0469 first=6.8317 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=1.79 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.6844 first=7.3309 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.8284 first=6.6858 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=2.24 | sec/step~1.12 | keep=1.00 | K=4 | llama: tf=8.3495 first=6.3805 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.9919 first=5.6238 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=2.64 | sec/step~0.52 | keep=1.00 | K=4 | llama: tf=8.7971 first=6.6912 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.9801 first=6.2539 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
Epoch 6/8
  step  10/334 | grad_norm=5.66 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.4661 first=7.4932 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4455 first=7.2839 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=2.70 | sec/step~0.96 | keep=1.00 | K=4 | llama: tf=8.2474 first=7.5773 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.4398 first=6.9991 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=5.69 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.7571 first=7.9200 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=7.3699 first=7.6174 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=4.15 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.3995 first=7.2806 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=6.9185 first=6.8311 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=1.24 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=8.0392 first=7.8504 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=5.8905 first=7.0247 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=2.70 | sec/step~1.20 | keep=1.00 | K=4 | llama: tf=8.2123 first=7.2283 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=6.7004 first=6.5825 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=4.96 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.0084 first=7.1693 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.5294 first=6.7533 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=12.90 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4756 first=7.1081 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.6820 first=5.9121 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=3.03 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.3189 first=7.0248 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.3837 first=6.3007 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=1.81 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.4442 first=7.1904 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4765 first=6.8646 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=6.26 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5669 first=7.1143 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4835 first=6.6901 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=8.80 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=9.0313 first=7.4777 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=7.8742 first=7.0295 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=1.80 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=7.8202 first=7.4325 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=6.2017 first=6.7592 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=3.67 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5879 first=6.6646 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.1196 first=6.1800 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=1.03 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.6837 first=7.6674 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=7.3834 first=6.7393 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=8.97 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.3436 first=7.1178 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.4573 first=6.8366 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=10.17 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.3904 first=6.8765 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.2295 first=6.8860 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=8.18 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=7.7813 first=7.5633 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=5.8942 first=7.1688 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=23.17 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.0772 first=7.3733 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.7334 first=7.0598 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=4.77 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=8.1974 first=7.4756 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.0161 first=7.5474 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=0.89 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=8.0218 first=7.3643 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.4229 first=6.5021 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=9.73 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.3014 first=7.4430 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.6455 first=6.5860 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=10.54 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.5473 first=7.3022 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.6711 first=6.8204 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=22.46 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.0997 first=6.5138 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=6.0832 first=5.7990 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=7.39 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.5871 first=7.1153 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.3370 first=6.7374 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=1.86 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3385 first=7.8914 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.2649 first=6.7404 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=7.76 | sec/step~1.20 | keep=1.00 | K=4 | llama: tf=8.3826 first=7.0507 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.2604 first=6.1475 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=4.25 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.3912 first=7.8581 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.6618 first=7.4868 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=1.49 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.6584 first=7.4588 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.6821 first=7.3654 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=6.35 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.6219 first=7.3738 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.9362 first=6.7069 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=6.33 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=8.1972 first=7.2113 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.5452 first=6.4856 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=13.56 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.3431 first=7.9132 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.7390 first=7.2168 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=5.49 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.3029 first=6.8513 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.9362 first=6.5833 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=8.02 | sec/step~0.55 | keep=1.00 | K=4 | llama: tf=8.3964 first=7.6867 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=7.4093 first=7.8532 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
Epoch 7/8
  step  10/334 | grad_norm=4.84 | sec/step~0.96 | keep=1.00 | K=4 | llama: tf=8.7902 first=7.0846 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=8.0038 first=6.2087 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=4.88 | sec/step~0.96 | keep=1.00 | K=4 | llama: tf=7.9527 first=7.1894 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=5.9982 first=6.5903 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=6.30 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.0796 first=6.8686 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.7439 first=6.6149 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=7.51 | sec/step~1.01 | keep=1.00 | K=4 | llama: tf=8.3457 first=7.1815 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.0052 first=6.5693 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=1.30 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=8.3289 first=6.7324 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.4377 first=6.2680 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=3.73 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4945 first=6.5991 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=7.9624 first=6.2613 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=3.81 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.2150 first=7.3766 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=5.8070 first=6.2675 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=7.37 | sec/step~1.18 | keep=1.00 | K=4 | llama: tf=8.4135 first=6.9954 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4722 first=6.8515 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=4.46 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4166 first=7.4068 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.8766 first=7.2512 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=7.96 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.6225 first=7.4750 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=7.1605 first=7.0136 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=23.39 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=7.9773 first=6.8727 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=6.0123 first=5.9857 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=12.76 | sec/step~0.93 | keep=1.00 | K=4 | llama: tf=8.5231 first=6.7814 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.8122 first=6.5618 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=2.34 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=8.4299 first=7.4575 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.6806 first=6.5551 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=8.98 | sec/step~1.10 | keep=1.00 | K=4 | llama: tf=8.5913 first=7.3173 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.8922 first=7.2884 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=4.67 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3515 first=8.3426 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.5837 first=8.0537 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=13.89 | sec/step~0.98 | keep=1.00 | K=4 | llama: tf=8.9283 first=6.6019 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.3435 first=6.1895 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=3.16 | sec/step~1.03 | keep=1.00 | K=4 | llama: tf=8.7696 first=7.2212 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.6717 first=6.8024 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=7.29 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=8.2081 first=8.0499 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=6.3925 first=7.6976 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=19.95 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=7.9419 first=7.2215 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=6.6185 first=6.9231 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=4.73 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=8.8979 first=7.0765 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=8.5280 first=6.2933 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=1.69 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.5073 first=7.2618 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.4968 first=6.4693 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=7.23 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.2505 first=7.7859 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.0494 first=7.3739 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=7.35 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.9718 first=7.2530 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4506 first=6.8346 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=23.11 | sec/step~1.02 | keep=1.00 | K=4 | llama: tf=8.2033 first=6.1797 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.2587 first=5.4091 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=7.68 | sec/step~1.07 | keep=1.00 | K=4 | llama: tf=7.9669 first=7.0963 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.0217 first=6.9128 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=5.45 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.4663 first=6.8271 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=6.9059 first=6.1279 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=15.82 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.4621 first=6.8566 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.5527e-15 | qwen: tf=7.9047 first=6.8373 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=5.43 | sec/step~1.11 | keep=1.00 | K=4 | llama: tf=8.4975 first=7.3932 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.7408e-13 | qwen: tf=7.1369 first=6.7916 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=4.92 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=8.3023 first=7.2651 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.9523 first=6.3888 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=15.45 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.0942 first=6.9368 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.5052 first=6.1881 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=12.49 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=7.9492 first=7.0197 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.2759 first=6.9428 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=34.37 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3420 first=7.5723 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.6959 first=7.1551 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=11.17 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.3509 first=7.2613 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.6818 first=6.6634 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=17.23 | sec/step~0.42 | keep=1.00 | K=4 | llama: tf=8.5355 first=6.8795 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.1342 first=6.2716 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
Epoch 8/8
  step  10/334 | grad_norm=19.66 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.3793 first=7.1302 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.1828 first=6.8171 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  20/334 | grad_norm=5.37 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.2675 first=7.4894 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.7395 first=7.1247 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  30/334 | grad_norm=19.88 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=8.4160 first=7.1707 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=7.0380 first=6.8490 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  40/334 | grad_norm=2.89 | sec/step~1.11 | keep=1.00 | K=4 | llama: tf=8.2642 first=7.0325 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.7939 first=6.5836 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  50/334 | grad_norm=2.62 | sec/step~0.86 | keep=1.00 | K=4 | llama: tf=8.4122 first=6.6657 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.1516 first=6.2588 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  60/334 | grad_norm=14.49 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.7662 first=7.0803 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.3182 first=6.7258 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  70/334 | grad_norm=4.14 | sec/step~1.00 | keep=1.00 | K=4 | llama: tf=8.3503 first=6.6765 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.3777 first=6.5668 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  80/334 | grad_norm=18.68 | sec/step~1.19 | keep=1.00 | K=4 | llama: tf=8.3846 first=7.2817 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4128 first=7.1714 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  90/334 | grad_norm=14.13 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.4198 first=7.8336 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.8655 first=7.7219 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  100/334 | grad_norm=3.47 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.0958 first=7.5335 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.2821 first=6.4392 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  110/334 | grad_norm=11.45 | sec/step~1.13 | keep=1.00 | K=4 | llama: tf=7.6728 first=7.1417 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.3751 first=6.6870 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  120/334 | grad_norm=6.46 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.3168 first=7.4699 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.0612 first=6.7843 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  130/334 | grad_norm=2.53 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=8.4127 first=6.7207 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.0686 first=6.7674 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  140/334 | grad_norm=15.98 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=8.7008 first=7.4797 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=7.4777 first=6.6741 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  150/334 | grad_norm=7.67 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.2578 first=7.4298 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.9486 first=6.6660 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  160/334 | grad_norm=19.73 | sec/step~0.97 | keep=1.00 | K=4 | llama: tf=8.6220 first=7.3962 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.3498 first=6.9375 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  170/334 | grad_norm=13.24 | sec/step~1.04 | keep=1.00 | K=4 | llama: tf=8.4488 first=7.0430 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.1668 first=6.4247 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  180/334 | grad_norm=8.96 | sec/step~1.08 | keep=1.00 | K=4 | llama: tf=7.9027 first=7.0478 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.3154 first=6.5377 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  190/334 | grad_norm=29.58 | sec/step~0.95 | keep=1.00 | K=4 | llama: tf=8.2615 first=8.0043 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.9055 first=7.0811 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  200/334 | grad_norm=10.21 | sec/step~0.87 | keep=1.00 | K=4 | llama: tf=8.2064 first=7.5611 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=6.7853 first=6.9125 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  210/334 | grad_norm=4.21 | sec/step~0.88 | keep=1.00 | K=4 | llama: tf=8.3864 first=6.8037 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=7.0847 first=6.1834 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  220/334 | grad_norm=18.18 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=7.8173 first=7.6339 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=5.6418 first=6.2223 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  230/334 | grad_norm=5.49 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.4167 first=8.1752 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.7512 first=7.1749 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  240/334 | grad_norm=12.92 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.9884 first=7.4034 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=8.4590 first=7.1156 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  250/334 | grad_norm=12.13 | sec/step~1.05 | keep=1.00 | K=4 | llama: tf=8.4976 first=6.7029 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.4844 first=6.1660 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  260/334 | grad_norm=5.93 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.4924 first=7.0111 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.3835 first=6.7206 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  270/334 | grad_norm=16.39 | sec/step~0.92 | keep=1.00 | K=4 | llama: tf=8.3708 first=7.6407 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=6.4164 first=6.8359 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  280/334 | grad_norm=5.93 | sec/step~1.09 | keep=1.00 | K=4 | llama: tf=8.3426 first=7.2896 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=7.3949 first=7.2449 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  290/334 | grad_norm=5.73 | sec/step~0.89 | keep=1.00 | K=4 | llama: tf=8.3432 first=6.7314 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=6.7634 first=5.9290 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  300/334 | grad_norm=12.33 | sec/step~0.96 | keep=1.00 | K=4 | llama: tf=8.4872 first=5.9927 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=7.1895 first=5.6205 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0003 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  310/334 | grad_norm=5.02 | sec/step~1.06 | keep=1.00 | K=4 | llama: tf=8.3373 first=7.0801 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=6.9434 first=6.2460 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  320/334 | grad_norm=16.66 | sec/step~0.91 | keep=1.00 | K=4 | llama: tf=7.9215 first=6.6747 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=6.0741 first=6.2659 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  330/334 | grad_norm=14.44 | sec/step~0.90 | keep=1.00 | K=4 | llama: tf=8.1944 first=7.1126 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=7.1629 first=7.0363 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
  step  334/334 | grad_norm=19.02 | sec/step~0.67 | keep=1.00 | K=4 | llama: tf=9.0140 first=6.9358 kCE=0.0000 KD=0.0000 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=7.6126 first=7.1328 kCE=0.0000 KD=0.0000 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~1.0002 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 1.7KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/scoped_softprompt_20250921_145505/ckpt/stageB
📝 Saved Prefix-Tuning adapters for Llama
📝 Saved Prefix-Tuning adapters for Qwen
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0002491287440953, 'rms_mean_cal': 0.01057133936876721, 'embed_rms': 0.01057521253824234, 'count': 2672}, 'qwen': {'rms_mean_raw': 1.0001685324900165, 'rms_mean_cal': 0.013640910707007046, 'embed_rms': 0.013643525540828705, 'count': 2672}}

=== Stage B -> refresh merged weights ===


=== Stage B -> restore LoRA adapters ===


=== Stage C: Eval ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/scoped_softprompt_20250921_145505/ckpt/stageB/training_stats.json
Building encoder and computing Z...

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2610.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.39s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[runs/scoped_softprompt_20250921_145505/ckpt/merged_llama] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3759.18it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[runs/scoped_softprompt_20250921_145505/ckpt/merged_qwen] hf_device_map: {'model.embed_tokens': 2, 'model.layers.0': 2, 'model.layers.1': 2, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 2, 'model.layers.5': 2, 'model.layers.6': 2, 'model.layers.7': 2, 'model.layers.8': 2, 'model.layers.9': 2, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 3, 'model.layers.16': 3, 'model.layers.17': 3, 'model.layers.18': 3, 'model.layers.19': 3, 'model.layers.20': 3, 'model.layers.21': 3, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
✓ Loaded Prefix-Tuning adapters for llama
✓ Loaded Prefix-Tuning adapters for qwen
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 1000  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 244.6 | (Qwen): 231.0 | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): 3.6x
Approx interlingua payload per example: 13312000 bytes (6-bit selected); fp16 reference: 32768000 bytes; fp32 reference: 65536000 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.428864790224976
Qwen   EM: 0.000   F1: 0.001   |  NLL/token (gold): 7.35328809342839
Wall clock: 78.75s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 9.513510367294108
       First-token acc: top1=0.048  top5=0.069
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 8.131046659855386
       First-token acc: top1=0.075  top5=0.170
Wall clock: 46.09s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Qwen   EM: 0.000   F1: 0.000
Wall clock: 48.80s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 1000,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 244.558,
    "qwen": 231.026
  },
  "compression": {
    "llama": 3.82121875,
    "qwen": 3.60978125
  },
  "payload_bytes": 13312000,
  "payload_bytes_detail": {
    "fp32": 65536000,
    "fp16": 32768000,
    "selected": 13312000
  },
  "wire": {
    "prompt_chars": {
      "llama": 1243778,
      "qwen": 1091778
    },
    "prompt_count": 1000,
    "latent_shape": [
      1000,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 65536000,
      "fp16": 32768000,
      "quantized": 12288000,
      "quantized_with_scales": 13312000
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 13312000,
    "base_latent_bytes": 65536,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 10.428864790224976
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0006666666622222223,
      "nll_token": 7.35328809342839
    },
    "wall_clock_sec": 78.7497968673706
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.513510367294108,
      "first_token_top1": 0.048,
      "first_token_top5": 0.069,
      "nll_token": 9.513510367294108
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 8.131046659855386,
      "first_token_top1": 0.075,
      "first_token_top5": 0.17,
      "nll_token": 8.131046659855386
    },
    "wall_clock_sec": 46.09353590011597
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 48.79520320892334
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.0,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
