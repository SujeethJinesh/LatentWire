Auto-detected device: mps
Using fp16 precision on MPS for memory efficiency
Loading Z from runs/squad_m16_small_20250909_225821/squad_eval/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [llama] text: 8/200 | 2.13 ex/s | elapsed=3.8s | eta=1m30s
  [llama] text: 16/200 | 2.11 ex/s | elapsed=7.6s | eta=1m27s
  [llama] text: 24/200 | 2.37 ex/s | elapsed=10.1s | eta=1m14s
  [llama] text: 32/200 | 2.55 ex/s | elapsed=12.5s | eta=1m05s
  [llama] text: 40/200 | 2.63 ex/s | elapsed=15.2s | eta=1m00s
  [llama] text: 48/200 | 2.76 ex/s | elapsed=17.4s | eta=55.1s
  [llama] text: 56/200 | 2.85 ex/s | elapsed=19.6s | eta=50.5s
  [llama] text: 64/200 | 2.93 ex/s | elapsed=21.8s | eta=46.4s
  [llama] text: 72/200 | 2.94 ex/s | elapsed=24.5s | eta=43.5s
  [llama] text: 80/200 | 2.97 ex/s | elapsed=27.0s | eta=40.4s
  [llama] text: 88/200 | 2.98 ex/s | elapsed=29.5s | eta=37.5s
  [llama] text: 96/200 | 2.92 ex/s | elapsed=32.9s | eta=35.6s
  [llama] text: 104/200 | 2.97 ex/s | elapsed=35.0s | eta=32.3s
  [llama] text: 112/200 | 2.98 ex/s | elapsed=37.6s | eta=29.5s
  [llama] text: 120/200 | 2.97 ex/s | elapsed=40.3s | eta=26.9s
  [llama] text: 128/200 | 3.05 ex/s | elapsed=42.0s | eta=23.6s
  [llama] text: 136/200 | 3.08 ex/s | elapsed=44.1s | eta=20.8s
  [llama] text: 144/200 | 3.13 ex/s | elapsed=46.0s | eta=17.9s
  [llama] text: 152/200 | 3.12 ex/s | elapsed=48.6s | eta=15.4s
  [llama] text: 160/200 | 3.14 ex/s | elapsed=50.9s | eta=12.7s
  [llama] text: 168/200 | 3.16 ex/s | elapsed=53.1s | eta=10.1s
  [llama] text: 176/200 | 3.16 ex/s | elapsed=55.7s | eta=7.6s
  [llama] text: 184/200 | 3.15 ex/s | elapsed=58.3s | eta=5.1s
  [llama] text: 192/200 | 3.18 ex/s | elapsed=1m00s | eta=2.5s
  [llama] text: 200/200 | 3.14 ex/s | elapsed=1m03s | eta=0.0s
[debug:llama] adapter.scale=0.1961 | Z.std=1.0121 Z.mean||=16.1931 | prefix.std=0.0077 prefix.mean||=0.3472
  [llama] latent: 8/200 | 3.86 ex/s | elapsed=2.1s | eta=49.7s
  [llama] latent: 16/200 | 4.73 ex/s | elapsed=3.4s | eta=38.9s
  [llama] latent: 24/200 | 5.20 ex/s | elapsed=4.6s | eta=33.8s
  [llama] latent: 32/200 | 5.50 ex/s | elapsed=5.8s | eta=30.5s
  [llama] latent: 40/200 | 5.66 ex/s | elapsed=7.1s | eta=28.2s
  [llama] latent: 48/200 | 5.80 ex/s | elapsed=8.3s | eta=26.2s
  [llama] latent: 56/200 | 5.87 ex/s | elapsed=9.5s | eta=24.5s
  [llama] latent: 64/200 | 5.96 ex/s | elapsed=10.7s | eta=22.8s
  [llama] latent: 72/200 | 5.98 ex/s | elapsed=12.0s | eta=21.4s
  [llama] latent: 80/200 | 6.04 ex/s | elapsed=13.2s | eta=19.9s
  [llama] latent: 88/200 | 6.10 ex/s | elapsed=14.4s | eta=18.4s
  [llama] latent: 96/200 | 6.15 ex/s | elapsed=15.6s | eta=16.9s
  [llama] latent: 104/200 | 6.19 ex/s | elapsed=16.8s | eta=15.5s
  [llama] latent: 112/200 | 6.22 ex/s | elapsed=18.0s | eta=14.2s
  [llama] latent: 120/200 | 6.24 ex/s | elapsed=19.2s | eta=12.8s
  [llama] latent: 128/200 | 6.25 ex/s | elapsed=20.5s | eta=11.5s
  [llama] latent: 136/200 | 6.24 ex/s | elapsed=21.8s | eta=10.3s
  [llama] latent: 144/200 | 6.26 ex/s | elapsed=23.0s | eta=8.9s
  [llama] latent: 152/200 | 6.28 ex/s | elapsed=24.2s | eta=7.6s
  [llama] latent: 160/200 | 6.31 ex/s | elapsed=25.4s | eta=6.3s
  [llama] latent: 168/200 | 6.33 ex/s | elapsed=26.6s | eta=5.1s
  [llama] latent: 176/200 | 6.33 ex/s | elapsed=27.8s | eta=3.8s
  [llama] latent: 184/200 | 6.35 ex/s | elapsed=29.0s | eta=2.5s
  [llama] latent: 192/200 | 6.37 ex/s | elapsed=30.2s | eta=1.3s
  [llama] latent: 200/200 | 6.38 ex/s | elapsed=31.3s | eta=0.0s
  [llama] text: 8/200 | 4.88 ex/s | elapsed=1.6s | eta=39.3s
  [llama] text: 16/200 | 6.01 ex/s | elapsed=2.7s | eta=30.6s
  [llama] text: 24/200 | 6.57 ex/s | elapsed=3.7s | eta=26.8s
  [llama] text: 32/200 | 6.88 ex/s | elapsed=4.7s | eta=24.4s
  [llama] text: 40/200 | 7.05 ex/s | elapsed=5.7s | eta=22.7s
  [llama] text: 48/200 | 7.16 ex/s | elapsed=6.7s | eta=21.2s
  [llama] text: 56/200 | 7.26 ex/s | elapsed=7.7s | eta=19.8s
  [llama] text: 64/200 | 7.35 ex/s | elapsed=8.7s | eta=18.5s
  [llama] text: 72/200 | 7.40 ex/s | elapsed=9.7s | eta=17.3s
  [llama] text: 80/200 | 7.46 ex/s | elapsed=10.7s | eta=16.1s
  [llama] text: 88/200 | 7.50 ex/s | elapsed=11.7s | eta=14.9s
  [llama] text: 96/200 | 7.51 ex/s | elapsed=12.8s | eta=13.8s
  [llama] text: 104/200 | 7.54 ex/s | elapsed=13.8s | eta=12.7s
  [llama] text: 112/200 | 7.57 ex/s | elapsed=14.8s | eta=11.6s
  [llama] text: 120/200 | 7.58 ex/s | elapsed=15.8s | eta=10.5s
  [llama] text: 128/200 | 7.61 ex/s | elapsed=16.8s | eta=9.5s
  [llama] text: 136/200 | 7.63 ex/s | elapsed=17.8s | eta=8.4s
  [llama] text: 144/200 | 7.65 ex/s | elapsed=18.8s | eta=7.3s
  [llama] text: 152/200 | 7.67 ex/s | elapsed=19.8s | eta=6.3s
  [llama] text: 160/200 | 7.68 ex/s | elapsed=20.8s | eta=5.2s
  [llama] text: 168/200 | 7.69 ex/s | elapsed=21.8s | eta=4.2s
  [llama] text: 176/200 | 7.70 ex/s | elapsed=22.9s | eta=3.1s
  [llama] text: 184/200 | 7.72 ex/s | elapsed=23.8s | eta=2.1s
  [llama] text: 192/200 | 7.71 ex/s | elapsed=24.9s | eta=1.0s
  [llama] text: 200/200 | 7.72 ex/s | elapsed=25.9s | eta=0.0s
Saved Llama results to runs/squad_m16_small_20250909_225821/squad_eval/llama_results.json

Evaluating Qwen...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [qwen] text: 8/200 | 1.73 ex/s | elapsed=4.6s | eta=1m50s
  [qwen] text: 16/200 | 1.98 ex/s | elapsed=8.1s | eta=1m33s
  [qwen] text: 24/200 | 2.16 ex/s | elapsed=11.1s | eta=1m21s
  [qwen] text: 32/200 | 2.28 ex/s | elapsed=14.1s | eta=1m13s
  [qwen] text: 40/200 | 2.33 ex/s | elapsed=17.2s | eta=1m08s
  [qwen] text: 48/200 | 2.41 ex/s | elapsed=19.9s | eta=1m03s
  [qwen] text: 56/200 | 2.52 ex/s | elapsed=22.3s | eta=57.3s
  [qwen] text: 64/200 | 2.57 ex/s | elapsed=24.9s | eta=53.0s
  [qwen] text: 72/200 | 2.56 ex/s | elapsed=28.1s | eta=50.0s
  [qwen] text: 80/200 | 2.56 ex/s | elapsed=31.3s | eta=46.9s
  [qwen] text: 88/200 | 2.68 ex/s | elapsed=32.8s | eta=41.8s
  [qwen] text: 96/200 | 2.65 ex/s | elapsed=36.3s | eta=39.3s
  [qwen] text: 104/200 | 2.64 ex/s | elapsed=39.3s | eta=36.3s
  [qwen] text: 112/200 | 2.63 ex/s | elapsed=42.5s | eta=33.4s
  [qwen] text: 120/200 | 2.66 ex/s | elapsed=45.2s | eta=30.1s
  [qwen] text: 128/200 | 2.68 ex/s | elapsed=47.7s | eta=26.8s
  [qwen] text: 136/200 | 2.69 ex/s | elapsed=50.6s | eta=23.8s
  [qwen] text: 144/200 | 2.70 ex/s | elapsed=53.3s | eta=20.7s
  [qwen] text: 152/200 | 2.72 ex/s | elapsed=55.8s | eta=17.6s
  [qwen] text: 160/200 | 2.77 ex/s | elapsed=57.8s | eta=14.4s
  [qwen] text: 168/200 | 2.78 ex/s | elapsed=1m00s | eta=11.5s
  [qwen] text: 176/200 | 2.79 ex/s | elapsed=1m03s | eta=8.6s
  [qwen] text: 184/200 | 2.79 ex/s | elapsed=1m05s | eta=5.7s
  [qwen] text: 192/200 | 2.79 ex/s | elapsed=1m08s | eta=2.9s
  [qwen] text: 200/200 | 2.76 ex/s | elapsed=1m12s | eta=0.0s
[debug:qwen] adapter.scale=0.1431 | Z.std=1.0121 Z.mean||=16.1931 | prefix.std=0.0084 prefix.mean||=0.2513
  [qwen] latent: 8/200 | 2.53 ex/s | elapsed=3.2s | eta=1m15s
  [qwen] latent: 16/200 | 3.59 ex/s | elapsed=4.5s | eta=51.2s
  [qwen] latent: 24/200 | 4.18 ex/s | elapsed=5.7s | eta=42.1s
  [qwen] latent: 32/200 | 4.55 ex/s | elapsed=7.0s | eta=36.9s
  [qwen] latent: 40/200 | 4.79 ex/s | elapsed=8.3s | eta=33.4s
  [qwen] latent: 48/200 | 4.99 ex/s | elapsed=9.6s | eta=30.5s
  [qwen] latent: 56/200 | 5.14 ex/s | elapsed=10.9s | eta=28.0s
  [qwen] latent: 64/200 | 5.28 ex/s | elapsed=12.1s | eta=25.8s
  [qwen] latent: 72/200 | 5.37 ex/s | elapsed=13.4s | eta=23.8s
  [qwen] latent: 80/200 | 5.42 ex/s | elapsed=14.8s | eta=22.2s
  [qwen] latent: 88/200 | 5.46 ex/s | elapsed=16.1s | eta=20.5s
  [qwen] latent: 96/200 | 5.50 ex/s | elapsed=17.4s | eta=18.9s
  [qwen] latent: 104/200 | 5.56 ex/s | elapsed=18.7s | eta=17.3s
  [qwen] latent: 112/200 | 5.59 ex/s | elapsed=20.0s | eta=15.7s
  [qwen] latent: 120/200 | 5.62 ex/s | elapsed=21.3s | eta=14.2s
  [qwen] latent: 128/200 | 5.65 ex/s | elapsed=22.6s | eta=12.7s
  [qwen] latent: 136/200 | 5.68 ex/s | elapsed=23.9s | eta=11.3s
  [qwen] latent: 144/200 | 5.72 ex/s | elapsed=25.2s | eta=9.8s
  [qwen] latent: 152/200 | 5.75 ex/s | elapsed=26.4s | eta=8.4s
  [qwen] latent: 160/200 | 5.78 ex/s | elapsed=27.7s | eta=6.9s
  [qwen] latent: 168/200 | 5.80 ex/s | elapsed=29.0s | eta=5.5s
  [qwen] latent: 176/200 | 5.82 ex/s | elapsed=30.3s | eta=4.1s
  [qwen] latent: 184/200 | 5.84 ex/s | elapsed=31.5s | eta=2.7s
  [qwen] latent: 192/200 | 5.85 ex/s | elapsed=32.8s | eta=1.4s
  [qwen] latent: 200/200 | 5.86 ex/s | elapsed=34.1s | eta=0.0s
  [qwen] text: 8/200 | 2.74 ex/s | elapsed=2.9s | eta=1m10s
  [qwen] text: 16/200 | 4.03 ex/s | elapsed=4.0s | eta=45.7s
  [qwen] text: 24/200 | 4.83 ex/s | elapsed=5.0s | eta=36.4s
  [qwen] text: 32/200 | 5.38 ex/s | elapsed=5.9s | eta=31.2s
  [qwen] text: 40/200 | 5.70 ex/s | elapsed=7.0s | eta=28.1s
  [qwen] text: 48/200 | 5.88 ex/s | elapsed=8.2s | eta=25.9s
  [qwen] text: 56/200 | 6.10 ex/s | elapsed=9.2s | eta=23.6s
  [qwen] text: 64/200 | 6.28 ex/s | elapsed=10.2s | eta=21.7s
  [qwen] text: 72/200 | 6.42 ex/s | elapsed=11.2s | eta=19.9s
  [qwen] text: 80/200 | 6.55 ex/s | elapsed=12.2s | eta=18.3s
  [qwen] text: 88/200 | 6.65 ex/s | elapsed=13.2s | eta=16.8s
  [qwen] text: 96/200 | 6.76 ex/s | elapsed=14.2s | eta=15.4s
  [qwen] text: 104/200 | 6.82 ex/s | elapsed=15.2s | eta=14.1s
  [qwen] text: 112/200 | 6.90 ex/s | elapsed=16.2s | eta=12.8s
  [qwen] text: 120/200 | 6.96 ex/s | elapsed=17.2s | eta=11.5s
  [qwen] text: 128/200 | 7.02 ex/s | elapsed=18.2s | eta=10.2s
  [qwen] text: 136/200 | 7.08 ex/s | elapsed=19.2s | eta=9.0s
  [qwen] text: 144/200 | 7.13 ex/s | elapsed=20.2s | eta=7.9s
  [qwen] text: 152/200 | 7.18 ex/s | elapsed=21.2s | eta=6.7s
  [qwen] text: 160/200 | 7.22 ex/s | elapsed=22.2s | eta=5.5s
  [qwen] text: 168/200 | 7.25 ex/s | elapsed=23.2s | eta=4.4s
  [qwen] text: 176/200 | 7.27 ex/s | elapsed=24.2s | eta=3.3s
  [qwen] text: 184/200 | 7.30 ex/s | elapsed=25.2s | eta=2.2s
  [qwen] text: 192/200 | 7.33 ex/s | elapsed=26.2s | eta=1.1s
  [qwen] text: 200/200 | 7.35 ex/s | elapsed=27.2s | eta=0.0s
Saved Qwen results to runs/squad_m16_small_20250909_225821/squad_eval/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: mps  |  Dtype: torch.float16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16)

— Baseline: Text prompting
Llama  EM: 0.005  F1: 0.057  |  NLL/token (gold): 15.681
Qwen   EM: 0.230   F1: 0.396   |  NLL/token (gold): 13.044
Wall clock: 136.05s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 8.880
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 10.285
Wall clock: 65.44s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.033
Qwen   EM: 0.000   F1: 0.052
Wall clock: 53.12s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.960
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "mps",
  "dtype": "torch.float16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    }
  },
  "text": {
    "llama": {
      "em": 0.005,
      "f1": 0.056785177241059595,
      "nll_token": 15.681324996504673
    },
    "qwen": {
      "em": 0.23,
      "f1": 0.39575623885918004,
      "nll_token": 13.044476828877889
    },
    "wall_clock_sec": 136.05494093894958
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 8.87982312268989
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 10.28483846042522
    },
    "wall_clock_sec": 65.43791604042053
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0,
      "f1": 0.03346892873208663
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.051748848373848376
    },
    "wall_clock_sec": 53.117454051971436
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.96,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 0.19612836837768555,
      "Z_std": 1.012070894241333,
      "Z_mean_norm": 16.193071365356445,
      "prefix_std": 0.007699538487941027,
      "prefix_mean_norm": 0.3471503257751465
    },
    "qwen": {
      "adapter_scale": 0.14308753609657288,
      "Z_std": 1.012070894241333,
      "Z_mean_norm": 16.193071365356445,
      "prefix_std": 0.00839131511747837,
      "prefix_mean_norm": 0.2512787878513336
    },
    "latent_anchor_text": "Answer: "
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_small_20250909_225821/squad_eval/predictions.jsonl
