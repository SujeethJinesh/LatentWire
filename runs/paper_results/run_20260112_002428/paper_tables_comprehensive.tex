% Table 1: Main Results - Method Comparison
\begin{table*}[t]
\centering
\caption{Classification accuracy (\%) on benchmark datasets. Results show mean $\pm$ std over 3 random seeds.
Bold indicates best result. Statistical significance vs. Bridge (8 tokens): *p<0.05, **p<0.01, ***p<0.001 (Bonferroni-corrected).}
\label{tab:main_results}
\begin{tabular}{lcccccc}
\toprule
Method & SST-2 & AG News & TREC & Avg & Params \\
\midrule
\textit{Random Chance} & 50.0 & 25.0 & 16.7 & 30.6 & -- \\
\midrule
\textbf{Individual Models (Zero-shot)} \\
\quad Llama-8B & 83.8 $\pm$ 0.0 & 71.0 $\pm$ 0.0 & 48.2 $\pm$ 0.0 & 67.7 & 8B \\
\quad Mistral-7B & 89.6 $\pm$ 0.0 & 71.1 $\pm$ 0.0 & 50.6 $\pm$ 0.0 & 70.4 & 7B \\
\midrule
\textbf{Text-Relay} \\
\quad Llama $\rightarrow$ text $\rightarrow$ Mistral & 41.3 $\pm$ 0.0 & 1.0 $\pm$ 0.0 & 4.0 $\pm$ 0.0 & 15.4 & 15B \\
\midrule
\textbf{Fine-tuning Baselines} \\
\quad Linear Probe (Llama layer-16) & 92.1 $\pm$ 0.0 & 86.8 $\pm$ 0.0 & 95.0 $\pm$ 0.0 & 91.3 & 33M \\
\quad Prompt Tuning (Mistral only) & 50.9 $\pm$ 0.0 & 25.0 $\pm$ 0.0 & 15.9 $\pm$ 4.1 & 30.6 & 33K \\
\quad LoRA (Mistral, rank-8) & 90.1 $\pm$ 6.0 & 40.8 $\pm$ 22.3 & 26.4 $\pm$ 27.4 & 52.4 & 4.2M \\
\midrule
\textbf{Telepathy Bridge (Ours)} \\
\quad 8 soft tokens & 93.3 $\pm$ 0.2 & 91.1 $\pm$ 1.1 & 61.5 $\pm$ 23.3 & 82.0 & 33.6M \\
\quad 32 soft tokens & 93.7 $\pm$ 0.3 & 90.7 $\pm$ 1.4 & 87.9 $\pm$ 3.3 & 90.7 & 33.8M \\
\bottomrule
\end{tabular}
\end{table*}


% Table 2: Token Count Ablation
\begin{table}[t]
\centering
\caption{Effect of soft token count on Bridge accuracy (\%). More tokens generally improve performance at the cost of higher bandwidth.}
\label{tab:token_ablation}
\begin{tabular}{lccc|c}
\toprule
Tokens & SST-2 & AG News & TREC & Compression \\
\midrule

8 & 93.3 & 91.1 & 61.5 & 0.04x \\

32 & 93.7 & 90.7 & 87.9 & 0.01x \\

\bottomrule
\end{tabular}
\end{table}


% Table 3: Fair Latency Comparison
\begin{table}[t]
\centering
\caption{End-to-end classification latency (ms). Both Bridge and Text-Relay perform the same task (Llama encodes, Mistral classifies). Speedup is computed as Text-Relay / Bridge.}
\label{tab:latency}
\begin{tabular}{lccc}
\toprule
Method & Latency (ms) & Task & Speedup \\
\midrule

Mistral Direct & 156.2 & Classify & 1.0x (baseline) \\

Text-Relay & 795.2 & Summarize + Classify & 0.20x \\

Bridge (Ours) & 170.6 & Encode + Classify & 4.66x \\

\bottomrule
\end{tabular}
\end{table}
