
=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 3147.70it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:02,  1.49s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.34s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.10s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
‚ö†Ô∏è  No valid checkpoint found to resume; starting fresh.
Epoch 1/4
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/40 | grad_norm=493.11 | sec/step~1.89 | keep=0.70 | K=4 | first_w=2.50 | llama(L): tf=11.7474 first=19.1619 kCE=9.4908 KD=6.5835 state=26.1982 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=111.51 | sec/step~2.12 | keep=0.70 | K=4 | first_w=2.50 | llama(L): tf=10.7615 first=15.4651 kCE=8.7537 KD=9.0103 state=26.7605 align=0.0000 | scale_pen(llama)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=408.23 | sec/step~2.26 | keep=0.71 | K=4 | first_w=2.50 | llama(L): tf=10.4712 first=15.3379 kCE=8.0188 KD=8.0567 state=26.6356 align=0.0000 | scale_pen(llama)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=184.64 | sec/step~2.10 | keep=0.72 | K=4 | first_w=2.50 | llama(L): tf=10.4992 first=11.9764 kCE=8.5410 KD=9.2954 state=25.5769 align=0.0000 | scale_pen(llama)=1.5667e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 2/4
  step  10/40 | grad_norm=65.29 | sec/step~2.09 | keep=0.73 | K=4 | first_w=2.50 | llama(L): tf=9.8637 first=10.3424 kCE=8.7036 KD=9.3509 state=25.3244 align=0.0000 | scale_pen(llama)=1.5667e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=18.54 | sec/step~2.70 | keep=0.74 | K=4 | first_w=2.46 | llama(L): tf=9.9139 first=9.7938 kCE=9.5244 KD=8.6952 state=26.7608 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=61.41 | sec/step~2.29 | keep=0.76 | K=4 | first_w=2.37 | llama(L): tf=10.1723 first=9.4564 kCE=9.4185 KD=9.4537 state=25.8235 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=26.25 | sec/step~1.79 | keep=0.77 | K=4 | first_w=2.23 | llama(L): tf=9.9386 first=9.4418 kCE=9.5704 KD=9.6684 state=23.1994 align=0.0000 | scale_pen(llama)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 3/4
  step  10/40 | grad_norm=28.37 | sec/step~2.13 | keep=0.79 | K=4 | first_w=2.06 | llama(L): tf=9.8221 first=8.8282 kCE=9.0104 KD=9.5315 state=24.3222 align=0.0000 | scale_pen(llama)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=10.14 | sec/step~2.15 | keep=0.82 | K=4 | first_w=1.85 | llama(L): tf=9.6935 first=8.3098 kCE=8.9780 KD=9.7681 state=23.6344 align=0.0000 | scale_pen(llama)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=23.88 | sec/step~1.94 | keep=0.84 | K=4 | first_w=1.65 | llama(L): tf=10.1526 first=8.5003 kCE=8.8750 KD=9.1988 state=23.4474 align=0.0000 | scale_pen(llama)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=11.93 | sec/step~2.71 | keep=0.87 | K=4 | first_w=1.44 | llama(L): tf=9.4230 first=9.0318 kCE=8.2580 KD=7.8933 state=24.5706 align=0.0000 | scale_pen(llama)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 4/4
  step  10/40 | grad_norm=12.10 | sec/step~2.09 | keep=0.90 | K=4 | first_w=1.27 | llama(L): tf=9.4403 first=8.0795 kCE=8.4498 KD=8.8743 state=21.5095 align=0.0000 | scale_pen(llama)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=6.81 | sec/step~2.14 | keep=0.93 | K=4 | first_w=1.13 | llama(L): tf=9.6093 first=7.9336 kCE=8.7212 KD=9.3895 state=20.2595 align=0.0000 | scale_pen(llama)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=19.37 | sec/step~1.87 | keep=0.96 | K=4 | first_w=1.04 | llama(L): tf=9.6411 first=8.2692 kCE=9.1740 KD=9.5365 state=20.3217 align=0.0000 | scale_pen(llama)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=9.06 | sec/step~2.03 | keep=1.00 | K=4 | first_w=1.00 | llama(L): tf=10.0281 first=7.9977 kCE=8.3781 KD=8.6263 state=20.7893 align=0.0000 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 1.8KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250924_171128/ckpt/stageA
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0000039506703615, 'rms_mean_cal': 0.010571266739862039, 'embed_rms': 0.01057521253824234, 'count': 160}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2977.32it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:05,  1.68s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.08it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.12s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 272,723,968 || all params: 8,302,985,216 || trainable%: 3.2846
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
‚è™ Resuming from: runs/llama_single_20250924_171128/ckpt/stageA/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 80 steps
Epoch 1/6
[warmup] step=0 mode=text
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/80 | (warm-up text) | align=0.0001 | text_tf=9.3322
[warmup] step=1 mode=latent
[warmup] step=2 mode=text
  step  3/80 | (warm-up text) | align=0.0001 | text_tf=9.6052
[warmup] step=3 mode=latent
[warmup] step=4 mode=text
  step  5/80 | (warm-up text) | align=0.0001 | text_tf=11.1511
[warmup] step=5 mode=latent
[warmup] step=6 mode=text
  step  7/80 | (warm-up text) | align=0.0001 | text_tf=10.7789
[warmup] step=7 mode=latent
[warmup] step=8 mode=text
  step  9/80 | (warm-up text) | align=0.0001 | text_tf=11.1129
[warmup] step=9 mode=latent
  step  10/80 | grad_norm=92.99 | sec/step~2.73 | keep=0.50 | K=4 | first_w=2.20 | llama(L): tf=11.0368 first=14.9156 kCE=10.0074 KD=2.9430 state=26.3209 align=0.0000 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  11/80 | (warm-up text) | align=0.0001 | text_tf=9.9096
  step  13/80 | (warm-up text) | align=0.0001 | text_tf=8.9452
  step  15/80 | (warm-up text) | align=0.0001 | text_tf=10.3443
  step  17/80 | (warm-up text) | align=0.0001 | text_tf=7.6076
  step  19/80 | (warm-up text) | align=0.0001 | text_tf=7.5153
  step  20/80 | grad_norm=5.26 | sec/step~2.39 | keep=0.50 | K=4 | first_w=2.20 | llama(L): tf=10.0845 first=10.6610 kCE=9.7451 KD=1.8772 state=23.6337 align=0.0000 | scale_pen(llama)=6.8781e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  21/80 | (warm-up text) | align=0.0001 | text_tf=8.3794
  step  23/80 | (warm-up text) | align=0.0001 | text_tf=9.3902
  step  25/80 | (warm-up text) | align=0.0001 | text_tf=9.9502
  step  27/80 | (warm-up text) | align=0.0001 | text_tf=9.4472
  step  29/80 | (warm-up text) | align=0.0001 | text_tf=8.9702
  step  30/80 | grad_norm=17.59 | sec/step~2.71 | keep=0.50 | K=4 | first_w=2.20 | llama(L): tf=10.6261 first=10.1734 kCE=9.7622 KD=1.6517 state=24.1333 align=0.0000 | scale_pen(llama)=6.8781e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  31/80 | (warm-up text) | align=0.0001 | text_tf=9.0036
  step  33/80 | (warm-up text) | align=0.0001 | text_tf=8.5289
  step  35/80 | (warm-up text) | align=0.0001 | text_tf=7.9959
  step  37/80 | (warm-up text) | align=0.0001 | text_tf=8.6163
  step  39/80 | (warm-up text) | align=0.0001 | text_tf=8.7370
  step  40/80 | grad_norm=6.01 | sec/step~2.25 | keep=0.50 | K=4 | first_w=2.20 | llama(L): tf=10.4080 first=8.8924 kCE=9.4526 KD=1.0545 state=18.3211 align=0.0000 | scale_pen(llama)=1.8468e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  41/80 | (warm-up text) | align=0.0001 | text_tf=8.1490
  step  43/80 | (warm-up text) | align=0.0001 | text_tf=8.4145
  step  45/80 | (warm-up text) | align=0.0001 | text_tf=7.4844
  step  47/80 | (warm-up text) | align=0.0001 | text_tf=8.2674
  step  49/80 | (warm-up text) | align=0.0001 | text_tf=8.6336
  step  50/80 | grad_norm=1.31 | sec/step~2.37 | keep=0.51 | K=4 | first_w=2.20 | llama(L): tf=9.4220 first=7.7288 kCE=9.3092 KD=0.8473 state=13.2893 align=0.0000 | scale_pen(llama)=1.1768e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  51/80 | (warm-up text) | align=0.0001 | text_tf=8.1377
  step  53/80 | (warm-up text) | align=0.0001 | text_tf=8.1539
  step  55/80 | (warm-up text) | align=0.0001 | text_tf=9.6590
  step  57/80 | (warm-up text) | align=0.0001 | text_tf=8.3231
  step  59/80 | (warm-up text) | align=0.0001 | text_tf=9.1805
  step  60/80 | grad_norm=7.43 | sec/step~2.31 | keep=0.51 | K=4 | first_w=2.20 | llama(L): tf=10.2860 first=8.5215 kCE=9.7782 KD=1.0638 state=12.7896 align=0.0000 | scale_pen(llama)=1.1768e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  61/80 | (warm-up text) | align=0.0001 | text_tf=9.0207
  step  63/80 | (warm-up text) | align=0.0001 | text_tf=8.4895
  step  65/80 | (warm-up text) | align=0.0001 | text_tf=7.8614
  step  67/80 | (warm-up text) | align=0.0001 | text_tf=7.6326
  step  69/80 | (warm-up text) | align=0.0001 | text_tf=9.1145
  step  70/80 | grad_norm=6.86 | sec/step~2.65 | keep=0.51 | K=4 | first_w=2.20 | llama(L): tf=9.3739 first=8.0433 kCE=8.8757 KD=1.0237 state=12.5795 align=0.0000 | scale_pen(llama)=6.3793e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  71/80 | (warm-up text) | align=0.0001 | text_tf=9.0641
  step  73/80 | (warm-up text) | align=0.0001 | text_tf=8.5556
  step  75/80 | (warm-up text) | align=0.0001 | text_tf=7.6748
  step  77/80 | (warm-up text) | align=0.0001 | text_tf=7.9156
  step  79/80 | (warm-up text) | align=0.0001 | text_tf=8.1949
  step  80/80 | grad_norm=17.73 | sec/step~2.51 | keep=0.51 | K=4 | first_w=2.20 | llama(L): tf=9.5438 first=7.8984 kCE=8.9979 KD=1.1099 state=12.5215 align=0.0000 | scale_pen(llama)=1.5667e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
Epoch 2/6
  step  10/80 | grad_norm=17.76 | sec/step~2.79 | keep=0.52 | K=4 | first_w=2.20 | llama(L): tf=8.7553 first=7.8212 kCE=8.2123 KD=0.8109 state=13.4423 align=0.0000 | scale_pen(llama)=1.5667e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/80 | grad_norm=2.26 | sec/step~3.01 | keep=0.52 | K=4 | first_w=2.20 | llama(L): tf=8.8261 first=6.8271 kCE=8.0302 KD=0.9781 state=12.9794 align=0.0000 | scale_pen(llama)=7.1637e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/80 | grad_norm=6.86 | sec/step~2.49 | keep=0.53 | K=4 | first_w=2.20 | llama(L): tf=9.5122 first=8.4871 kCE=8.1311 KD=1.1366 state=11.7784 align=0.0000 | scale_pen(llama)=7.1637e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/80 | grad_norm=7.45 | sec/step~2.32 | keep=0.53 | K=4 | first_w=2.20 | llama(L): tf=9.4560 first=8.3973 kCE=7.3342 KD=1.2553 state=11.7018 align=0.0000 | scale_pen(llama)=1.7408e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/80 | grad_norm=0.75 | sec/step~2.69 | keep=0.54 | K=4 | first_w=2.20 | llama(L): tf=9.2900 first=8.4242 kCE=6.3713 KD=1.0863 state=12.5619 align=0.0000 | scale_pen(llama)=4.9468e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/80 | grad_norm=2.79 | sec/step~2.29 | keep=0.54 | K=4 | first_w=2.20 | llama(L): tf=8.8411 first=7.3108 kCE=6.9022 KD=1.5425 state=10.0845 align=0.0000 | scale_pen(llama)=4.9468e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  70/80 | grad_norm=6.03 | sec/step~2.28 | keep=0.55 | K=4 | first_w=2.20 | llama(L): tf=8.9873 first=6.3970 kCE=6.5422 KD=1.3067 state=11.1102 align=0.0000 | scale_pen(llama)=3.4120e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  80/80 | grad_norm=15.83 | sec/step~2.73 | keep=0.56 | K=4 | first_w=2.20 | llama(L): tf=8.5690 first=8.0187 kCE=6.1029 KD=1.0563 state=12.2046 align=0.0000 | scale_pen(llama)=2.1325e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
Epoch 3/6
  step  10/80 | grad_norm=3.06 | sec/step~2.30 | keep=0.56 | K=4 | first_w=2.19 | llama(L): tf=8.8836 first=8.4120 kCE=5.9596 KD=1.3757 state=10.2948 align=0.0000 | scale_pen(llama)=2.1325e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/80 | grad_norm=1.74 | sec/step~2.30 | keep=0.57 | K=4 | first_w=2.17 | llama(L): tf=8.6905 first=7.5670 kCE=5.4940 KD=1.5484 state=9.7884 align=0.0000 | scale_pen(llama)=6.3793e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/80 | grad_norm=3.45 | sec/step~2.66 | keep=0.58 | K=4 | first_w=2.16 | llama(L): tf=8.5061 first=7.2289 kCE=5.0875 KD=1.1326 state=12.0972 align=0.0000 | scale_pen(llama)=6.3793e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/80 | grad_norm=5.49 | sec/step~2.92 | keep=0.59 | K=4 | first_w=2.14 | llama(L): tf=8.5832 first=7.7561 kCE=5.6208 KD=1.1976 state=11.9507 align=0.0000 | scale_pen(llama)=5.1159e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/80 | grad_norm=4.21 | sec/step~2.68 | keep=0.60 | K=4 | first_w=2.11 | llama(L): tf=9.2367 first=7.8304 kCE=5.6598 KD=1.0636 state=12.1720 align=0.0000 | scale_pen(llama)=2.9878e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/80 | grad_norm=22.57 | sec/step~2.52 | keep=0.60 | K=4 | first_w=2.08 | llama(L): tf=8.6360 first=7.7606 kCE=5.5935 KD=1.0207 state=11.6065 align=0.0000 | scale_pen(llama)=2.9878e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  70/80 | grad_norm=4.74 | sec/step~2.29 | keep=0.61 | K=4 | first_w=2.05 | llama(L): tf=8.4549 first=7.2006 kCE=5.4883 KD=1.2193 state=9.8503 align=0.0000 | scale_pen(llama)=5.4037e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  80/80 | grad_norm=15.83 | sec/step~2.69 | keep=0.62 | K=4 | first_w=2.02 | llama(L): tf=8.5688 first=7.1725 kCE=5.3873 KD=1.1178 state=11.4110 align=0.0000 | scale_pen(llama)=4.4521e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
Epoch 4/6
  step  10/80 | grad_norm=3.39 | sec/step~2.52 | keep=0.64 | K=4 | first_w=1.98 | llama(L): tf=8.4754 first=7.0387 kCE=5.4562 KD=1.1201 state=11.5549 align=0.0000 | scale_pen(llama)=4.4521e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  20/80 | grad_norm=6.83 | sec/step~3.01 | keep=0.65 | K=4 | first_w=1.94 | llama(L): tf=8.0637 first=7.0105 kCE=5.1080 KD=1.0980 state=12.3996 align=0.0000 | scale_pen(llama)=1.0360e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  30/80 | grad_norm=21.16 | sec/step~2.50 | keep=0.66 | K=4 | first_w=1.90 | llama(L): tf=8.7435 first=6.9589 kCE=5.6586 KD=1.1677 state=11.3508 align=0.0000 | scale_pen(llama)=1.0360e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  40/80 | grad_norm=2.40 | sec/step~2.64 | keep=0.67 | K=4 | first_w=1.85 | llama(L): tf=8.3694 first=7.8408 kCE=5.1780 KD=1.1229 state=11.6252 align=0.0000 | scale_pen(llama)=1.8146e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  50/80 | grad_norm=1.20 | sec/step~2.66 | keep=0.68 | K=4 | first_w=1.81 | llama(L): tf=8.4017 first=7.5388 kCE=5.7073 KD=1.2148 state=10.9875 align=0.0000 | scale_pen(llama)=1.2825e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  60/80 | grad_norm=6.21 | sec/step~2.76 | keep=0.69 | K=4 | first_w=1.76 | llama(L): tf=8.2413 first=7.2710 kCE=5.9306 KD=1.1895 state=10.4242 align=0.0000 | scale_pen(llama)=1.2825e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  step  70/80 | grad_norm=2.62 | sec/step~2.64 | keep=0.71 | K=4 | first_w=1.71 | llama(L): tf=8.7296 first=6.7599 kCE=5.1865 KD=1.0987 state=11.4702 align=0.0000 | scale_pen(llama)=3.1392e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/80 | grad_norm=8.74 | sec/step~2.52 | keep=0.72 | K=4 | first_w=1.67 | llama(L): tf=8.3720 first=7.0879 kCE=5.6281 KD=1.1866 state=10.9161 align=0.0000 | scale_pen(llama)=2.0464e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
Epoch 5/6
  step  10/80 | grad_norm=1.16 | sec/step~2.44 | keep=0.74 | K=4 | first_w=1.62 | llama(L): tf=8.2566 first=7.6917 kCE=4.8231 KD=1.3234 state=10.2551 align=0.0000 | scale_pen(llama)=2.0464e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  20/80 | grad_norm=3.95 | sec/step~2.83 | keep=0.75 | K=4 | first_w=1.58 | llama(L): tf=8.1585 first=6.8281 kCE=5.0135 KD=1.1942 state=11.9113 align=0.0000 | scale_pen(llama)=2.8777e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  30/80 | grad_norm=16.72 | sec/step~3.28 | keep=0.77 | K=4 | first_w=1.53 | llama(L): tf=8.1086 first=7.3641 kCE=4.7902 KD=1.2070 state=12.3461 align=0.0000 | scale_pen(llama)=2.8777e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/80 | grad_norm=2.21 | sec/step~2.59 | keep=0.78 | K=4 | first_w=1.49 | llama(L): tf=8.2531 first=7.0244 kCE=4.8863 KD=1.1436 state=10.8371 align=0.0000 | scale_pen(llama)=6.5711e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/80 | grad_norm=0.45 | sec/step~2.43 | keep=0.80 | K=4 | first_w=1.45 | llama(L): tf=8.5262 first=6.9322 kCE=5.2054 KD=1.1685 state=9.7771 align=0.0000 | scale_pen(llama)=2.2737e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/80 | grad_norm=1.39 | sec/step~2.58 | keep=0.81 | K=4 | first_w=1.41 | llama(L): tf=8.1093 first=7.5147 kCE=4.4073 KD=1.1956 state=10.9012 align=0.0000 | scale_pen(llama)=2.2737e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/80 | grad_norm=0.75 | sec/step~2.65 | keep=0.83 | K=4 | first_w=1.37 | llama(L): tf=8.4034 first=7.4110 kCE=4.4634 KD=1.0737 state=11.5542 align=0.0000 | scale_pen(llama)=8.5301e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/80 | grad_norm=1.46 | sec/step~2.43 | keep=0.85 | K=4 | first_w=1.34 | llama(L): tf=8.0529 first=7.0520 kCE=4.7370 KD=1.2231 state=10.2435 align=0.0000 | scale_pen(llama)=1.6914e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
Epoch 6/6
  step  10/80 | grad_norm=2.75 | sec/step~2.47 | keep=0.86 | K=4 | first_w=1.31 | llama(L): tf=8.5683 first=7.4834 kCE=4.8505 KD=1.1760 state=10.6686 align=0.0000 | scale_pen(llama)=1.6914e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  20/80 | grad_norm=0.58 | sec/step~2.29 | keep=0.88 | K=4 | first_w=1.28 | llama(L): tf=8.2788 first=7.5210 kCE=5.1076 KD=1.1121 state=9.6850 align=0.0000 | scale_pen(llama)=8.1855e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  30/80 | grad_norm=1.17 | sec/step~2.75 | keep=0.90 | K=4 | first_w=1.26 | llama(L): tf=7.8485 first=6.6909 kCE=4.9289 KD=0.9878 state=11.6847 align=0.0000 | scale_pen(llama)=8.1855e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  40/80 | grad_norm=5.64 | sec/step~2.58 | keep=0.92 | K=4 | first_w=1.24 | llama(L): tf=8.3683 first=7.4015 kCE=4.8740 KD=0.9724 state=10.7658 align=0.0000 | scale_pen(llama)=3.8426e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  50/80 | grad_norm=0.54 | sec/step~2.62 | keep=0.94 | K=4 | first_w=1.22 | llama(L): tf=7.9692 first=7.1984 kCE=4.9896 KD=1.0198 state=10.8007 align=0.0000 | scale_pen(llama)=1.6270e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  60/80 | grad_norm=1.95 | sec/step~2.27 | keep=0.96 | K=4 | first_w=1.21 | llama(L): tf=8.0472 first=6.5057 kCE=5.0646 KD=1.1188 state=9.3039 align=0.0000 | scale_pen(llama)=1.6270e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  70/80 | grad_norm=3.94 | sec/step~2.22 | keep=0.98 | K=4 | first_w=1.20 | llama(L): tf=8.3905 first=7.4240 kCE=4.6553 KD=1.1878 state=9.1662 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  step  80/80 | grad_norm=7.57 | sec/step~2.40 | keep=1.00 | K=4 | first_w=1.20 | llama(L): tf=8.3392 first=6.9609 kCE=5.2174 KD=1.1046 state=9.6694 align=0.0000 | scale_pen(llama)=1.8417e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 2.2KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250924_171128/ckpt/stageB
üìù Saved Prefix-Tuning adapters for Llama
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0001924994091194, 'rms_mean_cal': 0.010571490156386669, 'embed_rms': 0.010575395077466965, 'count': 480}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/llama_single_20250924_171128/ckpt/stageB/training_stats.json
Building encoder and computing Z...
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/eval.py", line 1471, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/eval.py", line 1310, in main
    encoder_wire.load_state_dict(_safe_load(os.path.join(ckpt_dir, "encoder.pt"), map_location=device))
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for STQueryEncoder:
	size mismatch for query: copying a param with shape torch.Size([64, 384]) from checkpoint, the shape in current model is torch.Size([72, 384]).
