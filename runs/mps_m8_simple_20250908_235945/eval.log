Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:15,  7.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:08,  8.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.61s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:29<00:00,  7.37s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Traceback (most recent call last):
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/eval.py", line 319, in <module>
    main()
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/eval.py", line 113, in main
    llama_text_preds = generate_text_baseline(llama, llama_chat_prompts, max_new_tokens=args.max_new_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/eval.py", line 23, in generate_text_baseline
    out_ids = wrapper.generate_from_text_manual(prompts, max_new_tokens=max_new_tokens, temperature=0.0)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/models.py", line 320, in generate_from_text_manual
    out = self.model(input_ids=input_ids, attention_mask=attn_mask, use_cache=True, return_dict=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: MPS backend out of memory (MPS allocated: 69.45 GB, other allocations: 688.00 KB, max allowed: 81.60 GB). Tried to allocate 14.09 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).
