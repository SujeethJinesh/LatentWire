Auto-detected device: mps
Using fp16 precision on MPS for memory efficiency
Loading Z from runs/squad_qwen_only_m16_20250910_013432/squad_eval/Z.pt

[Sequential Evaluation Mode - one model at a time]

ℹ️  Llama adapter missing at runs/squad_qwen_only_m16_20250910_013432/ckpt/adapter_llama.pt — evaluating TEXT + TOKEN-BUDGET only for Llama.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [llama] text: 8/200 | 2.13 ex/s | elapsed=3.8s | eta=1m30s
  [llama] text: 16/200 | 2.11 ex/s | elapsed=7.6s | eta=1m27s
  [llama] text: 24/200 | 2.35 ex/s | elapsed=10.2s | eta=1m14s
  [llama] text: 32/200 | 2.53 ex/s | elapsed=12.7s | eta=1m06s
  [llama] text: 40/200 | 2.60 ex/s | elapsed=15.4s | eta=1m01s
  [llama] text: 48/200 | 2.74 ex/s | elapsed=17.5s | eta=55.5s
  [llama] text: 56/200 | 2.83 ex/s | elapsed=19.8s | eta=50.9s
  [llama] text: 64/200 | 2.91 ex/s | elapsed=22.0s | eta=46.7s
  [llama] text: 72/200 | 2.92 ex/s | elapsed=24.7s | eta=43.8s
  [llama] text: 80/200 | 2.96 ex/s | elapsed=27.0s | eta=40.5s
  [llama] text: 88/200 | 2.98 ex/s | elapsed=29.5s | eta=37.6s
  [llama] text: 96/200 | 2.92 ex/s | elapsed=32.9s | eta=35.7s
  [llama] text: 104/200 | 2.95 ex/s | elapsed=35.2s | eta=32.5s
  [llama] text: 112/200 | 2.96 ex/s | elapsed=37.8s | eta=29.7s
  [llama] text: 120/200 | 2.96 ex/s | elapsed=40.6s | eta=27.0s
  [llama] text: 128/200 | 3.03 ex/s | elapsed=42.2s | eta=23.8s
  [llama] text: 136/200 | 3.06 ex/s | elapsed=44.4s | eta=20.9s
  [llama] text: 144/200 | 3.11 ex/s | elapsed=46.4s | eta=18.0s
  [llama] text: 152/200 | 3.10 ex/s | elapsed=49.0s | eta=15.5s
  [llama] text: 160/200 | 3.13 ex/s | elapsed=51.2s | eta=12.8s
  [llama] text: 168/200 | 3.15 ex/s | elapsed=53.4s | eta=10.2s
  [llama] text: 176/200 | 3.15 ex/s | elapsed=55.9s | eta=7.6s
  [llama] text: 184/200 | 3.14 ex/s | elapsed=58.6s | eta=5.1s
  [llama] text: 192/200 | 3.16 ex/s | elapsed=1m00s | eta=2.5s
  [llama] text: 200/200 | 3.13 ex/s | elapsed=1m03s | eta=0.0s
  [llama] text: 8/200 | 5.11 ex/s | elapsed=1.6s | eta=37.6s
  [llama] text: 16/200 | 6.21 ex/s | elapsed=2.6s | eta=29.6s
  [llama] text: 24/200 | 6.71 ex/s | elapsed=3.6s | eta=26.2s
  [llama] text: 32/200 | 6.96 ex/s | elapsed=4.6s | eta=24.1s
  [llama] text: 40/200 | 6.96 ex/s | elapsed=5.7s | eta=23.0s
  [llama] text: 48/200 | 7.08 ex/s | elapsed=6.8s | eta=21.5s
  [llama] text: 56/200 | 7.18 ex/s | elapsed=7.8s | eta=20.0s
  [llama] text: 64/200 | 7.27 ex/s | elapsed=8.8s | eta=18.7s
  [llama] text: 72/200 | 7.33 ex/s | elapsed=9.8s | eta=17.5s
  [llama] text: 80/200 | 7.39 ex/s | elapsed=10.8s | eta=16.2s
  [llama] text: 88/200 | 7.43 ex/s | elapsed=11.8s | eta=15.1s
  [llama] text: 96/200 | 7.46 ex/s | elapsed=12.9s | eta=13.9s
  [llama] text: 104/200 | 7.49 ex/s | elapsed=13.9s | eta=12.8s
  [llama] text: 112/200 | 7.52 ex/s | elapsed=14.9s | eta=11.7s
  [llama] text: 120/200 | 7.54 ex/s | elapsed=15.9s | eta=10.6s
  [llama] text: 128/200 | 7.57 ex/s | elapsed=16.9s | eta=9.5s
  [llama] text: 136/200 | 7.58 ex/s | elapsed=17.9s | eta=8.4s
  [llama] text: 144/200 | 7.60 ex/s | elapsed=19.0s | eta=7.4s
  [llama] text: 152/200 | 7.61 ex/s | elapsed=20.0s | eta=6.3s
  [llama] text: 160/200 | 7.62 ex/s | elapsed=21.0s | eta=5.2s
  [llama] text: 168/200 | 7.63 ex/s | elapsed=22.0s | eta=4.2s
  [llama] text: 176/200 | 7.63 ex/s | elapsed=23.1s | eta=3.1s
  [llama] text: 184/200 | 7.63 ex/s | elapsed=24.1s | eta=2.1s
  [llama] text: 192/200 | 7.63 ex/s | elapsed=25.2s | eta=1.0s
  [llama] text: 200/200 | 7.60 ex/s | elapsed=26.3s | eta=0.0s
Saved Llama results to runs/squad_qwen_only_m16_20250910_013432/squad_eval/llama_results.json

Evaluating Qwen...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [qwen] text: 8/200 | 2.04 ex/s | elapsed=3.9s | eta=1m34s
  [qwen] text: 16/200 | 2.16 ex/s | elapsed=7.4s | eta=1m25s
  [qwen] text: 24/200 | 2.29 ex/s | elapsed=10.5s | eta=1m16s
  [qwen] text: 32/200 | 2.41 ex/s | elapsed=13.3s | eta=1m09s
  [qwen] text: 40/200 | 2.43 ex/s | elapsed=16.4s | eta=1m05s
  [qwen] text: 48/200 | 2.49 ex/s | elapsed=19.3s | eta=1m01s
  [qwen] text: 56/200 | 2.58 ex/s | elapsed=21.7s | eta=55.7s
  [qwen] text: 64/200 | 2.63 ex/s | elapsed=24.4s | eta=51.8s
  [qwen] text: 72/200 | 2.62 ex/s | elapsed=27.4s | eta=48.8s
  [qwen] text: 80/200 | 2.62 ex/s | elapsed=30.5s | eta=45.7s
  [qwen] text: 88/200 | 2.75 ex/s | elapsed=31.9s | eta=40.7s
  [qwen] text: 96/200 | 2.71 ex/s | elapsed=35.4s | eta=38.4s
  [qwen] text: 104/200 | 2.70 ex/s | elapsed=38.5s | eta=35.6s
  [qwen] text: 112/200 | 2.69 ex/s | elapsed=41.6s | eta=32.7s
  [qwen] text: 120/200 | 2.71 ex/s | elapsed=44.2s | eta=29.5s
  [qwen] text: 128/200 | 2.74 ex/s | elapsed=46.6s | eta=26.2s
  [qwen] text: 136/200 | 2.75 ex/s | elapsed=49.5s | eta=23.3s
  [qwen] text: 144/200 | 2.76 ex/s | elapsed=52.1s | eta=20.3s
  [qwen] text: 152/200 | 2.78 ex/s | elapsed=54.6s | eta=17.2s
  [qwen] text: 160/200 | 2.83 ex/s | elapsed=56.5s | eta=14.1s
  [qwen] text: 168/200 | 2.84 ex/s | elapsed=59.1s | eta=11.3s
  [qwen] text: 176/200 | 2.85 ex/s | elapsed=1m01s | eta=8.4s
  [qwen] text: 184/200 | 2.87 ex/s | elapsed=1m04s | eta=5.6s
  [qwen] text: 192/200 | 2.86 ex/s | elapsed=1m07s | eta=2.8s
  [qwen] text: 200/200 | 2.84 ex/s | elapsed=1m10s | eta=0.0s
[debug:qwen] adapter.scale=0.1593 | Z.std=1.0296 Z.mean||=16.4733 | prefix.std=0.0166 prefix.mean||=0.4539
  [qwen] latent: 8/200 | 2.36 ex/s | elapsed=3.4s | eta=1m21s
  [qwen] latent: 16/200 | 3.44 ex/s | elapsed=4.6s | eta=53.4s
  [qwen] latent: 24/200 | 4.07 ex/s | elapsed=5.9s | eta=43.2s
  [qwen] latent: 32/200 | 4.48 ex/s | elapsed=7.1s | eta=37.5s
  [qwen] latent: 40/200 | 4.75 ex/s | elapsed=8.4s | eta=33.7s
  [qwen] latent: 48/200 | 4.97 ex/s | elapsed=9.7s | eta=30.6s
  [qwen] latent: 56/200 | 5.12 ex/s | elapsed=10.9s | eta=28.1s
  [qwen] latent: 64/200 | 5.24 ex/s | elapsed=12.2s | eta=26.0s
  [qwen] latent: 72/200 | 5.34 ex/s | elapsed=13.5s | eta=24.0s
  [qwen] latent: 80/200 | 5.43 ex/s | elapsed=14.7s | eta=22.1s
  [qwen] latent: 88/200 | 5.50 ex/s | elapsed=16.0s | eta=20.4s
  [qwen] latent: 96/200 | 5.57 ex/s | elapsed=17.2s | eta=18.7s
  [qwen] latent: 104/200 | 5.63 ex/s | elapsed=18.5s | eta=17.1s
  [qwen] latent: 112/200 | 5.68 ex/s | elapsed=19.7s | eta=15.5s
  [qwen] latent: 120/200 | 5.72 ex/s | elapsed=21.0s | eta=14.0s
  [qwen] latent: 128/200 | 5.76 ex/s | elapsed=22.2s | eta=12.5s
  [qwen] latent: 136/200 | 5.80 ex/s | elapsed=23.5s | eta=11.0s
  [qwen] latent: 144/200 | 5.83 ex/s | elapsed=24.7s | eta=9.6s
  [qwen] latent: 152/200 | 5.86 ex/s | elapsed=26.0s | eta=8.2s
  [qwen] latent: 160/200 | 5.88 ex/s | elapsed=27.2s | eta=6.8s
  [qwen] latent: 168/200 | 5.91 ex/s | elapsed=28.4s | eta=5.4s
  [qwen] latent: 176/200 | 5.93 ex/s | elapsed=29.7s | eta=4.0s
  [qwen] latent: 184/200 | 5.95 ex/s | elapsed=30.9s | eta=2.7s
  [qwen] latent: 192/200 | 5.97 ex/s | elapsed=32.1s | eta=1.3s
  [qwen] latent: 200/200 | 5.99 ex/s | elapsed=33.4s | eta=0.0s
  [qwen] text: 8/200 | 2.75 ex/s | elapsed=2.9s | eta=1m09s
  [qwen] text: 16/200 | 4.07 ex/s | elapsed=3.9s | eta=45.2s
  [qwen] text: 24/200 | 4.89 ex/s | elapsed=4.9s | eta=36.0s
  [qwen] text: 32/200 | 5.43 ex/s | elapsed=5.9s | eta=31.0s
  [qwen] text: 40/200 | 5.82 ex/s | elapsed=6.9s | eta=27.5s
  [qwen] text: 48/200 | 6.11 ex/s | elapsed=7.9s | eta=24.9s
  [qwen] text: 56/200 | 6.33 ex/s | elapsed=8.8s | eta=22.7s
  [qwen] text: 64/200 | 6.51 ex/s | elapsed=9.8s | eta=20.9s
  [qwen] text: 72/200 | 6.67 ex/s | elapsed=10.8s | eta=19.2s
  [qwen] text: 80/200 | 6.79 ex/s | elapsed=11.8s | eta=17.7s
  [qwen] text: 88/200 | 6.89 ex/s | elapsed=12.8s | eta=16.2s
  [qwen] text: 96/200 | 6.99 ex/s | elapsed=13.7s | eta=14.9s
  [qwen] text: 104/200 | 7.07 ex/s | elapsed=14.7s | eta=13.6s
  [qwen] text: 112/200 | 7.14 ex/s | elapsed=15.7s | eta=12.3s
  [qwen] text: 120/200 | 7.20 ex/s | elapsed=16.7s | eta=11.1s
  [qwen] text: 128/200 | 7.25 ex/s | elapsed=17.7s | eta=9.9s
  [qwen] text: 136/200 | 7.30 ex/s | elapsed=18.6s | eta=8.8s
  [qwen] text: 144/200 | 7.34 ex/s | elapsed=19.6s | eta=7.6s
  [qwen] text: 152/200 | 7.38 ex/s | elapsed=20.6s | eta=6.5s
  [qwen] text: 160/200 | 7.42 ex/s | elapsed=21.6s | eta=5.4s
  [qwen] text: 168/200 | 7.45 ex/s | elapsed=22.5s | eta=4.3s
  [qwen] text: 176/200 | 7.48 ex/s | elapsed=23.5s | eta=3.2s
  [qwen] text: 184/200 | 7.47 ex/s | elapsed=24.6s | eta=2.1s
  [qwen] text: 192/200 | 7.49 ex/s | elapsed=25.6s | eta=1.1s
  [qwen] text: 200/200 | 7.52 ex/s | elapsed=26.6s | eta=0.0s
Saved Qwen results to runs/squad_qwen_only_m16_20250910_013432/squad_eval/qwen_results.json

Joint rescoring...
ℹ️  Joint rescoring skipped — both adapters required.

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: mps  |  Dtype: torch.float16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16)

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.055  |  NLL/token (gold): 15.681
Qwen   EM: 0.065   F1: 0.298   |  NLL/token (gold): 13.044
Wall clock: 134.31s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): None
Qwen   EM: 0.000   F1: 0.002   |  NLL/token (gold): 9.144535482560515
Wall clock: 33.40s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.033
Qwen   EM: 0.000   F1: 0.053
Wall clock: 52.94s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.002

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "mps",
  "dtype": "torch.float16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    }
  },
  "text": {
    "llama": {
      "em": 0.0,
      "f1": 0.055118510574392925,
      "nll_token": 15.681324996504673
    },
    "qwen": {
      "em": 0.065,
      "f1": 0.29778076988371094,
      "nll_token": 13.044476828877889
    },
    "wall_clock_sec": 134.30719113349915
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": null
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0023333333333333335,
      "nll_token": 9.144535482560515
    },
    "wall_clock_sec": 33.39684820175171
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0,
      "f1": 0.03346892873208663
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.053228646353646346
    },
    "wall_clock_sec": 52.94118309020996
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {},
    "qwen": {
      "adapter_scale": 0.1593082845211029,
      "Z_std": 1.029585599899292,
      "Z_mean_norm": 16.473276138305664,
      "prefix_std": 0.01658441126346588,
      "prefix_mean_norm": 0.45389628410339355
    },
    "latent_anchor_text": "Answer: "
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0023333333333333335
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_qwen_only_m16_20250910_013432/squad_eval/predictions.jsonl
