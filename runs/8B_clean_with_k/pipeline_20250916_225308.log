
=========================================
Starting pipeline at Tue Sep 16 22:53:08 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 24 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_with_k/ckpt


=========================================
EPOCH 1/24
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8046.63it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.61s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5853.88it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: concatenation produced no new tokens
[WARN] t=0 alignment failed: concatenation produced no new tokens
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_clean_with_k/ckpt/state.pt
   -> failed to load weights from state.pt (Error(s) in loading state_dict for InterlinguaEncoder:
	Missing key(s) in state_dict: "backbone.encoder.layers.2.self_attn.in_proj_weight", "backbone.encoder.layers.2.self_attn.in_proj_bias", "backbone.encoder.layers.2.self_attn.out_proj.weight", "backbone.encoder.layers.2.self_attn.out_proj.bias", "backbone.encoder.layers.2.linear1.weight", "backbone.encoder.layers.2.linear1.bias", "backbone.encoder.layers.2.linear2.weight", "backbone.encoder.layers.2.linear2.bias", "backbone.encoder.layers.2.norm1.weight", "backbone.encoder.layers.2.norm1.bias", "backbone.encoder.layers.2.norm2.weight", "backbone.encoder.layers.2.norm2.bias", "backbone.encoder.layers.3.self_attn.in_proj_weight", "backbone.encoder.layers.3.self_attn.in_proj_bias", "backbone.encoder.layers.3.self_attn.out_proj.weight", "backbone.encoder.layers.3.self_attn.out_proj.bias", "backbone.encoder.layers.3.linear1.weight", "backbone.encoder.layers.3.linear1.bias", "backbone.encoder.layers.3.linear2.weight", "backbone.encoder.layers.3.linear2.bias", "backbone.encoder.layers.3.norm1.weight", "backbone.encoder.layers.3.norm1.bias", "backbone.encoder.layers.3.norm2.weight", "backbone.encoder.layers.3.norm2.bias", "backbone.encoder.layers.4.self_attn.in_proj_weight", "backbone.encoder.layers.4.self_attn.in_proj_bias", "backbone.encoder.layers.4.self_attn.out_proj.weight", "backbone.encoder.layers.4.self_attn.out_proj.bias", "backbone.encoder.layers.4.linear1.weight", "backbone.encoder.layers.4.linear1.bias", "backbone.encoder.layers.4.linear2.weight", "backbone.encoder.layers.4.linear2.bias", "backbone.encoder.layers.4.norm1.weight", "backbone.encoder.layers.4.norm1.bias", "backbone.encoder.layers.4.norm2.weight", "backbone.encoder.layers.4.norm2.bias", "backbone.encoder.layers.5.self_attn.in_proj_weight", "backbone.encoder.layers.5.self_attn.in_proj_bias", "backbone.encoder.layers.5.self_attn.out_proj.weight", "backbone.encoder.layers.5.self_attn.out_proj.bias", "backbone.encoder.layers.5.linear1.weight", "backbone.encoder.layers.5.linear1.bias", "backbone.encoder.layers.5.linear2.weight", "backbone.encoder.layers.5.linear2.bias", "backbone.encoder.layers.5.norm1.weight", "backbone.encoder.layers.5.norm1.bias", "backbone.encoder.layers.5.norm2.weight", "backbone.encoder.layers.5.norm2.bias", "shared_pooler.latent", "shared_pooler.cross_attn.in_proj_weight", "shared_pooler.cross_attn.in_proj_bias", "shared_pooler.cross_attn.out_proj.weight", "shared_pooler.cross_attn.out_proj.bias", "shared_pooler.ff.0.weight", "shared_pooler.ff.0.bias", "shared_pooler.ff.1.weight", "shared_pooler.ff.1.bias", "shared_pooler.ff.3.weight", "shared_pooler.ff.3.bias", "shared_pooler.gate.0.weight", "shared_pooler.gate.0.bias", "shared_pooler.gate.1.weight", "shared_pooler.gate.1.bias", "private_poolers.llama.latent", "private_poolers.llama.cross_attn.in_proj_weight", "private_poolers.llama.cross_attn.in_proj_bias", "private_poolers.llama.cross_attn.out_proj.weight", "private_poolers.llama.cross_attn.out_proj.bias", "private_poolers.llama.ff.0.weight", "private_poolers.llama.ff.0.bias", "private_poolers.llama.ff.1.weight", "private_poolers.llama.ff.1.bias", "private_poolers.llama.ff.3.weight", "private_poolers.llama.ff.3.bias", "private_poolers.llama.gate.0.weight", "private_poolers.llama.gate.0.bias", "private_poolers.llama.gate.1.weight", "private_poolers.llama.gate.1.bias", "private_poolers.qwen.latent", "private_poolers.qwen.cross_attn.in_proj_weight", "private_poolers.qwen.cross_attn.in_proj_bias", "private_poolers.qwen.cross_attn.out_proj.weight", "private_poolers.qwen.cross_attn.out_proj.bias", "private_poolers.qwen.ff.0.weight", "private_poolers.qwen.ff.0.bias", "private_poolers.qwen.ff.1.weight", "private_poolers.qwen.ff.1.bias", "private_poolers.qwen.ff.3.weight", "private_poolers.qwen.ff.3.bias", "private_poolers.qwen.gate.0.weight", "private_poolers.qwen.gate.0.bias", "private_poolers.qwen.gate.1.weight", "private_poolers.qwen.gate.1.bias". 
	Unexpected key(s) in state_dict: "pooler.latent", "pooler.cross_attn.in_proj_weight", "pooler.cross_attn.in_proj_bias", "pooler.cross_attn.out_proj.weight", "pooler.cross_attn.out_proj.bias", "pooler.ff.0.weight", "pooler.ff.0.bias", "pooler.ff.1.weight", "pooler.ff.1.bias", "pooler.ff.3.weight", "pooler.ff.3.bias". ); will try .pt files
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1080, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 603, in main
    epoch_loaded, global_loaded = load_checkpoint(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 124, in load_checkpoint
    encoder.load_state_dict(_safe_load(enc_path, map_location=device), strict=strict)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for InterlinguaEncoder:
	Missing key(s) in state_dict: "backbone.encoder.layers.2.self_attn.in_proj_weight", "backbone.encoder.layers.2.self_attn.in_proj_bias", "backbone.encoder.layers.2.self_attn.out_proj.weight", "backbone.encoder.layers.2.self_attn.out_proj.bias", "backbone.encoder.layers.2.linear1.weight", "backbone.encoder.layers.2.linear1.bias", "backbone.encoder.layers.2.linear2.weight", "backbone.encoder.layers.2.linear2.bias", "backbone.encoder.layers.2.norm1.weight", "backbone.encoder.layers.2.norm1.bias", "backbone.encoder.layers.2.norm2.weight", "backbone.encoder.layers.2.norm2.bias", "backbone.encoder.layers.3.self_attn.in_proj_weight", "backbone.encoder.layers.3.self_attn.in_proj_bias", "backbone.encoder.layers.3.self_attn.out_proj.weight", "backbone.encoder.layers.3.self_attn.out_proj.bias", "backbone.encoder.layers.3.linear1.weight", "backbone.encoder.layers.3.linear1.bias", "backbone.encoder.layers.3.linear2.weight", "backbone.encoder.layers.3.linear2.bias", "backbone.encoder.layers.3.norm1.weight", "backbone.encoder.layers.3.norm1.bias", "backbone.encoder.layers.3.norm2.weight", "backbone.encoder.layers.3.norm2.bias", "backbone.encoder.layers.4.self_attn.in_proj_weight", "backbone.encoder.layers.4.self_attn.in_proj_bias", "backbone.encoder.layers.4.self_attn.out_proj.weight", "backbone.encoder.layers.4.self_attn.out_proj.bias", "backbone.encoder.layers.4.linear1.weight", "backbone.encoder.layers.4.linear1.bias", "backbone.encoder.layers.4.linear2.weight", "backbone.encoder.layers.4.linear2.bias", "backbone.encoder.layers.4.norm1.weight", "backbone.encoder.layers.4.norm1.bias", "backbone.encoder.layers.4.norm2.weight", "backbone.encoder.layers.4.norm2.bias", "backbone.encoder.layers.5.self_attn.in_proj_weight", "backbone.encoder.layers.5.self_attn.in_proj_bias", "backbone.encoder.layers.5.self_attn.out_proj.weight", "backbone.encoder.layers.5.self_attn.out_proj.bias", "backbone.encoder.layers.5.linear1.weight", "backbone.encoder.layers.5.linear1.bias", "backbone.encoder.layers.5.linear2.weight", "backbone.encoder.layers.5.linear2.bias", "backbone.encoder.layers.5.norm1.weight", "backbone.encoder.layers.5.norm1.bias", "backbone.encoder.layers.5.norm2.weight", "backbone.encoder.layers.5.norm2.bias", "shared_pooler.latent", "shared_pooler.cross_attn.in_proj_weight", "shared_pooler.cross_attn.in_proj_bias", "shared_pooler.cross_attn.out_proj.weight", "shared_pooler.cross_attn.out_proj.bias", "shared_pooler.ff.0.weight", "shared_pooler.ff.0.bias", "shared_pooler.ff.1.weight", "shared_pooler.ff.1.bias", "shared_pooler.ff.3.weight", "shared_pooler.ff.3.bias", "shared_pooler.gate.0.weight", "shared_pooler.gate.0.bias", "shared_pooler.gate.1.weight", "shared_pooler.gate.1.bias", "private_poolers.llama.latent", "private_poolers.llama.cross_attn.in_proj_weight", "private_poolers.llama.cross_attn.in_proj_bias", "private_poolers.llama.cross_attn.out_proj.weight", "private_poolers.llama.cross_attn.out_proj.bias", "private_poolers.llama.ff.0.weight", "private_poolers.llama.ff.0.bias", "private_poolers.llama.ff.1.weight", "private_poolers.llama.ff.1.bias", "private_poolers.llama.ff.3.weight", "private_poolers.llama.ff.3.bias", "private_poolers.llama.gate.0.weight", "private_poolers.llama.gate.0.bias", "private_poolers.llama.gate.1.weight", "private_poolers.llama.gate.1.bias", "private_poolers.qwen.latent", "private_poolers.qwen.cross_attn.in_proj_weight", "private_poolers.qwen.cross_attn.in_proj_bias", "private_poolers.qwen.cross_attn.out_proj.weight", "private_poolers.qwen.cross_attn.out_proj.bias", "private_poolers.qwen.ff.0.weight", "private_poolers.qwen.ff.0.bias", "private_poolers.qwen.ff.1.weight", "private_poolers.qwen.ff.1.bias", "private_poolers.qwen.ff.3.weight", "private_poolers.qwen.ff.3.bias", "private_poolers.qwen.gate.0.weight", "private_poolers.qwen.gate.0.bias", "private_poolers.qwen.gate.1.weight", "private_poolers.qwen.gate.1.bias". 
	Unexpected key(s) in state_dict: "pooler.latent", "pooler.cross_attn.in_proj_weight", "pooler.cross_attn.in_proj_bias", "pooler.cross_attn.out_proj.weight", "pooler.cross_attn.out_proj.bias", "pooler.ff.0.weight", "pooler.ff.0.bias", "pooler.ff.1.weight", "pooler.ff.1.bias", "pooler.ff.3.weight", "pooler.ff.3.bias". 
