
=========================================
Starting pipeline at Tue Sep 16 14:11:16 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 24 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_with_k/ckpt


=========================================
EPOCH 1/24
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2792.01it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2983.68it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment could not be verified: list index out of range
[WARN] t=0 alignment could not be verified: list index out of range
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=12.8004 first=0.0000 kCE=10.8209 KD=5.9002 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.7034 first=0.0000 kCE=12.6090 KD=4.0217 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5580 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5629 rms_cal~0.0136 embed_rms~0.01363]
  step  20/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=11.8342 first=0.0000 kCE=12.0536 KD=5.6329 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.1963 first=0.0000 kCE=11.5910 KD=4.1372 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5580 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5630 rms_cal~0.0136 embed_rms~0.01363]
  step  30/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=12.0449 first=0.0000 kCE=11.7253 KD=6.2800 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.5586 first=0.0000 kCE=12.5924 KD=3.5464 | scale_pen(qwen)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5580 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5631 rms_cal~0.0136 embed_rms~0.01363]
  step  40/5475 | grad_norm=nan | sec/step~2.64 | llama: tf=12.4534 first=0.0000 kCE=12.3557 KD=4.3241 | scale_pen(llama)=2.4475e-09 | qwen: tf=9.0131 first=0.0000 kCE=11.8797 KD=3.1947 | scale_pen(qwen)=2.4475e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5578 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5634 rms_cal~0.0136 embed_rms~0.01363]
  step  50/5475 | grad_norm=nan | sec/step~2.87 | llama: tf=11.4877 first=0.0000 kCE=12.0270 KD=4.6835 | scale_pen(llama)=2.4475e-09 | qwen: tf=9.9456 first=0.0000 kCE=10.1319 KD=4.7486 | scale_pen(qwen)=2.4475e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5579 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5634 rms_cal~0.0136 embed_rms~0.01363]
  step  60/5475 | grad_norm=nan | sec/step~2.97 | llama: tf=11.0686 first=0.0000 kCE=11.5942 KD=5.3207 | scale_pen(llama)=2.4475e-09 | qwen: tf=11.4896 first=0.0000 kCE=11.4981 KD=4.4357 | scale_pen(qwen)=2.4475e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5579 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5634 rms_cal~0.0136 embed_rms~0.01363]
  step  70/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=12.3274 first=0.0000 kCE=12.8588 KD=4.5832 | scale_pen(llama)=9.6254e-09 | qwen: tf=12.1836 first=0.0000 kCE=11.4784 KD=3.8297 | scale_pen(qwen)=2.4016e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5581 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5633 rms_cal~0.0136 embed_rms~0.01363]
  step  80/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=11.5664 first=0.0000 kCE=11.4404 KD=4.7891 | scale_pen(llama)=9.6254e-09 | qwen: tf=9.7605 first=0.0000 kCE=10.2192 KD=4.1938 | scale_pen(qwen)=2.4016e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5582 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5633 rms_cal~0.0136 embed_rms~0.01363]
  step  90/5475 | grad_norm=nan | sec/step~3.20 | llama: tf=12.2282 first=0.0000 kCE=12.0012 KD=4.1576 | scale_pen(llama)=9.6254e-09 | qwen: tf=11.1270 first=0.0000 kCE=11.6870 KD=3.7520 | scale_pen(qwen)=2.4016e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5583 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5633 rms_cal~0.0136 embed_rms~0.01363]
  step  100/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=12.1272 first=0.0000 kCE=10.3746 KD=3.8481 | scale_pen(llama)=2.1430e-08 | qwen: tf=10.5815 first=0.0000 kCE=10.7978 KD=3.4001 | scale_pen(qwen)=4.5533e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5585 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5632 rms_cal~0.0136 embed_rms~0.01363]
  step  110/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=12.2354 first=0.0000 kCE=11.4372 KD=4.6382 | scale_pen(llama)=2.1430e-08 | qwen: tf=11.6242 first=0.0000 kCE=12.5964 KD=4.2648 | scale_pen(qwen)=4.5533e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5587 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5630 rms_cal~0.0136 embed_rms~0.01363]
  step  120/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=11.5521 first=0.0000 kCE=11.4315 KD=3.7342 | scale_pen(llama)=2.1430e-08 | qwen: tf=10.7320 first=0.0000 kCE=11.5987 KD=3.3860 | scale_pen(qwen)=4.5533e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5589 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5629 rms_cal~0.0136 embed_rms~0.01363]
  step  130/5475 | grad_norm=nan | sec/step~2.29 | llama: tf=12.1411 first=0.0000 kCE=11.2913 KD=4.4049 | scale_pen(llama)=3.0085e-08 | qwen: tf=11.0778 first=0.0000 kCE=10.1362 KD=4.3625 | scale_pen(qwen)=2.5367e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5590 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5628 rms_cal~0.0136 embed_rms~0.01363]
  step  140/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=12.3956 first=0.0000 kCE=10.4661 KD=4.1392 | scale_pen(llama)=3.0085e-08 | qwen: tf=9.7241 first=0.0000 kCE=9.1576 KD=3.7075 | scale_pen(qwen)=2.5367e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5592 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5628 rms_cal~0.0136 embed_rms~0.01363]
  step  150/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=12.1864 first=0.0000 kCE=10.9455 KD=4.4935 | scale_pen(llama)=3.0085e-08 | qwen: tf=11.8053 first=0.0000 kCE=11.1094 KD=3.9009 | scale_pen(qwen)=2.5367e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5593 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  160/5475 | grad_norm=12.17 | sec/step~2.85 | llama: tf=11.3308 first=0.0000 kCE=11.0804 KD=4.3611 | scale_pen(llama)=4.1846e-08 | qwen: tf=10.9532 first=0.0000 kCE=9.7833 KD=4.4540 | scale_pen(qwen)=4.4565e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5594 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  170/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=12.4681 first=0.0000 kCE=10.5249 KD=4.4169 | scale_pen(llama)=4.1846e-08 | qwen: tf=11.2718 first=0.0000 kCE=10.5396 KD=4.1232 | scale_pen(qwen)=4.4565e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5595 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 172
  step  180/5475 | grad_norm=nan | sec/step~2.20 | llama: tf=11.8310 first=0.0000 kCE=11.5899 KD=4.6257 | scale_pen(llama)=4.1846e-08 | qwen: tf=10.8672 first=0.0000 kCE=10.0373 KD=4.2813 | scale_pen(qwen)=4.4565e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5597 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5626 rms_cal~0.0136 embed_rms~0.01363]
  step  190/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=12.6790 first=0.0000 kCE=10.7192 KD=4.7443 | scale_pen(llama)=4.1846e-08 | qwen: tf=10.2639 first=0.0000 kCE=10.2733 KD=3.8817 | scale_pen(qwen)=4.4565e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5598 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5626 rms_cal~0.0136 embed_rms~0.01363]
  step  200/5475 | grad_norm=nan | sec/step~2.20 | llama: tf=11.8687 first=0.0000 kCE=9.7376 KD=3.9329 | scale_pen(llama)=5.5656e-08 | qwen: tf=10.5137 first=0.0000 kCE=10.2336 KD=3.3967 | scale_pen(qwen)=5.4099e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5599 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  210/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=12.3618 first=0.0000 kCE=10.3690 KD=4.0077 | scale_pen(llama)=5.5656e-08 | qwen: tf=11.1915 first=0.0000 kCE=10.2502 KD=3.9455 | scale_pen(qwen)=5.4099e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5600 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  220/5475 | grad_norm=nan | sec/step~2.23 | llama: tf=12.5531 first=0.0000 kCE=10.0882 KD=3.9912 | scale_pen(llama)=5.5656e-08 | qwen: tf=10.7835 first=0.0000 kCE=10.3939 KD=3.8501 | scale_pen(qwen)=5.4099e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5600 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  230/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=12.1513 first=0.0000 kCE=10.8418 KD=4.5720 | scale_pen(llama)=6.4534e-08 | qwen: tf=10.8502 first=0.0000 kCE=10.5821 KD=3.8003 | scale_pen(qwen)=4.5848e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5601 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5627 rms_cal~0.0136 embed_rms~0.01363]
  step  240/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=13.8085 first=0.0000 kCE=9.9728 KD=4.7593 | scale_pen(llama)=6.4534e-08 | qwen: tf=14.0739 first=0.0000 kCE=11.2794 KD=4.0227 | scale_pen(qwen)=4.5848e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5601 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5628 rms_cal~0.0136 embed_rms~0.01363]
  step  250/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=13.0243 first=0.0000 kCE=10.4800 KD=4.8291 | scale_pen(llama)=6.4534e-08 | qwen: tf=10.3594 first=0.0000 kCE=10.5669 KD=3.8730 | scale_pen(qwen)=4.5848e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5602 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5628 rms_cal~0.0136 embed_rms~0.01363]
  step  260/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=13.1722 first=0.0000 kCE=10.9920 KD=5.5385 | scale_pen(llama)=6.8968e-08 | qwen: tf=10.1022 first=0.0000 kCE=9.7571 KD=4.0081 | scale_pen(qwen)=3.6457e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5603 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5629 rms_cal~0.0136 embed_rms~0.01363]
  step  270/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=10.9980 first=0.0000 kCE=11.1780 KD=5.2383 | scale_pen(llama)=6.8968e-08 | qwen: tf=9.0666 first=0.0000 kCE=8.6158 KD=4.0416 | scale_pen(qwen)=3.6457e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5603 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5630 rms_cal~0.0136 embed_rms~0.01363]
  step  280/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=11.4696 first=0.0000 kCE=11.3443 KD=4.7686 | scale_pen(llama)=6.8968e-08 | qwen: tf=9.1432 first=0.0000 kCE=8.1935 KD=4.0974 | scale_pen(qwen)=3.6457e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5603 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5631 rms_cal~0.0136 embed_rms~0.01363]
  step  290/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=12.8112 first=0.0000 kCE=10.2004 KD=4.4323 | scale_pen(llama)=6.5324e-08 | qwen: tf=11.6233 first=0.0000 kCE=10.1957 KD=3.6373 | scale_pen(qwen)=2.8713e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5632 rms_cal~0.0136 embed_rms~0.01363]
  step  300/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=12.6272 first=0.0000 kCE=9.9414 KD=4.9138 | scale_pen(llama)=6.5324e-08 | qwen: tf=12.3692 first=0.0000 kCE=10.7136 KD=4.3144 | scale_pen(qwen)=2.8713e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5633 rms_cal~0.0136 embed_rms~0.01363]
  step  310/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=11.9026 first=0.0000 kCE=10.5371 KD=4.3891 | scale_pen(llama)=6.5324e-08 | qwen: tf=10.9423 first=0.0000 kCE=9.1393 KD=4.0598 | scale_pen(qwen)=2.8713e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5634 rms_cal~0.0136 embed_rms~0.01363]
  step  320/5475 | grad_norm=58.57 | sec/step~2.69 | llama: tf=12.0307 first=0.0000 kCE=10.2084 KD=4.2137 | scale_pen(llama)=5.7528e-08 | qwen: tf=9.4777 first=0.0000 kCE=8.6791 KD=3.7650 | scale_pen(qwen)=1.5617e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5635 rms_cal~0.0136 embed_rms~0.01363]
  step  330/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=11.0175 first=0.0000 kCE=10.1160 KD=4.1687 | scale_pen(llama)=5.7528e-08 | qwen: tf=9.9460 first=0.0000 kCE=9.3593 KD=4.3994 | scale_pen(qwen)=1.5617e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5637 rms_cal~0.0136 embed_rms~0.01363]
  step  340/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=12.2347 first=0.0000 kCE=10.1887 KD=4.3553 | scale_pen(llama)=5.7528e-08 | qwen: tf=10.6972 first=0.0000 kCE=10.1833 KD=4.3936 | scale_pen(qwen)=1.5617e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5638 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 344
  step  350/5475 | grad_norm=nan | sec/step~3.15 | llama: tf=12.0776 first=0.0000 kCE=10.3066 KD=4.1348 | scale_pen(llama)=5.7528e-08 | qwen: tf=10.1052 first=0.0000 kCE=9.7686 KD=4.6486 | scale_pen(qwen)=1.5617e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5640 rms_cal~0.0136 embed_rms~0.01363]
  step  360/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=12.5923 first=0.0000 kCE=9.6965 KD=4.3775 | scale_pen(llama)=5.1355e-08 | qwen: tf=11.8836 first=0.0000 kCE=10.6010 KD=4.3144 | scale_pen(qwen)=7.1304e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5642 rms_cal~0.0136 embed_rms~0.01363]
  step  370/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=11.0804 first=0.0000 kCE=10.9143 KD=3.9703 | scale_pen(llama)=5.1355e-08 | qwen: tf=9.4558 first=0.0000 kCE=9.1655 KD=3.7506 | scale_pen(qwen)=7.1304e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5643 rms_cal~0.0136 embed_rms~0.01363]
  step  380/5475 | grad_norm=nan | sec/step~3.22 | llama: tf=11.9419 first=0.0000 kCE=10.2086 KD=3.9358 | scale_pen(llama)=5.1355e-08 | qwen: tf=10.4342 first=0.0000 kCE=10.4289 KD=4.6206 | scale_pen(qwen)=7.1304e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5644 rms_cal~0.0136 embed_rms~0.01363]
  step  390/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=12.1993 first=0.0000 kCE=10.2843 KD=4.1971 | scale_pen(llama)=4.4874e-08 | qwen: tf=10.6996 first=0.0000 kCE=9.7376 KD=4.6472 | scale_pen(qwen)=8.9816e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5645 rms_cal~0.0136 embed_rms~0.01363]
  step  400/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=11.9404 first=0.0000 kCE=10.4122 KD=4.6359 | scale_pen(llama)=4.4874e-08 | qwen: tf=10.1998 first=0.0000 kCE=8.6672 KD=5.0299 | scale_pen(qwen)=8.9816e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5647 rms_cal~0.0136 embed_rms~0.01363]
  step  410/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=12.8212 first=0.0000 kCE=10.9191 KD=4.3520 | scale_pen(llama)=4.4874e-08 | qwen: tf=10.3995 first=0.0000 kCE=10.2816 KD=4.5336 | scale_pen(qwen)=8.9816e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5648 rms_cal~0.0136 embed_rms~0.01363]
  step  420/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=13.2358 first=0.0000 kCE=11.0238 KD=4.4471 | scale_pen(llama)=3.8830e-08 | qwen: tf=10.1348 first=0.0000 kCE=10.1649 KD=4.8254 | scale_pen(qwen)=2.3309e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5649 rms_cal~0.0136 embed_rms~0.01363]
  step  430/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=12.8945 first=0.0000 kCE=9.7824 KD=4.4369 | scale_pen(llama)=3.8830e-08 | qwen: tf=10.6257 first=0.0000 kCE=10.2424 KD=3.4001 | scale_pen(qwen)=2.3309e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5650 rms_cal~0.0136 embed_rms~0.01363]
  step  440/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=12.6662 first=0.0000 kCE=10.0625 KD=4.2048 | scale_pen(llama)=3.8830e-08 | qwen: tf=11.9079 first=0.0000 kCE=9.7362 KD=4.7881 | scale_pen(qwen)=2.3309e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5652 rms_cal~0.0136 embed_rms~0.01363]
  step  450/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=12.7018 first=0.0000 kCE=10.0374 KD=4.4360 | scale_pen(llama)=3.3571e-08 | qwen: tf=10.8768 first=0.0000 kCE=10.2749 KD=4.4401 | scale_pen(qwen)=3.2742e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5653 rms_cal~0.0136 embed_rms~0.01363]
  step  460/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=12.9274 first=0.0000 kCE=10.5391 KD=4.1072 | scale_pen(llama)=3.3571e-08 | qwen: tf=10.4350 first=0.0000 kCE=9.6738 KD=3.9092 | scale_pen(qwen)=3.2742e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5654 rms_cal~0.0136 embed_rms~0.01363]
  step  470/5475 | grad_norm=nan | sec/step~2.78 | llama: tf=12.7875 first=0.0000 kCE=10.5089 KD=4.1106 | scale_pen(llama)=3.3571e-08 | qwen: tf=10.6071 first=0.0000 kCE=9.4713 KD=3.6032 | scale_pen(qwen)=3.2742e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5655 rms_cal~0.0136 embed_rms~0.01363]
  step  480/5475 | grad_norm=75.90 | sec/step~2.83 | llama: tf=11.4870 first=0.0000 kCE=9.9919 KD=3.7400 | scale_pen(llama)=2.8897e-08 | qwen: tf=9.7823 first=0.0000 kCE=8.8504 KD=3.7996 | scale_pen(qwen)=3.7757e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5656 rms_cal~0.0136 embed_rms~0.01363]
  step  490/5475 | grad_norm=nan | sec/step~2.92 | llama: tf=10.3697 first=0.0000 kCE=10.8474 KD=4.2925 | scale_pen(llama)=2.8897e-08 | qwen: tf=8.0698 first=0.0000 kCE=8.3276 KD=4.6459 | scale_pen(qwen)=3.7757e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5657 rms_cal~0.0136 embed_rms~0.01363]
  step  500/5475 | grad_norm=nan | sec/step~2.23 | llama: tf=12.5444 first=0.0000 kCE=11.1389 KD=4.1013 | scale_pen(llama)=2.8897e-08 | qwen: tf=9.1462 first=0.0000 kCE=9.0218 KD=3.6286 | scale_pen(qwen)=3.7757e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5659 rms_cal~0.0136 embed_rms~0.01363]
  step  510/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=12.4501 first=0.0000 kCE=10.8357 KD=4.2409 | scale_pen(llama)=2.8897e-08 | qwen: tf=9.4866 first=0.0000 kCE=10.4169 KD=4.0356 | scale_pen(qwen)=3.7757e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5604 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5660 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 516
  step  520/5475 | grad_norm=nan | sec/step~2.97 | llama: tf=12.4474 first=0.0000 kCE=11.1353 KD=4.5389 | scale_pen(llama)=2.3906e-08 | qwen: tf=10.1952 first=0.0000 kCE=10.1315 KD=4.5756 | scale_pen(qwen)=6.5690e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5661 rms_cal~0.0136 embed_rms~0.01363]
  step  530/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=12.5302 first=0.0000 kCE=10.1824 KD=4.3725 | scale_pen(llama)=2.3906e-08 | qwen: tf=8.7116 first=0.0000 kCE=8.3431 KD=3.8394 | scale_pen(qwen)=6.5690e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5662 rms_cal~0.0136 embed_rms~0.01363]
  step  540/5475 | grad_norm=nan | sec/step~2.36 | llama: tf=12.8990 first=0.0000 kCE=10.9396 KD=4.3470 | scale_pen(llama)=2.3906e-08 | qwen: tf=9.7698 first=0.0000 kCE=8.6991 KD=3.9740 | scale_pen(qwen)=6.5690e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5663 rms_cal~0.0136 embed_rms~0.01363]
  step  550/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=12.1076 first=0.0000 kCE=10.8734 KD=4.2495 | scale_pen(llama)=1.9122e-08 | qwen: tf=8.7875 first=0.0000 kCE=9.1472 KD=4.6423 | scale_pen(qwen)=7.9821e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5664 rms_cal~0.0136 embed_rms~0.01363]
  step  560/5475 | grad_norm=nan | sec/step~2.15 | llama: tf=11.9011 first=0.0000 kCE=10.0450 KD=4.9160 | scale_pen(llama)=1.9122e-08 | qwen: tf=10.5667 first=0.0000 kCE=10.7218 KD=4.1912 | scale_pen(qwen)=7.9821e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5665 rms_cal~0.0136 embed_rms~0.01363]
  step  570/5475 | grad_norm=nan | sec/step~2.82 | llama: tf=12.3773 first=0.0000 kCE=10.1492 KD=3.7204 | scale_pen(llama)=1.9122e-08 | qwen: tf=10.3291 first=0.0000 kCE=8.4840 KD=4.0564 | scale_pen(qwen)=7.9821e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5666 rms_cal~0.0136 embed_rms~0.01363]
  step  580/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=12.4305 first=0.0000 kCE=9.8733 KD=5.0281 | scale_pen(llama)=1.5817e-08 | qwen: tf=10.0286 first=0.0000 kCE=9.1712 KD=5.2126 | scale_pen(qwen)=5.3484e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5666 rms_cal~0.0136 embed_rms~0.01363]
  step  590/5475 | grad_norm=nan | sec/step~2.24 | llama: tf=11.1898 first=0.0000 kCE=10.2566 KD=4.2077 | scale_pen(llama)=1.5817e-08 | qwen: tf=8.9459 first=0.0000 kCE=9.0724 KD=3.8412 | scale_pen(qwen)=5.3484e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5667 rms_cal~0.0136 embed_rms~0.01363]
  step  600/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=12.1840 first=0.0000 kCE=9.0496 KD=4.3941 | scale_pen(llama)=1.5817e-08 | qwen: tf=10.3204 first=0.0000 kCE=10.4396 KD=3.4912 | scale_pen(qwen)=5.3484e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5605 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5668 rms_cal~0.0136 embed_rms~0.01363]
  step  610/5475 | grad_norm=nan | sec/step~2.87 | llama: tf=12.9275 first=0.0000 kCE=10.2853 KD=3.9160 | scale_pen(llama)=1.1105e-08 | qwen: tf=9.9481 first=0.0000 kCE=9.6582 KD=4.0951 | scale_pen(qwen)=3.5028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5669 rms_cal~0.0136 embed_rms~0.01363]
  step  620/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=13.3991 first=0.0000 kCE=8.8220 KD=4.4784 | scale_pen(llama)=1.1105e-08 | qwen: tf=11.2953 first=0.0000 kCE=9.8910 KD=4.2471 | scale_pen(qwen)=3.5028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5669 rms_cal~0.0136 embed_rms~0.01363]
  step  630/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=14.2351 first=0.0000 kCE=9.7966 KD=4.0808 | scale_pen(llama)=1.1105e-08 | qwen: tf=8.7764 first=0.0000 kCE=9.3960 KD=3.6783 | scale_pen(qwen)=3.5028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5670 rms_cal~0.0136 embed_rms~0.01363]
  step  640/5475 | grad_norm=32.26 | sec/step~2.42 | llama: tf=13.3484 first=0.0000 kCE=9.6084 KD=3.9041 | scale_pen(llama)=7.0632e-09 | qwen: tf=7.9166 first=0.0000 kCE=9.5760 KD=3.6175 | scale_pen(qwen)=1.4785e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5671 rms_cal~0.0136 embed_rms~0.01363]
  step  650/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=12.8630 first=0.0000 kCE=10.2155 KD=4.1411 | scale_pen(llama)=7.0632e-09 | qwen: tf=8.9568 first=0.0000 kCE=8.7609 KD=3.7420 | scale_pen(qwen)=1.4785e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5672 rms_cal~0.0136 embed_rms~0.01363]
  step  660/5475 | grad_norm=nan | sec/step~2.24 | llama: tf=11.2651 first=0.0000 kCE=9.9442 KD=4.3784 | scale_pen(llama)=7.0632e-09 | qwen: tf=8.5515 first=0.0000 kCE=9.1550 KD=3.3620 | scale_pen(qwen)=1.4785e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5672 rms_cal~0.0136 embed_rms~0.01363]
  step  670/5475 | grad_norm=nan | sec/step~2.16 | llama: tf=12.3350 first=0.0000 kCE=9.6710 KD=4.7677 | scale_pen(llama)=7.0632e-09 | qwen: tf=10.9513 first=0.0000 kCE=10.1576 KD=4.3912 | scale_pen(qwen)=1.4785e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5673 rms_cal~0.0136 embed_rms~0.01363]
  step  680/5475 | grad_norm=nan | sec/step~2.66 | llama: tf=12.0757 first=0.0000 kCE=9.7184 KD=4.3077 | scale_pen(llama)=3.7691e-09 | qwen: tf=9.1960 first=0.0000 kCE=9.4245 KD=4.5612 | scale_pen(qwen)=2.3648e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5606 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5673 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 688
  step  690/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=13.0739 first=0.0000 kCE=9.9624 KD=4.0254 | scale_pen(llama)=3.7691e-09 | qwen: tf=8.8969 first=0.0000 kCE=9.3760 KD=3.7385 | scale_pen(qwen)=2.3648e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5607 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5674 rms_cal~0.0136 embed_rms~0.01363]
  step  700/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=11.7632 first=0.0000 kCE=10.0771 KD=3.7601 | scale_pen(llama)=3.7691e-09 | qwen: tf=9.6755 first=0.0000 kCE=8.7927 KD=4.5203 | scale_pen(qwen)=2.3648e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5607 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5674 rms_cal~0.0136 embed_rms~0.01363]
  step  710/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=11.3961 first=0.0000 kCE=10.2334 KD=4.3162 | scale_pen(llama)=1.2034e-09 | qwen: tf=9.7521 first=0.0000 kCE=9.3321 KD=4.9189 | scale_pen(qwen)=1.2028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5607 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5675 rms_cal~0.0136 embed_rms~0.01363]
  step  720/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=11.8139 first=0.0000 kCE=9.6800 KD=3.5331 | scale_pen(llama)=1.2034e-09 | qwen: tf=9.3775 first=0.0000 kCE=8.9398 KD=3.3772 | scale_pen(qwen)=1.2028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5607 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5675 rms_cal~0.0136 embed_rms~0.01363]
  step  730/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=12.8646 first=0.0000 kCE=10.1751 KD=4.5283 | scale_pen(llama)=1.2034e-09 | qwen: tf=11.9162 first=0.0000 kCE=10.7443 KD=4.4611 | scale_pen(qwen)=1.2028e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5607 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5676 rms_cal~0.0136 embed_rms~0.01363]
  step  740/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=11.1425 first=0.0000 kCE=8.4695 KD=3.9967 | scale_pen(llama)=7.3669e-11 | qwen: tf=9.1538 first=0.0000 kCE=9.1007 KD=3.7938 | scale_pen(qwen)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5676 rms_cal~0.0136 embed_rms~0.01363]
  step  750/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=11.6236 first=0.0000 kCE=10.0260 KD=4.4853 | scale_pen(llama)=7.3669e-11 | qwen: tf=8.9134 first=0.0000 kCE=8.3123 KD=4.7620 | scale_pen(qwen)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5677 rms_cal~0.0136 embed_rms~0.01363]
  step  760/5475 | grad_norm=nan | sec/step~3.01 | llama: tf=11.9486 first=0.0000 kCE=9.2600 KD=3.7297 | scale_pen(llama)=7.3669e-11 | qwen: tf=8.8581 first=0.0000 kCE=8.4373 KD=3.5460 | scale_pen(qwen)=9.0949e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5677 rms_cal~0.0136 embed_rms~0.01363]
  step  770/5475 | grad_norm=nan | sec/step~2.91 | llama: tf=11.0647 first=0.0000 kCE=9.4822 KD=3.8622 | scale_pen(llama)=3.0500e-10 | qwen: tf=9.6254 first=0.0000 kCE=8.8435 KD=4.0700 | scale_pen(qwen)=1.1256e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5678 rms_cal~0.0136 embed_rms~0.01363]
  step  780/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=11.7743 first=0.0000 kCE=10.3191 KD=4.3789 | scale_pen(llama)=3.0500e-10 | qwen: tf=9.4721 first=0.0000 kCE=8.3686 KD=4.6822 | scale_pen(qwen)=1.1256e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5678 rms_cal~0.0136 embed_rms~0.01363]
  step  790/5475 | grad_norm=nan | sec/step~4.46 | llama: tf=11.7199 first=0.0000 kCE=9.0599 KD=3.1831 | scale_pen(llama)=3.0500e-10 | qwen: tf=10.0215 first=0.0000 kCE=8.9476 KD=4.5511 | scale_pen(qwen)=1.1256e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5608 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5678 rms_cal~0.0136 embed_rms~0.01363]
  step  800/5475 | grad_norm=25.16 | sec/step~2.54 | llama: tf=11.2916 first=0.0000 kCE=9.4732 KD=4.0019 | scale_pen(llama)=1.9297e-09 | qwen: tf=8.3628 first=0.0000 kCE=9.3064 KD=3.6677 | scale_pen(qwen)=2.0464e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5679 rms_cal~0.0136 embed_rms~0.01363]
  step  810/5475 | grad_norm=nan | sec/step~2.20 | llama: tf=11.2390 first=0.0000 kCE=9.0809 KD=4.1705 | scale_pen(llama)=1.9297e-09 | qwen: tf=8.1802 first=0.0000 kCE=8.9185 KD=4.0040 | scale_pen(qwen)=2.0464e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5679 rms_cal~0.0136 embed_rms~0.01363]
  step  820/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=12.1000 first=0.0000 kCE=9.0929 KD=4.1657 | scale_pen(llama)=1.9297e-09 | qwen: tf=8.2912 first=0.0000 kCE=9.0080 KD=3.9107 | scale_pen(qwen)=2.0464e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5680 rms_cal~0.0136 embed_rms~0.01363]
  step  830/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=11.0111 first=0.0000 kCE=9.5473 KD=3.7428 | scale_pen(llama)=1.9297e-09 | qwen: tf=8.9570 first=0.0000 kCE=7.9489 KD=5.0648 | scale_pen(qwen)=2.0464e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5680 rms_cal~0.0136 embed_rms~0.01363]
  step  840/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=11.6305 first=0.0000 kCE=10.4627 KD=4.0359 | scale_pen(llama)=5.9488e-09 | qwen: tf=8.3910 first=0.0000 kCE=7.9470 KD=3.5444 | scale_pen(qwen)=4.2286e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5680 rms_cal~0.0136 embed_rms~0.01363]
  step  850/5475 | grad_norm=nan | sec/step~2.75 | llama: tf=10.9318 first=0.0000 kCE=9.6003 KD=4.1774 | scale_pen(llama)=5.9488e-09 | qwen: tf=8.2953 first=0.0000 kCE=8.4969 KD=3.6767 | scale_pen(qwen)=4.2286e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5609 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5681 rms_cal~0.0136 embed_rms~0.01363]
  step  860/5475 | grad_norm=nan | sec/step~2.64 | llama: tf=10.6499 first=0.0000 kCE=9.3572 KD=4.0505 | scale_pen(llama)=5.9488e-09 | qwen: tf=9.0292 first=0.0000 kCE=8.0688 KD=4.4743 | scale_pen(qwen)=4.2286e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5610 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5681 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 860
  step  870/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=12.3455 first=0.0000 kCE=9.4522 KD=4.1624 | scale_pen(llama)=1.2839e-08 | qwen: tf=9.6239 first=0.0000 kCE=9.3759 KD=4.1480 | scale_pen(qwen)=5.8562e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5610 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5681 rms_cal~0.0136 embed_rms~0.01363]
  step  880/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=12.3096 first=0.0000 kCE=8.3643 KD=4.2023 | scale_pen(llama)=1.2839e-08 | qwen: tf=8.6258 first=0.0000 kCE=9.2044 KD=3.7663 | scale_pen(qwen)=5.8562e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5610 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5682 rms_cal~0.0136 embed_rms~0.01363]
  step  890/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=12.2420 first=0.0000 kCE=10.5463 KD=4.4211 | scale_pen(llama)=1.2839e-08 | qwen: tf=10.5402 first=0.0000 kCE=8.9511 KD=4.3759 | scale_pen(qwen)=5.8562e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5610 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5682 rms_cal~0.0136 embed_rms~0.01363]
  step  900/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=11.9653 first=0.0000 kCE=9.4974 KD=4.1775 | scale_pen(llama)=2.3411e-08 | qwen: tf=8.7335 first=0.0000 kCE=8.0666 KD=4.6635 | scale_pen(qwen)=7.7813e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5610 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5683 rms_cal~0.0136 embed_rms~0.01363]
  step  910/5475 | grad_norm=nan | sec/step~2.38 | llama: tf=11.8603 first=0.0000 kCE=9.0928 KD=4.3641 | scale_pen(llama)=2.3411e-08 | qwen: tf=8.5204 first=0.0000 kCE=9.0289 KD=3.5425 | scale_pen(qwen)=7.7813e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5611 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5683 rms_cal~0.0136 embed_rms~0.01363]
  step  920/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=11.7206 first=0.0000 kCE=9.3881 KD=4.1777 | scale_pen(llama)=2.3411e-08 | qwen: tf=9.3047 first=0.0000 kCE=9.2323 KD=3.9057 | scale_pen(qwen)=7.7813e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5611 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5683 rms_cal~0.0136 embed_rms~0.01363]
  step  930/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=12.0317 first=0.0000 kCE=9.6872 KD=4.3059 | scale_pen(llama)=3.8361e-08 | qwen: tf=9.9649 first=0.0000 kCE=9.8145 KD=3.7950 | scale_pen(qwen)=1.1062e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5611 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5684 rms_cal~0.0136 embed_rms~0.01363]
  step  940/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=11.8171 first=0.0000 kCE=9.9074 KD=4.7900 | scale_pen(llama)=3.8361e-08 | qwen: tf=10.3718 first=0.0000 kCE=8.7143 KD=4.6354 | scale_pen(qwen)=1.1062e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5611 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5684 rms_cal~0.0136 embed_rms~0.01363]
  step  950/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=12.2028 first=0.0000 kCE=9.8402 KD=4.1680 | scale_pen(llama)=3.8361e-08 | qwen: tf=9.1050 first=0.0000 kCE=9.2252 KD=3.7500 | scale_pen(qwen)=1.1062e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5611 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5684 rms_cal~0.0136 embed_rms~0.01363]
  step  960/5475 | grad_norm=10.56 | sec/step~2.69 | llama: tf=11.4618 first=0.0000 kCE=10.3431 KD=4.4238 | scale_pen(llama)=5.8850e-08 | qwen: tf=9.8917 first=0.0000 kCE=8.7202 KD=4.6632 | scale_pen(qwen)=1.3350e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5612 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5685 rms_cal~0.0136 embed_rms~0.01363]
  step  970/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=11.0095 first=0.0000 kCE=9.3492 KD=3.9956 | scale_pen(llama)=5.8850e-08 | qwen: tf=10.1129 first=0.0000 kCE=10.8000 KD=3.1776 | scale_pen(qwen)=1.3350e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5612 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5685 rms_cal~0.0136 embed_rms~0.01363]
  step  980/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=10.8791 first=0.0000 kCE=9.6144 KD=3.7158 | scale_pen(llama)=5.8850e-08 | qwen: tf=9.0039 first=0.0000 kCE=8.4178 KD=3.8396 | scale_pen(qwen)=1.3350e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5612 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5685 rms_cal~0.0136 embed_rms~0.01363]
  step  990/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=10.2911 first=0.0000 kCE=9.8459 KD=4.7861 | scale_pen(llama)=5.8850e-08 | qwen: tf=10.0918 first=0.0000 kCE=8.7765 KD=5.2261 | scale_pen(qwen)=1.3350e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5612 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5686 rms_cal~0.0136 embed_rms~0.01363]
  step  1000/5475 | grad_norm=nan | sec/step~3.29 | llama: tf=11.9009 first=0.0000 kCE=9.4299 KD=3.8983 | scale_pen(llama)=8.3362e-08 | qwen: tf=10.5913 first=0.0000 kCE=8.7754 KD=4.0649 | scale_pen(qwen)=1.8881e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5612 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5686 rms_cal~0.0136 embed_rms~0.01363]
  step  1010/5475 | grad_norm=nan | sec/step~3.01 | llama: tf=12.0874 first=0.0000 kCE=9.3201 KD=3.8530 | scale_pen(llama)=8.3362e-08 | qwen: tf=10.3706 first=0.0000 kCE=8.7474 KD=4.4444 | scale_pen(qwen)=1.8881e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5613 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5686 rms_cal~0.0136 embed_rms~0.01363]
  step  1020/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=11.7457 first=0.0000 kCE=10.3631 KD=4.3339 | scale_pen(llama)=8.3362e-08 | qwen: tf=11.2745 first=0.0000 kCE=9.9131 KD=4.4267 | scale_pen(qwen)=1.8881e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5613 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5687 rms_cal~0.0136 embed_rms~0.01363]
  step  1030/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=11.8606 first=0.0000 kCE=10.0534 KD=3.8909 | scale_pen(llama)=1.0755e-07 | qwen: tf=10.1766 first=0.0000 kCE=9.4313 KD=4.4248 | scale_pen(qwen)=2.2060e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5613 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5687 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1032
  step  1040/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=11.1976 first=0.0000 kCE=9.8175 KD=4.1918 | scale_pen(llama)=1.0755e-07 | qwen: tf=9.4572 first=0.0000 kCE=7.5770 KD=4.4333 | scale_pen(qwen)=2.2060e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5614 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5687 rms_cal~0.0136 embed_rms~0.01363]
  step  1050/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=11.2804 first=0.0000 kCE=11.4294 KD=4.4456 | scale_pen(llama)=1.0755e-07 | qwen: tf=9.3596 first=0.0000 kCE=7.8908 KD=4.5162 | scale_pen(qwen)=2.2060e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5614 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5687 rms_cal~0.0136 embed_rms~0.01363]
  step  1060/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=12.2712 first=0.0000 kCE=9.5153 KD=4.1412 | scale_pen(llama)=1.3064e-07 | qwen: tf=10.2152 first=0.0000 kCE=9.2473 KD=4.1811 | scale_pen(qwen)=2.1837e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5614 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5688 rms_cal~0.0136 embed_rms~0.01363]
  step  1070/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=12.8954 first=0.0000 kCE=9.0384 KD=4.0395 | scale_pen(llama)=1.3064e-07 | qwen: tf=9.1485 first=0.0000 kCE=8.7384 KD=3.4088 | scale_pen(qwen)=2.1837e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5614 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5688 rms_cal~0.0136 embed_rms~0.01363]
  step  1080/5475 | grad_norm=nan | sec/step~2.99 | llama: tf=11.5876 first=0.0000 kCE=8.9031 KD=3.8799 | scale_pen(llama)=1.3064e-07 | qwen: tf=8.7247 first=0.0000 kCE=7.6975 KD=3.8279 | scale_pen(qwen)=2.1837e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5615 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5688 rms_cal~0.0136 embed_rms~0.01363]
  step  1090/5475 | grad_norm=nan | sec/step~3.19 | llama: tf=11.3481 first=0.0000 kCE=8.8381 KD=3.8618 | scale_pen(llama)=1.4996e-07 | qwen: tf=9.0040 first=0.0000 kCE=7.5869 KD=4.2118 | scale_pen(qwen)=1.8315e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5615 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5689 rms_cal~0.0136 embed_rms~0.01363]
  step  1100/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=11.2351 first=0.0000 kCE=9.1999 KD=4.3799 | scale_pen(llama)=1.4996e-07 | qwen: tf=9.3843 first=0.0000 kCE=8.1501 KD=4.3603 | scale_pen(qwen)=1.8315e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5615 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5689 rms_cal~0.0136 embed_rms~0.01363]
  step  1110/5475 | grad_norm=nan | sec/step~2.77 | llama: tf=10.7734 first=0.0000 kCE=9.9798 KD=3.9314 | scale_pen(llama)=1.4996e-07 | qwen: tf=9.3567 first=0.0000 kCE=8.1926 KD=3.8205 | scale_pen(qwen)=1.8315e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5616 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5689 rms_cal~0.0136 embed_rms~0.01363]
  step  1120/5475 | grad_norm=9.32 | sec/step~3.04 | llama: tf=10.4930 first=0.0000 kCE=9.0100 KD=4.1361 | scale_pen(llama)=1.6573e-07 | qwen: tf=9.4244 first=0.0000 kCE=9.4569 KD=4.1364 | scale_pen(qwen)=1.0131e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5616 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5690 rms_cal~0.0136 embed_rms~0.01363]
  step  1130/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=10.7770 first=0.0000 kCE=8.8872 KD=4.2116 | scale_pen(llama)=1.6573e-07 | qwen: tf=9.9698 first=0.0000 kCE=7.9141 KD=3.9937 | scale_pen(qwen)=1.0131e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5616 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5690 rms_cal~0.0136 embed_rms~0.01363]
  step  1140/5475 | grad_norm=nan | sec/step~3.29 | llama: tf=10.5820 first=0.0000 kCE=9.0267 KD=4.1303 | scale_pen(llama)=1.6573e-07 | qwen: tf=9.0168 first=0.0000 kCE=8.2604 KD=4.0615 | scale_pen(qwen)=1.0131e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5616 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5690 rms_cal~0.0136 embed_rms~0.01363]
  step  1150/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=11.7062 first=0.0000 kCE=9.2839 KD=4.6233 | scale_pen(llama)=1.6573e-07 | qwen: tf=11.0773 first=0.0000 kCE=9.6169 KD=4.6786 | scale_pen(qwen)=1.0131e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5617 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5691 rms_cal~0.0136 embed_rms~0.01363]
  step  1160/5475 | grad_norm=nan | sec/step~3.02 | llama: tf=10.8353 first=0.0000 kCE=9.1495 KD=4.1043 | scale_pen(llama)=1.8173e-07 | qwen: tf=10.0981 first=0.0000 kCE=8.5813 KD=5.1660 | scale_pen(qwen)=6.4776e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5617 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5691 rms_cal~0.0136 embed_rms~0.01363]
  step  1170/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=10.7893 first=0.0000 kCE=10.6102 KD=4.1302 | scale_pen(llama)=1.8173e-07 | qwen: tf=8.2868 first=0.0000 kCE=7.6542 KD=3.6936 | scale_pen(qwen)=6.4776e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5617 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5691 rms_cal~0.0136 embed_rms~0.01363]
  step  1180/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=11.4913 first=0.0000 kCE=8.6509 KD=4.1917 | scale_pen(llama)=1.8173e-07 | qwen: tf=11.9620 first=0.0000 kCE=8.9506 KD=3.5094 | scale_pen(qwen)=6.4776e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5618 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5691 rms_cal~0.0136 embed_rms~0.01363]
  step  1190/5475 | grad_norm=nan | sec/step~3.12 | llama: tf=11.0310 first=0.0000 kCE=9.2784 KD=4.3606 | scale_pen(llama)=1.9931e-07 | qwen: tf=10.6489 first=0.0000 kCE=8.5037 KD=4.5671 | scale_pen(qwen)=4.1069e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5618 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5692 rms_cal~0.0136 embed_rms~0.01363]
  step  1200/5475 | grad_norm=nan | sec/step~3.12 | llama: tf=11.0992 first=0.0000 kCE=9.0306 KD=4.1459 | scale_pen(llama)=1.9931e-07 | qwen: tf=9.3399 first=0.0000 kCE=7.1599 KD=4.2899 | scale_pen(qwen)=4.1069e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5618 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5692 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1204
  step  1210/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=11.5770 first=0.0000 kCE=9.3108 KD=3.8329 | scale_pen(llama)=1.9931e-07 | qwen: tf=8.0846 first=0.0000 kCE=8.6150 KD=3.2689 | scale_pen(qwen)=4.1069e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5618 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5693 rms_cal~0.0136 embed_rms~0.01363]
  step  1220/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=10.2714 first=0.0000 kCE=9.0455 KD=4.2575 | scale_pen(llama)=2.1581e-07 | qwen: tf=8.3371 first=0.0000 kCE=8.1885 KD=3.5654 | scale_pen(qwen)=3.8924e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5619 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5693 rms_cal~0.0136 embed_rms~0.01363]
  step  1230/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=11.7488 first=0.0000 kCE=8.8467 KD=4.7319 | scale_pen(llama)=2.1581e-07 | qwen: tf=10.5148 first=0.0000 kCE=9.2290 KD=4.4009 | scale_pen(qwen)=3.8924e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5619 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5693 rms_cal~0.0136 embed_rms~0.01363]
  step  1240/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=11.4909 first=0.0000 kCE=8.7188 KD=4.2086 | scale_pen(llama)=2.1581e-07 | qwen: tf=9.8557 first=0.0000 kCE=9.2044 KD=4.1357 | scale_pen(qwen)=3.8924e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5619 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5694 rms_cal~0.0136 embed_rms~0.01363]
  step  1250/5475 | grad_norm=nan | sec/step~3.13 | llama: tf=10.6628 first=0.0000 kCE=9.5548 KD=4.1415 | scale_pen(llama)=2.3028e-07 | qwen: tf=8.0674 first=0.0000 kCE=7.3741 KD=4.7548 | scale_pen(qwen)=1.5370e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5620 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5694 rms_cal~0.0136 embed_rms~0.01363]
  step  1260/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=11.2814 first=0.0000 kCE=9.6657 KD=4.3175 | scale_pen(llama)=2.3028e-07 | qwen: tf=10.3193 first=0.0000 kCE=8.4705 KD=4.1135 | scale_pen(qwen)=1.5370e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5620 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5694 rms_cal~0.0136 embed_rms~0.01363]
  step  1270/5475 | grad_norm=nan | sec/step~2.78 | llama: tf=10.8332 first=0.0000 kCE=9.1124 KD=4.0897 | scale_pen(llama)=2.3028e-07 | qwen: tf=10.2713 first=0.0000 kCE=9.7491 KD=4.0900 | scale_pen(qwen)=1.5370e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5620 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5695 rms_cal~0.0136 embed_rms~0.01363]
  step  1280/5475 | grad_norm=10.80 | sec/step~3.27 | llama: tf=10.3364 first=0.0000 kCE=9.2011 KD=4.0313 | scale_pen(llama)=2.3958e-07 | qwen: tf=8.7300 first=0.0000 kCE=8.3255 KD=4.3022 | scale_pen(qwen)=1.7408e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5621 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5695 rms_cal~0.0136 embed_rms~0.01363]
  step  1290/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=10.8771 first=0.0000 kCE=9.4004 KD=3.9884 | scale_pen(llama)=2.3958e-07 | qwen: tf=8.1439 first=0.0000 kCE=8.2758 KD=3.9931 | scale_pen(qwen)=1.7408e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5621 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5695 rms_cal~0.0136 embed_rms~0.01363]
  step  1300/5475 | grad_norm=nan | sec/step~2.18 | llama: tf=11.0090 first=0.0000 kCE=9.5104 KD=4.4623 | scale_pen(llama)=2.3958e-07 | qwen: tf=9.4888 first=0.0000 kCE=8.8395 KD=4.0663 | scale_pen(qwen)=1.7408e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5621 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5696 rms_cal~0.0136 embed_rms~0.01363]
  step  1310/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=10.2703 first=0.0000 kCE=9.5771 KD=4.4223 | scale_pen(llama)=2.3958e-07 | qwen: tf=7.3121 first=0.0000 kCE=7.4213 KD=3.7378 | scale_pen(qwen)=1.7408e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5622 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5696 rms_cal~0.0136 embed_rms~0.01363]
  step  1320/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=11.5043 first=0.0000 kCE=8.4486 KD=4.3225 | scale_pen(llama)=2.4275e-07 | qwen: tf=8.7423 first=0.0000 kCE=7.9109 KD=4.0887 | scale_pen(qwen)=5.2387e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5622 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5696 rms_cal~0.0136 embed_rms~0.01363]
  step  1330/5475 | grad_norm=nan | sec/step~2.18 | llama: tf=10.6288 first=0.0000 kCE=9.7790 KD=4.0775 | scale_pen(llama)=2.4275e-07 | qwen: tf=10.6750 first=0.0000 kCE=9.8015 KD=4.1284 | scale_pen(qwen)=5.2387e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5622 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5697 rms_cal~0.0136 embed_rms~0.01363]
  step  1340/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=10.6075 first=0.0000 kCE=9.2177 KD=4.5261 | scale_pen(llama)=2.4275e-07 | qwen: tf=9.1204 first=0.0000 kCE=7.7337 KD=4.2936 | scale_pen(qwen)=5.2387e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5622 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5697 rms_cal~0.0136 embed_rms~0.01363]
  step  1350/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=12.3654 first=0.0000 kCE=9.4071 KD=3.9929 | scale_pen(llama)=2.4157e-07 | qwen: tf=11.7162 first=0.0000 kCE=9.2229 KD=3.5475 | scale_pen(qwen)=2.3772e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5623 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5697 rms_cal~0.0136 embed_rms~0.01363]
  step  1360/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=10.0435 first=0.0000 kCE=9.0517 KD=4.2705 | scale_pen(llama)=2.4157e-07 | qwen: tf=8.9395 first=0.0000 kCE=7.7829 KD=4.1136 | scale_pen(qwen)=2.3772e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5623 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5698 rms_cal~0.0136 embed_rms~0.01363]
  step  1370/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=10.7045 first=0.0000 kCE=9.3901 KD=4.4177 | scale_pen(llama)=2.4157e-07 | qwen: tf=8.2051 first=0.0000 kCE=7.7430 KD=4.0846 | scale_pen(qwen)=2.3772e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5623 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5698 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1376
  step  1380/5475 | grad_norm=nan | sec/step~2.91 | llama: tf=10.2006 first=0.0000 kCE=9.2153 KD=4.3914 | scale_pen(llama)=2.3639e-07 | qwen: tf=9.4748 first=0.0000 kCE=7.7241 KD=4.6945 | scale_pen(qwen)=5.5334e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5624 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5698 rms_cal~0.0136 embed_rms~0.01363]
  step  1390/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=11.0747 first=0.0000 kCE=9.6777 KD=4.0708 | scale_pen(llama)=2.3639e-07 | qwen: tf=11.2710 first=0.0000 kCE=9.2724 KD=3.9482 | scale_pen(qwen)=5.5334e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5624 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5699 rms_cal~0.0136 embed_rms~0.01363]
  step  1400/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=10.5614 first=0.0000 kCE=8.3354 KD=4.1353 | scale_pen(llama)=2.3639e-07 | qwen: tf=9.9750 first=0.0000 kCE=9.1166 KD=3.6448 | scale_pen(qwen)=5.5334e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5624 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5699 rms_cal~0.0136 embed_rms~0.01363]
  step  1410/5475 | grad_norm=nan | sec/step~3.05 | llama: tf=10.7127 first=0.0000 kCE=9.7130 KD=3.8802 | scale_pen(llama)=2.3108e-07 | qwen: tf=11.2782 first=0.0000 kCE=9.5585 KD=3.9866 | scale_pen(qwen)=8.1220e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5625 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5700 rms_cal~0.0136 embed_rms~0.01363]
  step  1420/5475 | grad_norm=nan | sec/step~3.60 | llama: tf=10.9841 first=0.0000 kCE=9.8370 KD=3.8964 | scale_pen(llama)=2.3108e-07 | qwen: tf=10.7215 first=0.0000 kCE=8.4034 KD=4.3147 | scale_pen(qwen)=8.1220e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5625 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5700 rms_cal~0.0136 embed_rms~0.01363]
  step  1430/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=11.4643 first=0.0000 kCE=9.5422 KD=4.6348 | scale_pen(llama)=2.3108e-07 | qwen: tf=12.1677 first=0.0000 kCE=9.8605 KD=4.9748 | scale_pen(qwen)=8.1220e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5625 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5701 rms_cal~0.0136 embed_rms~0.01363]
  step  1440/5475 | grad_norm=25.88 | sec/step~3.01 | llama: tf=10.2686 first=0.0000 kCE=9.2248 KD=4.1097 | scale_pen(llama)=2.2336e-07 | qwen: tf=9.6472 first=0.0000 kCE=7.8039 KD=4.6467 | scale_pen(qwen)=9.5321e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5625 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5701 rms_cal~0.0136 embed_rms~0.01363]
  step  1450/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=10.7501 first=0.0000 kCE=8.8471 KD=5.2348 | scale_pen(llama)=2.2336e-07 | qwen: tf=10.7459 first=0.0000 kCE=9.0217 KD=4.9951 | scale_pen(qwen)=9.5321e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5626 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5701 rms_cal~0.0136 embed_rms~0.01363]
  step  1460/5475 | grad_norm=nan | sec/step~3.22 | llama: tf=10.4456 first=0.0000 kCE=8.9109 KD=3.7829 | scale_pen(llama)=2.2336e-07 | qwen: tf=8.5350 first=0.0000 kCE=8.0710 KD=4.1289 | scale_pen(qwen)=9.5321e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5626 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5702 rms_cal~0.0136 embed_rms~0.01363]
  step  1470/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=10.8991 first=0.0000 kCE=8.5353 KD=4.5851 | scale_pen(llama)=2.2336e-07 | qwen: tf=10.9489 first=0.0000 kCE=9.5439 KD=3.9296 | scale_pen(qwen)=9.5321e-09 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5627 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5702 rms_cal~0.0136 embed_rms~0.01363]
  step  1480/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=10.8880 first=0.0000 kCE=9.0735 KD=4.5897 | scale_pen(llama)=2.1294e-07 | qwen: tf=10.4092 first=0.0000 kCE=9.1216 KD=4.5045 | scale_pen(qwen)=1.1105e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5627 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5702 rms_cal~0.0136 embed_rms~0.01363]
  step  1490/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=10.6760 first=0.0000 kCE=8.0875 KD=4.8962 | scale_pen(llama)=2.1294e-07 | qwen: tf=10.2749 first=0.0000 kCE=8.6442 KD=4.6131 | scale_pen(qwen)=1.1105e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5627 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5703 rms_cal~0.0136 embed_rms~0.01363]
  step  1500/5475 | grad_norm=nan | sec/step~3.29 | llama: tf=10.9912 first=0.0000 kCE=8.5947 KD=3.7877 | scale_pen(llama)=2.1294e-07 | qwen: tf=7.6361 first=0.0000 kCE=7.2985 KD=4.0537 | scale_pen(qwen)=1.1105e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5628 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5703 rms_cal~0.0136 embed_rms~0.01363]
  step  1510/5475 | grad_norm=nan | sec/step~2.27 | llama: tf=11.0252 first=0.0000 kCE=8.7966 KD=4.6170 | scale_pen(llama)=2.0715e-07 | qwen: tf=10.4515 first=0.0000 kCE=8.7176 KD=3.9548 | scale_pen(qwen)=1.1485e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5628 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5703 rms_cal~0.0136 embed_rms~0.01363]
  step  1520/5475 | grad_norm=nan | sec/step~3.02 | llama: tf=10.0714 first=0.0000 kCE=9.1480 KD=3.6571 | scale_pen(llama)=2.0715e-07 | qwen: tf=9.4539 first=0.0000 kCE=8.2020 KD=4.6490 | scale_pen(qwen)=1.1485e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5628 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5704 rms_cal~0.0136 embed_rms~0.01363]
  step  1530/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=11.0975 first=0.0000 kCE=8.0632 KD=3.8441 | scale_pen(llama)=2.0715e-07 | qwen: tf=8.9084 first=0.0000 kCE=8.9536 KD=3.7343 | scale_pen(qwen)=1.1485e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5629 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5704 rms_cal~0.0136 embed_rms~0.01363]
  step  1540/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=10.2785 first=0.0000 kCE=9.1512 KD=4.3818 | scale_pen(llama)=2.0429e-07 | qwen: tf=9.3893 first=0.0000 kCE=7.9040 KD=4.5866 | scale_pen(qwen)=1.2852e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5629 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5704 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1548
  step  1550/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=10.8155 first=0.0000 kCE=9.3358 KD=4.4757 | scale_pen(llama)=2.0429e-07 | qwen: tf=9.6360 first=0.0000 kCE=7.9808 KD=3.9743 | scale_pen(qwen)=1.2852e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5630 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5705 rms_cal~0.0136 embed_rms~0.01363]
  step  1560/5475 | grad_norm=nan | sec/step~2.98 | llama: tf=9.9134 first=0.0000 kCE=9.5249 KD=3.7953 | scale_pen(llama)=2.0429e-07 | qwen: tf=8.8795 first=0.0000 kCE=8.1539 KD=4.2125 | scale_pen(qwen)=1.2852e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5630 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5705 rms_cal~0.0136 embed_rms~0.01363]
  step  1570/5475 | grad_norm=nan | sec/step~3.62 | llama: tf=10.4260 first=0.0000 kCE=9.2709 KD=3.7250 | scale_pen(llama)=2.0547e-07 | qwen: tf=8.0379 first=0.0000 kCE=7.5010 KD=3.8594 | scale_pen(qwen)=1.4756e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5630 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5705 rms_cal~0.0136 embed_rms~0.01363]
  step  1580/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.8160 first=0.0000 kCE=8.6564 KD=4.0036 | scale_pen(llama)=2.0547e-07 | qwen: tf=8.3820 first=0.0000 kCE=8.0125 KD=3.6161 | scale_pen(qwen)=1.4756e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5631 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5706 rms_cal~0.0136 embed_rms~0.01363]
  step  1590/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=11.5115 first=0.0000 kCE=9.7333 KD=4.1502 | scale_pen(llama)=2.0547e-07 | qwen: tf=9.5941 first=0.0000 kCE=8.7690 KD=4.0991 | scale_pen(qwen)=1.4756e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5631 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5706 rms_cal~0.0136 embed_rms~0.01363]
  step  1600/5475 | grad_norm=5.28 | sec/step~2.65 | llama: tf=10.1813 first=0.0000 kCE=8.9869 KD=4.4556 | scale_pen(llama)=2.1620e-07 | qwen: tf=8.8393 first=0.0000 kCE=8.2599 KD=4.1886 | scale_pen(qwen)=1.7954e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5632 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5706 rms_cal~0.0136 embed_rms~0.01363]
  step  1610/5475 | grad_norm=nan | sec/step~2.95 | llama: tf=10.0297 first=0.0000 kCE=9.3848 KD=4.1078 | scale_pen(llama)=2.1620e-07 | qwen: tf=9.1481 first=0.0000 kCE=7.3821 KD=4.9100 | scale_pen(qwen)=1.7954e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5632 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5707 rms_cal~0.0136 embed_rms~0.01363]
  step  1620/5475 | grad_norm=nan | sec/step~2.79 | llama: tf=10.6003 first=0.0000 kCE=9.2114 KD=4.3049 | scale_pen(llama)=2.1620e-07 | qwen: tf=9.6065 first=0.0000 kCE=7.9950 KD=4.2299 | scale_pen(qwen)=1.7954e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5633 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5707 rms_cal~0.0136 embed_rms~0.01363]
  step  1630/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.5488 first=0.0000 kCE=9.3557 KD=4.2710 | scale_pen(llama)=2.1620e-07 | qwen: tf=8.5306 first=0.0000 kCE=7.9504 KD=4.3386 | scale_pen(qwen)=1.7954e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5633 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5707 rms_cal~0.0136 embed_rms~0.01363]
  step  1640/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=10.1972 first=0.0000 kCE=8.8950 KD=4.1198 | scale_pen(llama)=2.2982e-07 | qwen: tf=10.1676 first=0.0000 kCE=9.1747 KD=4.1162 | scale_pen(qwen)=2.1082e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5634 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5708 rms_cal~0.0136 embed_rms~0.01363]
  step  1650/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=10.5254 first=0.0000 kCE=8.5801 KD=4.8290 | scale_pen(llama)=2.2982e-07 | qwen: tf=9.4027 first=0.0000 kCE=7.8122 KD=4.3013 | scale_pen(qwen)=2.1082e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5634 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5708 rms_cal~0.0136 embed_rms~0.01363]
  step  1660/5475 | grad_norm=nan | sec/step~3.02 | llama: tf=10.0863 first=0.0000 kCE=9.2415 KD=4.6035 | scale_pen(llama)=2.2982e-07 | qwen: tf=9.1132 first=0.0000 kCE=8.3614 KD=4.2594 | scale_pen(qwen)=2.1082e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5634 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5708 rms_cal~0.0136 embed_rms~0.01363]
  step  1670/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=10.7148 first=0.0000 kCE=9.0253 KD=3.8051 | scale_pen(llama)=2.3953e-07 | qwen: tf=10.7985 first=0.0000 kCE=9.4139 KD=3.6976 | scale_pen(qwen)=2.4239e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5635 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5708 rms_cal~0.0136 embed_rms~0.01363]
  step  1680/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=10.1112 first=0.0000 kCE=8.8750 KD=4.4052 | scale_pen(llama)=2.3953e-07 | qwen: tf=10.3175 first=0.0000 kCE=9.7966 KD=4.1959 | scale_pen(qwen)=2.4239e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5635 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5709 rms_cal~0.0136 embed_rms~0.01363]
  step  1690/5475 | grad_norm=nan | sec/step~2.41 | llama: tf=9.9622 first=0.0000 kCE=9.0781 KD=4.2198 | scale_pen(llama)=2.3953e-07 | qwen: tf=7.9352 first=0.0000 kCE=8.1732 KD=3.4924 | scale_pen(qwen)=2.4239e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5636 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5709 rms_cal~0.0136 embed_rms~0.01363]
  step  1700/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=10.8397 first=0.0000 kCE=8.6315 KD=4.7533 | scale_pen(llama)=2.4634e-07 | qwen: tf=9.6258 first=0.0000 kCE=7.8111 KD=4.4942 | scale_pen(qwen)=3.1423e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5636 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5709 rms_cal~0.0136 embed_rms~0.01363]
  step  1710/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=10.6378 first=0.0000 kCE=9.0750 KD=4.5788 | scale_pen(llama)=2.4634e-07 | qwen: tf=9.5197 first=0.0000 kCE=8.1400 KD=4.7267 | scale_pen(qwen)=3.1423e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5637 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5709 rms_cal~0.0136 embed_rms~0.01363]
  step  1720/5475 | grad_norm=nan | sec/step~2.73 | llama: tf=10.3876 first=0.0000 kCE=9.6724 KD=4.7498 | scale_pen(llama)=2.4634e-07 | qwen: tf=10.9148 first=0.0000 kCE=8.8808 KD=5.0753 | scale_pen(qwen)=3.1423e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5637 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5710 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1720
  step  1730/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=10.4095 first=0.0000 kCE=7.9731 KD=4.3766 | scale_pen(llama)=2.4640e-07 | qwen: tf=9.8506 first=0.0000 kCE=8.9933 KD=4.1526 | scale_pen(qwen)=4.0396e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5638 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5710 rms_cal~0.0136 embed_rms~0.01363]
  step  1740/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=10.9849 first=0.0000 kCE=8.3399 KD=4.3796 | scale_pen(llama)=2.4640e-07 | qwen: tf=9.6322 first=0.0000 kCE=7.9385 KD=3.7879 | scale_pen(qwen)=4.0396e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5638 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5710 rms_cal~0.0136 embed_rms~0.01363]
  step  1750/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=10.3441 first=0.0000 kCE=9.0174 KD=3.9710 | scale_pen(llama)=2.4640e-07 | qwen: tf=10.3584 first=0.0000 kCE=8.4928 KD=4.6830 | scale_pen(qwen)=4.0396e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5639 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5710 rms_cal~0.0136 embed_rms~0.01363]
  step  1760/5475 | grad_norm=6.16 | sec/step~2.44 | llama: tf=10.2542 first=0.0000 kCE=8.7147 KD=4.2892 | scale_pen(llama)=2.4017e-07 | qwen: tf=9.1771 first=0.0000 kCE=7.4443 KD=3.9156 | scale_pen(qwen)=4.4672e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5639 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5711 rms_cal~0.0136 embed_rms~0.01363]
  step  1770/5475 | grad_norm=nan | sec/step~2.17 | llama: tf=10.6363 first=0.0000 kCE=9.2548 KD=4.4905 | scale_pen(llama)=2.4017e-07 | qwen: tf=9.3778 first=0.0000 kCE=9.1721 KD=3.6540 | scale_pen(qwen)=4.4672e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5639 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5711 rms_cal~0.0136 embed_rms~0.01363]
  step  1780/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=11.2271 first=0.0000 kCE=9.0863 KD=4.5285 | scale_pen(llama)=2.4017e-07 | qwen: tf=9.9205 first=0.0000 kCE=7.8812 KD=4.4350 | scale_pen(qwen)=4.4672e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5640 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5711 rms_cal~0.0136 embed_rms~0.01363]
  step  1790/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=10.5133 first=0.0000 kCE=9.4467 KD=4.1349 | scale_pen(llama)=2.4017e-07 | qwen: tf=9.9252 first=0.0000 kCE=7.7919 KD=4.1097 | scale_pen(qwen)=4.4672e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5640 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5711 rms_cal~0.0136 embed_rms~0.01363]
  step  1800/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=11.1992 first=0.0000 kCE=8.1531 KD=4.5413 | scale_pen(llama)=2.4087e-07 | qwen: tf=10.4533 first=0.0000 kCE=8.8613 KD=3.9063 | scale_pen(qwen)=4.9058e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5641 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5711 rms_cal~0.0136 embed_rms~0.01363]
  step  1810/5475 | grad_norm=nan | sec/step~2.24 | llama: tf=10.2835 first=0.0000 kCE=9.0349 KD=4.4836 | scale_pen(llama)=2.4087e-07 | qwen: tf=9.6567 first=0.0000 kCE=9.1648 KD=3.9150 | scale_pen(qwen)=4.9058e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5641 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5712 rms_cal~0.0136 embed_rms~0.01363]
  step  1820/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=10.1458 first=0.0000 kCE=9.1925 KD=4.6039 | scale_pen(llama)=2.4087e-07 | qwen: tf=10.0754 first=0.0000 kCE=8.6383 KD=4.7264 | scale_pen(qwen)=4.9058e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5642 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5712 rms_cal~0.0136 embed_rms~0.01363]
  step  1830/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=10.4754 first=0.0000 kCE=8.7335 KD=4.5424 | scale_pen(llama)=2.4599e-07 | qwen: tf=9.7131 first=0.0000 kCE=8.0526 KD=4.6559 | scale_pen(qwen)=4.8795e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5642 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5712 rms_cal~0.0136 embed_rms~0.01363]
  step  1840/5475 | grad_norm=nan | sec/step~2.81 | llama: tf=9.7575 first=0.0000 kCE=8.7207 KD=4.7266 | scale_pen(llama)=2.4599e-07 | qwen: tf=8.8591 first=0.0000 kCE=7.4895 KD=4.7483 | scale_pen(qwen)=4.8795e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5643 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5712 rms_cal~0.0136 embed_rms~0.01363]
  step  1850/5475 | grad_norm=nan | sec/step~2.94 | llama: tf=10.0060 first=0.0000 kCE=8.6140 KD=4.7508 | scale_pen(llama)=2.4599e-07 | qwen: tf=9.2999 first=0.0000 kCE=8.3871 KD=4.6344 | scale_pen(qwen)=4.8795e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5643 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5712 rms_cal~0.0136 embed_rms~0.01363]
  step  1860/5475 | grad_norm=nan | sec/step~2.29 | llama: tf=9.6485 first=0.0000 kCE=7.4660 KD=4.1308 | scale_pen(llama)=2.5271e-07 | qwen: tf=8.6866 first=0.0000 kCE=8.3014 KD=3.8289 | scale_pen(qwen)=4.3920e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5644 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
  step  1870/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=10.2698 first=0.0000 kCE=8.2187 KD=4.4877 | scale_pen(llama)=2.5271e-07 | qwen: tf=9.2112 first=0.0000 kCE=8.6399 KD=3.5743 | scale_pen(qwen)=4.3920e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5644 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
  step  1880/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=10.2788 first=0.0000 kCE=8.0613 KD=4.3029 | scale_pen(llama)=2.5271e-07 | qwen: tf=10.1850 first=0.0000 kCE=8.1463 KD=3.9870 | scale_pen(qwen)=4.3920e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5645 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
  step  1890/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=10.9397 first=0.0000 kCE=8.1165 KD=4.3767 | scale_pen(llama)=2.5911e-07 | qwen: tf=10.4917 first=0.0000 kCE=8.6929 KD=4.6294 | scale_pen(qwen)=4.1749e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5645 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1892
  step  1900/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=10.6209 first=0.0000 kCE=8.6548 KD=4.6453 | scale_pen(llama)=2.5911e-07 | qwen: tf=9.9341 first=0.0000 kCE=8.6296 KD=4.0649 | scale_pen(qwen)=4.1749e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5646 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
  step  1910/5475 | grad_norm=nan | sec/step~2.96 | llama: tf=11.5601 first=0.0000 kCE=8.4593 KD=3.5409 | scale_pen(llama)=2.5911e-07 | qwen: tf=9.3473 first=0.0000 kCE=8.7232 KD=3.4840 | scale_pen(qwen)=4.1749e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5646 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5713 rms_cal~0.0136 embed_rms~0.01363]
  step  1920/5475 | grad_norm=10.97 | sec/step~2.88 | llama: tf=10.3926 first=0.0000 kCE=7.8028 KD=4.7582 | scale_pen(llama)=2.6496e-07 | qwen: tf=9.9526 first=0.0000 kCE=8.2676 KD=4.3576 | scale_pen(qwen)=4.3173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5647 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1930/5475 | grad_norm=nan | sec/step~2.64 | llama: tf=10.4064 first=0.0000 kCE=8.3974 KD=4.3984 | scale_pen(llama)=2.6496e-07 | qwen: tf=10.0726 first=0.0000 kCE=8.4469 KD=4.1845 | scale_pen(qwen)=4.3173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5647 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1940/5475 | grad_norm=nan | sec/step~3.06 | llama: tf=10.4910 first=0.0000 kCE=8.3476 KD=3.7469 | scale_pen(llama)=2.6496e-07 | qwen: tf=9.4185 first=0.0000 kCE=8.9117 KD=3.4474 | scale_pen(qwen)=4.3173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5648 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1950/5475 | grad_norm=nan | sec/step~2.72 | llama: tf=10.1230 first=0.0000 kCE=8.7249 KD=4.1837 | scale_pen(llama)=2.6496e-07 | qwen: tf=9.4260 first=0.0000 kCE=7.8704 KD=4.5485 | scale_pen(qwen)=4.3173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5648 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1960/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.2240 first=0.0000 kCE=8.5468 KD=4.1181 | scale_pen(llama)=2.6952e-07 | qwen: tf=8.5620 first=0.0000 kCE=7.9233 KD=4.4044 | scale_pen(qwen)=4.8322e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5649 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1970/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=9.7266 first=0.0000 kCE=7.5745 KD=4.3457 | scale_pen(llama)=2.6952e-07 | qwen: tf=7.9976 first=0.0000 kCE=8.2556 KD=4.1785 | scale_pen(qwen)=4.8322e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5650 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5714 rms_cal~0.0136 embed_rms~0.01363]
  step  1980/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=10.3295 first=0.0000 kCE=8.3343 KD=3.8023 | scale_pen(llama)=2.6952e-07 | qwen: tf=8.9111 first=0.0000 kCE=7.7142 KD=3.8716 | scale_pen(qwen)=4.8322e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5650 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5715 rms_cal~0.0136 embed_rms~0.01363]
  step  1990/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=10.1102 first=0.0000 kCE=8.1705 KD=4.1711 | scale_pen(llama)=2.7256e-07 | qwen: tf=8.3390 first=0.0000 kCE=6.9546 KD=4.3940 | scale_pen(qwen)=5.3044e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5651 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5715 rms_cal~0.0136 embed_rms~0.01363]
  step  2000/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=9.9662 first=0.0000 kCE=9.0021 KD=4.7007 | scale_pen(llama)=2.7256e-07 | qwen: tf=9.9981 first=0.0000 kCE=7.9729 KD=4.7389 | scale_pen(qwen)=5.3044e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5651 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5715 rms_cal~0.0136 embed_rms~0.01363]
  step  2010/5475 | grad_norm=nan | sec/step~2.75 | llama: tf=9.7648 first=0.0000 kCE=8.7198 KD=4.0364 | scale_pen(llama)=2.7256e-07 | qwen: tf=7.3724 first=0.0000 kCE=7.6608 KD=3.3763 | scale_pen(qwen)=5.3044e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5652 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5715 rms_cal~0.0136 embed_rms~0.01363]
  step  2020/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=9.5246 first=0.0000 kCE=8.1242 KD=4.3191 | scale_pen(llama)=2.7481e-07 | qwen: tf=10.3403 first=0.0000 kCE=8.5681 KD=3.7692 | scale_pen(qwen)=5.2824e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5652 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5715 rms_cal~0.0136 embed_rms~0.01363]
  step  2030/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=10.2062 first=0.0000 kCE=8.6425 KD=4.5400 | scale_pen(llama)=2.7481e-07 | qwen: tf=10.3108 first=0.0000 kCE=10.1531 KD=4.1877 | scale_pen(qwen)=5.2824e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5653 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
  step  2040/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=10.0300 first=0.0000 kCE=8.0322 KD=3.8211 | scale_pen(llama)=2.7481e-07 | qwen: tf=9.3390 first=0.0000 kCE=7.9884 KD=3.2778 | scale_pen(qwen)=5.2824e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5653 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
  step  2050/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=10.2096 first=0.0000 kCE=8.1264 KD=4.8635 | scale_pen(llama)=2.8015e-07 | qwen: tf=10.2256 first=0.0000 kCE=9.0565 KD=4.6056 | scale_pen(qwen)=5.1572e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5654 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
  step  2060/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=10.9119 first=0.0000 kCE=8.6146 KD=3.7061 | scale_pen(llama)=2.8015e-07 | qwen: tf=9.3356 first=0.0000 kCE=8.5983 KD=2.7834 | scale_pen(qwen)=5.1572e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5654 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2064
  step  2070/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=10.9549 first=0.0000 kCE=8.8111 KD=4.5086 | scale_pen(llama)=2.8015e-07 | qwen: tf=10.7329 first=0.0000 kCE=9.0178 KD=4.2994 | scale_pen(qwen)=5.1572e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5655 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
  step  2080/5475 | grad_norm=15.83 | sec/step~2.59 | llama: tf=9.8028 first=0.0000 kCE=8.6891 KD=3.8495 | scale_pen(llama)=2.8796e-07 | qwen: tf=10.7022 first=0.0000 kCE=9.6513 KD=3.3350 | scale_pen(qwen)=4.8900e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5655 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5716 rms_cal~0.0136 embed_rms~0.01363]
  step  2090/5475 | grad_norm=nan | sec/step~2.81 | llama: tf=10.3956 first=0.0000 kCE=9.0031 KD=4.0828 | scale_pen(llama)=2.8796e-07 | qwen: tf=8.2182 first=0.0000 kCE=8.0543 KD=3.7417 | scale_pen(qwen)=4.8900e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5656 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2100/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=10.9437 first=0.0000 kCE=7.9646 KD=4.2572 | scale_pen(llama)=2.8796e-07 | qwen: tf=10.5365 first=0.0000 kCE=8.5833 KD=4.1819 | scale_pen(qwen)=4.8900e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5657 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2110/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=10.3092 first=0.0000 kCE=8.6144 KD=3.7908 | scale_pen(llama)=2.8796e-07 | qwen: tf=9.5362 first=0.0000 kCE=8.5956 KD=3.7626 | scale_pen(qwen)=4.8900e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5657 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2120/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=10.0814 first=0.0000 kCE=7.7840 KD=4.0275 | scale_pen(llama)=2.9803e-07 | qwen: tf=9.9917 first=0.0000 kCE=8.1021 KD=4.1800 | scale_pen(qwen)=5.1788e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5658 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2130/5475 | grad_norm=nan | sec/step~2.95 | llama: tf=10.2533 first=0.0000 kCE=8.1750 KD=4.2456 | scale_pen(llama)=2.9803e-07 | qwen: tf=8.3429 first=0.0000 kCE=7.2757 KD=4.3527 | scale_pen(qwen)=5.1788e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5658 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2140/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=9.3648 first=0.0000 kCE=8.4479 KD=4.3909 | scale_pen(llama)=2.9803e-07 | qwen: tf=8.9663 first=0.0000 kCE=7.6321 KD=4.5942 | scale_pen(qwen)=5.1788e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5659 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2150/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=10.3508 first=0.0000 kCE=9.5151 KD=4.3156 | scale_pen(llama)=3.0747e-07 | qwen: tf=12.1463 first=0.0000 kCE=10.0015 KD=4.2482 | scale_pen(qwen)=6.1541e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5660 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5717 rms_cal~0.0136 embed_rms~0.01363]
  step  2160/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=9.6776 first=0.0000 kCE=9.0288 KD=3.8528 | scale_pen(llama)=3.0747e-07 | qwen: tf=8.2861 first=0.0000 kCE=7.6090 KD=4.0259 | scale_pen(qwen)=6.1541e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5660 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2170/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.9163 first=0.0000 kCE=8.3969 KD=4.8296 | scale_pen(llama)=3.0747e-07 | qwen: tf=9.3255 first=0.0000 kCE=8.5658 KD=4.5755 | scale_pen(qwen)=6.1541e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5661 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2180/5475 | grad_norm=nan | sec/step~2.62 | llama: tf=10.3128 first=0.0000 kCE=7.8302 KD=3.8145 | scale_pen(llama)=3.1901e-07 | qwen: tf=9.6977 first=0.0000 kCE=8.6218 KD=4.1646 | scale_pen(qwen)=7.1241e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5661 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2190/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=11.3240 first=0.0000 kCE=8.8548 KD=5.3092 | scale_pen(llama)=3.1901e-07 | qwen: tf=11.2278 first=0.0000 kCE=8.2026 KD=5.1430 | scale_pen(qwen)=7.1241e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5662 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2200/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=10.3675 first=0.0000 kCE=10.2707 KD=4.5964 | scale_pen(llama)=3.1901e-07 | qwen: tf=9.0499 first=0.0000 kCE=8.2589 KD=4.1502 | scale_pen(qwen)=7.1241e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5662 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2210/5475 | grad_norm=nan | sec/step~2.77 | llama: tf=10.6295 first=0.0000 kCE=9.4252 KD=4.1276 | scale_pen(llama)=3.3194e-07 | qwen: tf=9.4912 first=0.0000 kCE=7.0392 KD=4.4401 | scale_pen(qwen)=7.9686e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5663 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5718 rms_cal~0.0136 embed_rms~0.01363]
  step  2220/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=10.3359 first=0.0000 kCE=8.0248 KD=3.7312 | scale_pen(llama)=3.3194e-07 | qwen: tf=7.3435 first=0.0000 kCE=7.6420 KD=3.5329 | scale_pen(qwen)=7.9686e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5664 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2230/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=9.9325 first=0.0000 kCE=7.8627 KD=3.9001 | scale_pen(llama)=3.3194e-07 | qwen: tf=9.2913 first=0.0000 kCE=8.1411 KD=4.0722 | scale_pen(qwen)=7.9686e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5664 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2236
  step  2240/5475 | grad_norm=13.27 | sec/step~2.79 | llama: tf=9.8804 first=0.0000 kCE=8.7187 KD=3.8139 | scale_pen(llama)=3.5187e-07 | qwen: tf=9.9747 first=0.0000 kCE=8.5167 KD=4.1687 | scale_pen(qwen)=8.6489e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5665 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2250/5475 | grad_norm=nan | sec/step~2.36 | llama: tf=10.0662 first=0.0000 kCE=9.0437 KD=4.7215 | scale_pen(llama)=3.5187e-07 | qwen: tf=9.6346 first=0.0000 kCE=7.5740 KD=4.4734 | scale_pen(qwen)=8.6489e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5665 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2260/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=10.0491 first=0.0000 kCE=8.9686 KD=4.5639 | scale_pen(llama)=3.5187e-07 | qwen: tf=10.0688 first=0.0000 kCE=8.7566 KD=3.9801 | scale_pen(qwen)=8.6489e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5666 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2270/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.9978 first=0.0000 kCE=8.4812 KD=4.2873 | scale_pen(llama)=3.5187e-07 | qwen: tf=9.5291 first=0.0000 kCE=7.0772 KD=4.2249 | scale_pen(qwen)=8.6489e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5667 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2280/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=9.9601 first=0.0000 kCE=8.8021 KD=5.2213 | scale_pen(llama)=3.7874e-07 | qwen: tf=9.3023 first=0.0000 kCE=7.1043 KD=5.9421 | scale_pen(qwen)=8.9387e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5667 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5719 rms_cal~0.0136 embed_rms~0.01363]
  step  2290/5475 | grad_norm=nan | sec/step~2.22 | llama: tf=10.2956 first=0.0000 kCE=9.1983 KD=4.5854 | scale_pen(llama)=3.7874e-07 | qwen: tf=10.4048 first=0.0000 kCE=8.9587 KD=4.4584 | scale_pen(qwen)=8.9387e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5668 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2300/5475 | grad_norm=nan | sec/step~2.68 | llama: tf=11.0011 first=0.0000 kCE=8.5625 KD=5.0242 | scale_pen(llama)=3.7874e-07 | qwen: tf=9.8460 first=0.0000 kCE=7.0467 KD=4.7465 | scale_pen(qwen)=8.9387e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5668 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2310/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=10.5994 first=0.0000 kCE=8.4199 KD=4.4782 | scale_pen(llama)=4.1508e-07 | qwen: tf=9.0734 first=0.0000 kCE=7.3136 KD=4.7231 | scale_pen(qwen)=9.3351e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5669 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2320/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=8.8679 first=0.0000 kCE=8.6903 KD=4.0244 | scale_pen(llama)=4.1508e-07 | qwen: tf=6.8050 first=0.0000 kCE=6.3676 KD=3.4241 | scale_pen(qwen)=9.3351e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5670 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2330/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=10.4084 first=0.0000 kCE=8.1538 KD=4.8739 | scale_pen(llama)=4.1508e-07 | qwen: tf=10.6472 first=0.0000 kCE=9.1553 KD=3.9283 | scale_pen(qwen)=9.3351e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5670 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2340/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.7713 first=0.0000 kCE=8.2142 KD=4.9345 | scale_pen(llama)=4.5308e-07 | qwen: tf=9.6840 first=0.0000 kCE=8.7232 KD=4.6462 | scale_pen(qwen)=9.0173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5671 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5720 rms_cal~0.0136 embed_rms~0.01363]
  step  2350/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=9.7780 first=0.0000 kCE=7.8078 KD=4.2951 | scale_pen(llama)=4.5308e-07 | qwen: tf=10.3570 first=0.0000 kCE=9.0322 KD=4.3308 | scale_pen(qwen)=9.0173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5671 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2360/5475 | grad_norm=nan | sec/step~2.38 | llama: tf=10.4082 first=0.0000 kCE=7.6999 KD=4.2103 | scale_pen(llama)=4.5308e-07 | qwen: tf=9.7341 first=0.0000 kCE=7.3301 KD=4.1675 | scale_pen(qwen)=9.0173e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5672 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2370/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=9.9161 first=0.0000 kCE=7.7962 KD=4.0401 | scale_pen(llama)=4.8883e-07 | qwen: tf=10.1031 first=0.0000 kCE=8.3883 KD=3.8818 | scale_pen(qwen)=8.2744e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5672 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2380/5475 | grad_norm=nan | sec/step~2.27 | llama: tf=10.3557 first=0.0000 kCE=7.8707 KD=4.3232 | scale_pen(llama)=4.8883e-07 | qwen: tf=8.7377 first=0.0000 kCE=7.8826 KD=3.7347 | scale_pen(qwen)=8.2744e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5673 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2390/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=10.7098 first=0.0000 kCE=7.6658 KD=4.4598 | scale_pen(llama)=4.8883e-07 | qwen: tf=11.4324 first=0.0000 kCE=8.9084 KD=4.3979 | scale_pen(qwen)=8.2744e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5674 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2400/5475 | grad_norm=9.18 | sec/step~2.77 | llama: tf=9.6815 first=0.0000 kCE=8.8903 KD=4.1232 | scale_pen(llama)=5.1424e-07 | qwen: tf=8.1055 first=0.0000 kCE=7.7657 KD=4.4051 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5674 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2408
  step  2410/5475 | grad_norm=nan | sec/step~3.14 | llama: tf=10.0540 first=0.0000 kCE=7.5818 KD=4.2199 | scale_pen(llama)=5.1424e-07 | qwen: tf=10.7590 first=0.0000 kCE=8.9474 KD=3.7323 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5675 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5721 rms_cal~0.0136 embed_rms~0.01363]
  step  2420/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=8.6586 first=0.0000 kCE=7.5519 KD=3.7102 | scale_pen(llama)=5.1424e-07 | qwen: tf=8.1913 first=0.0000 kCE=7.3781 KD=3.8058 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5675 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2430/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=9.9381 first=0.0000 kCE=8.4233 KD=3.7881 | scale_pen(llama)=5.1424e-07 | qwen: tf=9.6577 first=0.0000 kCE=7.8657 KD=3.9166 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5676 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2440/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=10.3451 first=0.0000 kCE=8.3974 KD=4.3010 | scale_pen(llama)=5.3740e-07 | qwen: tf=9.4274 first=0.0000 kCE=7.2495 KD=3.6037 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5676 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2450/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=9.9095 first=0.0000 kCE=9.4032 KD=4.2840 | scale_pen(llama)=5.3740e-07 | qwen: tf=10.3568 first=0.0000 kCE=8.9936 KD=4.2595 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5677 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2460/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=10.2072 first=0.0000 kCE=8.9295 KD=4.4865 | scale_pen(llama)=5.3740e-07 | qwen: tf=8.5878 first=0.0000 kCE=8.1937 KD=3.4365 | scale_pen(qwen)=7.5699e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5678 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2470/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.8979 first=0.0000 kCE=6.7188 KD=3.8406 | scale_pen(llama)=5.5894e-07 | qwen: tf=9.0170 first=0.0000 kCE=8.3662 KD=3.0769 | scale_pen(qwen)=8.2470e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5678 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5722 rms_cal~0.0136 embed_rms~0.01363]
  step  2480/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=10.1430 first=0.0000 kCE=9.3740 KD=4.4015 | scale_pen(llama)=5.5894e-07 | qwen: tf=10.1505 first=0.0000 kCE=8.5814 KD=4.3069 | scale_pen(qwen)=8.2470e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5679 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2490/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=9.9744 first=0.0000 kCE=8.4680 KD=4.0594 | scale_pen(llama)=5.5894e-07 | qwen: tf=9.5158 first=0.0000 kCE=8.2145 KD=3.4294 | scale_pen(qwen)=8.2470e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5679 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2500/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=9.0016 first=0.0000 kCE=9.1253 KD=3.8885 | scale_pen(llama)=5.7826e-07 | qwen: tf=8.8566 first=0.0000 kCE=7.2759 KD=4.5770 | scale_pen(qwen)=9.4593e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5680 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2510/5475 | grad_norm=nan | sec/step~2.82 | llama: tf=10.0497 first=0.0000 kCE=8.2523 KD=4.3226 | scale_pen(llama)=5.7826e-07 | qwen: tf=9.2182 first=0.0000 kCE=7.3542 KD=4.3138 | scale_pen(qwen)=9.4593e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5680 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2520/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=10.2424 first=0.0000 kCE=8.9299 KD=4.4430 | scale_pen(llama)=5.7826e-07 | qwen: tf=11.0827 first=0.0000 kCE=8.6350 KD=4.6672 | scale_pen(qwen)=9.4593e-08 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5681 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2530/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=9.5079 first=0.0000 kCE=8.4231 KD=4.9165 | scale_pen(llama)=5.9313e-07 | qwen: tf=8.0670 first=0.0000 kCE=7.2118 KD=4.8567 | scale_pen(qwen)=1.1022e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5681 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5723 rms_cal~0.0136 embed_rms~0.01363]
  step  2540/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.8159 first=0.0000 kCE=7.9567 KD=4.1450 | scale_pen(llama)=5.9313e-07 | qwen: tf=9.5116 first=0.0000 kCE=7.8422 KD=3.9908 | scale_pen(qwen)=1.1022e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5682 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5724 rms_cal~0.0136 embed_rms~0.01363]
  step  2550/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=9.7645 first=0.0000 kCE=8.0162 KD=3.8403 | scale_pen(llama)=5.9313e-07 | qwen: tf=9.4321 first=0.0000 kCE=7.7785 KD=3.1693 | scale_pen(qwen)=1.1022e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5682 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5724 rms_cal~0.0136 embed_rms~0.01363]
  step  2560/5475 | grad_norm=19.03 | sec/step~2.20 | llama: tf=10.1450 first=0.0000 kCE=8.2480 KD=4.7717 | scale_pen(llama)=6.0467e-07 | qwen: tf=9.5107 first=0.0000 kCE=8.2427 KD=3.9875 | scale_pen(qwen)=1.2150e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5683 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5724 rms_cal~0.0136 embed_rms~0.01363]
  step  2570/5475 | grad_norm=nan | sec/step~2.66 | llama: tf=9.9617 first=0.0000 kCE=8.2632 KD=4.0822 | scale_pen(llama)=6.0467e-07 | qwen: tf=9.5264 first=0.0000 kCE=7.2009 KD=4.3081 | scale_pen(qwen)=1.2150e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5684 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5724 rms_cal~0.0136 embed_rms~0.01363]
  step  2580/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=9.7125 first=0.0000 kCE=8.4232 KD=4.5492 | scale_pen(llama)=6.0467e-07 | qwen: tf=9.9027 first=0.0000 kCE=7.5438 KD=4.7267 | scale_pen(qwen)=1.2150e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5684 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5725 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2580
  step  2590/5475 | grad_norm=nan | sec/step~2.73 | llama: tf=10.6333 first=0.0000 kCE=8.2462 KD=4.6767 | scale_pen(llama)=6.0467e-07 | qwen: tf=10.0089 first=0.0000 kCE=7.9906 KD=4.1722 | scale_pen(qwen)=1.2150e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5685 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5725 rms_cal~0.0136 embed_rms~0.01363]
  step  2600/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=9.7005 first=0.0000 kCE=7.5585 KD=3.9700 | scale_pen(llama)=6.0596e-07 | qwen: tf=7.4140 first=0.0000 kCE=6.6998 KD=3.3483 | scale_pen(qwen)=1.2586e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5685 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5725 rms_cal~0.0136 embed_rms~0.01363]
  step  2610/5475 | grad_norm=nan | sec/step~2.17 | llama: tf=9.9083 first=0.0000 kCE=8.0691 KD=4.4058 | scale_pen(llama)=6.0596e-07 | qwen: tf=10.1914 first=0.0000 kCE=8.9350 KD=4.0365 | scale_pen(qwen)=1.2586e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5686 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5725 rms_cal~0.0136 embed_rms~0.01363]
  step  2620/5475 | grad_norm=nan | sec/step~3.32 | llama: tf=9.6312 first=0.0000 kCE=8.5023 KD=3.8796 | scale_pen(llama)=6.0596e-07 | qwen: tf=8.9768 first=0.0000 kCE=7.5057 KD=3.9883 | scale_pen(qwen)=1.2586e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5686 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5725 rms_cal~0.0136 embed_rms~0.01363]
  step  2630/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=10.0854 first=0.0000 kCE=8.8690 KD=4.1884 | scale_pen(llama)=6.0578e-07 | qwen: tf=9.9367 first=0.0000 kCE=9.0759 KD=3.9151 | scale_pen(qwen)=1.2944e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5687 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5726 rms_cal~0.0136 embed_rms~0.01363]
  step  2640/5475 | grad_norm=nan | sec/step~2.86 | llama: tf=9.8101 first=0.0000 kCE=7.6787 KD=3.5962 | scale_pen(llama)=6.0578e-07 | qwen: tf=9.6058 first=0.0000 kCE=8.3391 KD=4.1195 | scale_pen(qwen)=1.2944e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5687 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5726 rms_cal~0.0136 embed_rms~0.01363]
  step  2650/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=10.0062 first=0.0000 kCE=7.6418 KD=3.8394 | scale_pen(llama)=6.0578e-07 | qwen: tf=9.2411 first=0.0000 kCE=8.0624 KD=3.6847 | scale_pen(qwen)=1.2944e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5688 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5726 rms_cal~0.0136 embed_rms~0.01363]
  step  2660/5475 | grad_norm=nan | sec/step~2.97 | llama: tf=10.2547 first=0.0000 kCE=9.0704 KD=4.3187 | scale_pen(llama)=6.2109e-07 | qwen: tf=9.9849 first=0.0000 kCE=7.4852 KD=4.0582 | scale_pen(qwen)=1.3064e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5688 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5726 rms_cal~0.0136 embed_rms~0.01363]
  step  2670/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=9.9766 first=0.0000 kCE=8.6257 KD=4.4853 | scale_pen(llama)=6.2109e-07 | qwen: tf=9.1622 first=0.0000 kCE=8.3860 KD=4.7102 | scale_pen(qwen)=1.3064e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5689 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5726 rms_cal~0.0136 embed_rms~0.01363]
  step  2680/5475 | grad_norm=nan | sec/step~3.03 | llama: tf=9.7995 first=0.0000 kCE=8.9762 KD=3.4694 | scale_pen(llama)=6.2109e-07 | qwen: tf=7.9700 first=0.0000 kCE=8.6472 KD=3.2286 | scale_pen(qwen)=1.3064e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5689 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5727 rms_cal~0.0136 embed_rms~0.01363]
  step  2690/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=9.8190 first=0.0000 kCE=8.8315 KD=3.8032 | scale_pen(llama)=6.4604e-07 | qwen: tf=8.7747 first=0.0000 kCE=7.9311 KD=3.7372 | scale_pen(qwen)=1.3090e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5690 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5727 rms_cal~0.0136 embed_rms~0.01363]
  step  2700/5475 | grad_norm=nan | sec/step~2.86 | llama: tf=10.1037 first=0.0000 kCE=8.0497 KD=3.6187 | scale_pen(llama)=6.4604e-07 | qwen: tf=10.2668 first=0.0000 kCE=8.8445 KD=3.7311 | scale_pen(qwen)=1.3090e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5691 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5727 rms_cal~0.0136 embed_rms~0.01363]
  step  2710/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=10.5090 first=0.0000 kCE=9.2575 KD=4.1751 | scale_pen(llama)=6.4604e-07 | qwen: tf=8.3556 first=0.0000 kCE=7.2315 KD=4.5114 | scale_pen(qwen)=1.3090e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5691 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5727 rms_cal~0.0136 embed_rms~0.01363]
  step  2720/5475 | grad_norm=17.22 | sec/step~2.48 | llama: tf=10.2772 first=0.0000 kCE=8.7390 KD=4.6192 | scale_pen(llama)=6.6632e-07 | qwen: tf=9.9147 first=0.0000 kCE=9.2081 KD=4.3036 | scale_pen(qwen)=1.2807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5692 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5727 rms_cal~0.0136 embed_rms~0.01363]
  step  2730/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=10.0193 first=0.0000 kCE=7.8018 KD=4.2900 | scale_pen(llama)=6.6632e-07 | qwen: tf=10.4368 first=0.0000 kCE=8.1267 KD=4.7795 | scale_pen(qwen)=1.2807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5692 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
  step  2740/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=10.0372 first=0.0000 kCE=8.2045 KD=3.8474 | scale_pen(llama)=6.6632e-07 | qwen: tf=9.7953 first=0.0000 kCE=8.2754 KD=3.7426 | scale_pen(qwen)=1.2807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5693 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
  step  2750/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=10.0566 first=0.0000 kCE=7.8453 KD=3.7492 | scale_pen(llama)=6.6632e-07 | qwen: tf=7.8743 first=0.0000 kCE=7.1937 KD=3.1875 | scale_pen(qwen)=1.2807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5693 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2752
  step  2760/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=10.1919 first=0.0000 kCE=9.2550 KD=4.0878 | scale_pen(llama)=6.7511e-07 | qwen: tf=8.9323 first=0.0000 kCE=6.5540 KD=4.2816 | scale_pen(qwen)=1.2493e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5694 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
  step  2770/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.3835 first=0.0000 kCE=8.2357 KD=3.8942 | scale_pen(llama)=6.7511e-07 | qwen: tf=9.3065 first=0.0000 kCE=7.4206 KD=4.0511 | scale_pen(qwen)=1.2493e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5694 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
  step  2780/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=9.5827 first=0.0000 kCE=8.2137 KD=4.3822 | scale_pen(llama)=6.7511e-07 | qwen: tf=9.8713 first=0.0000 kCE=8.9109 KD=4.1105 | scale_pen(qwen)=1.2493e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5695 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5728 rms_cal~0.0136 embed_rms~0.01363]
  step  2790/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=8.7668 first=0.0000 kCE=8.3541 KD=3.8771 | scale_pen(llama)=6.9355e-07 | qwen: tf=7.5533 first=0.0000 kCE=6.6511 KD=3.4313 | scale_pen(qwen)=1.2208e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5695 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2800/5475 | grad_norm=nan | sec/step~2.73 | llama: tf=9.6523 first=0.0000 kCE=8.3801 KD=3.5128 | scale_pen(llama)=6.9355e-07 | qwen: tf=8.5657 first=0.0000 kCE=8.1271 KD=4.1523 | scale_pen(qwen)=1.2208e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5696 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2810/5475 | grad_norm=nan | sec/step~2.73 | llama: tf=9.7490 first=0.0000 kCE=9.0195 KD=4.0977 | scale_pen(llama)=6.9355e-07 | qwen: tf=9.0538 first=0.0000 kCE=6.4340 KD=4.7566 | scale_pen(qwen)=1.2208e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5696 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2820/5475 | grad_norm=nan | sec/step~2.12 | llama: tf=10.2834 first=0.0000 kCE=8.2504 KD=4.2826 | scale_pen(llama)=7.1637e-07 | qwen: tf=10.5770 first=0.0000 kCE=8.2482 KD=4.2707 | scale_pen(qwen)=1.1927e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5697 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2830/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=10.5505 first=0.0000 kCE=7.1808 KD=3.8020 | scale_pen(llama)=7.1637e-07 | qwen: tf=10.3337 first=0.0000 kCE=8.6874 KD=3.1802 | scale_pen(qwen)=1.1927e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5697 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2840/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.3056 first=0.0000 kCE=7.5686 KD=4.1439 | scale_pen(llama)=7.1637e-07 | qwen: tf=9.7726 first=0.0000 kCE=7.0783 KD=4.1924 | scale_pen(qwen)=1.1927e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5697 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5729 rms_cal~0.0136 embed_rms~0.01363]
  step  2850/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=10.5022 first=0.0000 kCE=7.9975 KD=4.2950 | scale_pen(llama)=7.4038e-07 | qwen: tf=8.8119 first=0.0000 kCE=7.1076 KD=3.8687 | scale_pen(qwen)=1.2183e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5698 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2860/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.7364 first=0.0000 kCE=8.4291 KD=3.9233 | scale_pen(llama)=7.4038e-07 | qwen: tf=9.5419 first=0.0000 kCE=8.3319 KD=4.2245 | scale_pen(qwen)=1.2183e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5698 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2870/5475 | grad_norm=nan | sec/step~2.24 | llama: tf=10.1052 first=0.0000 kCE=8.2672 KD=4.2385 | scale_pen(llama)=7.4038e-07 | qwen: tf=8.0918 first=0.0000 kCE=7.1616 KD=3.7558 | scale_pen(qwen)=1.2183e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5699 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2880/5475 | grad_norm=16.58 | sec/step~2.80 | llama: tf=10.0204 first=0.0000 kCE=7.9571 KD=4.3925 | scale_pen(llama)=7.6197e-07 | qwen: tf=8.0343 first=0.0000 kCE=7.2988 KD=4.3743 | scale_pen(qwen)=1.2850e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5699 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2890/5475 | grad_norm=nan | sec/step~3.10 | llama: tf=10.3365 first=0.0000 kCE=8.4216 KD=3.7784 | scale_pen(llama)=7.6197e-07 | qwen: tf=11.1180 first=0.0000 kCE=9.0809 KD=3.9858 | scale_pen(qwen)=1.2850e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5700 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2900/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=9.8740 first=0.0000 kCE=8.9139 KD=4.5778 | scale_pen(llama)=7.6197e-07 | qwen: tf=9.5649 first=0.0000 kCE=8.4806 KD=4.6122 | scale_pen(qwen)=1.2850e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5700 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2910/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.9165 first=0.0000 kCE=7.8688 KD=4.0500 | scale_pen(llama)=7.6197e-07 | qwen: tf=10.1188 first=0.0000 kCE=7.6443 KD=3.9510 | scale_pen(qwen)=1.2850e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5701 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5730 rms_cal~0.0136 embed_rms~0.01363]
  step  2920/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=10.1184 first=0.0000 kCE=8.9835 KD=4.6301 | scale_pen(llama)=7.8345e-07 | qwen: tf=10.1752 first=0.0000 kCE=8.3642 KD=4.2617 | scale_pen(qwen)=1.4083e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5701 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2924
  step  2930/5475 | grad_norm=nan | sec/step~2.92 | llama: tf=9.2911 first=0.0000 kCE=8.0146 KD=3.6760 | scale_pen(llama)=7.8345e-07 | qwen: tf=8.9336 first=0.0000 kCE=6.6831 KD=4.1660 | scale_pen(qwen)=1.4083e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5702 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2940/5475 | grad_norm=nan | sec/step~2.22 | llama: tf=9.9981 first=0.0000 kCE=7.1699 KD=4.1515 | scale_pen(llama)=7.8345e-07 | qwen: tf=9.1628 first=0.0000 kCE=8.2209 KD=3.5022 | scale_pen(qwen)=1.4083e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5702 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2950/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=8.9632 first=0.0000 kCE=8.7851 KD=4.0081 | scale_pen(llama)=7.9723e-07 | qwen: tf=8.0659 first=0.0000 kCE=7.5664 KD=3.8401 | scale_pen(qwen)=1.5476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5703 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2960/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=10.0047 first=0.0000 kCE=8.9075 KD=4.0749 | scale_pen(llama)=7.9723e-07 | qwen: tf=10.4142 first=0.0000 kCE=8.2988 KD=4.0365 | scale_pen(qwen)=1.5476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5703 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2970/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=9.8671 first=0.0000 kCE=7.7068 KD=4.0306 | scale_pen(llama)=7.9723e-07 | qwen: tf=9.3808 first=0.0000 kCE=7.4774 KD=3.7865 | scale_pen(qwen)=1.5476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5703 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2980/5475 | grad_norm=nan | sec/step~2.64 | llama: tf=9.6976 first=0.0000 kCE=8.7118 KD=4.3205 | scale_pen(llama)=8.1081e-07 | qwen: tf=9.1908 first=0.0000 kCE=7.8708 KD=3.8973 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5704 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5731 rms_cal~0.0136 embed_rms~0.01363]
  step  2990/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=9.9775 first=0.0000 kCE=8.3913 KD=4.0848 | scale_pen(llama)=8.1081e-07 | qwen: tf=10.0291 first=0.0000 kCE=8.3211 KD=4.2604 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5704 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3000/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=9.7489 first=0.0000 kCE=8.9622 KD=4.4472 | scale_pen(llama)=8.1081e-07 | qwen: tf=8.6726 first=0.0000 kCE=6.7647 KD=4.3162 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5705 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3010/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=9.7371 first=0.0000 kCE=7.6836 KD=4.0859 | scale_pen(llama)=8.2612e-07 | qwen: tf=9.1859 first=0.0000 kCE=7.9056 KD=3.7921 | scale_pen(qwen)=1.7798e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5705 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3020/5475 | grad_norm=nan | sec/step~2.11 | llama: tf=9.6879 first=0.0000 kCE=8.0422 KD=4.7297 | scale_pen(llama)=8.2612e-07 | qwen: tf=9.7612 first=0.0000 kCE=7.9927 KD=4.2140 | scale_pen(qwen)=1.7798e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5706 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3030/5475 | grad_norm=nan | sec/step~3.77 | llama: tf=9.8170 first=0.0000 kCE=8.7772 KD=3.6501 | scale_pen(llama)=8.2612e-07 | qwen: tf=9.8538 first=0.0000 kCE=8.8596 KD=3.8004 | scale_pen(qwen)=1.7798e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5706 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3040/5475 | grad_norm=14.30 | sec/step~2.48 | llama: tf=9.5650 first=0.0000 kCE=7.9670 KD=3.6840 | scale_pen(llama)=8.4103e-07 | qwen: tf=8.5069 first=0.0000 kCE=8.2568 KD=3.0890 | scale_pen(qwen)=1.8540e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5706 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5732 rms_cal~0.0136 embed_rms~0.01363]
  step  3050/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=10.3228 first=0.0000 kCE=8.6350 KD=4.1689 | scale_pen(llama)=8.4103e-07 | qwen: tf=9.1336 first=0.0000 kCE=6.7339 KD=3.7219 | scale_pen(qwen)=1.8540e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5707 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3060/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.5661 first=0.0000 kCE=8.6414 KD=3.8825 | scale_pen(llama)=8.4103e-07 | qwen: tf=9.4948 first=0.0000 kCE=8.2270 KD=3.4726 | scale_pen(qwen)=1.8540e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5707 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3070/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=9.5807 first=0.0000 kCE=8.0860 KD=4.4827 | scale_pen(llama)=8.4103e-07 | qwen: tf=9.7627 first=0.0000 kCE=8.3328 KD=4.5639 | scale_pen(qwen)=1.8540e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5708 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3080/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.6768 first=0.0000 kCE=8.4537 KD=3.7865 | scale_pen(llama)=8.6359e-07 | qwen: tf=6.7624 first=0.0000 kCE=7.5235 KD=3.0448 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5708 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3090/5475 | grad_norm=nan | sec/step~2.82 | llama: tf=9.5399 first=0.0000 kCE=8.3278 KD=4.1892 | scale_pen(llama)=8.6359e-07 | qwen: tf=10.2108 first=0.0000 kCE=8.0191 KD=4.8791 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5708 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3096
  step  3100/5475 | grad_norm=nan | sec/step~2.40 | llama: tf=8.9824 first=0.0000 kCE=9.1096 KD=3.5306 | scale_pen(llama)=8.6359e-07 | qwen: tf=8.3639 first=0.0000 kCE=7.9959 KD=3.1910 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5709 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3110/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.7902 first=0.0000 kCE=8.8025 KD=3.8362 | scale_pen(llama)=8.9545e-07 | qwen: tf=10.4133 first=0.0000 kCE=8.2802 KD=3.9039 | scale_pen(qwen)=1.9308e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5709 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5733 rms_cal~0.0136 embed_rms~0.01363]
  step  3120/5475 | grad_norm=nan | sec/step~2.25 | llama: tf=9.8343 first=0.0000 kCE=9.0055 KD=4.3434 | scale_pen(llama)=8.9545e-07 | qwen: tf=10.3143 first=0.0000 kCE=7.6404 KD=4.7108 | scale_pen(qwen)=1.9308e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5710 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3130/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=9.1710 first=0.0000 kCE=8.8530 KD=3.7634 | scale_pen(llama)=8.9545e-07 | qwen: tf=7.3136 first=0.0000 kCE=7.6494 KD=3.4950 | scale_pen(qwen)=1.9308e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5710 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3140/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=9.5808 first=0.0000 kCE=9.3398 KD=3.8780 | scale_pen(llama)=9.4972e-07 | qwen: tf=8.8536 first=0.0000 kCE=7.2050 KD=4.3709 | scale_pen(qwen)=1.9193e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5710 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3150/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=9.6082 first=0.0000 kCE=8.4534 KD=3.4672 | scale_pen(llama)=9.4972e-07 | qwen: tf=8.8301 first=0.0000 kCE=8.6774 KD=3.3636 | scale_pen(qwen)=1.9193e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5711 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3160/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=9.7986 first=0.0000 kCE=9.3471 KD=3.3588 | scale_pen(llama)=9.4972e-07 | qwen: tf=9.0458 first=0.0000 kCE=9.1183 KD=3.5060 | scale_pen(qwen)=1.9193e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5711 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3170/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=9.7129 first=0.0000 kCE=8.8967 KD=3.7928 | scale_pen(llama)=9.9759e-07 | qwen: tf=10.0653 first=0.0000 kCE=7.9417 KD=4.5598 | scale_pen(qwen)=1.8808e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5712 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3180/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=9.8895 first=0.0000 kCE=8.9243 KD=4.0409 | scale_pen(llama)=9.9759e-07 | qwen: tf=10.6817 first=0.0000 kCE=7.3912 KD=4.7040 | scale_pen(qwen)=1.8808e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5712 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5734 rms_cal~0.0136 embed_rms~0.01363]
  step  3190/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=8.8094 first=0.0000 kCE=8.8529 KD=3.9696 | scale_pen(llama)=9.9759e-07 | qwen: tf=8.4171 first=0.0000 kCE=8.2214 KD=4.1241 | scale_pen(qwen)=1.8808e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5712 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3200/5475 | grad_norm=32.24 | sec/step~2.61 | llama: tf=10.0825 first=0.0000 kCE=8.7971 KD=3.5683 | scale_pen(llama)=1.0324e-06 | qwen: tf=10.5117 first=0.0000 kCE=7.5975 KD=3.9853 | scale_pen(qwen)=1.8787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5713 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3210/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.4564 first=0.0000 kCE=8.5173 KD=3.4156 | scale_pen(llama)=1.0324e-06 | qwen: tf=7.5013 first=0.0000 kCE=7.0223 KD=3.3881 | scale_pen(qwen)=1.8787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5713 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3220/5475 | grad_norm=nan | sec/step~3.16 | llama: tf=8.7909 first=0.0000 kCE=8.9695 KD=3.2918 | scale_pen(llama)=1.0324e-06 | qwen: tf=8.8311 first=0.0000 kCE=6.9639 KD=4.0706 | scale_pen(qwen)=1.8787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5714 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3230/5475 | grad_norm=nan | sec/step~2.81 | llama: tf=10.3321 first=0.0000 kCE=7.8641 KD=3.7368 | scale_pen(llama)=1.0324e-06 | qwen: tf=10.2995 first=0.0000 kCE=7.0641 KD=4.3345 | scale_pen(qwen)=1.8787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5714 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3240/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=9.3259 first=0.0000 kCE=8.2309 KD=3.8243 | scale_pen(llama)=1.0708e-06 | qwen: tf=9.6118 first=0.0000 kCE=7.2936 KD=5.0926 | scale_pen(qwen)=1.8746e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5714 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3250/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=9.2975 first=0.0000 kCE=9.7754 KD=3.8984 | scale_pen(llama)=1.0708e-06 | qwen: tf=9.4304 first=0.0000 kCE=7.9092 KD=4.2156 | scale_pen(qwen)=1.8746e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5715 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
  step  3260/5475 | grad_norm=nan | sec/step~3.16 | llama: tf=9.8980 first=0.0000 kCE=8.7205 KD=3.4140 | scale_pen(llama)=1.0708e-06 | qwen: tf=11.8804 first=0.0000 kCE=9.3703 KD=3.9060 | scale_pen(qwen)=1.8746e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5715 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5735 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3268
  step  3270/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=9.1658 first=0.0000 kCE=8.4906 KD=3.4592 | scale_pen(llama)=1.1222e-06 | qwen: tf=8.0200 first=0.0000 kCE=6.2641 KD=3.6804 | scale_pen(qwen)=1.8530e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5715 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3280/5475 | grad_norm=nan | sec/step~2.97 | llama: tf=10.1072 first=0.0000 kCE=9.1001 KD=3.5658 | scale_pen(llama)=1.1222e-06 | qwen: tf=10.5436 first=0.0000 kCE=8.7018 KD=4.5333 | scale_pen(qwen)=1.8530e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5716 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3290/5475 | grad_norm=nan | sec/step~2.94 | llama: tf=9.0190 first=0.0000 kCE=8.4566 KD=3.2094 | scale_pen(llama)=1.1222e-06 | qwen: tf=9.0933 first=0.0000 kCE=7.2727 KD=3.7023 | scale_pen(qwen)=1.8530e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5716 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3300/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=8.6757 first=0.0000 kCE=8.6291 KD=3.3082 | scale_pen(llama)=1.1772e-06 | qwen: tf=7.6129 first=0.0000 kCE=6.6365 KD=3.7049 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5717 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3310/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=8.8984 first=0.0000 kCE=9.0430 KD=3.9419 | scale_pen(llama)=1.1772e-06 | qwen: tf=7.5901 first=0.0000 kCE=6.2932 KD=4.1780 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5717 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3320/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=9.1035 first=0.0000 kCE=8.3442 KD=3.2648 | scale_pen(llama)=1.1772e-06 | qwen: tf=9.8360 first=0.0000 kCE=7.2667 KD=3.9528 | scale_pen(qwen)=1.8984e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5717 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3330/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=9.0954 first=0.0000 kCE=8.7248 KD=3.4897 | scale_pen(llama)=1.2076e-06 | qwen: tf=8.3621 first=0.0000 kCE=6.4262 KD=3.6368 | scale_pen(qwen)=1.9539e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5718 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5736 rms_cal~0.0136 embed_rms~0.01363]
  step  3340/5475 | grad_norm=nan | sec/step~3.30 | llama: tf=8.8874 first=0.0000 kCE=8.6396 KD=3.1062 | scale_pen(llama)=1.2076e-06 | qwen: tf=8.5355 first=0.0000 kCE=7.3658 KD=4.4441 | scale_pen(qwen)=1.9539e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5718 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3350/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.8044 first=0.0000 kCE=8.6225 KD=3.5163 | scale_pen(llama)=1.2076e-06 | qwen: tf=9.6207 first=0.0000 kCE=7.5132 KD=3.6731 | scale_pen(qwen)=1.9539e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5718 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3360/5475 | grad_norm=52.00 | sec/step~2.49 | llama: tf=9.0701 first=0.0000 kCE=8.2992 KD=3.5879 | scale_pen(llama)=1.2157e-06 | qwen: tf=7.5145 first=0.0000 kCE=7.5618 KD=3.8168 | scale_pen(qwen)=1.9995e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5719 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3370/5475 | grad_norm=nan | sec/step~2.17 | llama: tf=8.6979 first=0.0000 kCE=8.6580 KD=3.6602 | scale_pen(llama)=1.2157e-06 | qwen: tf=9.1650 first=0.0000 kCE=7.9739 KD=4.2606 | scale_pen(qwen)=1.9995e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5719 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3380/5475 | grad_norm=nan | sec/step~3.09 | llama: tf=9.3964 first=0.0000 kCE=8.7994 KD=3.3197 | scale_pen(llama)=1.2157e-06 | qwen: tf=10.2752 first=0.0000 kCE=8.7274 KD=4.3962 | scale_pen(qwen)=1.9995e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5719 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3390/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.0058 first=0.0000 kCE=7.7821 KD=3.5539 | scale_pen(llama)=1.2157e-06 | qwen: tf=8.8244 first=0.0000 kCE=7.4561 KD=3.3765 | scale_pen(qwen)=1.9995e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5720 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3400/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=9.3525 first=0.0000 kCE=8.2993 KD=3.4027 | scale_pen(llama)=1.2254e-06 | qwen: tf=9.4648 first=0.0000 kCE=6.5703 KD=3.9197 | scale_pen(qwen)=1.9592e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5720 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3410/5475 | grad_norm=nan | sec/step~2.75 | llama: tf=9.0187 first=0.0000 kCE=8.5672 KD=3.3320 | scale_pen(llama)=1.2254e-06 | qwen: tf=8.0889 first=0.0000 kCE=7.7085 KD=3.4001 | scale_pen(qwen)=1.9592e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5720 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5737 rms_cal~0.0136 embed_rms~0.01363]
  step  3420/5475 | grad_norm=nan | sec/step~2.19 | llama: tf=9.3555 first=0.0000 kCE=7.4279 KD=4.3650 | scale_pen(llama)=1.2254e-06 | qwen: tf=9.4068 first=0.0000 kCE=8.4369 KD=4.4116 | scale_pen(qwen)=1.9592e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5721 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3430/5475 | grad_norm=nan | sec/step~2.66 | llama: tf=9.7908 first=0.0000 kCE=8.2434 KD=3.9991 | scale_pen(llama)=1.2422e-06 | qwen: tf=11.3755 first=0.0000 kCE=7.8865 KD=5.0825 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5721 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3440/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=9.6270 first=0.0000 kCE=7.8303 KD=3.4907 | scale_pen(llama)=1.2422e-06 | qwen: tf=8.8293 first=0.0000 kCE=7.0712 KD=4.1332 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5722 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3440
  step  3450/5475 | grad_norm=nan | sec/step~2.16 | llama: tf=9.0359 first=0.0000 kCE=8.2411 KD=3.7059 | scale_pen(llama)=1.2422e-06 | qwen: tf=9.0518 first=0.0000 kCE=7.4700 KD=3.7873 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5722 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3460/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=9.8599 first=0.0000 kCE=7.8324 KD=3.8087 | scale_pen(llama)=1.2547e-06 | qwen: tf=10.5390 first=0.0000 kCE=6.4430 KD=5.5452 | scale_pen(qwen)=1.9276e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5722 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3470/5475 | grad_norm=nan | sec/step~3.26 | llama: tf=9.2674 first=0.0000 kCE=8.8032 KD=3.2209 | scale_pen(llama)=1.2547e-06 | qwen: tf=9.9710 first=0.0000 kCE=6.9876 KD=4.4117 | scale_pen(qwen)=1.9276e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5723 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3480/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=9.1407 first=0.0000 kCE=7.5530 KD=3.0200 | scale_pen(llama)=1.2547e-06 | qwen: tf=9.8875 first=0.0000 kCE=7.8640 KD=3.5004 | scale_pen(qwen)=1.9276e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5723 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3490/5475 | grad_norm=nan | sec/step~2.21 | llama: tf=9.1548 first=0.0000 kCE=9.0162 KD=3.9006 | scale_pen(llama)=1.2617e-06 | qwen: tf=8.6003 first=0.0000 kCE=7.4533 KD=4.4692 | scale_pen(qwen)=1.9486e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5723 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5738 rms_cal~0.0136 embed_rms~0.01363]
  step  3500/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=9.3109 first=0.0000 kCE=7.9628 KD=3.9260 | scale_pen(llama)=1.2617e-06 | qwen: tf=9.3335 first=0.0000 kCE=7.0226 KD=4.5236 | scale_pen(qwen)=1.9486e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5724 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3510/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=8.8400 first=0.0000 kCE=8.2157 KD=3.1764 | scale_pen(llama)=1.2617e-06 | qwen: tf=8.4270 first=0.0000 kCE=5.9062 KD=3.3080 | scale_pen(qwen)=1.9486e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5724 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3520/5475 | grad_norm=60.42 | sec/step~2.78 | llama: tf=9.7213 first=0.0000 kCE=7.9565 KD=3.5119 | scale_pen(llama)=1.2526e-06 | qwen: tf=9.6503 first=0.0000 kCE=7.2175 KD=4.1127 | scale_pen(qwen)=1.9666e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5724 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3530/5475 | grad_norm=nan | sec/step~2.12 | llama: tf=9.1657 first=0.0000 kCE=8.5353 KD=4.1465 | scale_pen(llama)=1.2526e-06 | qwen: tf=9.8167 first=0.0000 kCE=7.0446 KD=4.8120 | scale_pen(qwen)=1.9666e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5725 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3540/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=8.6932 first=0.0000 kCE=8.1254 KD=3.5054 | scale_pen(llama)=1.2526e-06 | qwen: tf=9.2232 first=0.0000 kCE=6.6698 KD=4.2250 | scale_pen(qwen)=1.9666e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5725 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3550/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=8.5953 first=0.0000 kCE=8.7522 KD=3.4767 | scale_pen(llama)=1.2526e-06 | qwen: tf=8.9818 first=0.0000 kCE=7.2845 KD=3.7240 | scale_pen(qwen)=1.9666e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5725 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3560/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.1103 first=0.0000 kCE=8.9995 KD=3.0074 | scale_pen(llama)=1.2279e-06 | qwen: tf=8.8747 first=0.0000 kCE=7.5062 KD=3.6988 | scale_pen(qwen)=1.9846e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5726 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3570/5475 | grad_norm=nan | sec/step~2.86 | llama: tf=8.6362 first=0.0000 kCE=8.5654 KD=3.5639 | scale_pen(llama)=1.2279e-06 | qwen: tf=8.2623 first=0.0000 kCE=7.0812 KD=4.3518 | scale_pen(qwen)=1.9846e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5726 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3580/5475 | grad_norm=nan | sec/step~2.72 | llama: tf=9.2501 first=0.0000 kCE=8.1771 KD=2.7133 | scale_pen(llama)=1.2279e-06 | qwen: tf=8.7402 first=0.0000 kCE=7.6598 KD=3.5177 | scale_pen(qwen)=1.9846e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5726 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5739 rms_cal~0.0136 embed_rms~0.01363]
  step  3590/5475 | grad_norm=nan | sec/step~3.10 | llama: tf=9.0047 first=0.0000 kCE=8.2095 KD=3.1286 | scale_pen(llama)=1.1881e-06 | qwen: tf=8.7203 first=0.0000 kCE=6.4812 KD=3.6713 | scale_pen(qwen)=1.9867e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5727 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3600/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=9.3939 first=0.0000 kCE=7.2036 KD=3.1931 | scale_pen(llama)=1.1881e-06 | qwen: tf=9.6877 first=0.0000 kCE=7.0300 KD=3.7107 | scale_pen(qwen)=1.9867e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5727 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3610/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=9.0582 first=0.0000 kCE=7.3237 KD=2.7454 | scale_pen(llama)=1.1881e-06 | qwen: tf=7.7696 first=0.0000 kCE=6.2798 KD=2.9203 | scale_pen(qwen)=1.9867e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5727 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3612
  step  3620/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=9.5511 first=0.0000 kCE=8.5681 KD=4.0053 | scale_pen(llama)=1.1416e-06 | qwen: tf=10.5437 first=0.0000 kCE=7.3418 KD=5.1775 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5728 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3630/5475 | grad_norm=nan | sec/step~3.01 | llama: tf=8.8058 first=0.0000 kCE=8.8410 KD=3.1073 | scale_pen(llama)=1.1416e-06 | qwen: tf=7.7430 first=0.0000 kCE=7.0447 KD=3.9463 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5728 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3640/5475 | grad_norm=nan | sec/step~2.77 | llama: tf=9.2234 first=0.0000 kCE=8.3266 KD=3.7334 | scale_pen(llama)=1.1416e-06 | qwen: tf=9.1002 first=0.0000 kCE=7.9964 KD=4.2088 | scale_pen(qwen)=1.9350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5728 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3650/5475 | grad_norm=nan | sec/step~2.85 | llama: tf=9.7835 first=0.0000 kCE=7.7777 KD=3.1454 | scale_pen(llama)=1.0936e-06 | qwen: tf=9.2850 first=0.0000 kCE=6.5633 KD=3.3211 | scale_pen(qwen)=1.8458e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5729 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3660/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=9.5859 first=0.0000 kCE=8.0372 KD=3.3928 | scale_pen(llama)=1.0936e-06 | qwen: tf=10.7973 first=0.0000 kCE=7.4932 KD=4.8719 | scale_pen(qwen)=1.8458e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5729 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3670/5475 | grad_norm=nan | sec/step~2.87 | llama: tf=8.9150 first=0.0000 kCE=8.2932 KD=3.3580 | scale_pen(llama)=1.0936e-06 | qwen: tf=8.7201 first=0.0000 kCE=6.3403 KD=4.7127 | scale_pen(qwen)=1.8458e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5729 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3680/5475 | grad_norm=9.55 | sec/step~3.10 | llama: tf=9.3671 first=0.0000 kCE=8.2894 KD=3.0058 | scale_pen(llama)=1.0482e-06 | qwen: tf=9.0134 first=0.0000 kCE=6.4006 KD=3.9258 | scale_pen(qwen)=1.7829e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5730 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5740 rms_cal~0.0136 embed_rms~0.01363]
  step  3690/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.3180 first=0.0000 kCE=8.2400 KD=3.3721 | scale_pen(llama)=1.0482e-06 | qwen: tf=8.4023 first=0.0000 kCE=7.0130 KD=4.2680 | scale_pen(qwen)=1.7829e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5730 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3700/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=9.7966 first=0.0000 kCE=7.2261 KD=2.9313 | scale_pen(llama)=1.0482e-06 | qwen: tf=9.7768 first=0.0000 kCE=6.9213 KD=3.6844 | scale_pen(qwen)=1.7829e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5730 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3710/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=9.0802 first=0.0000 kCE=8.2226 KD=3.3870 | scale_pen(llama)=1.0482e-06 | qwen: tf=8.3851 first=0.0000 kCE=6.7919 KD=4.1529 | scale_pen(qwen)=1.7829e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5731 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3720/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=9.6204 first=0.0000 kCE=8.1886 KD=3.4609 | scale_pen(llama)=1.0087e-06 | qwen: tf=8.9436 first=0.0000 kCE=6.4206 KD=4.2488 | scale_pen(qwen)=1.7319e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5731 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3730/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=9.3551 first=0.0000 kCE=7.3349 KD=2.8987 | scale_pen(llama)=1.0087e-06 | qwen: tf=8.8183 first=0.0000 kCE=6.4320 KD=3.9677 | scale_pen(qwen)=1.7319e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5731 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3740/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=9.5024 first=0.0000 kCE=7.8517 KD=3.2980 | scale_pen(llama)=1.0087e-06 | qwen: tf=10.6707 first=0.0000 kCE=7.8084 KD=3.8007 | scale_pen(qwen)=1.7319e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5732 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3750/5475 | grad_norm=nan | sec/step~3.04 | llama: tf=9.0628 first=0.0000 kCE=9.0110 KD=3.7768 | scale_pen(llama)=9.6828e-07 | qwen: tf=8.9725 first=0.0000 kCE=8.0098 KD=4.0388 | scale_pen(qwen)=1.7309e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5732 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3760/5475 | grad_norm=nan | sec/step~2.35 | llama: tf=8.8999 first=0.0000 kCE=8.1526 KD=3.5292 | scale_pen(llama)=9.6828e-07 | qwen: tf=9.8310 first=0.0000 kCE=8.0427 KD=4.2402 | scale_pen(qwen)=1.7309e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5732 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3770/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=8.8687 first=0.0000 kCE=7.6888 KD=2.8608 | scale_pen(llama)=9.6828e-07 | qwen: tf=9.6060 first=0.0000 kCE=7.1885 KD=3.9517 | scale_pen(qwen)=1.7309e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5733 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3780/5475 | grad_norm=nan | sec/step~3.02 | llama: tf=9.6387 first=0.0000 kCE=8.3471 KD=2.8898 | scale_pen(llama)=9.2686e-07 | qwen: tf=10.3651 first=0.0000 kCE=7.5574 KD=3.9581 | scale_pen(qwen)=1.7052e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5733 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3784
  step  3790/5475 | grad_norm=nan | sec/step~2.91 | llama: tf=9.3933 first=0.0000 kCE=8.0692 KD=3.0433 | scale_pen(llama)=9.2686e-07 | qwen: tf=9.2287 first=0.0000 kCE=7.2099 KD=4.0903 | scale_pen(qwen)=1.7052e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5733 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5741 rms_cal~0.0136 embed_rms~0.01363]
  step  3800/5475 | grad_norm=nan | sec/step~2.78 | llama: tf=9.4495 first=0.0000 kCE=7.4508 KD=3.0706 | scale_pen(llama)=9.2686e-07 | qwen: tf=9.8863 first=0.0000 kCE=7.1767 KD=3.7784 | scale_pen(qwen)=1.7052e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5734 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3810/5475 | grad_norm=nan | sec/step~3.20 | llama: tf=9.5437 first=0.0000 kCE=7.8926 KD=3.3478 | scale_pen(llama)=8.8219e-07 | qwen: tf=10.0306 first=0.0000 kCE=6.7559 KD=4.2386 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5734 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3820/5475 | grad_norm=nan | sec/step~2.29 | llama: tf=9.1338 first=0.0000 kCE=7.6300 KD=3.8986 | scale_pen(llama)=8.8219e-07 | qwen: tf=10.1396 first=0.0000 kCE=7.2779 KD=4.5943 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5734 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3830/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=9.0706 first=0.0000 kCE=8.1229 KD=3.6606 | scale_pen(llama)=8.8219e-07 | qwen: tf=8.9636 first=0.0000 kCE=6.8765 KD=3.7836 | scale_pen(qwen)=1.6964e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5735 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3840/5475 | grad_norm=16.04 | sec/step~3.05 | llama: tf=9.2266 first=0.0000 kCE=7.7359 KD=3.2612 | scale_pen(llama)=8.4409e-07 | qwen: tf=9.6572 first=0.0000 kCE=6.7718 KD=4.2977 | scale_pen(qwen)=1.6225e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5735 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3850/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=8.9074 first=0.0000 kCE=8.1316 KD=3.3661 | scale_pen(llama)=8.4409e-07 | qwen: tf=8.0910 first=0.0000 kCE=6.6329 KD=4.0087 | scale_pen(qwen)=1.6225e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5735 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3860/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=9.3912 first=0.0000 kCE=7.6400 KD=3.5737 | scale_pen(llama)=8.4409e-07 | qwen: tf=9.6037 first=0.0000 kCE=6.4129 KD=4.3726 | scale_pen(qwen)=1.6225e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5735 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3870/5475 | grad_norm=nan | sec/step~2.29 | llama: tf=8.8091 first=0.0000 kCE=7.7811 KD=3.0928 | scale_pen(llama)=8.4409e-07 | qwen: tf=7.8689 first=0.0000 kCE=5.9966 KD=3.7075 | scale_pen(qwen)=1.6225e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5736 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3880/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=8.9781 first=0.0000 kCE=7.6146 KD=3.3011 | scale_pen(llama)=8.1467e-07 | qwen: tf=8.1605 first=0.0000 kCE=5.2536 KD=4.4837 | scale_pen(qwen)=1.5317e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5736 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3890/5475 | grad_norm=nan | sec/step~2.93 | llama: tf=8.7033 first=0.0000 kCE=7.5443 KD=3.1472 | scale_pen(llama)=8.1467e-07 | qwen: tf=8.6725 first=0.0000 kCE=6.3138 KD=4.1356 | scale_pen(qwen)=1.5317e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5736 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3900/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=8.9336 first=0.0000 kCE=7.3021 KD=3.7358 | scale_pen(llama)=8.1467e-07 | qwen: tf=8.8483 first=0.0000 kCE=6.1841 KD=4.4560 | scale_pen(qwen)=1.5317e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5737 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3910/5475 | grad_norm=nan | sec/step~3.80 | llama: tf=9.2950 first=0.0000 kCE=8.2978 KD=3.0693 | scale_pen(llama)=7.8853e-07 | qwen: tf=9.4533 first=0.0000 kCE=6.0968 KD=4.6109 | scale_pen(qwen)=1.4506e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5737 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3920/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=9.9023 first=0.0000 kCE=7.4369 KD=2.6062 | scale_pen(llama)=7.8853e-07 | qwen: tf=9.2105 first=0.0000 kCE=5.4716 KD=4.0562 | scale_pen(qwen)=1.4506e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5737 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5742 rms_cal~0.0136 embed_rms~0.01363]
  step  3930/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=9.3464 first=0.0000 kCE=7.0173 KD=3.4369 | scale_pen(llama)=7.8853e-07 | qwen: tf=7.7139 first=0.0000 kCE=5.4774 KD=4.0989 | scale_pen(qwen)=1.4506e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5738 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  3940/5475 | grad_norm=nan | sec/step~2.23 | llama: tf=9.6711 first=0.0000 kCE=7.8964 KD=3.7041 | scale_pen(llama)=7.6437e-07 | qwen: tf=9.9124 first=0.0000 kCE=6.6545 KD=4.3354 | scale_pen(qwen)=1.3807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5738 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  3950/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.0476 first=0.0000 kCE=7.5126 KD=3.4139 | scale_pen(llama)=7.6437e-07 | qwen: tf=9.8145 first=0.0000 kCE=6.6746 KD=4.3907 | scale_pen(qwen)=1.3807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5738 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 3956
  step  3960/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=9.9905 first=0.0000 kCE=7.2617 KD=3.3251 | scale_pen(llama)=7.6437e-07 | qwen: tf=10.9926 first=0.0000 kCE=6.2740 KD=4.8750 | scale_pen(qwen)=1.3807e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5739 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  3970/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=9.2594 first=0.0000 kCE=7.2251 KD=4.5850 | scale_pen(llama)=7.3905e-07 | qwen: tf=9.4027 first=0.0000 kCE=7.1390 KD=5.4764 | scale_pen(qwen)=1.3411e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5739 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  3980/5475 | grad_norm=nan | sec/step~2.87 | llama: tf=9.0343 first=0.0000 kCE=7.9994 KD=3.3134 | scale_pen(llama)=7.3905e-07 | qwen: tf=9.0367 first=0.0000 kCE=6.0229 KD=4.2552 | scale_pen(qwen)=1.3411e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5739 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  3990/5475 | grad_norm=nan | sec/step~2.77 | llama: tf=9.5049 first=0.0000 kCE=7.7990 KD=3.5276 | scale_pen(llama)=7.3905e-07 | qwen: tf=9.4543 first=0.0000 kCE=6.5927 KD=4.1213 | scale_pen(qwen)=1.3411e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5740 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4000/5475 | grad_norm=16.14 | sec/step~2.60 | llama: tf=9.4763 first=0.0000 kCE=7.6428 KD=3.6261 | scale_pen(llama)=7.2710e-07 | qwen: tf=8.8550 first=0.0000 kCE=6.6964 KD=4.0237 | scale_pen(qwen)=1.3613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5740 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4010/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.3141 first=0.0000 kCE=7.7189 KD=3.4162 | scale_pen(llama)=7.2710e-07 | qwen: tf=10.4405 first=0.0000 kCE=7.4819 KD=4.2167 | scale_pen(qwen)=1.3613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5740 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4020/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=8.8758 first=0.0000 kCE=8.0499 KD=2.9568 | scale_pen(llama)=7.2710e-07 | qwen: tf=7.7627 first=0.0000 kCE=6.0081 KD=3.9330 | scale_pen(qwen)=1.3613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5741 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4030/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=9.3707 first=0.0000 kCE=8.2345 KD=3.7367 | scale_pen(llama)=7.2710e-07 | qwen: tf=9.4201 first=0.0000 kCE=6.8997 KD=4.3651 | scale_pen(qwen)=1.3613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5741 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4040/5475 | grad_norm=nan | sec/step~3.28 | llama: tf=8.7722 first=0.0000 kCE=7.7852 KD=2.8397 | scale_pen(llama)=7.1385e-07 | qwen: tf=8.1837 first=0.0000 kCE=6.0383 KD=3.9542 | scale_pen(qwen)=1.3376e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5741 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4050/5475 | grad_norm=nan | sec/step~2.90 | llama: tf=9.5664 first=0.0000 kCE=7.4464 KD=2.8381 | scale_pen(llama)=7.1385e-07 | qwen: tf=8.0614 first=0.0000 kCE=6.9823 KD=3.3538 | scale_pen(qwen)=1.3376e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5742 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4060/5475 | grad_norm=nan | sec/step~3.50 | llama: tf=9.3714 first=0.0000 kCE=8.0300 KD=2.6238 | scale_pen(llama)=7.1385e-07 | qwen: tf=8.3188 first=0.0000 kCE=6.3011 KD=4.2217 | scale_pen(qwen)=1.3376e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5742 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5743 rms_cal~0.0136 embed_rms~0.01363]
  step  4070/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=9.4460 first=0.0000 kCE=7.2418 KD=3.4861 | scale_pen(llama)=7.1667e-07 | qwen: tf=9.1152 first=0.0000 kCE=6.3280 KD=4.5657 | scale_pen(qwen)=1.3569e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5742 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4080/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=8.8645 first=0.0000 kCE=7.8122 KD=3.7681 | scale_pen(llama)=7.1667e-07 | qwen: tf=8.1712 first=0.0000 kCE=6.0766 KD=4.3473 | scale_pen(qwen)=1.3569e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5743 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4090/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.6033 first=0.0000 kCE=7.7081 KD=3.6827 | scale_pen(llama)=7.1667e-07 | qwen: tf=9.0251 first=0.0000 kCE=6.8005 KD=4.1518 | scale_pen(qwen)=1.3569e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5743 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4100/5475 | grad_norm=nan | sec/step~2.95 | llama: tf=9.5190 first=0.0000 kCE=7.4006 KD=3.1619 | scale_pen(llama)=7.1930e-07 | qwen: tf=9.3218 first=0.0000 kCE=5.9487 KD=3.8664 | scale_pen(qwen)=1.3168e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5743 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4110/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.7939 first=0.0000 kCE=7.9272 KD=3.3945 | scale_pen(llama)=7.1930e-07 | qwen: tf=9.4786 first=0.0000 kCE=7.1484 KD=4.0536 | scale_pen(qwen)=1.3168e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5744 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4120/5475 | grad_norm=nan | sec/step~2.18 | llama: tf=8.8639 first=0.0000 kCE=8.5693 KD=3.2417 | scale_pen(llama)=7.1930e-07 | qwen: tf=8.5704 first=0.0000 kCE=6.7865 KD=3.7010 | scale_pen(qwen)=1.3168e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5744 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4128
  step  4130/5475 | grad_norm=nan | sec/step~2.57 | llama: tf=9.1372 first=0.0000 kCE=7.1564 KD=3.0102 | scale_pen(llama)=7.2112e-07 | qwen: tf=10.5596 first=0.0000 kCE=6.6596 KD=4.2797 | scale_pen(qwen)=1.3021e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5744 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4140/5475 | grad_norm=nan | sec/step~2.41 | llama: tf=9.1860 first=0.0000 kCE=7.3099 KD=3.9570 | scale_pen(llama)=7.2112e-07 | qwen: tf=9.8296 first=0.0000 kCE=6.7969 KD=4.9312 | scale_pen(qwen)=1.3021e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5745 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4150/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.3564 first=0.0000 kCE=7.5834 KD=2.9444 | scale_pen(llama)=7.2112e-07 | qwen: tf=8.8498 first=0.0000 kCE=5.8677 KD=3.7032 | scale_pen(qwen)=1.3021e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5745 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4160/5475 | grad_norm=20.66 | sec/step~2.65 | llama: tf=9.1244 first=0.0000 kCE=7.5623 KD=3.7020 | scale_pen(llama)=7.3006e-07 | qwen: tf=8.2236 first=0.0000 kCE=6.2744 KD=4.5895 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5745 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4170/5475 | grad_norm=nan | sec/step~2.64 | llama: tf=9.3065 first=0.0000 kCE=8.0313 KD=3.2255 | scale_pen(llama)=7.3006e-07 | qwen: tf=9.6932 first=0.0000 kCE=5.9274 KD=4.9448 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5746 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4180/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=10.2201 first=0.0000 kCE=7.9197 KD=3.1728 | scale_pen(llama)=7.3006e-07 | qwen: tf=8.2681 first=0.0000 kCE=6.6603 KD=3.6028 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5746 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4190/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=8.8760 first=0.0000 kCE=7.7489 KD=3.3836 | scale_pen(llama)=7.3006e-07 | qwen: tf=8.5784 first=0.0000 kCE=6.3950 KD=3.6611 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5746 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5744 rms_cal~0.0136 embed_rms~0.01363]
  step  4200/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=8.9931 first=0.0000 kCE=6.9672 KD=3.2071 | scale_pen(llama)=7.5026e-07 | qwen: tf=8.6236 first=0.0000 kCE=5.8494 KD=3.4231 | scale_pen(qwen)=1.3394e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5747 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4210/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=9.5647 first=0.0000 kCE=7.9532 KD=3.4935 | scale_pen(llama)=7.5026e-07 | qwen: tf=9.5245 first=0.0000 kCE=6.7400 KD=4.1584 | scale_pen(qwen)=1.3394e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5747 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4220/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=9.3275 first=0.0000 kCE=7.7860 KD=3.1082 | scale_pen(llama)=7.5026e-07 | qwen: tf=9.5453 first=0.0000 kCE=6.3083 KD=4.0071 | scale_pen(qwen)=1.3394e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5747 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4230/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=8.6688 first=0.0000 kCE=7.4381 KD=2.8276 | scale_pen(llama)=7.7987e-07 | qwen: tf=7.9309 first=0.0000 kCE=5.9144 KD=3.9145 | scale_pen(qwen)=1.2884e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5748 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4240/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=9.7753 first=0.0000 kCE=7.5076 KD=3.4121 | scale_pen(llama)=7.7987e-07 | qwen: tf=9.0844 first=0.0000 kCE=6.5153 KD=3.6851 | scale_pen(qwen)=1.2884e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5748 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4250/5475 | grad_norm=nan | sec/step~2.36 | llama: tf=9.1935 first=0.0000 kCE=7.7660 KD=3.7554 | scale_pen(llama)=7.7987e-07 | qwen: tf=8.1697 first=0.0000 kCE=6.8384 KD=3.7896 | scale_pen(qwen)=1.2884e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5748 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4260/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=9.3380 first=0.0000 kCE=6.7392 KD=3.3058 | scale_pen(llama)=8.1038e-07 | qwen: tf=10.7008 first=0.0000 kCE=5.9646 KD=4.0486 | scale_pen(qwen)=1.2901e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5749 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4270/5475 | grad_norm=nan | sec/step~3.17 | llama: tf=9.3741 first=0.0000 kCE=7.2903 KD=2.7685 | scale_pen(llama)=8.1038e-07 | qwen: tf=8.8224 first=0.0000 kCE=6.1058 KD=3.4666 | scale_pen(qwen)=1.2901e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5749 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4280/5475 | grad_norm=nan | sec/step~2.41 | llama: tf=9.4776 first=0.0000 kCE=8.1349 KD=3.1969 | scale_pen(llama)=8.1038e-07 | qwen: tf=8.6652 first=0.0000 kCE=6.0941 KD=4.8076 | scale_pen(qwen)=1.2901e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5749 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4290/5475 | grad_norm=nan | sec/step~2.08 | llama: tf=9.0298 first=0.0000 kCE=7.9863 KD=3.9183 | scale_pen(llama)=8.4212e-07 | qwen: tf=9.8069 first=0.0000 kCE=7.0043 KD=5.2641 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5750 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4300/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=9.0365 first=0.0000 kCE=6.8955 KD=2.9248 | scale_pen(llama)=8.4212e-07 | qwen: tf=10.1002 first=0.0000 kCE=6.5891 KD=3.5791 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5750 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4300
  step  4310/5475 | grad_norm=nan | sec/step~2.92 | llama: tf=8.6601 first=0.0000 kCE=7.6794 KD=2.8301 | scale_pen(llama)=8.4212e-07 | qwen: tf=8.3058 first=0.0000 kCE=5.2838 KD=3.9661 | scale_pen(qwen)=1.3542e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5750 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4320/5475 | grad_norm=18.18 | sec/step~2.66 | llama: tf=9.6987 first=0.0000 kCE=6.8933 KD=2.6499 | scale_pen(llama)=8.7951e-07 | qwen: tf=9.6259 first=0.0000 kCE=5.7213 KD=3.7404 | scale_pen(qwen)=1.4497e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5751 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4330/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.2238 first=0.0000 kCE=7.4877 KD=3.1938 | scale_pen(llama)=8.7951e-07 | qwen: tf=7.6174 first=0.0000 kCE=6.6824 KD=4.2827 | scale_pen(qwen)=1.4497e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5751 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4340/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=8.8246 first=0.0000 kCE=7.4819 KD=3.6693 | scale_pen(llama)=8.7951e-07 | qwen: tf=8.2514 first=0.0000 kCE=5.5610 KD=4.0632 | scale_pen(qwen)=1.4497e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5751 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5745 rms_cal~0.0136 embed_rms~0.01363]
  step  4350/5475 | grad_norm=nan | sec/step~3.36 | llama: tf=9.6833 first=0.0000 kCE=7.0054 KD=3.6438 | scale_pen(llama)=8.7951e-07 | qwen: tf=10.3269 first=0.0000 kCE=6.9113 KD=4.6252 | scale_pen(qwen)=1.4497e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5752 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4360/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=9.3512 first=0.0000 kCE=7.3128 KD=3.0258 | scale_pen(llama)=9.2262e-07 | qwen: tf=9.6247 first=0.0000 kCE=6.4077 KD=4.0070 | scale_pen(qwen)=1.4525e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5752 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4370/5475 | grad_norm=nan | sec/step~2.15 | llama: tf=8.9764 first=0.0000 kCE=7.3461 KD=3.5732 | scale_pen(llama)=9.2262e-07 | qwen: tf=9.1535 first=0.0000 kCE=5.9082 KD=4.2283 | scale_pen(qwen)=1.4525e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5752 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4380/5475 | grad_norm=nan | sec/step~2.23 | llama: tf=8.7590 first=0.0000 kCE=7.3983 KD=3.1175 | scale_pen(llama)=9.2262e-07 | qwen: tf=8.6702 first=0.0000 kCE=5.7896 KD=3.7766 | scale_pen(qwen)=1.4525e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5752 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4390/5475 | grad_norm=nan | sec/step~3.00 | llama: tf=8.1797 first=0.0000 kCE=7.7164 KD=2.6096 | scale_pen(llama)=9.6922e-07 | qwen: tf=7.0537 first=0.0000 kCE=5.4732 KD=3.4186 | scale_pen(qwen)=1.3727e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5753 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4400/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=9.2276 first=0.0000 kCE=7.8850 KD=3.6512 | scale_pen(llama)=9.6922e-07 | qwen: tf=9.6296 first=0.0000 kCE=6.9937 KD=4.4379 | scale_pen(qwen)=1.3727e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5753 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4410/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=8.9692 first=0.0000 kCE=7.7509 KD=3.2118 | scale_pen(llama)=9.6922e-07 | qwen: tf=8.4213 first=0.0000 kCE=5.8701 KD=4.1235 | scale_pen(qwen)=1.3727e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5753 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4420/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=9.0004 first=0.0000 kCE=7.7551 KD=3.4647 | scale_pen(llama)=1.0217e-06 | qwen: tf=8.0190 first=0.0000 kCE=6.7527 KD=4.5157 | scale_pen(qwen)=1.2832e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5754 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4430/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=9.9917 first=0.0000 kCE=7.5311 KD=4.0719 | scale_pen(llama)=1.0217e-06 | qwen: tf=10.2821 first=0.0000 kCE=7.5040 KD=4.1288 | scale_pen(qwen)=1.2832e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5754 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4440/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=9.2211 first=0.0000 kCE=7.4866 KD=3.0202 | scale_pen(llama)=1.0217e-06 | qwen: tf=8.8240 first=0.0000 kCE=5.6562 KD=3.7927 | scale_pen(qwen)=1.2832e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5754 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4450/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=8.3444 first=0.0000 kCE=8.1296 KD=2.8054 | scale_pen(llama)=1.0720e-06 | qwen: tf=7.6985 first=0.0000 kCE=6.9761 KD=3.5189 | scale_pen(qwen)=1.2739e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5755 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4460/5475 | grad_norm=nan | sec/step~2.78 | llama: tf=8.5984 first=0.0000 kCE=8.5943 KD=3.4419 | scale_pen(llama)=1.0720e-06 | qwen: tf=8.6491 first=0.0000 kCE=7.1736 KD=4.0501 | scale_pen(qwen)=1.2739e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5755 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4470/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=9.0557 first=0.0000 kCE=7.0435 KD=3.0988 | scale_pen(llama)=1.0720e-06 | qwen: tf=9.9100 first=0.0000 kCE=6.6444 KD=4.0361 | scale_pen(qwen)=1.2739e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5755 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4472
  step  4480/5475 | grad_norm=40.66 | sec/step~2.13 | llama: tf=8.6244 first=0.0000 kCE=8.1067 KD=3.2962 | scale_pen(llama)=1.1236e-06 | qwen: tf=8.4785 first=0.0000 kCE=5.8833 KD=3.8685 | scale_pen(qwen)=1.3350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5756 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4490/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=8.7549 first=0.0000 kCE=6.9613 KD=3.0910 | scale_pen(llama)=1.1236e-06 | qwen: tf=8.1971 first=0.0000 kCE=6.0267 KD=3.6745 | scale_pen(qwen)=1.3350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5756 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4500/5475 | grad_norm=nan | sec/step~2.65 | llama: tf=9.2032 first=0.0000 kCE=7.2657 KD=3.4063 | scale_pen(llama)=1.1236e-06 | qwen: tf=10.0060 first=0.0000 kCE=6.5396 KD=4.1631 | scale_pen(qwen)=1.3350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5756 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5746 rms_cal~0.0136 embed_rms~0.01363]
  step  4510/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=9.0236 first=0.0000 kCE=7.9308 KD=3.4181 | scale_pen(llama)=1.1236e-06 | qwen: tf=9.1322 first=0.0000 kCE=5.9451 KD=4.5427 | scale_pen(qwen)=1.3350e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5756 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4520/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=9.5231 first=0.0000 kCE=7.5951 KD=3.7514 | scale_pen(llama)=1.1781e-06 | qwen: tf=10.7635 first=0.0000 kCE=7.2729 KD=4.5082 | scale_pen(qwen)=1.4534e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5757 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4530/5475 | grad_norm=nan | sec/step~2.48 | llama: tf=9.3031 first=0.0000 kCE=7.1090 KD=2.7583 | scale_pen(llama)=1.1781e-06 | qwen: tf=9.9645 first=0.0000 kCE=6.1368 KD=4.0904 | scale_pen(qwen)=1.4534e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5757 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4540/5475 | grad_norm=nan | sec/step~2.31 | llama: tf=9.6447 first=0.0000 kCE=7.9594 KD=3.4819 | scale_pen(llama)=1.1781e-06 | qwen: tf=9.6788 first=0.0000 kCE=6.5435 KD=4.2047 | scale_pen(qwen)=1.4534e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5757 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4550/5475 | grad_norm=nan | sec/step~2.67 | llama: tf=8.9924 first=0.0000 kCE=7.1107 KD=2.8110 | scale_pen(llama)=1.2517e-06 | qwen: tf=7.6956 first=0.0000 kCE=5.3743 KD=3.5983 | scale_pen(qwen)=1.5787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5758 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4560/5475 | grad_norm=nan | sec/step~2.89 | llama: tf=9.1115 first=0.0000 kCE=6.6556 KD=3.0167 | scale_pen(llama)=1.2517e-06 | qwen: tf=9.0490 first=0.0000 kCE=5.3370 KD=4.7906 | scale_pen(qwen)=1.5787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5758 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4570/5475 | grad_norm=nan | sec/step~2.76 | llama: tf=8.9959 first=0.0000 kCE=6.9412 KD=3.3066 | scale_pen(llama)=1.2517e-06 | qwen: tf=8.7774 first=0.0000 kCE=5.9483 KD=3.6640 | scale_pen(qwen)=1.5787e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5758 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4580/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=8.2624 first=0.0000 kCE=7.8479 KD=3.1347 | scale_pen(llama)=1.3224e-06 | qwen: tf=7.8324 first=0.0000 kCE=6.3196 KD=3.5679 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5759 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4590/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=9.3425 first=0.0000 kCE=6.5821 KD=3.0621 | scale_pen(llama)=1.3224e-06 | qwen: tf=7.9191 first=0.0000 kCE=4.3427 KD=4.0017 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5759 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4600/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=9.4404 first=0.0000 kCE=7.2268 KD=2.8518 | scale_pen(llama)=1.3224e-06 | qwen: tf=8.7894 first=0.0000 kCE=5.4651 KD=3.9351 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5759 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4610/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.5520 first=0.0000 kCE=7.8426 KD=4.0688 | scale_pen(llama)=1.3747e-06 | qwen: tf=10.4665 first=0.0000 kCE=6.7537 KD=5.1574 | scale_pen(qwen)=1.8684e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5759 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4620/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=9.9286 first=0.0000 kCE=7.4902 KD=3.6906 | scale_pen(llama)=1.3747e-06 | qwen: tf=10.0094 first=0.0000 kCE=6.5811 KD=4.6319 | scale_pen(qwen)=1.8684e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5760 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4630/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=9.4855 first=0.0000 kCE=6.8670 KD=2.8375 | scale_pen(llama)=1.3747e-06 | qwen: tf=8.5010 first=0.0000 kCE=5.0116 KD=3.7025 | scale_pen(qwen)=1.8684e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5760 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5747 rms_cal~0.0136 embed_rms~0.01363]
  step  4640/5475 | grad_norm=13.21 | sec/step~2.51 | llama: tf=8.9300 first=0.0000 kCE=6.8013 KD=3.0990 | scale_pen(llama)=1.4248e-06 | qwen: tf=8.5336 first=0.0000 kCE=5.6591 KD=3.6290 | scale_pen(qwen)=1.8932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5760 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4644
  step  4650/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=10.1018 first=0.0000 kCE=7.1816 KD=3.0381 | scale_pen(llama)=1.4248e-06 | qwen: tf=9.7474 first=0.0000 kCE=5.7557 KD=3.8780 | scale_pen(qwen)=1.8932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5761 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4660/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=8.5625 first=0.0000 kCE=6.8799 KD=3.3108 | scale_pen(llama)=1.4248e-06 | qwen: tf=8.5479 first=0.0000 kCE=5.4501 KD=4.4954 | scale_pen(qwen)=1.8932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5761 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4670/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=9.2772 first=0.0000 kCE=6.9245 KD=3.3238 | scale_pen(llama)=1.4248e-06 | qwen: tf=10.5510 first=0.0000 kCE=5.7652 KD=4.5210 | scale_pen(qwen)=1.8932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5761 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4680/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=8.9949 first=0.0000 kCE=6.3760 KD=3.0289 | scale_pen(llama)=1.5034e-06 | qwen: tf=8.3728 first=0.0000 kCE=4.7764 KD=4.1133 | scale_pen(qwen)=1.8499e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5761 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4690/5475 | grad_norm=nan | sec/step~2.44 | llama: tf=8.6574 first=0.0000 kCE=6.4687 KD=3.2398 | scale_pen(llama)=1.5034e-06 | qwen: tf=9.0739 first=0.0000 kCE=5.6395 KD=4.2838 | scale_pen(qwen)=1.8499e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5762 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4700/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=8.9529 first=0.0000 kCE=7.0470 KD=3.2505 | scale_pen(llama)=1.5034e-06 | qwen: tf=8.8535 first=0.0000 kCE=5.7876 KD=4.0617 | scale_pen(qwen)=1.8499e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5762 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4710/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=8.9551 first=0.0000 kCE=7.3323 KD=4.0932 | scale_pen(llama)=1.6117e-06 | qwen: tf=9.0737 first=0.0000 kCE=5.9282 KD=5.1573 | scale_pen(qwen)=1.7578e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5762 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4720/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=9.0308 first=0.0000 kCE=7.4296 KD=3.2699 | scale_pen(llama)=1.6117e-06 | qwen: tf=9.2095 first=0.0000 kCE=5.8167 KD=3.9776 | scale_pen(qwen)=1.7578e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5762 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4730/5475 | grad_norm=nan | sec/step~2.25 | llama: tf=8.8276 first=0.0000 kCE=7.0173 KD=3.3393 | scale_pen(llama)=1.6117e-06 | qwen: tf=8.2332 first=0.0000 kCE=5.4073 KD=3.5073 | scale_pen(qwen)=1.7578e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5763 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4740/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=9.6763 first=0.0000 kCE=7.6016 KD=3.3672 | scale_pen(llama)=1.7011e-06 | qwen: tf=10.4923 first=0.0000 kCE=6.2427 KD=4.1131 | scale_pen(qwen)=1.6408e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5763 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5748 rms_cal~0.0136 embed_rms~0.01363]
  step  4750/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=8.7309 first=0.0000 kCE=6.2291 KD=2.7410 | scale_pen(llama)=1.7011e-06 | qwen: tf=8.4341 first=0.0000 kCE=4.7773 KD=3.6753 | scale_pen(qwen)=1.6408e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5763 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4760/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=8.3933 first=0.0000 kCE=6.9277 KD=3.2246 | scale_pen(llama)=1.7011e-06 | qwen: tf=7.9400 first=0.0000 kCE=5.7113 KD=4.0558 | scale_pen(qwen)=1.6408e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5764 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4770/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.3743 first=0.0000 kCE=7.4905 KD=3.2251 | scale_pen(llama)=1.7852e-06 | qwen: tf=9.4115 first=0.0000 kCE=7.0190 KD=4.7029 | scale_pen(qwen)=1.5326e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5764 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4780/5475 | grad_norm=nan | sec/step~2.75 | llama: tf=10.0942 first=0.0000 kCE=7.1849 KD=3.8889 | scale_pen(llama)=1.7852e-06 | qwen: tf=11.6459 first=0.0000 kCE=6.9784 KD=4.3939 | scale_pen(qwen)=1.5326e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5764 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4790/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=9.3908 first=0.0000 kCE=7.5349 KD=3.6481 | scale_pen(llama)=1.7852e-06 | qwen: tf=8.6237 first=0.0000 kCE=6.1599 KD=4.3760 | scale_pen(qwen)=1.5326e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5764 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4800/5475 | grad_norm=17.33 | sec/step~2.65 | llama: tf=9.2930 first=0.0000 kCE=7.2878 KD=3.3591 | scale_pen(llama)=1.8730e-06 | qwen: tf=8.8967 first=0.0000 kCE=5.9269 KD=4.6253 | scale_pen(qwen)=1.5149e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5765 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4810/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=9.1583 first=0.0000 kCE=7.5103 KD=3.4953 | scale_pen(llama)=1.8730e-06 | qwen: tf=8.9459 first=0.0000 kCE=6.9354 KD=4.2368 | scale_pen(qwen)=1.5149e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5765 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4816
  step  4820/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=9.7568 first=0.0000 kCE=6.8220 KD=3.0348 | scale_pen(llama)=1.8730e-06 | qwen: tf=9.8301 first=0.0000 kCE=6.4183 KD=3.6775 | scale_pen(qwen)=1.5149e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5765 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4830/5475 | grad_norm=nan | sec/step~3.82 | llama: tf=9.0517 first=0.0000 kCE=6.6750 KD=2.8610 | scale_pen(llama)=1.8730e-06 | qwen: tf=10.6977 first=0.0000 kCE=7.0148 KD=3.4036 | scale_pen(qwen)=1.5149e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5765 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4840/5475 | grad_norm=nan | sec/step~2.41 | llama: tf=9.5047 first=0.0000 kCE=6.3727 KD=2.9063 | scale_pen(llama)=1.9595e-06 | qwen: tf=9.0515 first=0.0000 kCE=6.1487 KD=3.1088 | scale_pen(qwen)=1.5560e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5766 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5749 rms_cal~0.0136 embed_rms~0.01363]
  step  4850/5475 | grad_norm=nan | sec/step~2.84 | llama: tf=9.0216 first=0.0000 kCE=7.2059 KD=3.3086 | scale_pen(llama)=1.9595e-06 | qwen: tf=9.6915 first=0.0000 kCE=6.1214 KD=4.2512 | scale_pen(qwen)=1.5560e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5766 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4860/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=8.9916 first=0.0000 kCE=6.7708 KD=3.1961 | scale_pen(llama)=1.9595e-06 | qwen: tf=9.9055 first=0.0000 kCE=6.3126 KD=3.7607 | scale_pen(qwen)=1.5560e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5766 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4870/5475 | grad_norm=nan | sec/step~3.14 | llama: tf=9.2089 first=0.0000 kCE=6.7208 KD=2.5608 | scale_pen(llama)=2.0522e-06 | qwen: tf=9.1812 first=0.0000 kCE=5.1643 KD=3.7696 | scale_pen(qwen)=1.6476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5767 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4880/5475 | grad_norm=nan | sec/step~2.14 | llama: tf=9.4792 first=0.0000 kCE=7.4031 KD=3.6177 | scale_pen(llama)=2.0522e-06 | qwen: tf=9.3179 first=0.0000 kCE=6.1888 KD=3.8214 | scale_pen(qwen)=1.6476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5767 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4890/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=9.2861 first=0.0000 kCE=6.7463 KD=3.6681 | scale_pen(llama)=2.0522e-06 | qwen: tf=10.5554 first=0.0000 kCE=6.9957 KD=4.7480 | scale_pen(qwen)=1.6476e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5767 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4900/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.4535 first=0.0000 kCE=6.8029 KD=3.1701 | scale_pen(llama)=2.1874e-06 | qwen: tf=9.7734 first=0.0000 kCE=5.3627 KD=4.1315 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5767 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4910/5475 | grad_norm=nan | sec/step~3.43 | llama: tf=9.2677 first=0.0000 kCE=7.4871 KD=2.8322 | scale_pen(llama)=2.1874e-06 | qwen: tf=8.5214 first=0.0000 kCE=5.5091 KD=4.1329 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5768 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4920/5475 | grad_norm=nan | sec/step~2.38 | llama: tf=10.4606 first=0.0000 kCE=7.1931 KD=3.3700 | scale_pen(llama)=2.1874e-06 | qwen: tf=8.9510 first=0.0000 kCE=5.9947 KD=3.5145 | scale_pen(qwen)=1.7688e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5768 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4930/5475 | grad_norm=nan | sec/step~2.22 | llama: tf=8.5414 first=0.0000 kCE=6.9835 KD=3.2155 | scale_pen(llama)=2.3449e-06 | qwen: tf=8.2057 first=0.0000 kCE=5.5014 KD=4.4848 | scale_pen(qwen)=1.8356e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5768 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4940/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=9.5908 first=0.0000 kCE=6.4681 KD=3.0974 | scale_pen(llama)=2.3449e-06 | qwen: tf=9.6170 first=0.0000 kCE=4.7773 KD=3.8535 | scale_pen(qwen)=1.8356e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5768 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5750 rms_cal~0.0136 embed_rms~0.01363]
  step  4950/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=9.4352 first=0.0000 kCE=7.0696 KD=2.8031 | scale_pen(llama)=2.3449e-06 | qwen: tf=10.0916 first=0.0000 kCE=5.5970 KD=4.4147 | scale_pen(qwen)=1.8356e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5769 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  4960/5475 | grad_norm=35.58 | sec/step~3.04 | llama: tf=9.0754 first=0.0000 kCE=6.4932 KD=2.7235 | scale_pen(llama)=2.4840e-06 | qwen: tf=8.8837 first=0.0000 kCE=5.5586 KD=3.5429 | scale_pen(qwen)=1.9571e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5769 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  4970/5475 | grad_norm=nan | sec/step~2.19 | llama: tf=9.1690 first=0.0000 kCE=6.4482 KD=3.6565 | scale_pen(llama)=2.4840e-06 | qwen: tf=9.5941 first=0.0000 kCE=5.9113 KD=4.3437 | scale_pen(qwen)=1.9571e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5769 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  4980/5475 | grad_norm=nan | sec/step~2.41 | llama: tf=8.7243 first=0.0000 kCE=6.9092 KD=3.4346 | scale_pen(llama)=2.4840e-06 | qwen: tf=8.1267 first=0.0000 kCE=4.8268 KD=3.7697 | scale_pen(qwen)=1.9571e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5769 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 4988
  step  4990/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=9.5691 first=0.0000 kCE=7.5347 KD=4.0240 | scale_pen(llama)=2.4840e-06 | qwen: tf=10.0552 first=0.0000 kCE=5.9621 KD=5.2643 | scale_pen(qwen)=1.9571e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5770 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  5000/5475 | grad_norm=nan | sec/step~3.79 | llama: tf=9.4899 first=0.0000 kCE=7.2688 KD=2.8895 | scale_pen(llama)=2.5936e-06 | qwen: tf=8.6068 first=0.0000 kCE=5.1982 KD=3.6788 | scale_pen(qwen)=2.0607e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5770 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  5010/5475 | grad_norm=nan | sec/step~2.46 | llama: tf=9.3644 first=0.0000 kCE=6.5989 KD=2.8305 | scale_pen(llama)=2.5936e-06 | qwen: tf=8.6581 first=0.0000 kCE=5.1952 KD=4.0277 | scale_pen(qwen)=2.0607e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5770 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  5020/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=9.2169 first=0.0000 kCE=7.3477 KD=3.2530 | scale_pen(llama)=2.5936e-06 | qwen: tf=9.0085 first=0.0000 kCE=6.5395 KD=3.8734 | scale_pen(qwen)=2.0607e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5770 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  5030/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=9.8137 first=0.0000 kCE=6.7688 KD=3.3544 | scale_pen(llama)=2.6942e-06 | qwen: tf=9.5936 first=0.0000 kCE=5.1541 KD=4.0338 | scale_pen(qwen)=2.1086e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5771 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5751 rms_cal~0.0136 embed_rms~0.01363]
  step  5040/5475 | grad_norm=nan | sec/step~2.83 | llama: tf=8.9352 first=0.0000 kCE=6.4146 KD=3.2003 | scale_pen(llama)=2.6942e-06 | qwen: tf=9.2287 first=0.0000 kCE=5.5316 KD=4.2446 | scale_pen(qwen)=2.1086e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5771 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5050/5475 | grad_norm=nan | sec/step~2.23 | llama: tf=8.9402 first=0.0000 kCE=6.9337 KD=3.0131 | scale_pen(llama)=2.6942e-06 | qwen: tf=8.9046 first=0.0000 kCE=5.8349 KD=4.2165 | scale_pen(qwen)=2.1086e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5771 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5060/5475 | grad_norm=nan | sec/step~2.80 | llama: tf=8.9297 first=0.0000 kCE=7.1050 KD=3.4371 | scale_pen(llama)=2.8087e-06 | qwen: tf=8.9173 first=0.0000 kCE=5.1196 KD=4.8461 | scale_pen(qwen)=2.1372e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5772 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5070/5475 | grad_norm=nan | sec/step~2.34 | llama: tf=8.7786 first=0.0000 kCE=6.6438 KD=3.3123 | scale_pen(llama)=2.8087e-06 | qwen: tf=9.5780 first=0.0000 kCE=4.9854 KD=4.3936 | scale_pen(qwen)=2.1372e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5772 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5080/5475 | grad_norm=nan | sec/step~2.47 | llama: tf=8.8617 first=0.0000 kCE=7.0841 KD=3.4122 | scale_pen(llama)=2.8087e-06 | qwen: tf=8.8131 first=0.0000 kCE=5.6528 KD=3.9426 | scale_pen(qwen)=2.1372e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5772 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5090/5475 | grad_norm=nan | sec/step~5.12 | llama: tf=8.2197 first=0.0000 kCE=7.5580 KD=2.6044 | scale_pen(llama)=2.9490e-06 | qwen: tf=8.3852 first=0.0000 kCE=6.2351 KD=4.0525 | scale_pen(qwen)=2.1637e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5772 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5100/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=8.6807 first=0.0000 kCE=6.6829 KD=3.7467 | scale_pen(llama)=2.9490e-06 | qwen: tf=8.5599 first=0.0000 kCE=5.1742 KD=4.2677 | scale_pen(qwen)=2.1637e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5773 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5110/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=9.1280 first=0.0000 kCE=6.2846 KD=2.9389 | scale_pen(llama)=2.9490e-06 | qwen: tf=10.5252 first=0.0000 kCE=5.8393 KD=4.0763 | scale_pen(qwen)=2.1637e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5773 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5752 rms_cal~0.0136 embed_rms~0.01363]
  step  5120/5475 | grad_norm=7.38 | sec/step~2.45 | llama: tf=9.2109 first=0.0000 kCE=6.2131 KD=3.7134 | scale_pen(llama)=3.0796e-06 | qwen: tf=8.7594 first=0.0000 kCE=4.9308 KD=4.1038 | scale_pen(qwen)=2.1748e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5773 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5130/5475 | grad_norm=nan | sec/step~2.24 | llama: tf=8.8707 first=0.0000 kCE=7.0877 KD=3.4239 | scale_pen(llama)=3.0796e-06 | qwen: tf=8.2214 first=0.0000 kCE=5.7208 KD=3.9733 | scale_pen(qwen)=2.1748e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5773 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5140/5475 | grad_norm=nan | sec/step~2.45 | llama: tf=9.0230 first=0.0000 kCE=7.0238 KD=3.3955 | scale_pen(llama)=3.0796e-06 | qwen: tf=8.1880 first=0.0000 kCE=5.5658 KD=3.8944 | scale_pen(qwen)=2.1748e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5774 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5150/5475 | grad_norm=nan | sec/step~3.01 | llama: tf=9.4359 first=0.0000 kCE=6.6659 KD=3.2385 | scale_pen(llama)=3.0796e-06 | qwen: tf=9.4503 first=0.0000 kCE=4.9313 KD=3.9053 | scale_pen(qwen)=2.1748e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5774 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5160/5475 | grad_norm=nan | sec/step~2.88 | llama: tf=9.1565 first=0.0000 kCE=7.1376 KD=2.8953 | scale_pen(llama)=3.1785e-06 | qwen: tf=9.7801 first=0.0000 kCE=6.6379 KD=3.2557 | scale_pen(qwen)=2.1316e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5774 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5160
  step  5170/5475 | grad_norm=nan | sec/step~3.06 | llama: tf=9.0418 first=0.0000 kCE=6.7460 KD=3.1384 | scale_pen(llama)=3.1785e-06 | qwen: tf=8.4165 first=0.0000 kCE=4.9474 KD=4.1580 | scale_pen(qwen)=2.1316e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5774 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5180/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=9.3273 first=0.0000 kCE=7.1141 KD=2.7681 | scale_pen(llama)=3.1785e-06 | qwen: tf=10.2368 first=0.0000 kCE=6.4812 KD=3.4520 | scale_pen(qwen)=2.1316e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5775 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5190/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=8.9916 first=0.0000 kCE=7.0897 KD=3.2791 | scale_pen(llama)=3.2772e-06 | qwen: tf=8.7117 first=0.0000 kCE=6.2210 KD=4.1511 | scale_pen(qwen)=2.0531e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5775 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5753 rms_cal~0.0136 embed_rms~0.01363]
  step  5200/5475 | grad_norm=nan | sec/step~3.29 | llama: tf=9.4377 first=0.0000 kCE=6.9807 KD=3.3423 | scale_pen(llama)=3.2772e-06 | qwen: tf=8.5499 first=0.0000 kCE=5.7096 KD=3.5773 | scale_pen(qwen)=2.0531e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5775 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5210/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=9.6745 first=0.0000 kCE=6.6407 KD=3.1800 | scale_pen(llama)=3.2772e-06 | qwen: tf=8.4713 first=0.0000 kCE=5.6256 KD=4.0338 | scale_pen(qwen)=2.0531e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5775 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5220/5475 | grad_norm=nan | sec/step~2.25 | llama: tf=8.4859 first=0.0000 kCE=6.9936 KD=2.8606 | scale_pen(llama)=3.4502e-06 | qwen: tf=8.6578 first=0.0000 kCE=6.3446 KD=3.3367 | scale_pen(qwen)=2.0715e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5776 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5230/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=8.6425 first=0.0000 kCE=7.1718 KD=3.0146 | scale_pen(llama)=3.4502e-06 | qwen: tf=7.7324 first=0.0000 kCE=6.3474 KD=3.6703 | scale_pen(qwen)=2.0715e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5776 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5240/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=9.0159 first=0.0000 kCE=6.3573 KD=3.2227 | scale_pen(llama)=3.4502e-06 | qwen: tf=8.3109 first=0.0000 kCE=5.1435 KD=3.8272 | scale_pen(qwen)=2.0715e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5776 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5250/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=8.8153 first=0.0000 kCE=7.1241 KD=3.2154 | scale_pen(llama)=3.6614e-06 | qwen: tf=9.3280 first=0.0000 kCE=5.6391 KD=4.3650 | scale_pen(qwen)=2.1141e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5776 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5260/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.1275 first=0.0000 kCE=6.8616 KD=3.1687 | scale_pen(llama)=3.6614e-06 | qwen: tf=9.5255 first=0.0000 kCE=6.0007 KD=3.9260 | scale_pen(qwen)=2.1141e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5777 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5270/5475 | grad_norm=nan | sec/step~2.61 | llama: tf=8.7912 first=0.0000 kCE=7.0876 KD=3.1581 | scale_pen(llama)=3.6614e-06 | qwen: tf=9.6575 first=0.0000 kCE=6.6255 KD=4.4515 | scale_pen(qwen)=2.1141e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5777 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5754 rms_cal~0.0136 embed_rms~0.01363]
  step  5280/5475 | grad_norm=59.49 | sec/step~2.47 | llama: tf=8.7700 first=0.0000 kCE=6.7617 KD=3.7345 | scale_pen(llama)=3.8602e-06 | qwen: tf=9.1966 first=0.0000 kCE=5.4844 KD=3.9102 | scale_pen(qwen)=2.2184e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5777 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5290/5475 | grad_norm=nan | sec/step~3.79 | llama: tf=9.0521 first=0.0000 kCE=6.8935 KD=2.8790 | scale_pen(llama)=3.8602e-06 | qwen: tf=8.7664 first=0.0000 kCE=5.5450 KD=3.3800 | scale_pen(qwen)=2.2184e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5777 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5300/5475 | grad_norm=nan | sec/step~2.07 | llama: tf=9.8708 first=0.0000 kCE=6.5721 KD=3.3360 | scale_pen(llama)=3.8602e-06 | qwen: tf=12.5599 first=0.0000 kCE=6.0980 KD=3.4815 | scale_pen(qwen)=2.2184e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5778 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5310/5475 | grad_norm=nan | sec/step~3.15 | llama: tf=8.8530 first=0.0000 kCE=6.5283 KD=3.4819 | scale_pen(llama)=3.8602e-06 | qwen: tf=9.5040 first=0.0000 kCE=5.0446 KD=4.6353 | scale_pen(qwen)=2.2184e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5778 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5320/5475 | grad_norm=nan | sec/step~2.19 | llama: tf=8.5079 first=0.0000 kCE=6.6386 KD=3.0975 | scale_pen(llama)=4.0420e-06 | qwen: tf=8.2494 first=0.0000 kCE=5.0321 KD=3.9879 | scale_pen(qwen)=2.3702e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5778 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5330/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=9.3636 first=0.0000 kCE=7.3160 KD=3.3666 | scale_pen(llama)=4.0420e-06 | qwen: tf=10.9463 first=0.0000 kCE=6.7612 KD=3.7622 | scale_pen(qwen)=2.3702e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5778 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5332
  step  5340/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=8.9143 first=0.0000 kCE=7.0270 KD=3.2011 | scale_pen(llama)=4.0420e-06 | qwen: tf=8.1174 first=0.0000 kCE=5.7706 KD=3.3749 | scale_pen(qwen)=2.3702e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5779 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5350/5475 | grad_norm=nan | sec/step~3.28 | llama: tf=9.1977 first=0.0000 kCE=7.1554 KD=2.8489 | scale_pen(llama)=4.1997e-06 | qwen: tf=9.0209 first=0.0000 kCE=5.6049 KD=4.0059 | scale_pen(qwen)=2.5475e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5779 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5755 rms_cal~0.0136 embed_rms~0.01363]
  step  5360/5475 | grad_norm=nan | sec/step~3.28 | llama: tf=8.5583 first=0.0000 kCE=7.4528 KD=2.5921 | scale_pen(llama)=4.1997e-06 | qwen: tf=8.6650 first=0.0000 kCE=5.5369 KD=4.0333 | scale_pen(qwen)=2.5475e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5779 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5370/5475 | grad_norm=nan | sec/step~2.16 | llama: tf=9.1229 first=0.0000 kCE=6.5727 KD=3.3845 | scale_pen(llama)=4.1997e-06 | qwen: tf=9.5818 first=0.0000 kCE=5.6802 KD=4.1018 | scale_pen(qwen)=2.5475e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5779 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5380/5475 | grad_norm=nan | sec/step~3.03 | llama: tf=9.3283 first=0.0000 kCE=6.9560 KD=3.6690 | scale_pen(llama)=4.3670e-06 | qwen: tf=10.5496 first=0.0000 kCE=5.9464 KD=4.4719 | scale_pen(qwen)=2.7675e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5780 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5390/5475 | grad_norm=nan | sec/step~3.19 | llama: tf=9.0159 first=0.0000 kCE=6.6591 KD=3.3354 | scale_pen(llama)=4.3670e-06 | qwen: tf=7.7495 first=0.0000 kCE=4.7866 KD=3.8548 | scale_pen(qwen)=2.7675e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5780 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5400/5475 | grad_norm=nan | sec/step~3.04 | llama: tf=9.3133 first=0.0000 kCE=7.0222 KD=2.6828 | scale_pen(llama)=4.3670e-06 | qwen: tf=9.1412 first=0.0000 kCE=5.4889 KD=3.4410 | scale_pen(qwen)=2.7675e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5780 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5410/5475 | grad_norm=nan | sec/step~3.50 | llama: tf=9.5918 first=0.0000 kCE=6.6048 KD=2.7130 | scale_pen(llama)=4.5485e-06 | qwen: tf=9.1753 first=0.0000 kCE=5.2400 KD=3.1868 | scale_pen(qwen)=2.8969e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5780 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5420/5475 | grad_norm=nan | sec/step~2.43 | llama: tf=9.3492 first=0.0000 kCE=6.7188 KD=3.2417 | scale_pen(llama)=4.5485e-06 | qwen: tf=9.8032 first=0.0000 kCE=5.3775 KD=4.4532 | scale_pen(qwen)=2.8969e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5781 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5756 rms_cal~0.0136 embed_rms~0.01363]
  step  5430/5475 | grad_norm=nan | sec/step~2.94 | llama: tf=9.1527 first=0.0000 kCE=7.0852 KD=3.2726 | scale_pen(llama)=4.5485e-06 | qwen: tf=9.7092 first=0.0000 kCE=5.7661 KD=4.7861 | scale_pen(qwen)=2.8969e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5781 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
  step  5440/5475 | grad_norm=20.37 | sec/step~3.07 | llama: tf=9.0784 first=0.0000 kCE=6.3283 KD=3.5352 | scale_pen(llama)=4.7682e-06 | qwen: tf=9.0105 first=0.0000 kCE=4.6590 KD=4.2552 | scale_pen(qwen)=2.9342e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5781 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
  step  5450/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=9.2893 first=0.0000 kCE=6.6513 KD=3.5191 | scale_pen(llama)=4.7682e-06 | qwen: tf=9.3975 first=0.0000 kCE=5.7452 KD=4.8173 | scale_pen(qwen)=2.9342e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5781 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
  step  5460/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=9.6490 first=0.0000 kCE=7.5055 KD=3.4508 | scale_pen(llama)=4.7682e-06 | qwen: tf=10.4505 first=0.0000 kCE=6.7670 KD=4.6250 | scale_pen(qwen)=2.9342e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5781 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
  step  5470/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=8.9884 first=0.0000 kCE=6.1685 KD=3.1183 | scale_pen(llama)=4.7682e-06 | qwen: tf=9.1559 first=0.0000 kCE=4.9627 KD=3.9874 | scale_pen(qwen)=2.9342e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5782 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
  step  5475/5475 | grad_norm=2.77 | sec/step~2.34 | llama: tf=10.1298 first=0.0000 kCE=6.0785 KD=3.4239 | scale_pen(llama)=5.1970e-06 | qwen: tf=11.8595 first=0.0000 kCE=5.5276 KD=3.9095 | scale_pen(qwen)=2.9809e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5782 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.5757 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_with_k/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.5781840841193178, 'rms_mean_cal': 0.01057134444089649, 'embed_rms': 0.01057521253824234, 'count': 5475}, 'qwen': {'rms_mean_raw': 0.5757188950063975, 'rms_mean_cal': 0.01364105942416681, 'embed_rms': 0.013628026470541954, 'count': 5475}}

Evaluating epoch 1 checkpoint...
Evaluating: runs/8B_clean_with_k/epoch1 -> runs/8B_clean_with_k/eval_epoch1
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_with_k/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_with_k/eval_epoch1/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3000.22it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw'] | nlls={'raw': 9.19357649431207} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_with_k/eval_epoch1/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.59120 -> target=0.01057
[debug:llama] adapter.scale=0.9977 | Z.std=0.5284 Z.mean||=8.5125 | prefix.std=0.0122 prefix.mean||=0.7782 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'January 1, 1, 1, 1'
  1: '1945. 1945. 1945'
  2: 'to be used in the production of a new type of explosive'
  3: 'fifteen years after the death of the Prophet, the'
  4: 'October 1, 1, 1, 1'
Saved Llama results to runs/8B_clean_with_k/eval_epoch1/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3512.82it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.14it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw'] | nlls={'raw': 8.734972384083209} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_with_k/eval_epoch1/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.58500 -> target=0.01363
[debug:qwen] adapter.scale=1.0005 | Z.std=0.5284 Z.mean||=8.5125 | prefix.std=0.0157 prefix.mean||=0.9382 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '666666666666'
  1: '# 1: The first sentence is a statement about'
  2: 'not not not not not not not not not not not not'
  3: '0 0 0 0 0 0'
  4: '# 1: The first sentence is a statement about'
Saved Qwen results to runs/8B_clean_with_k/eval_epoch1/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3177.50it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5966.29it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 32768 bytes (fp32), and 16400 bytes (fp16); latent/text bytes (one-copy, fp16): 13.03x
Selected latent bytes (6-bit): 6672 bytes

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.20s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.008  |  NLL/token (gold): 9.433984365052376
       First-token acc: top1=0.000  top5=0.030
Qwen   EM: 0.000   F1: 0.000  |  NLL/token (gold): 8.748859485778858
       First-token acc: top1=0.000  top5=0.075
Wall clock: 13.31s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 11.23s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.001
Inter-model agreement (normalized): 0.015
Oracle upper bound:  EM 0.000  F1 0.008

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 32768,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 32784,
      "fp16": 16400,
      "int8": 8720,
      "int6": 6672,
      "int4": 4624
    },
    "selected_latent_bytes": 6672,
    "wire_ratio": {
      "latent_over_onecopy_fp16": 13.026211278792692,
      "latent_over_onecopy_fp32": 26.039714058776806,
      "selected_over_onecopy": 5.299444003177125
    }
  },
  "text": {
    "wall_clock_sec": 16.201311111450195,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.305988550186157,
    "llama": {
      "em": 0.0,
      "f1": 0.008140534903692799,
      "nll_token": 9.433984365052376,
      "first_token_top1": 0.0,
      "first_token_top5": 0.029999999329447746
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 8.748859485778858,
      "first_token_top1": 0.0,
      "first_token_top5": 0.07499999552965164
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04086507936507937
    },
    "wall_clock_sec": 11.225218772888184,
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711954711955
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0006666666666666666,
    "agreement": 0.015,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 0.9977203011512756,
      "Z_std": 0.528378427028656,
      "Z_mean_norm": 8.512521743774414,
      "prefix_std": 0.012158080004155636,
      "prefix_mean_norm": 0.7781969904899597,
      "embed_rms": 0.010569815523922443,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0005459785461426,
      "Z_std": 0.528378427028656,
      "Z_mean_norm": 8.512521743774414,
      "prefix_std": 0.015670046210289,
      "prefix_mean_norm": 0.9381944537162781,
      "embed_rms": 0.01364040095359087,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.15,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "no",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.95,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.008140534903692799
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_with_k/eval_epoch1/predictions.jsonl

✓ Metrics from: runs/8B_clean_with_k/eval_epoch1/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.008 | Qwen 0.000
  FirstTok@1:  Llama 0.000 | Qwen 0.000
  FirstTok@5:  Llama 0.030 | Qwen 0.075


=========================================
EPOCH 2/24
=========================================

Training epoch 2...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7034.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.03it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4110.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.17it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment could not be verified: list index out of range
[WARN] t=0 alignment could not be verified: list index out of range
⏪ Resuming from: runs/8B_clean_with_k/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=1, global_step=5475
Epoch 2/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/5475 | grad_norm=nan | sec/step~2.94 | llama: tf=9.0729 first=0.0000 kCE=6.3314 KD=3.4996 | scale_pen(llama)=5.1970e-06 | qwen: tf=7.9061 first=0.0000 kCE=4.4534 KD=4.3215 | scale_pen(qwen)=2.9809e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  20/5475 | grad_norm=nan | sec/step~2.89 | llama: tf=9.6075 first=0.0000 kCE=7.1588 KD=3.4435 | scale_pen(llama)=5.1970e-06 | qwen: tf=9.5749 first=0.0000 kCE=6.1917 KD=4.2847 | scale_pen(qwen)=2.9809e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5504
  step  30/5475 | grad_norm=nan | sec/step~2.88 | llama: tf=9.2817 first=0.0000 kCE=6.7004 KD=2.4994 | scale_pen(llama)=5.1970e-06 | qwen: tf=8.7245 first=0.0000 kCE=5.7523 KD=4.3359 | scale_pen(qwen)=2.9809e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5916 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  40/5475 | grad_norm=nan | sec/step~2.59 | llama: tf=9.2677 first=0.0000 kCE=6.6041 KD=3.3522 | scale_pen(llama)=5.3638e-06 | qwen: tf=7.2131 first=0.0000 kCE=4.5781 KD=3.6955 | scale_pen(qwen)=3.0609e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5837 rms_cal~0.0136 embed_rms~0.01364]
  step  50/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=8.8639 first=0.0000 kCE=6.6199 KD=3.7233 | scale_pen(llama)=5.3638e-06 | qwen: tf=9.8187 first=0.0000 kCE=5.2085 KD=4.5010 | scale_pen(qwen)=3.0609e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  60/5475 | grad_norm=nan | sec/step~2.33 | llama: tf=8.8684 first=0.0000 kCE=6.1501 KD=3.4768 | scale_pen(llama)=5.3638e-06 | qwen: tf=7.4097 first=0.0000 kCE=4.0784 KD=4.1731 | scale_pen(qwen)=3.0609e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  70/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=9.2677 first=0.0000 kCE=6.6666 KD=3.4119 | scale_pen(llama)=5.5738e-06 | qwen: tf=9.9022 first=0.0000 kCE=5.7831 KD=3.7913 | scale_pen(qwen)=3.1419e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5835 rms_cal~0.0136 embed_rms~0.01364]
  step  80/5475 | grad_norm=nan | sec/step~3.23 | llama: tf=8.9857 first=0.0000 kCE=7.0572 KD=3.4079 | scale_pen(llama)=5.5738e-06 | qwen: tf=10.3261 first=0.0000 kCE=6.2838 KD=4.9383 | scale_pen(qwen)=3.1419e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  90/5475 | grad_norm=nan | sec/step~2.30 | llama: tf=9.2546 first=0.0000 kCE=6.6259 KD=3.6488 | scale_pen(llama)=5.5738e-06 | qwen: tf=10.2012 first=0.0000 kCE=6.2846 KD=4.2776 | scale_pen(qwen)=3.1419e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  100/5475 | grad_norm=nan | sec/step~2.50 | llama: tf=8.9581 first=0.0000 kCE=6.6914 KD=3.3286 | scale_pen(llama)=5.8579e-06 | qwen: tf=8.8297 first=0.0000 kCE=5.6321 KD=4.5182 | scale_pen(qwen)=3.1888e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  110/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=9.4623 first=0.0000 kCE=6.1604 KD=3.3746 | scale_pen(llama)=5.8579e-06 | qwen: tf=8.2547 first=0.0000 kCE=4.8401 KD=4.2135 | scale_pen(qwen)=3.1888e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5836 rms_cal~0.0136 embed_rms~0.01364]
  step  120/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=8.7690 first=0.0000 kCE=6.6786 KD=3.2053 | scale_pen(llama)=5.8579e-06 | qwen: tf=8.2083 first=0.0000 kCE=4.8858 KD=4.1817 | scale_pen(qwen)=3.1888e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5837 rms_cal~0.0136 embed_rms~0.01364]
  step  130/5475 | grad_norm=nan | sec/step~2.39 | llama: tf=8.8674 first=0.0000 kCE=6.5921 KD=2.8067 | scale_pen(llama)=6.1358e-06 | qwen: tf=8.6234 first=0.0000 kCE=4.7822 KD=3.3162 | scale_pen(qwen)=3.3207e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5837 rms_cal~0.0136 embed_rms~0.01364]
  step  140/5475 | grad_norm=nan | sec/step~2.71 | llama: tf=9.1087 first=0.0000 kCE=6.8336 KD=3.2560 | scale_pen(llama)=6.1358e-06 | qwen: tf=9.1145 first=0.0000 kCE=5.5708 KD=4.2767 | scale_pen(qwen)=3.3207e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5837 rms_cal~0.0136 embed_rms~0.01364]
  step  150/5475 | grad_norm=nan | sec/step~3.13 | llama: tf=8.9853 first=0.0000 kCE=6.3701 KD=2.8992 | scale_pen(llama)=6.1358e-06 | qwen: tf=9.5758 first=0.0000 kCE=5.3649 KD=4.1329 | scale_pen(qwen)=3.3207e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5837 rms_cal~0.0136 embed_rms~0.01364]
  step  160/5475 | grad_norm=16.04 | sec/step~2.73 | llama: tf=8.8538 first=0.0000 kCE=6.5262 KD=2.7500 | scale_pen(llama)=6.3644e-06 | qwen: tf=7.7265 first=0.0000 kCE=4.4146 KD=3.8033 | scale_pen(qwen)=3.3731e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5838 rms_cal~0.0136 embed_rms~0.01364]
  step  170/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=8.7535 first=0.0000 kCE=6.8153 KD=3.4245 | scale_pen(llama)=6.3644e-06 | qwen: tf=8.5897 first=0.0000 kCE=4.8975 KD=4.1555 | scale_pen(qwen)=3.3731e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5838 rms_cal~0.0136 embed_rms~0.01364]
  step  180/5475 | grad_norm=nan | sec/step~3.38 | llama: tf=8.9966 first=0.0000 kCE=6.6138 KD=2.9509 | scale_pen(llama)=6.3644e-06 | qwen: tf=8.9625 first=0.0000 kCE=5.0822 KD=4.1176 | scale_pen(qwen)=3.3731e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5838 rms_cal~0.0136 embed_rms~0.01364]
  step  190/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.3454 first=0.0000 kCE=7.2825 KD=3.4915 | scale_pen(llama)=6.3644e-06 | qwen: tf=9.9925 first=0.0000 kCE=6.6060 KD=4.4328 | scale_pen(qwen)=3.3731e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  200/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=8.8572 first=0.0000 kCE=6.6658 KD=3.3897 | scale_pen(llama)=6.5223e-06 | qwen: tf=8.4180 first=0.0000 kCE=5.1203 KD=4.1940 | scale_pen(qwen)=3.3828e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5676
  step  210/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=8.7648 first=0.0000 kCE=6.5305 KD=3.3400 | scale_pen(llama)=6.5223e-06 | qwen: tf=8.8103 first=0.0000 kCE=4.9805 KD=4.2952 | scale_pen(qwen)=3.3828e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  220/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=8.9665 first=0.0000 kCE=7.0181 KD=3.2431 | scale_pen(llama)=6.5223e-06 | qwen: tf=7.2772 first=0.0000 kCE=5.0971 KD=3.7677 | scale_pen(qwen)=3.3828e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5917 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  230/5475 | grad_norm=nan | sec/step~2.40 | llama: tf=9.0044 first=0.0000 kCE=6.4062 KD=3.7600 | scale_pen(llama)=6.7127e-06 | qwen: tf=8.9614 first=0.0000 kCE=5.4606 KD=4.3561 | scale_pen(qwen)=3.4708e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  240/5475 | grad_norm=nan | sec/step~2.70 | llama: tf=9.4912 first=0.0000 kCE=6.1662 KD=3.3055 | scale_pen(llama)=6.7127e-06 | qwen: tf=9.8163 first=0.0000 kCE=5.3641 KD=4.5064 | scale_pen(qwen)=3.4708e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  250/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=8.8335 first=0.0000 kCE=6.4877 KD=3.5428 | scale_pen(llama)=6.7127e-06 | qwen: tf=9.0744 first=0.0000 kCE=5.6468 KD=4.2812 | scale_pen(qwen)=3.4708e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  260/5475 | grad_norm=nan | sec/step~2.07 | llama: tf=9.4615 first=0.0000 kCE=6.4286 KD=3.4410 | scale_pen(llama)=6.9188e-06 | qwen: tf=9.1720 first=0.0000 kCE=4.8131 KD=3.9951 | scale_pen(qwen)=3.6327e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  270/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=8.8298 first=0.0000 kCE=6.3617 KD=3.1310 | scale_pen(llama)=6.9188e-06 | qwen: tf=8.5471 first=0.0000 kCE=5.6097 KD=3.8792 | scale_pen(qwen)=3.6327e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5839 rms_cal~0.0136 embed_rms~0.01364]
  step  280/5475 | grad_norm=nan | sec/step~2.55 | llama: tf=9.2584 first=0.0000 kCE=6.4872 KD=3.1116 | scale_pen(llama)=6.9188e-06 | qwen: tf=8.9278 first=0.0000 kCE=5.5386 KD=4.1871 | scale_pen(qwen)=3.6327e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  290/5475 | grad_norm=nan | sec/step~2.51 | llama: tf=9.4776 first=0.0000 kCE=6.6991 KD=3.2112 | scale_pen(llama)=7.1199e-06 | qwen: tf=8.4659 first=0.0000 kCE=5.4062 KD=3.7605 | scale_pen(qwen)=3.7618e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  300/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=9.3684 first=0.0000 kCE=7.6063 KD=3.4694 | scale_pen(llama)=7.1199e-06 | qwen: tf=9.7770 first=0.0000 kCE=6.6387 KD=4.0797 | scale_pen(qwen)=3.7618e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  310/5475 | grad_norm=nan | sec/step~2.53 | llama: tf=9.2576 first=0.0000 kCE=6.2773 KD=3.7883 | scale_pen(llama)=7.1199e-06 | qwen: tf=9.3746 first=0.0000 kCE=5.0850 KD=4.9812 | scale_pen(qwen)=3.7618e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5918 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  320/5475 | grad_norm=13.72 | sec/step~2.50 | llama: tf=9.3147 first=0.0000 kCE=6.6063 KD=3.5596 | scale_pen(llama)=7.2670e-06 | qwen: tf=9.2443 first=0.0000 kCE=5.7762 KD=4.1151 | scale_pen(qwen)=3.8678e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  330/5475 | grad_norm=nan | sec/step~2.78 | llama: tf=8.8203 first=0.0000 kCE=6.4565 KD=3.5899 | scale_pen(llama)=7.2670e-06 | qwen: tf=8.3552 first=0.0000 kCE=5.4970 KD=4.8166 | scale_pen(qwen)=3.8678e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  340/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.3564 first=0.0000 kCE=5.7965 KD=3.4171 | scale_pen(llama)=7.2670e-06 | qwen: tf=9.3317 first=0.0000 kCE=5.2243 KD=3.9237 | scale_pen(qwen)=3.8678e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  350/5475 | grad_norm=nan | sec/step~2.56 | llama: tf=9.0989 first=0.0000 kCE=6.9714 KD=3.5277 | scale_pen(llama)=7.2670e-06 | qwen: tf=8.7985 first=0.0000 kCE=5.2431 KD=4.2803 | scale_pen(qwen)=3.8678e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  360/5475 | grad_norm=nan | sec/step~2.28 | llama: tf=9.3789 first=0.0000 kCE=6.6232 KD=3.4292 | scale_pen(llama)=7.3977e-06 | qwen: tf=9.8312 first=0.0000 kCE=5.7957 KD=4.0463 | scale_pen(qwen)=3.9738e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5840 rms_cal~0.0136 embed_rms~0.01364]
  step  370/5475 | grad_norm=nan | sec/step~2.63 | llama: tf=9.0632 first=0.0000 kCE=6.8608 KD=3.4063 | scale_pen(llama)=7.3977e-06 | qwen: tf=8.4079 first=0.0000 kCE=5.0607 KD=3.9742 | scale_pen(qwen)=3.9738e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 5848
  step  380/5475 | grad_norm=nan | sec/step~3.64 | llama: tf=9.1063 first=0.0000 kCE=6.0825 KD=2.5115 | scale_pen(llama)=7.3977e-06 | qwen: tf=8.7276 first=0.0000 kCE=4.4075 KD=3.4390 | scale_pen(qwen)=3.9738e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  390/5475 | grad_norm=nan | sec/step~2.40 | llama: tf=9.2052 first=0.0000 kCE=7.0866 KD=3.4073 | scale_pen(llama)=7.5624e-06 | qwen: tf=9.5082 first=0.0000 kCE=5.5684 KD=4.3627 | scale_pen(qwen)=4.0660e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5919 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  400/5475 | grad_norm=nan | sec/step~2.32 | llama: tf=8.7857 first=0.0000 kCE=7.0463 KD=3.1587 | scale_pen(llama)=7.5624e-06 | qwen: tf=8.4727 first=0.0000 kCE=5.5472 KD=3.7765 | scale_pen(qwen)=4.0660e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  410/5475 | grad_norm=nan | sec/step~2.68 | llama: tf=8.8117 first=0.0000 kCE=6.5089 KD=2.8586 | scale_pen(llama)=7.5624e-06 | qwen: tf=8.1433 first=0.0000 kCE=4.5676 KD=4.4586 | scale_pen(qwen)=4.0660e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  420/5475 | grad_norm=nan | sec/step~2.16 | llama: tf=9.2600 first=0.0000 kCE=6.4085 KD=2.9221 | scale_pen(llama)=7.7428e-06 | qwen: tf=9.4876 first=0.0000 kCE=4.8993 KD=3.4675 | scale_pen(qwen)=4.1117e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  430/5475 | grad_norm=nan | sec/step~2.49 | llama: tf=9.7138 first=0.0000 kCE=6.8411 KD=2.8087 | scale_pen(llama)=7.7428e-06 | qwen: tf=9.8070 first=0.0000 kCE=6.3586 KD=3.8765 | scale_pen(qwen)=4.1117e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  440/5475 | grad_norm=nan | sec/step~2.60 | llama: tf=9.5251 first=0.0000 kCE=6.7277 KD=2.5996 | scale_pen(llama)=7.7428e-06 | qwen: tf=8.7190 first=0.0000 kCE=4.7862 KD=3.3826 | scale_pen(qwen)=4.1117e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  450/5475 | grad_norm=nan | sec/step~2.52 | llama: tf=8.6478 first=0.0000 kCE=6.6074 KD=3.2214 | scale_pen(llama)=7.9343e-06 | qwen: tf=8.5482 first=0.0000 kCE=4.7970 KD=3.5994 | scale_pen(qwen)=4.2442e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  460/5475 | grad_norm=nan | sec/step~2.69 | llama: tf=8.5301 first=0.0000 kCE=6.6305 KD=3.2521 | scale_pen(llama)=7.9343e-06 | qwen: tf=7.9571 first=0.0000 kCE=4.9666 KD=3.8800 | scale_pen(qwen)=4.2442e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  470/5475 | grad_norm=nan | sec/step~2.74 | llama: tf=8.7553 first=0.0000 kCE=6.4664 KD=2.9940 | scale_pen(llama)=7.9343e-06 | qwen: tf=7.3268 first=0.0000 kCE=5.0845 KD=3.8541 | scale_pen(qwen)=4.2442e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  480/5475 | grad_norm=15.08 | sec/step~2.65 | llama: tf=8.7804 first=0.0000 kCE=6.9804 KD=3.1867 | scale_pen(llama)=8.1266e-06 | qwen: tf=9.4120 first=0.0000 kCE=6.0334 KD=4.2969 | scale_pen(qwen)=4.4026e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5920 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  490/5475 | grad_norm=nan | sec/step~2.58 | llama: tf=9.1120 first=0.0000 kCE=6.5397 KD=3.2617 | scale_pen(llama)=8.1266e-06 | qwen: tf=8.8031 first=0.0000 kCE=5.3197 KD=4.1384 | scale_pen(qwen)=4.4026e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  500/5475 | grad_norm=nan | sec/step~2.25 | llama: tf=9.7525 first=0.0000 kCE=6.2010 KD=3.6653 | scale_pen(llama)=8.1266e-06 | qwen: tf=10.4790 first=0.0000 kCE=5.1743 KD=4.1857 | scale_pen(qwen)=4.4026e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  510/5475 | grad_norm=nan | sec/step~2.29 | llama: tf=9.3230 first=0.0000 kCE=7.5968 KD=4.0403 | scale_pen(llama)=8.1266e-06 | qwen: tf=9.2850 first=0.0000 kCE=5.6478 KD=4.9971 | scale_pen(qwen)=4.4026e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  520/5475 | grad_norm=nan | sec/step~3.36 | llama: tf=9.0823 first=0.0000 kCE=7.1574 KD=2.4610 | scale_pen(llama)=8.3180e-06 | qwen: tf=9.1326 first=0.0000 kCE=5.5764 KD=3.8399 | scale_pen(qwen)=4.4980e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  530/5475 | grad_norm=nan | sec/step~2.05 | llama: tf=9.7968 first=0.0000 kCE=7.2874 KD=4.0449 | scale_pen(llama)=8.3180e-06 | qwen: tf=9.1156 first=0.0000 kCE=5.6426 KD=4.5333 | scale_pen(qwen)=4.4980e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  540/5475 | grad_norm=nan | sec/step~2.82 | llama: tf=8.8516 first=0.0000 kCE=6.6942 KD=3.1778 | scale_pen(llama)=8.3180e-06 | qwen: tf=7.5353 first=0.0000 kCE=5.2779 KD=3.8275 | scale_pen(qwen)=4.4980e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 6020
  step  550/5475 | grad_norm=nan | sec/step~2.27 | llama: tf=9.2581 first=0.0000 kCE=5.8052 KD=3.5437 | scale_pen(llama)=8.5106e-06 | qwen: tf=9.6089 first=0.0000 kCE=4.2195 KD=4.4312 | scale_pen(qwen)=4.4613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  560/5475 | grad_norm=nan | sec/step~2.81 | llama: tf=9.5909 first=0.0000 kCE=6.4631 KD=3.5817 | scale_pen(llama)=8.5106e-06 | qwen: tf=10.3048 first=0.0000 kCE=5.8450 KD=4.6849 | scale_pen(qwen)=4.4613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  570/5475 | grad_norm=nan | sec/step~3.15 | llama: tf=9.6151 first=0.0000 kCE=6.3554 KD=3.1090 | scale_pen(llama)=8.5106e-06 | qwen: tf=9.4961 first=0.0000 kCE=5.5617 KD=4.0166 | scale_pen(qwen)=4.4613e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  580/5475 | grad_norm=nan | sec/step~3.07 | llama: tf=8.5571 first=0.0000 kCE=6.6369 KD=3.0129 | scale_pen(llama)=8.6889e-06 | qwen: tf=8.3887 first=0.0000 kCE=5.0921 KD=4.5146 | scale_pen(qwen)=4.4932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  590/5475 | grad_norm=nan | sec/step~2.42 | llama: tf=9.1511 first=0.0000 kCE=6.7604 KD=3.6184 | scale_pen(llama)=8.6889e-06 | qwen: tf=9.0628 first=0.0000 kCE=5.6644 KD=4.5714 | scale_pen(qwen)=4.4932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  600/5475 | grad_norm=nan | sec/step~2.81 | llama: tf=9.4048 first=0.0000 kCE=6.9136 KD=3.0698 | scale_pen(llama)=8.6889e-06 | qwen: tf=9.6069 first=0.0000 kCE=5.2780 KD=3.9266 | scale_pen(qwen)=4.4932e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  610/5475 | grad_norm=nan | sec/step~2.54 | llama: tf=8.9991 first=0.0000 kCE=6.8389 KD=3.1344 | scale_pen(llama)=8.8832e-06 | qwen: tf=8.6187 first=0.0000 kCE=5.4688 KD=4.3465 | scale_pen(qwen)=4.6074e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  620/5475 | grad_norm=nan | sec/step~2.37 | llama: tf=8.9156 first=0.0000 kCE=6.2405 KD=2.8181 | scale_pen(llama)=8.8832e-06 | qwen: tf=7.3218 first=0.0000 kCE=4.0854 KD=3.6086 | scale_pen(qwen)=4.6074e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5841 rms_cal~0.0136 embed_rms~0.01364]
  step  630/5475 | grad_norm=nan | sec/step~3.78 | llama: tf=8.6255 first=0.0000 kCE=6.5541 KD=2.4970 | scale_pen(llama)=8.8832e-06 | qwen: tf=8.5791 first=0.0000 kCE=5.2411 KD=3.5803 | scale_pen(qwen)=4.6074e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5842 rms_cal~0.0136 embed_rms~0.01364]
  step  640/5475 | grad_norm=10.90 | sec/step~2.32 | llama: tf=9.2118 first=0.0000 kCE=5.9661 KD=3.2893 | scale_pen(llama)=9.1603e-06 | qwen: tf=8.4992 first=0.0000 kCE=4.7920 KD=3.8698 | scale_pen(qwen)=4.7312e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5842 rms_cal~0.0136 embed_rms~0.01364]
  step  650/5475 | grad_norm=nan | sec/step~2.91 | llama: tf=9.3049 first=0.0000 kCE=6.5368 KD=3.1595 | scale_pen(llama)=9.1603e-06 | qwen: tf=7.7876 first=0.0000 kCE=4.8150 KD=3.4444 | scale_pen(qwen)=4.7312e-07 | K=4 tau=1.00 | stats=[llama: rms_raw~0.5921 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.5842 rms_cal~0.0136 embed_rms~0.01364]
