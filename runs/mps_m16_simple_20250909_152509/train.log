Loading dataset subset...
Loading HotpotQA subset...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.65s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:05<00:05,  2.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.79s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.19s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.09s/it]
Llama hidden size: 4096, Qwen hidden size: 4096
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/5
  step 10/256 | loss_llama=3.8243 | loss_qwen=1.6124 | grad_norm=13.94 | sec/step~6.65
  step 20/256 | loss_llama=2.1783 | loss_qwen=1.8410 | grad_norm=2.00 | sec/step~6.40
  step 30/256 | loss_llama=1.9329 | loss_qwen=1.6821 | grad_norm=1.56 | sec/step~6.32
  step 40/256 | loss_llama=1.9651 | loss_qwen=1.8168 | grad_norm=1.01 | sec/step~6.27
  step 50/256 | loss_llama=2.0444 | loss_qwen=1.5816 | grad_norm=1.10 | sec/step~6.26
  step 60/256 | loss_llama=1.5426 | loss_qwen=1.2648 | grad_norm=0.91 | sec/step~6.26
  step 70/256 | loss_llama=1.6884 | loss_qwen=1.4612 | grad_norm=1.00 | sec/step~6.25
  step 80/256 | loss_llama=1.9321 | loss_qwen=1.6588 | grad_norm=1.20 | sec/step~6.25
  step 90/256 | loss_llama=1.7645 | loss_qwen=1.4778 | grad_norm=0.99 | sec/step~6.25
  step 100/256 | loss_llama=1.7484 | loss_qwen=1.5197 | grad_norm=0.80 | sec/step~6.25
  step 110/256 | loss_llama=1.5203 | loss_qwen=1.1635 | grad_norm=0.75 | sec/step~6.25
  step 120/256 | loss_llama=1.8315 | loss_qwen=1.5521 | grad_norm=1.27 | sec/step~6.25
  step 130/256 | loss_llama=1.6069 | loss_qwen=1.3126 | grad_norm=0.78 | sec/step~6.26
  step 140/256 | loss_llama=1.7300 | loss_qwen=1.3942 | grad_norm=0.89 | sec/step~6.32
  step 150/256 | loss_llama=1.7239 | loss_qwen=1.3286 | grad_norm=1.00 | sec/step~6.34
  step 160/256 | loss_llama=1.3881 | loss_qwen=1.0125 | grad_norm=0.81 | sec/step~6.37
  step 170/256 | loss_llama=1.6942 | loss_qwen=1.3814 | grad_norm=0.87 | sec/step~6.38
  step 180/256 | loss_llama=1.3646 | loss_qwen=1.1503 | grad_norm=0.87 | sec/step~6.37
  step 190/256 | loss_llama=1.4279 | loss_qwen=1.1806 | grad_norm=1.39 | sec/step~6.38
  step 200/256 | loss_llama=1.6118 | loss_qwen=1.3299 | grad_norm=0.97 | sec/step~6.38
  ✅ Saved checkpoint at step 200
  step 210/256 | loss_llama=1.4258 | loss_qwen=1.2718 | grad_norm=0.95 | sec/step~6.38
  step 220/256 | loss_llama=1.3422 | loss_qwen=1.2238 | grad_norm=1.09 | sec/step~6.36
  step 230/256 | loss_llama=1.7326 | loss_qwen=1.5294 | grad_norm=0.86 | sec/step~6.35
  step 240/256 | loss_llama=1.3978 | loss_qwen=1.2685 | grad_norm=1.20 | sec/step~8.10
  step 250/256 | loss_llama=1.2865 | loss_qwen=1.1935 | grad_norm=0.77 | sec/step~16.12
  step 256/256 | loss_llama=1.7974 | loss_qwen=1.7394 | grad_norm=1.03 | sec/step~14.83
Epoch 2/5
  step 10/256 | loss_llama=1.4847 | loss_qwen=1.3600 | grad_norm=0.94 | sec/step~16.28
  step 20/256 | loss_llama=1.8542 | loss_qwen=1.7156 | grad_norm=1.18 | sec/step~17.31
  step 30/256 | loss_llama=1.3426 | loss_qwen=1.2286 | grad_norm=1.43 | sec/step~10.14
  step 40/256 | loss_llama=1.3297 | loss_qwen=1.1829 | grad_norm=0.95 | sec/step~9.92
  step 50/256 | loss_llama=1.5354 | loss_qwen=1.3554 | grad_norm=1.06 | sec/step~14.15
  step 60/256 | loss_llama=1.3730 | loss_qwen=1.2262 | grad_norm=0.76 | sec/step~10.40
  step 70/256 | loss_llama=1.4206 | loss_qwen=1.3804 | grad_norm=0.99 | sec/step~7.76
  step 80/256 | loss_llama=0.9842 | loss_qwen=0.9268 | grad_norm=0.59 | sec/step~14.12
  step 90/256 | loss_llama=1.3925 | loss_qwen=1.2917 | grad_norm=0.91 | sec/step~14.48
  step 100/256 | loss_llama=1.3464 | loss_qwen=1.3593 | grad_norm=0.66 | sec/step~15.96
  step 110/256 | loss_llama=1.0552 | loss_qwen=1.0051 | grad_norm=1.00 | sec/step~19.71
  step 120/256 | loss_llama=1.6411 | loss_qwen=1.6295 | grad_norm=1.19 | sec/step~11.86
  step 130/256 | loss_llama=1.0999 | loss_qwen=1.0168 | grad_norm=0.68 | sec/step~13.42
  step 140/256 | loss_llama=1.0945 | loss_qwen=1.0016 | grad_norm=1.33 | sec/step~17.15
  ✅ Saved checkpoint at step 400
  step 150/256 | loss_llama=1.3123 | loss_qwen=1.2732 | grad_norm=0.94 | sec/step~10.24
  step 160/256 | loss_llama=2.1504 | loss_qwen=2.2993 | grad_norm=1.67 | sec/step~7.83
  step 170/256 | loss_llama=1.4630 | loss_qwen=1.3482 | grad_norm=0.81 | sec/step~6.97
  step 180/256 | loss_llama=1.3323 | loss_qwen=1.3181 | grad_norm=0.68 | sec/step~6.69
  step 190/256 | loss_llama=0.9758 | loss_qwen=0.9811 | grad_norm=0.48 | sec/step~6.59
  step 200/256 | loss_llama=1.2311 | loss_qwen=1.2235 | grad_norm=0.68 | sec/step~6.54
  step 210/256 | loss_llama=0.9039 | loss_qwen=0.8827 | grad_norm=1.18 | sec/step~6.39
  step 220/256 | loss_llama=1.7257 | loss_qwen=1.7494 | grad_norm=0.93 | sec/step~6.33
  step 230/256 | loss_llama=1.1905 | loss_qwen=1.1657 | grad_norm=0.83 | sec/step~6.30
  step 240/256 | loss_llama=1.9090 | loss_qwen=1.8209 | grad_norm=1.38 | sec/step~6.30
  step 250/256 | loss_llama=1.2714 | loss_qwen=1.2125 | grad_norm=0.58 | sec/step~6.30
  step 256/256 | loss_llama=0.8644 | loss_qwen=0.8739 | grad_norm=0.70 | sec/step~6.30
Epoch 3/5
  step 10/256 | loss_llama=1.2324 | loss_qwen=1.1434 | grad_norm=0.97 | sec/step~6.30
  step 20/256 | loss_llama=1.2307 | loss_qwen=1.2053 | grad_norm=0.64 | sec/step~6.30
  step 30/256 | loss_llama=1.1331 | loss_qwen=1.0759 | grad_norm=0.77 | sec/step~6.29
  step 40/256 | loss_llama=1.2976 | loss_qwen=1.2641 | grad_norm=0.85 | sec/step~6.29
  step 50/256 | loss_llama=1.3618 | loss_qwen=1.3568 | grad_norm=0.86 | sec/step~6.30
  step 60/256 | loss_llama=1.8474 | loss_qwen=1.8354 | grad_norm=0.76 | sec/step~6.29
  step 70/256 | loss_llama=0.8257 | loss_qwen=0.7818 | grad_norm=0.64 | sec/step~6.30
  step 80/256 | loss_llama=1.0517 | loss_qwen=1.0225 | grad_norm=0.67 | sec/step~6.29
  ✅ Saved checkpoint at step 600
  step 90/256 | loss_llama=1.3604 | loss_qwen=1.2904 | grad_norm=0.95 | sec/step~6.31
  step 100/256 | loss_llama=1.2607 | loss_qwen=1.2547 | grad_norm=0.83 | sec/step~6.30
  step 110/256 | loss_llama=1.4597 | loss_qwen=1.5027 | grad_norm=0.82 | sec/step~6.31
  step 120/256 | loss_llama=1.1807 | loss_qwen=1.1336 | grad_norm=0.88 | sec/step~6.30
  step 130/256 | loss_llama=0.9596 | loss_qwen=0.9235 | grad_norm=1.32 | sec/step~6.30
  step 140/256 | loss_llama=1.3327 | loss_qwen=1.3365 | grad_norm=1.25 | sec/step~6.30
  step 150/256 | loss_llama=0.9682 | loss_qwen=0.8885 | grad_norm=0.53 | sec/step~6.30
  step 160/256 | loss_llama=0.7748 | loss_qwen=0.7626 | grad_norm=0.52 | sec/step~6.30
  step 170/256 | loss_llama=1.0949 | loss_qwen=1.1228 | grad_norm=1.72 | sec/step~6.29
  step 180/256 | loss_llama=1.0057 | loss_qwen=0.9222 | grad_norm=0.76 | sec/step~6.30
  step 190/256 | loss_llama=1.0632 | loss_qwen=1.0367 | grad_norm=0.60 | sec/step~6.30
  step 200/256 | loss_llama=1.1581 | loss_qwen=1.0849 | grad_norm=0.68 | sec/step~6.30
  step 210/256 | loss_llama=1.1618 | loss_qwen=1.0987 | grad_norm=0.53 | sec/step~6.29
  step 220/256 | loss_llama=1.0557 | loss_qwen=1.0594 | grad_norm=0.88 | sec/step~6.30
  step 230/256 | loss_llama=1.4478 | loss_qwen=1.4185 | grad_norm=0.77 | sec/step~6.29
  step 240/256 | loss_llama=1.0011 | loss_qwen=0.9660 | grad_norm=0.56 | sec/step~6.30
  step 250/256 | loss_llama=0.9962 | loss_qwen=0.9703 | grad_norm=0.99 | sec/step~7.26
  step 256/256 | loss_llama=1.1694 | loss_qwen=1.1144 | grad_norm=0.51 | sec/step~6.81
Epoch 4/5
  step 10/256 | loss_llama=1.1253 | loss_qwen=1.1316 | grad_norm=0.75 | sec/step~6.48
  step 20/256 | loss_llama=1.1491 | loss_qwen=1.0586 | grad_norm=0.54 | sec/step~6.32
  step 30/256 | loss_llama=1.5081 | loss_qwen=1.5143 | grad_norm=1.12 | sec/step~6.25
  ✅ Saved checkpoint at step 800
  step 40/256 | loss_llama=1.2610 | loss_qwen=1.2030 | grad_norm=0.85 | sec/step~6.23
  step 50/256 | loss_llama=1.0217 | loss_qwen=0.9993 | grad_norm=0.48 | sec/step~6.22
  step 60/256 | loss_llama=1.3332 | loss_qwen=1.3733 | grad_norm=0.69 | sec/step~6.24
  step 70/256 | loss_llama=0.9207 | loss_qwen=0.8508 | grad_norm=0.62 | sec/step~6.23
  step 80/256 | loss_llama=1.0193 | loss_qwen=1.0036 | grad_norm=0.91 | sec/step~6.23
  step 90/256 | loss_llama=1.1749 | loss_qwen=1.1308 | grad_norm=0.75 | sec/step~6.27
  step 100/256 | loss_llama=1.2269 | loss_qwen=1.1421 | grad_norm=0.60 | sec/step~6.25
  step 110/256 | loss_llama=1.2421 | loss_qwen=1.3214 | grad_norm=0.58 | sec/step~6.23
  step 120/256 | loss_llama=1.1660 | loss_qwen=1.1422 | grad_norm=0.76 | sec/step~6.28
  step 130/256 | loss_llama=1.1969 | loss_qwen=1.2061 | grad_norm=0.74 | sec/step~6.29
  step 140/256 | loss_llama=0.9421 | loss_qwen=0.9096 | grad_norm=0.80 | sec/step~6.29
  step 150/256 | loss_llama=0.9782 | loss_qwen=0.9785 | grad_norm=0.63 | sec/step~6.29
  step 160/256 | loss_llama=1.1593 | loss_qwen=1.0967 | grad_norm=0.75 | sec/step~6.29
  step 170/256 | loss_llama=1.1038 | loss_qwen=1.0928 | grad_norm=0.83 | sec/step~6.29
  step 180/256 | loss_llama=1.2047 | loss_qwen=1.1978 | grad_norm=0.64 | sec/step~6.28
  step 190/256 | loss_llama=0.9396 | loss_qwen=0.9295 | grad_norm=0.56 | sec/step~6.27
  step 200/256 | loss_llama=1.0768 | loss_qwen=1.0854 | grad_norm=0.59 | sec/step~6.25
  step 210/256 | loss_llama=1.2505 | loss_qwen=1.2795 | grad_norm=0.64 | sec/step~6.26
  step 220/256 | loss_llama=1.1671 | loss_qwen=1.2400 | grad_norm=0.65 | sec/step~6.23
  step 230/256 | loss_llama=1.6793 | loss_qwen=1.6623 | grad_norm=0.99 | sec/step~6.21
  ✅ Saved checkpoint at step 1000
  step 240/256 | loss_llama=1.0418 | loss_qwen=1.0453 | grad_norm=0.64 | sec/step~6.21
  step 250/256 | loss_llama=1.4254 | loss_qwen=1.4562 | grad_norm=0.70 | sec/step~6.20
  step 256/256 | loss_llama=1.7436 | loss_qwen=1.7000 | grad_norm=0.70 | sec/step~6.20
Epoch 5/5
  step 10/256 | loss_llama=1.0536 | loss_qwen=1.0322 | grad_norm=0.94 | sec/step~6.20
  step 20/256 | loss_llama=0.9813 | loss_qwen=0.9698 | grad_norm=0.51 | sec/step~6.25
  step 30/256 | loss_llama=0.9437 | loss_qwen=0.9212 | grad_norm=0.59 | sec/step~6.30
  step 40/256 | loss_llama=0.9671 | loss_qwen=0.9378 | grad_norm=0.74 | sec/step~6.31
  step 50/256 | loss_llama=1.0718 | loss_qwen=1.0598 | grad_norm=0.59 | sec/step~6.26
  step 60/256 | loss_llama=1.1069 | loss_qwen=1.1285 | grad_norm=0.80 | sec/step~6.22
  step 70/256 | loss_llama=0.9362 | loss_qwen=0.9787 | grad_norm=0.73 | sec/step~6.22
  step 80/256 | loss_llama=0.9649 | loss_qwen=0.9390 | grad_norm=0.49 | sec/step~6.21
  step 90/256 | loss_llama=1.0962 | loss_qwen=1.1011 | grad_norm=1.00 | sec/step~6.30
  step 100/256 | loss_llama=1.1161 | loss_qwen=1.0898 | grad_norm=0.51 | sec/step~6.27
  step 110/256 | loss_llama=0.9735 | loss_qwen=1.0072 | grad_norm=0.76 | sec/step~6.21
  step 120/256 | loss_llama=1.1040 | loss_qwen=1.1001 | grad_norm=0.59 | sec/step~6.21
  step 130/256 | loss_llama=0.9723 | loss_qwen=0.9798 | grad_norm=0.57 | sec/step~6.22
  step 140/256 | loss_llama=0.9434 | loss_qwen=0.9232 | grad_norm=0.48 | sec/step~6.44
  step 150/256 | loss_llama=1.0424 | loss_qwen=0.9935 | grad_norm=0.81 | sec/step~6.41
  step 160/256 | loss_llama=1.1479 | loss_qwen=1.1597 | grad_norm=0.65 | sec/step~6.34
  step 170/256 | loss_llama=1.0132 | loss_qwen=0.9695 | grad_norm=0.89 | sec/step~6.33
  ✅ Saved checkpoint at step 1200
  step 180/256 | loss_llama=1.2410 | loss_qwen=1.2935 | grad_norm=0.74 | sec/step~6.32
  step 190/256 | loss_llama=1.0523 | loss_qwen=1.0545 | grad_norm=0.72 | sec/step~6.31
  step 200/256 | loss_llama=1.6033 | loss_qwen=1.6401 | grad_norm=0.88 | sec/step~6.30
  step 210/256 | loss_llama=1.0528 | loss_qwen=1.0190 | grad_norm=0.61 | sec/step~6.29
  step 220/256 | loss_llama=1.1480 | loss_qwen=1.0997 | grad_norm=0.52 | sec/step~6.29
  step 230/256 | loss_llama=1.3335 | loss_qwen=1.3410 | grad_norm=0.71 | sec/step~6.29
  step 240/256 | loss_llama=1.0580 | loss_qwen=1.0713 | grad_norm=0.57 | sec/step~6.28
  step 250/256 | loss_llama=1.0665 | loss_qwen=1.0792 | grad_norm=0.62 | sec/step~6.51
  step 256/256 | loss_llama=1.1902 | loss_qwen=1.1787 | grad_norm=0.50 | sec/step~6.41
Saved encoder, adapters, and state to runs/mps_m16_simple_20250909_152509/ckpt
