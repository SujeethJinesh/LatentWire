W1104 20:30:05.911000 1490975 torch/distributed/run.py:793] 
W1104 20:30:05.911000 1490975 torch/distributed/run.py:793] *****************************************
W1104 20:30:05.911000 1490975 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1104 20:30:05.911000 1490975 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
translator_type: cross_attn
soft_tokens: 32
depth: 2
heads: 8
lr: 0.0005
weight_decay: 0.0
train_steps: 2000
warmup_steps: 100
per_device_batch: 24
eval_every: 200
eval_samples: 200
max_new_tokens: 256
seed: 1234
bf16: True
save_path: runs/cross_attention/translator_checkpoint.pt
Loading source model/tokenizer...
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 661.70it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1542.78it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 827.99it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 4446.26it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.39it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.38it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.36it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.09s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.09s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:02<00:01,  1.10s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.10it/s]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 801.78it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 366.15it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8947.85it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8081.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.19it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.19it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.17it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.43it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.40it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.35it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.68it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.56it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.64it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.50it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]
Source hidden dim: 4096 | Target hidden dim: 4096
Loading GSM8K...
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 512, in <module>
[rank2]:     main()
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 463, in main
[rank2]:     data = build_batch_inputs(samples, src_model, src_tok, tgt_model, tgt_tok,
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 259, in build_batch_inputs
[rank2]:     src_enc = src_tok([s.src_prompt for s in samples], return_tensors="pt", padding=True, truncation=True)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
[rank2]:     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
[rank2]:     return self.batch_encode_plus(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3302, in batch_encode_plus
[rank2]:     padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2923, in _get_padding_truncation_strategies
[rank2]:     raise ValueError(
[rank2]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 512, in <module>
[rank1]:     main()
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 463, in main
[rank1]:     data = build_batch_inputs(samples, src_model, src_tok, tgt_model, tgt_tok,
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 259, in build_batch_inputs
[rank1]:     src_enc = src_tok([s.src_prompt for s in samples], return_tensors="pt", padding=True, truncation=True)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
[rank1]:     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
[rank1]:     return self.batch_encode_plus(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3302, in batch_encode_plus
[rank1]:     padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2923, in _get_padding_truncation_strategies
[rank1]:     raise ValueError(
[rank1]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 512, in <module>
[rank3]:     main()
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 463, in main
[rank3]:     data = build_batch_inputs(samples, src_model, src_tok, tgt_model, tgt_tok,
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 259, in build_batch_inputs
[rank3]:     src_enc = src_tok([s.src_prompt for s in samples], return_tensors="pt", padding=True, truncation=True)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
[rank3]:     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
[rank3]:     return self.batch_encode_plus(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3302, in batch_encode_plus
[rank3]:     padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2923, in _get_padding_truncation_strategies
[rank3]:     raise ValueError(
[rank3]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 512, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 463, in main
[rank0]:     data = build_batch_inputs(samples, src_model, src_tok, tgt_model, tgt_tok,
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/cross_model/experiments/cross_attention.py", line 259, in build_batch_inputs
[rank0]:     src_enc = src_tok([s.src_prompt for s in samples], return_tensors="pt", padding=True, truncation=True)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3021, in __call__
[rank0]:     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3109, in _call_one
[rank0]:     return self.batch_encode_plus(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3302, in batch_encode_plus
[rank0]:     padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2923, in _get_padding_truncation_strategies
[rank0]:     raise ValueError(
[rank0]: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.
[rank0]:[W1104 20:33:59.090762039 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1104 20:34:00.104000 1490975 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1490991 closing signal SIGTERM
W1104 20:34:00.107000 1490975 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1490992 closing signal SIGTERM
W1104 20:34:00.107000 1490975 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1490994 closing signal SIGTERM
E1104 20:34:00.760000 1490975 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 1490993) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
cross_model/experiments/cross_attention.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-04_20:34:00
  host      : n03.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1490993)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
