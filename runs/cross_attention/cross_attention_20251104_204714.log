W1104 20:47:15.256000 1514231 torch/distributed/run.py:793] 
W1104 20:47:15.256000 1514231 torch/distributed/run.py:793] *****************************************
W1104 20:47:15.256000 1514231 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1104 20:47:15.256000 1514231 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
translator_type: cross_attn
soft_tokens: 32
depth: 2
heads: 8
lr: 0.0005
weight_decay: 0.0
train_steps: 2000
warmup_steps: 100
per_device_batch: 10
eval_every: 200
eval_samples: 200
max_new_tokens: 256
seed: 1234
bf16: True
save_path: runs/cross_attention/translator_checkpoint.pt
Loading source model/tokenizer...
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 2927.62it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.47it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.94it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.34it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.08it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 3813.00it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1700.39it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 7149.38it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.41it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.39it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.10it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.58it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.57it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.31it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.79it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.61it/s]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3556.76it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8568.55it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 809.16it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 788.40it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.17it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.19it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.18it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.35it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.36it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.42it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.39it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.49it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.57it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.49it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.42it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.39it/s]
Source hidden dim: 4096 | Target hidden dim: 4096
Loading GSM8K...
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Step 20/2000 | Loss (avg over last 20): 1.2276
Step 40/2000 | Loss (avg over last 20): 0.5634
Step 60/2000 | Loss (avg over last 20): 0.4997
Step 80/2000 | Loss (avg over last 20): 0.4576
Step 100/2000 | Loss (avg over last 20): 0.4708
Step 120/2000 | Loss (avg over last 20): 0.4594
Step 140/2000 | Loss (avg over last 20): 0.4638
Step 160/2000 | Loss (avg over last 20): 0.4623
Step 180/2000 | Loss (avg over last 20): 0.4431
Step 200/2000 | Loss (avg over last 20): 0.4654
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 200 | Target-alone acc: 0.715 | Bridged acc: 0.015
Step 220/2000 | Loss (avg over last 20): 0.4421
Step 240/2000 | Loss (avg over last 20): 0.4605
Step 260/2000 | Loss (avg over last 20): 0.4567
Step 280/2000 | Loss (avg over last 20): 0.4622
Step 300/2000 | Loss (avg over last 20): 0.4461
Step 320/2000 | Loss (avg over last 20): 0.4468
Step 340/2000 | Loss (avg over last 20): 0.4411
Step 360/2000 | Loss (avg over last 20): 0.4533
Step 380/2000 | Loss (avg over last 20): 0.4445
Step 400/2000 | Loss (avg over last 20): 0.4685
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 400 | Target-alone acc: 0.715 | Bridged acc: 0.010
Step 420/2000 | Loss (avg over last 20): 0.4622
Step 440/2000 | Loss (avg over last 20): 0.4340
Step 460/2000 | Loss (avg over last 20): 0.4562
Step 480/2000 | Loss (avg over last 20): 0.4509
Step 500/2000 | Loss (avg over last 20): 0.4666
Step 520/2000 | Loss (avg over last 20): 0.4794
Step 540/2000 | Loss (avg over last 20): 0.4541
Step 560/2000 | Loss (avg over last 20): 0.4478
Step 580/2000 | Loss (avg over last 20): 0.4788
Step 600/2000 | Loss (avg over last 20): 0.4588
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 600 | Target-alone acc: 0.715 | Bridged acc: 0.000
Step 620/2000 | Loss (avg over last 20): 0.4402
Step 640/2000 | Loss (avg over last 20): 0.4459
Step 660/2000 | Loss (avg over last 20): 0.4332
Step 680/2000 | Loss (avg over last 20): 0.4466
Step 700/2000 | Loss (avg over last 20): 0.4310
Step 720/2000 | Loss (avg over last 20): 0.4640
Step 740/2000 | Loss (avg over last 20): 0.4366
Step 760/2000 | Loss (avg over last 20): 0.4745
Step 780/2000 | Loss (avg over last 20): 0.4235
Step 800/2000 | Loss (avg over last 20): 0.4286
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 800 | Target-alone acc: 0.715 | Bridged acc: 0.015
Step 820/2000 | Loss (avg over last 20): 0.4273
Step 840/2000 | Loss (avg over last 20): 0.4866
Step 860/2000 | Loss (avg over last 20): 0.4529
Step 880/2000 | Loss (avg over last 20): 0.4356
Step 900/2000 | Loss (avg over last 20): 0.4472
Step 920/2000 | Loss (avg over last 20): 0.4642
Step 940/2000 | Loss (avg over last 20): 0.4327
Step 960/2000 | Loss (avg over last 20): 0.4389
Step 980/2000 | Loss (avg over last 20): 0.4616
Step 1000/2000 | Loss (avg over last 20): 0.4580
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1000 | Target-alone acc: 0.715 | Bridged acc: 0.035
Step 1020/2000 | Loss (avg over last 20): 0.4446
Step 1040/2000 | Loss (avg over last 20): 0.4271
Step 1060/2000 | Loss (avg over last 20): 0.4442
Step 1080/2000 | Loss (avg over last 20): 0.4246
Step 1100/2000 | Loss (avg over last 20): 0.4517
Step 1120/2000 | Loss (avg over last 20): 0.4507
Step 1140/2000 | Loss (avg over last 20): 0.4331
Step 1160/2000 | Loss (avg over last 20): 0.4594
Step 1180/2000 | Loss (avg over last 20): 0.4697
Step 1200/2000 | Loss (avg over last 20): 0.4019
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1200 | Target-alone acc: 0.715 | Bridged acc: 0.010
Step 1220/2000 | Loss (avg over last 20): 0.4441
Step 1240/2000 | Loss (avg over last 20): 0.4527
Step 1260/2000 | Loss (avg over last 20): 0.4422
Step 1280/2000 | Loss (avg over last 20): 0.4207
Step 1300/2000 | Loss (avg over last 20): 0.4479
Step 1320/2000 | Loss (avg over last 20): 0.4445
Step 1340/2000 | Loss (avg over last 20): 0.4506
Step 1360/2000 | Loss (avg over last 20): 0.4322
Step 1380/2000 | Loss (avg over last 20): 0.4597
Step 1400/2000 | Loss (avg over last 20): 0.4218
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1400 | Target-alone acc: 0.715 | Bridged acc: 0.015
Step 1420/2000 | Loss (avg over last 20): 0.4445
Step 1440/2000 | Loss (avg over last 20): 0.4408
Step 1460/2000 | Loss (avg over last 20): 0.4699
Step 1480/2000 | Loss (avg over last 20): 0.4368
Step 1500/2000 | Loss (avg over last 20): 0.4715
Step 1520/2000 | Loss (avg over last 20): 0.4490
Step 1540/2000 | Loss (avg over last 20): 0.4386
Step 1560/2000 | Loss (avg over last 20): 0.4539
Step 1580/2000 | Loss (avg over last 20): 0.4570
Step 1600/2000 | Loss (avg over last 20): 0.4233
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1600 | Target-alone acc: 0.715 | Bridged acc: 0.005
Step 1620/2000 | Loss (avg over last 20): 0.4369
Step 1640/2000 | Loss (avg over last 20): 0.4246
Step 1660/2000 | Loss (avg over last 20): 0.4478
Step 1680/2000 | Loss (avg over last 20): 0.4409
Step 1700/2000 | Loss (avg over last 20): 0.4445
Step 1720/2000 | Loss (avg over last 20): 0.4551
Step 1740/2000 | Loss (avg over last 20): 0.4291
Step 1760/2000 | Loss (avg over last 20): 0.4240
Step 1780/2000 | Loss (avg over last 20): 0.4172
Step 1800/2000 | Loss (avg over last 20): 0.4423
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1800 | Target-alone acc: 0.715 | Bridged acc: 0.005
Step 1820/2000 | Loss (avg over last 20): 0.4295
Step 1840/2000 | Loss (avg over last 20): 0.4334
Step 1860/2000 | Loss (avg over last 20): 0.4280
Step 1880/2000 | Loss (avg over last 20): 0.4247
Step 1900/2000 | Loss (avg over last 20): 0.4429
Step 1920/2000 | Loss (avg over last 20): 0.4359
Step 1940/2000 | Loss (avg over last 20): 0.4319
Step 1960/2000 | Loss (avg over last 20): 0.4406
Step 1980/2000 | Loss (avg over last 20): 0.4250
Step 2000/2000 | Loss (avg over last 20): 0.4199
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 2000 | Target-alone acc: 0.715 | Bridged acc: 0.005
Saved translator to runs/cross_attention/translator_checkpoint.pt
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Final Eval] Target-alone acc: 0.715 | Bridged acc: 0.005
