W1104 20:04:35.309000 1459561 torch/distributed/run.py:793] 
W1104 20:04:35.309000 1459561 torch/distributed/run.py:793] *****************************************
W1104 20:04:35.309000 1459561 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1104 20:04:35.309000 1459561 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
==== Config ====
source_model: Qwen/Qwen2.5-1.5B-Instruct
target_model: meta-llama/Llama-3.2-1B-Instruct
translator_type: cross_attn
soft_tokens: 32
depth: 2
heads: 8
lr: 0.0003
weight_decay: 0.0
train_steps: 2000
warmup_steps: 100
per_device_batch: 2
eval_every: 200
eval_samples: 200
max_new_tokens: 256
seed: 1234
bf16: True
save_path: runs/cross_attention/translator_checkpoint.pt
Loading source model/tokenizer...
Loading target model/tokenizer...
Source hidden dim: 1536 | Target hidden dim: 2048
Loading GSM8K...
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Step 20/2000 | Loss (avg over last 20): 2.5449
Step 40/2000 | Loss (avg over last 20): 0.7028
Step 60/2000 | Loss (avg over last 20): 0.7240
Step 80/2000 | Loss (avg over last 20): 0.6242
Step 100/2000 | Loss (avg over last 20): 0.6072
Step 120/2000 | Loss (avg over last 20): 0.6322
Step 140/2000 | Loss (avg over last 20): 0.5908
Step 160/2000 | Loss (avg over last 20): 0.5441
Step 180/2000 | Loss (avg over last 20): 0.5538
Step 200/2000 | Loss (avg over last 20): 0.6081
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 200 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 220/2000 | Loss (avg over last 20): 0.6240
Step 240/2000 | Loss (avg over last 20): 0.5764
Step 260/2000 | Loss (avg over last 20): 0.5555
Step 280/2000 | Loss (avg over last 20): 0.5538
Step 300/2000 | Loss (avg over last 20): 0.5114
Step 320/2000 | Loss (avg over last 20): 0.5318
Step 340/2000 | Loss (avg over last 20): 0.5622
Step 360/2000 | Loss (avg over last 20): 0.5285
Step 380/2000 | Loss (avg over last 20): 0.5957
Step 400/2000 | Loss (avg over last 20): 0.4690
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 400 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 420/2000 | Loss (avg over last 20): 0.5632
Step 440/2000 | Loss (avg over last 20): 0.5161
Step 460/2000 | Loss (avg over last 20): 0.5395
Step 480/2000 | Loss (avg over last 20): 0.5650
Step 500/2000 | Loss (avg over last 20): 0.5367
Step 520/2000 | Loss (avg over last 20): 0.6151
Step 540/2000 | Loss (avg over last 20): 0.5056
Step 560/2000 | Loss (avg over last 20): 0.5583
Step 580/2000 | Loss (avg over last 20): 0.5709
Step 600/2000 | Loss (avg over last 20): 0.4955
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 600 | Target-alone acc: 0.035 | Bridged acc: 0.015
Step 620/2000 | Loss (avg over last 20): 0.5958
Step 640/2000 | Loss (avg over last 20): 0.5163
Step 660/2000 | Loss (avg over last 20): 0.5196
Step 680/2000 | Loss (avg over last 20): 0.5042
Step 700/2000 | Loss (avg over last 20): 0.5189
Step 720/2000 | Loss (avg over last 20): 0.5848
Step 740/2000 | Loss (avg over last 20): 0.5589
Step 760/2000 | Loss (avg over last 20): 0.5360
Step 780/2000 | Loss (avg over last 20): 0.4949
Step 800/2000 | Loss (avg over last 20): 0.5498
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 800 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 820/2000 | Loss (avg over last 20): 0.4845
Step 840/2000 | Loss (avg over last 20): 0.5642
Step 860/2000 | Loss (avg over last 20): 0.5433
Step 880/2000 | Loss (avg over last 20): 0.5190
Step 900/2000 | Loss (avg over last 20): 0.4853
Step 920/2000 | Loss (avg over last 20): 0.5637
Step 940/2000 | Loss (avg over last 20): 0.5299
Step 960/2000 | Loss (avg over last 20): 0.5762
Step 980/2000 | Loss (avg over last 20): 0.4710
Step 1000/2000 | Loss (avg over last 20): 0.5883
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1000 | Target-alone acc: 0.035 | Bridged acc: 0.005
Step 1020/2000 | Loss (avg over last 20): 0.5491
Step 1040/2000 | Loss (avg over last 20): 0.5668
Step 1060/2000 | Loss (avg over last 20): 0.4835
Step 1080/2000 | Loss (avg over last 20): 0.5466
Step 1100/2000 | Loss (avg over last 20): 0.4758
Step 1120/2000 | Loss (avg over last 20): 0.5564
Step 1140/2000 | Loss (avg over last 20): 0.5938
Step 1160/2000 | Loss (avg over last 20): 0.5308
Step 1180/2000 | Loss (avg over last 20): 0.5355
Step 1200/2000 | Loss (avg over last 20): 0.4990
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1200 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 1220/2000 | Loss (avg over last 20): 0.5020
Step 1240/2000 | Loss (avg over last 20): 0.5435
Step 1260/2000 | Loss (avg over last 20): 0.5542
Step 1280/2000 | Loss (avg over last 20): 0.5109
Step 1300/2000 | Loss (avg over last 20): 0.5133
Step 1320/2000 | Loss (avg over last 20): 0.4953
Step 1340/2000 | Loss (avg over last 20): 0.6257
Step 1360/2000 | Loss (avg over last 20): 0.5429
Step 1380/2000 | Loss (avg over last 20): 0.5525
Step 1400/2000 | Loss (avg over last 20): 0.5240
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1400 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 1420/2000 | Loss (avg over last 20): 0.4228
Step 1440/2000 | Loss (avg over last 20): 0.5873
Step 1460/2000 | Loss (avg over last 20): 0.5628
Step 1480/2000 | Loss (avg over last 20): 0.5156
Step 1500/2000 | Loss (avg over last 20): 0.4816
Step 1520/2000 | Loss (avg over last 20): 0.5309
Step 1540/2000 | Loss (avg over last 20): 0.4877
Step 1560/2000 | Loss (avg over last 20): 0.5417
Step 1580/2000 | Loss (avg over last 20): 0.5119
Step 1600/2000 | Loss (avg over last 20): 0.5292
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1600 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 1620/2000 | Loss (avg over last 20): 0.5614
Step 1640/2000 | Loss (avg over last 20): 0.4989
Step 1660/2000 | Loss (avg over last 20): 0.5897
Step 1680/2000 | Loss (avg over last 20): 0.4685
Step 1700/2000 | Loss (avg over last 20): 0.4885
Step 1720/2000 | Loss (avg over last 20): 0.5402
Step 1740/2000 | Loss (avg over last 20): 0.5249
Step 1760/2000 | Loss (avg over last 20): 0.5321
Step 1780/2000 | Loss (avg over last 20): 0.5354
Step 1800/2000 | Loss (avg over last 20): 0.5180
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 1800 | Target-alone acc: 0.035 | Bridged acc: 0.000
Step 1820/2000 | Loss (avg over last 20): 0.5311
Step 1840/2000 | Loss (avg over last 20): 0.6052
Step 1860/2000 | Loss (avg over last 20): 0.5021
Step 1880/2000 | Loss (avg over last 20): 0.4745
Step 1900/2000 | Loss (avg over last 20): 0.5210
Step 1920/2000 | Loss (avg over last 20): 0.5109
Step 1940/2000 | Loss (avg over last 20): 0.5778
Step 1960/2000 | Loss (avg over last 20): 0.5444
Step 1980/2000 | Loss (avg over last 20): 0.5591
Step 2000/2000 | Loss (avg over last 20): 0.5431
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Eval] Step 2000 | Target-alone acc: 0.035 | Bridged acc: 0.000
Saved translator to runs/cross_attention/translator_checkpoint.pt
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.
[Final Eval] Target-alone acc: 0.035 | Bridged acc: 0.000
