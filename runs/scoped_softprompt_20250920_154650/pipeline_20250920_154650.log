
=== Stage A: LoRA (tiny) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 378.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:17, 45.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:49<01:52, 56.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:46<00:56, 56.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:58<00:00, 38.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:58<00:00, 44.58s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 385.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:17, 45.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:26<01:25, 42.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:17<00:46, 46.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:59<00:00, 44.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:59<00:00, 44.94s/it]
The 8-bit optimizer is not available on your device, only available on CUDA for now.
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
