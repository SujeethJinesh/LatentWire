
=========================================
Starting pipeline at Thu Sep 18 21:14:23 PDT 2025
=========================================


=========================================
TRAINING (3 epochs, 1600 samples, latent M=48)
=========================================

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: train.py [-h] [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
                [--llama_device_map LLAMA_DEVICE_MAP]
                [--qwen_device_map QWEN_DEVICE_MAP]
                [--dataset {hotpot,squad,squad_v2}]
                [--hotpot_config HOTPOT_CONFIG] [--samples SAMPLES]
                [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                [--grad_accum_steps GRAD_ACCUM_STEPS] [--seed SEED]
                [--data_seed DATA_SEED] [--latent_len LATENT_LEN]
                [--latent_shared_len LATENT_SHARED_LEN]
                [--latent_private_len LATENT_PRIVATE_LEN] [--d_z D_Z]
                [--max_bytes MAX_BYTES] [--encoder_type {byte,simple-st,stq}]
                [--encoder_use_chat_template]
                [--encoder_backbone ENCODER_BACKBONE]
                [--hf_encoder_id HF_ENCODER_ID]
                [--max_enc_tokens MAX_ENC_TOKENS]
                [--max_answer_tokens MAX_ANSWER_TOKENS] [--lr LR]
                [--scale_l2 SCALE_L2] [--adapter_rms_l2 ADAPTER_RMS_L2]
                [--max_grad_norm MAX_GRAD_NORM] [--adapter_freeze_scale]
                [--first_token_ce_weight FIRST_TOKEN_CE_WEIGHT]
                [--train_append_bos_after_prefix {auto,yes,no}]
                [--adapter_hidden_mult ADAPTER_HIDDEN_MULT]
                [--adapter_colorize] [--no_adapter_metadata]
                [--manifold_stat_weight MANIFOLD_STAT_WEIGHT]
                [--state_kd_weight STATE_KD_WEIGHT]
                [--state_kd_layers STATE_KD_LAYERS] [--K K]
                [--adaptive_k_start ADAPTIVE_K_START]
                [--adaptive_k_end ADAPTIVE_K_END]
                [--latent_keep_start LATENT_KEEP_START]
                [--latent_keep_end LATENT_KEEP_END]
                [--latent_keep_power LATENT_KEEP_POWER]
                [--k_ce_weight K_CE_WEIGHT]
                [--kd_first_k_weight KD_FIRST_K_WEIGHT] [--kd_tau KD_TAU]
                [--load_4bit] [--sequential_models]
                [--llama_devices LLAMA_DEVICES] [--qwen_devices QWEN_DEVICES]
                [--gpu_mem_gib GPU_MEM_GIB] [--grad_ckpt] [--fp16_mps]
                [--warm_anchor_text WARM_ANCHOR_TEXT] [--debug]
                [--save_dir SAVE_DIR] [--save_every SAVE_EVERY]
                [--resume_from RESUME_FROM] [--auto_resume]
                [--no_load_optimizer] [--save_training_stats]
train.py: error: argument --encoder_type: invalid choice: 'simple-stq' (choose from 'byte', 'simple-st', 'stq')
