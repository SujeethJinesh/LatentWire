
>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=llama_single_20250926_170438

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2808.37it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.38s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.14s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
‚ö†Ô∏è  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 16 steps
Epoch 1/4
[warmup] step=0 mode=text (warm-up)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/32 | (warm-up text) | align=0.0002 | text_tf=13.4584 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/32 | (warm-up text) | align=0.0002 | text_tf=13.6803 | latent_scale=0.03
[warmup] step=2 mode=text (warm-up)
  step  3/32 | (warm-up text) | align=0.0002 | text_tf=13.4812 | latent_scale=0.06
[warmup] step=3 mode=text (warm-up)
  step  4/32 | (warm-up text) | align=0.0002 | text_tf=13.5367 | latent_scale=0.09
[warmup] step=4 mode=text (warm-up)
  step  5/32 | (warm-up text) | align=0.0002 | text_tf=14.5049 | latent_scale=0.12
[warmup] step=5 mode=text (warm-up)
  step  6/32 | (warm-up text) | align=0.0002 | text_tf=13.5405 | latent_scale=0.16
[warmup] step=6 mode=text (warm-up)
  step  7/32 | (warm-up text) | align=0.0002 | text_tf=14.9243 | latent_scale=0.19
[warmup] step=7 mode=text (warm-up)
  step  8/32 | (warm-up text) | align=0.0002 | text_tf=13.7110 | latent_scale=0.22
[warmup] step=8 mode=text (warm-up)
  step  9/32 | (warm-up text) | align=0.0002 | text_tf=12.3362 | latent_scale=0.25
[warmup] step=9 mode=text (warm-up)
  step  10/32 | (warm-up text) | align=0.0002 | text_tf=14.6672 | latent_scale=0.28
  step  10/32 | grad_norm=32.42 | sec/step~3.56 | keep=0.70 | K=8 | first_w=4.00 | llama(T): tf=3.8909 first=3.8410 kCE=3.7223 KD=0.0000 acc=0.000 state=4.0125 align=0.0002 latA=0.0000 latP=0.0000 gist=1.0001 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  11/32 | (warm-up text) | align=0.0002 | text_tf=13.0599 | latent_scale=0.31
  step  12/32 | (warm-up text) | align=0.0002 | text_tf=13.5308 | latent_scale=0.34
  step  13/32 | (warm-up text) | align=0.0002 | text_tf=13.7707 | latent_scale=0.38
  step  14/32 | (warm-up text) | align=0.0002 | text_tf=15.2113 | latent_scale=0.41
  step  15/32 | (warm-up text) | align=0.0002 | text_tf=14.3971 | latent_scale=0.44
  step  16/32 | (warm-up text) | align=0.0002 | text_tf=15.7620 | latent_scale=0.47
  step  20/32 | grad_norm=309.51 | sec/step~4.10 | keep=0.71 | K=8 | first_w=4.00 | llama(L): tf=33.8284 first=32.4556 kCE=45.6097 KD=65.6913 acc=0.000 state=15.9167 align=0.0000 latA=0.5018 latP=0.2501 gist=0.9998 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/32 | grad_norm=1100.59 | sec/step~3.23 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=33.1791 first=37.7537 kCE=44.0993 KD=62.9015 acc=0.000 state=13.5564 align=0.0000 latA=0.5024 latP=0.2503 gist=0.9998 | scale_pen(llama)=0.0000e+00 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  32/32 | grad_norm=1261.69 | sec/step~4.08 | keep=0.72 | K=8 | first_w=4.00 | llama(L): tf=27.6399 first=31.1186 kCE=41.4680 KD=62.5712 acc=0.000 state=13.8993 align=0.0000 latA=0.4978 latP=0.2499 gist=0.9998 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 2/4
  step  10/32 | grad_norm=206.14 | sec/step~3.53 | keep=0.73 | K=8 | first_w=3.99 | llama(L): tf=14.9823 first=13.4180 kCE=21.7719 KD=32.0561 acc=0.000 state=14.9011 align=0.0000 latA=0.5008 latP=0.2496 gist=0.9996 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/32 | grad_norm=44.37 | sec/step~3.85 | keep=0.75 | K=8 | first_w=3.87 | llama(L): tf=14.7410 first=11.1843 kCE=14.9277 KD=24.1575 acc=0.000 state=14.6823 align=0.0000 latA=0.4982 latP=0.2491 gist=0.9993 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/32 | grad_norm=153.63 | sec/step~3.90 | keep=0.77 | K=8 | first_w=3.62 | llama(L): tf=13.1935 first=9.8038 kCE=13.9952 KD=23.8343 acc=0.000 state=14.9236 align=0.0000 latA=0.4994 latP=0.2491 gist=0.9993 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  32/32 | grad_norm=175.36 | sec/step~3.29 | keep=0.77 | K=8 | first_w=3.55 | llama(L): tf=14.3864 first=12.0455 kCE=14.7840 KD=24.6114 acc=0.000 state=13.8417 align=0.0000 latA=0.5005 latP=0.2490 gist=0.9993 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 3/4
  step  10/32 | grad_norm=82.08 | sec/step~3.53 | keep=0.80 | K=8 | first_w=3.18 | llama(L): tf=15.5189 first=12.1113 kCE=13.4072 KD=24.2490 acc=0.000 state=16.0649 align=0.0000 latA=0.4994 latP=0.2486 gist=0.9990 | scale_pen(llama)=5.6843e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/32 | grad_norm=24.54 | sec/step~3.27 | keep=0.83 | K=8 | first_w=2.75 | llama(L): tf=15.4498 first=12.1083 kCE=12.7283 KD=23.0862 acc=0.000 state=17.8994 align=0.0000 latA=0.4985 latP=0.2483 gist=0.9988 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/32 | grad_norm=78.69 | sec/step~3.43 | keep=0.86 | K=8 | first_w=2.32 | llama(L): tf=15.6517 first=12.6214 kCE=13.0702 KD=24.0460 acc=0.000 state=16.6963 align=0.0000 latA=0.4998 latP=0.2482 gist=0.9988 | scale_pen(llama)=1.4211e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  32/32 | grad_norm=88.93 | sec/step~4.14 | keep=0.87 | K=8 | first_w=2.24 | llama(L): tf=15.5394 first=12.0844 kCE=12.8158 KD=22.5651 acc=0.000 state=19.1290 align=0.0000 latA=0.4991 latP=0.2480 gist=0.9988 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 4/4
  step  10/32 | grad_norm=33.88 | sec/step~3.52 | keep=0.91 | K=8 | first_w=1.88 | llama(L): tf=14.3330 first=11.2161 kCE=12.2227 KD=22.5832 acc=0.000 state=19.2861 align=0.0000 latA=0.5014 latP=0.2480 gist=0.9985 | scale_pen(llama)=2.2737e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/32 | grad_norm=10.20 | sec/step~3.38 | keep=0.95 | K=8 | first_w=1.63 | llama(L): tf=14.7824 first=11.3550 kCE=12.1738 KD=22.6768 acc=0.000 state=19.5732 align=0.0000 latA=0.4987 latP=0.2475 gist=0.9983 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/32 | grad_norm=33.11 | sec/step~3.98 | keep=0.99 | K=8 | first_w=1.51 | llama(L): tf=13.7057 first=11.4320 kCE=12.0541 KD=22.6438 acc=0.000 state=20.0149 align=0.0000 latA=0.5024 latP=0.2473 gist=0.9983 | scale_pen(llama)=1.2790e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  32/32 | grad_norm=38.01 | sec/step~3.66 | keep=1.00 | K=8 | first_w=1.50 | llama(L): tf=12.1004 first=10.0386 kCE=10.8725 KD=25.6929 acc=0.000 state=20.5446 align=0.0000 latA=0.4992 latP=0.2473 gist=0.9983 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 3.0KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, gist_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250926_170438/ckpt/stageA
üìù Saved LoRA adapters for Llama
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 0.9999927598983049, 'rms_mean_cal': 0.010571631282800809, 'embed_rms': 0.01057521253824234, 'count': 128}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 3119.60it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.07s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.24s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.
  warnings.warn(
trainable params: 272,723,968 || all params: 8,323,956,736 || trainable%: 3.2764
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[INFO] llama anchor tokens: 3
‚è™ Resuming from: runs/llama_single_20250926_170438/ckpt/stageA/state.pt
   -> loaded encoder/adapters/deep_prefix/refiner/gist FROM state.pt
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 64 steps
Epoch 1/6
[warmup] step=0 mode=text (warm-up)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/64 | (warm-up text) | align=0.0003 | text_tf=9.1890 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/64 | (warm-up text) | align=0.0003 | text_tf=9.4550 | latent_scale=0.02
[warmup] step=2 mode=text (warm-up)
  step  3/64 | (warm-up text) | align=0.0003 | text_tf=9.7178 | latent_scale=0.03
[warmup] step=3 mode=text (warm-up)
  step  4/64 | (warm-up text) | align=0.0003 | text_tf=11.0655 | latent_scale=0.05
[warmup] step=4 mode=text (warm-up)
  step  5/64 | (warm-up text) | align=0.0003 | text_tf=9.5898 | latent_scale=0.06
[warmup] step=5 mode=text (warm-up)
  step  6/64 | (warm-up text) | align=0.0003 | text_tf=10.1655 | latent_scale=0.08
[warmup] step=6 mode=text (warm-up)
  step  7/64 | (warm-up text) | align=0.0003 | text_tf=10.7247 | latent_scale=0.09
[warmup] step=7 mode=text (warm-up)
  step  8/64 | (warm-up text) | align=0.0003 | text_tf=9.2095 | latent_scale=0.11
[warmup] step=8 mode=text (warm-up)
  step  9/64 | (warm-up text) | align=0.0003 | text_tf=9.5371 | latent_scale=0.12
[warmup] step=9 mode=text (warm-up)
  step  10/64 | (warm-up text) | align=0.0003 | text_tf=8.6736 | latent_scale=0.14
  step  10/64 | grad_norm=0.00 | sec/step~3.35 | keep=0.50 | K=8 | first_w=8.00 | llama(T): tf=1.5416 first=1.9226 kCE=1.3190 KD=0.0000 acc=0.000 state=3.9892 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9981 | scale_pen(llama)=8.8818e-14 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  11/64 | (warm-up text) | align=0.0003 | text_tf=9.0538 | latent_scale=0.16
  step  12/64 | (warm-up text) | align=0.0003 | text_tf=9.9524 | latent_scale=0.17
  step  13/64 | (warm-up text) | align=0.0003 | text_tf=9.8088 | latent_scale=0.19
  step  14/64 | (warm-up text) | align=0.0003 | text_tf=8.4260 | latent_scale=0.20
  step  15/64 | (warm-up text) | align=0.0003 | text_tf=8.6466 | latent_scale=0.22
  step  16/64 | (warm-up text) | align=0.0003 | text_tf=8.5393 | latent_scale=0.23
  step  17/64 | (warm-up text) | align=0.0003 | text_tf=8.1994 | latent_scale=0.25
  step  18/64 | (warm-up text) | align=0.0003 | text_tf=9.0513 | latent_scale=0.27
  step  19/64 | (warm-up text) | align=0.0003 | text_tf=8.6303 | latent_scale=0.28
  step  20/64 | (warm-up text) | align=0.0003 | text_tf=9.3579 | latent_scale=0.30
  step  20/64 | grad_norm=0.00 | sec/step~3.53 | keep=0.50 | K=8 | first_w=8.00 | llama(T): tf=3.8607 first=3.8115 kCE=4.3935 KD=0.0000 acc=0.000 state=6.2808 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9977 | scale_pen(llama)=1.3481e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  21/64 | (warm-up text) | align=0.0003 | text_tf=8.8108 | latent_scale=0.31
  step  22/64 | (warm-up text) | align=0.0003 | text_tf=9.0625 | latent_scale=0.33
  step  23/64 | (warm-up text) | align=0.0003 | text_tf=9.0751 | latent_scale=0.34
  step  24/64 | (warm-up text) | align=0.0003 | text_tf=9.0320 | latent_scale=0.36
  step  25/64 | (warm-up text) | align=0.0003 | text_tf=9.6289 | latent_scale=0.38
  step  26/64 | (warm-up text) | align=0.0003 | text_tf=8.5045 | latent_scale=0.39
  step  27/64 | (warm-up text) | align=0.0003 | text_tf=8.4430 | latent_scale=0.41
  step  28/64 | (warm-up text) | align=0.0003 | text_tf=8.3308 | latent_scale=0.42
  step  29/64 | (warm-up text) | align=0.0003 | text_tf=9.2421 | latent_scale=0.44
  step  30/64 | (warm-up text) | align=0.0003 | text_tf=9.2274 | latent_scale=0.45
  step  30/64 | grad_norm=0.00 | sec/step~3.36 | keep=0.50 | K=8 | first_w=8.00 | llama(T): tf=5.8455 first=5.9724 kCE=6.2449 KD=0.0000 acc=0.000 state=9.9818 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9977 | scale_pen(llama)=1.3481e-09 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/64 | (warm-up text) | align=0.0003 | text_tf=9.7134 | latent_scale=0.47
  step  32/64 | (warm-up text) | align=0.0003 | text_tf=8.8283 | latent_scale=0.48
  step  33/64 | (warm-up text) | align=0.0003 | text_tf=8.5140 | latent_scale=0.50
  step  34/64 | (warm-up text) | align=0.0003 | text_tf=8.9047 | latent_scale=0.52
  step  35/64 | (warm-up text) | align=0.0003 | text_tf=9.2441 | latent_scale=0.53
  step  36/64 | (warm-up text) | align=0.0003 | text_tf=8.3158 | latent_scale=0.55
  step  37/64 | (warm-up text) | align=0.0003 | text_tf=8.4055 | latent_scale=0.56
  step  38/64 | (warm-up text) | align=0.0003 | text_tf=8.3476 | latent_scale=0.58
  step  39/64 | (warm-up text) | align=0.0003 | text_tf=8.7602 | latent_scale=0.59
  step  40/64 | (warm-up text) | align=0.0003 | text_tf=8.4584 | latent_scale=0.61
  step  40/64 | grad_norm=0.00 | sec/step~3.59 | keep=0.51 | K=8 | first_w=8.00 | llama(T): tf=6.3212 first=6.0158 kCE=8.4284 KD=0.0000 acc=0.150 state=14.5774 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9974 | scale_pen(llama)=2.8777e-13 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/64 | (warm-up text) | align=0.0003 | text_tf=8.4344 | latent_scale=0.62
  step  42/64 | (warm-up text) | align=0.0003 | text_tf=9.2986 | latent_scale=0.64
  step  43/64 | (warm-up text) | align=0.0003 | text_tf=8.4651 | latent_scale=0.66
  step  44/64 | (warm-up text) | align=0.0003 | text_tf=10.6612 | latent_scale=0.67
  step  45/64 | (warm-up text) | align=0.0003 | text_tf=9.0875 | latent_scale=0.69
  step  46/64 | (warm-up text) | align=0.0003 | text_tf=8.9733 | latent_scale=0.70
  step  47/64 | (warm-up text) | align=0.0003 | text_tf=9.2053 | latent_scale=0.72
  step  48/64 | (warm-up text) | align=0.0003 | text_tf=9.0644 | latent_scale=0.73
  step  49/64 | (warm-up text) | align=0.0003 | text_tf=9.2644 | latent_scale=0.75
  step  50/64 | (warm-up text) | align=0.0003 | text_tf=8.1799 | latent_scale=0.77
  step  50/64 | grad_norm=0.00 | sec/step~3.08 | keep=0.51 | K=8 | first_w=8.00 | llama(T): tf=7.2891 first=6.7889 kCE=9.7641 KD=0.0000 acc=0.000 state=14.6217 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9971 | scale_pen(llama)=8.3914e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/64 | (warm-up text) | align=0.0003 | text_tf=8.7470 | latent_scale=0.78
  step  52/64 | (warm-up text) | align=0.0003 | text_tf=8.6076 | latent_scale=0.80
  step  53/64 | (warm-up text) | align=0.0003 | text_tf=8.8979 | latent_scale=0.81
  step  54/64 | (warm-up text) | align=0.0003 | text_tf=8.5986 | latent_scale=0.83
  step  55/64 | (warm-up text) | align=0.0003 | text_tf=10.0770 | latent_scale=0.84
  step  56/64 | (warm-up text) | align=0.0003 | text_tf=9.7907 | latent_scale=0.86
  step  57/64 | (warm-up text) | align=0.0003 | text_tf=9.4358 | latent_scale=0.88
  step  58/64 | (warm-up text) | align=0.0003 | text_tf=9.3307 | latent_scale=0.89
  step  59/64 | (warm-up text) | align=0.0003 | text_tf=9.0252 | latent_scale=0.91
  step  60/64 | (warm-up text) | align=0.0003 | text_tf=8.8768 | latent_scale=0.92
  step  60/64 | grad_norm=0.00 | sec/step~3.65 | keep=0.51 | K=8 | first_w=8.00 | llama(T): tf=8.8532 first=8.1028 kCE=11.0877 KD=0.0000 acc=0.000 state=18.9883 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9971 | scale_pen(llama)=8.3914e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/64 | (warm-up text) | align=0.0003 | text_tf=8.4371 | latent_scale=0.94
  step  62/64 | (warm-up text) | align=0.0003 | text_tf=8.2512 | latent_scale=0.95
  step  63/64 | (warm-up text) | align=0.0003 | text_tf=9.7609 | latent_scale=0.97
  step  64/64 | (warm-up text) | align=0.0003 | text_tf=9.0749 | latent_scale=0.98
  step  64/64 | grad_norm=0.00 | sec/step~3.71 | keep=0.51 | K=8 | first_w=8.00 | llama(T): tf=9.9505 first=8.2061 kCE=11.9146 KD=0.0000 acc=0.050 state=19.2913 align=0.0003 latA=0.0000 latP=0.0000 gist=0.9971 | scale_pen(llama)=8.7050e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 2/6
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  step  10/64 | grad_norm=10.14 | sec/step~4.78 | keep=0.52 | K=8 | first_w=8.00 | llama(L): tf=9.3679 first=8.1890 kCE=11.9860 KD=22.1509 acc=0.000 state=20.8196 align=0.0000 latA=0.9764 latP=0.4950 gist=0.9970 | scale_pen(llama)=8.7050e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  20/64 | grad_norm=5.80 | sec/step~4.18 | keep=0.52 | K=8 | first_w=8.00 | llama(L): tf=9.7455 first=7.9034 kCE=13.0790 KD=15.4902 acc=0.050 state=17.8507 align=0.0000 latA=0.9834 latP=0.4943 gist=0.9967 | scale_pen(llama)=8.3569e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  30/64 | grad_norm=20.48 | sec/step~4.79 | keep=0.53 | K=8 | first_w=8.00 | llama(L): tf=9.4089 first=7.9354 kCE=12.5198 KD=15.5171 acc=0.050 state=21.8510 align=0.0000 latA=0.9762 latP=0.4940 gist=0.9967 | scale_pen(llama)=8.3569e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  40/64 | grad_norm=66.90 | sec/step~4.39 | keep=0.54 | K=8 | first_w=8.00 | llama(L): tf=9.7416 first=8.5253 kCE=12.8182 KD=27.4124 acc=0.050 state=20.3509 align=0.0000 latA=0.9755 latP=0.4934 gist=0.9965 | scale_pen(llama)=7.7481e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  50/64 | grad_norm=1.13 | sec/step~4.08 | keep=0.54 | K=8 | first_w=8.00 | llama(L): tf=9.2637 first=7.8125 kCE=12.5916 KD=11.4131 acc=0.000 state=18.1010 align=0.0000 latA=0.9821 latP=0.4933 gist=0.9962 | scale_pen(llama)=7.1942e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  60/64 | grad_norm=6.38 | sec/step~4.17 | keep=0.55 | K=8 | first_w=8.00 | llama(L): tf=9.1778 first=7.6242 kCE=12.0901 KD=13.9092 acc=0.050 state=19.7889 align=0.0000 latA=0.9750 latP=0.4928 gist=0.9962 | scale_pen(llama)=7.1942e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  64/64 | grad_norm=8.79 | sec/step~4.28 | keep=0.55 | K=8 | first_w=8.00 | llama(L): tf=8.7814 first=7.9492 kCE=12.0214 KD=14.4141 acc=0.050 state=19.3200 align=0.0000 latA=0.9831 latP=0.4929 gist=0.9962 | scale_pen(llama)=5.9430e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
Epoch 3/6
  step  10/64 | grad_norm=9.74 | sec/step~4.19 | keep=0.56 | K=8 | first_w=8.00 | llama(L): tf=9.3909 first=7.7297 kCE=13.0085 KD=14.5082 acc=0.150 state=18.0079 align=0.0000 latA=0.9833 latP=0.4928 gist=0.9960 | scale_pen(llama)=5.9430e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  20/64 | grad_norm=3.33 | sec/step~4.05 | keep=0.57 | K=8 | first_w=8.00 | llama(L): tf=8.7027 first=7.7565 kCE=11.4221 KD=16.2572 acc=0.050 state=17.8211 align=0.0000 latA=0.9817 latP=0.4921 gist=0.9958 | scale_pen(llama)=4.6043e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  30/64 | grad_norm=11.37 | sec/step~4.34 | keep=0.58 | K=8 | first_w=8.00 | llama(L): tf=8.7980 first=7.6436 kCE=11.2360 KD=15.2370 acc=0.050 state=18.3523 align=0.0000 latA=0.9862 latP=0.4911 gist=0.9957 | scale_pen(llama)=4.6043e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  40/64 | grad_norm=5.85 | sec/step~4.46 | keep=0.60 | K=8 | first_w=7.98 | llama(L): tf=9.1281 first=7.9160 kCE=12.0200 KD=15.9663 acc=0.050 state=19.4154 align=0.0000 latA=0.9812 latP=0.4907 gist=0.9955 | scale_pen(llama)=3.2617e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  50/64 | grad_norm=1.38 | sec/step~4.57 | keep=0.61 | K=8 | first_w=7.95 | llama(L): tf=8.9927 first=7.6013 kCE=11.1250 KD=15.3235 acc=0.000 state=18.9470 align=0.0000 latA=0.9862 latP=0.4899 gist=0.9953 | scale_pen(llama)=2.0464e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  60/64 | grad_norm=8.07 | sec/step~4.45 | keep=0.62 | K=8 | first_w=7.90 | llama(L): tf=8.7691 first=7.6560 kCE=11.0163 KD=17.1590 acc=0.050 state=19.0717 align=0.0000 latA=0.9729 latP=0.4888 gist=0.9953 | scale_pen(llama)=2.0464e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
  step  64/64 | grad_norm=10.79 | sec/step~4.61 | keep=0.62 | K=8 | first_w=7.87 | llama(L): tf=8.9882 first=7.3709 kCE=11.2487 KD=14.8225 acc=0.100 state=19.1348 align=0.0000 latA=0.9845 latP=0.4896 gist=0.9953 | scale_pen(llama)=1.0510e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01057]
Epoch 4/6
  step  10/64 | grad_norm=6.47 | sec/step~4.08 | keep=0.64 | K=8 | first_w=7.80 | llama(L): tf=8.7647 first=7.1680 kCE=11.2151 KD=13.8286 acc=0.000 state=16.9784 align=0.0000 latA=0.9790 latP=0.4885 gist=0.9951 | scale_pen(llama)=1.0510e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  20/64 | grad_norm=2.51 | sec/step~4.54 | keep=0.65 | K=8 | first_w=7.71 | llama(L): tf=8.9814 first=7.6868 kCE=11.0461 KD=14.7443 acc=0.050 state=19.2295 align=0.0000 latA=0.9804 latP=0.4878 gist=0.9949 | scale_pen(llama)=3.7691e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  30/64 | grad_norm=9.20 | sec/step~4.31 | keep=0.67 | K=8 | first_w=7.61 | llama(L): tf=8.8125 first=6.8915 kCE=11.0628 KD=12.9335 acc=0.050 state=17.6349 align=0.0000 latA=0.9732 latP=0.4873 gist=0.9949 | scale_pen(llama)=3.7691e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  40/64 | grad_norm=4.58 | sec/step~4.67 | keep=0.68 | K=8 | first_w=7.50 | llama(L): tf=8.8183 first=7.2851 kCE=10.1956 KD=11.6187 acc=0.100 state=18.1051 align=0.0000 latA=0.9818 latP=0.4865 gist=0.9947 | scale_pen(llama)=4.8637e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  50/64 | grad_norm=0.81 | sec/step~4.45 | keep=0.70 | K=8 | first_w=7.37 | llama(L): tf=8.9699 first=7.4673 kCE=10.0244 KD=10.6817 acc=0.000 state=18.7616 align=0.0000 latA=0.9813 latP=0.4864 gist=0.9946 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  60/64 | grad_norm=4.02 | sec/step~4.05 | keep=0.71 | K=8 | first_w=7.24 | llama(L): tf=9.2590 first=8.3360 kCE=10.4531 KD=10.7258 acc=0.000 state=18.4803 align=0.0000 latA=0.9857 latP=0.4855 gist=0.9946 | scale_pen(llama)=2.0464e-12 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  64/64 | grad_norm=5.27 | sec/step~4.53 | keep=0.72 | K=8 | first_w=7.19 | llama(L): tf=8.8758 first=7.4532 kCE=9.9152 KD=12.0167 acc=0.000 state=18.1057 align=0.0000 latA=0.9755 latP=0.4856 gist=0.9946 | scale_pen(llama)=2.0520e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 5/6
  step  10/64 | grad_norm=11.02 | sec/step~4.60 | keep=0.74 | K=8 | first_w=7.05 | llama(L): tf=9.0369 first=8.1315 kCE=9.7896 KD=11.3356 acc=0.000 state=18.9503 align=0.0000 latA=0.9831 latP=0.4850 gist=0.9944 | scale_pen(llama)=2.0520e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  20/64 | grad_norm=5.37 | sec/step~4.28 | keep=0.76 | K=8 | first_w=6.92 | llama(L): tf=8.3786 first=7.5394 kCE=8.8297 KD=10.1889 acc=0.000 state=18.1065 align=0.0000 latA=0.9826 latP=0.4837 gist=0.9943 | scale_pen(llama)=5.1159e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  30/64 | grad_norm=17.53 | sec/step~4.48 | keep=0.78 | K=8 | first_w=6.78 | llama(L): tf=8.0234 first=7.4066 kCE=8.7674 KD=9.9641 acc=0.050 state=19.7942 align=0.0000 latA=0.9782 latP=0.4837 gist=0.9943 | scale_pen(llama)=5.1159e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  40/64 | grad_norm=13.41 | sec/step~4.26 | keep=0.80 | K=8 | first_w=6.65 | llama(L): tf=8.9303 first=7.3130 kCE=8.4812 KD=10.2077 acc=0.050 state=16.4192 align=0.0000 latA=0.9792 latP=0.4818 gist=0.9941 | scale_pen(llama)=8.6459e-11 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  50/64 | grad_norm=4.86 | sec/step~4.23 | keep=0.82 | K=8 | first_w=6.53 | llama(L): tf=8.8205 first=7.2085 kCE=8.1660 KD=10.2243 acc=0.100 state=16.3253 align=0.0000 latA=0.9758 latP=0.4820 gist=0.9939 | scale_pen(llama)=1.2291e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  60/64 | grad_norm=29.99 | sec/step~4.11 | keep=0.84 | K=8 | first_w=6.41 | llama(L): tf=7.9536 first=6.7476 kCE=7.2897 KD=10.3877 acc=0.150 state=15.9505 align=0.0000 latA=0.9749 latP=0.4812 gist=0.9939 | scale_pen(llama)=1.2291e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  64/64 | grad_norm=40.64 | sec/step~4.46 | keep=0.85 | K=8 | first_w=6.37 | llama(L): tf=8.5688 first=7.2429 kCE=7.7796 KD=12.2844 acc=0.050 state=16.9818 align=0.0000 latA=0.9796 latP=0.4825 gist=0.9939 | scale_pen(llama)=1.5667e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 6/6
  step  10/64 | grad_norm=13.05 | sec/step~4.20 | keep=0.87 | K=8 | first_w=6.27 | llama(L): tf=8.8330 first=7.8721 kCE=7.1972 KD=11.6012 acc=0.000 state=18.2018 align=0.0000 latA=0.9807 latP=0.4800 gist=0.9937 | scale_pen(llama)=1.5667e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  20/64 | grad_norm=5.99 | sec/step~4.11 | keep=0.89 | K=8 | first_w=6.18 | llama(L): tf=9.1832 first=7.7412 kCE=7.4288 KD=9.4885 acc=0.050 state=16.4198 align=0.0000 latA=0.9770 latP=0.4801 gist=0.9934 | scale_pen(llama)=1.8146e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  30/64 | grad_norm=20.56 | sec/step~4.03 | keep=0.92 | K=8 | first_w=6.11 | llama(L): tf=8.8496 first=7.2521 kCE=7.4066 KD=9.6863 acc=0.050 state=16.5140 align=0.0000 latA=0.9782 latP=0.4801 gist=0.9934 | scale_pen(llama)=1.8146e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  40/64 | grad_norm=11.30 | sec/step~4.35 | keep=0.94 | K=8 | first_w=6.06 | llama(L): tf=8.7416 first=7.3723 kCE=7.2451 KD=9.7172 acc=0.150 state=17.8276 align=0.0000 latA=0.9795 latP=0.4791 gist=0.9931 | scale_pen(llama)=1.9453e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  50/64 | grad_norm=0.89 | sec/step~4.39 | keep=0.96 | K=8 | first_w=6.02 | llama(L): tf=9.0933 first=7.0225 kCE=6.1892 KD=8.8739 acc=0.050 state=17.6403 align=0.0000 latA=0.9698 latP=0.4771 gist=0.9929 | scale_pen(llama)=1.9453e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  60/64 | grad_norm=5.00 | sec/step~4.26 | keep=0.99 | K=8 | first_w=6.00 | llama(L): tf=9.1967 first=7.4615 kCE=6.8884 KD=9.6206 acc=0.000 state=17.1706 align=0.0000 latA=0.9760 latP=0.4772 gist=0.9929 | scale_pen(llama)=1.9453e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  64/64 | grad_norm=7.19 | sec/step~3.91 | keep=1.00 | K=8 | first_w=6.00 | llama(L): tf=8.7275 first=7.1365 kCE=6.4397 KD=9.2735 acc=0.100 state=16.3892 align=0.0000 latA=0.9728 latP=0.4778 gist=0.9929 | scale_pen(llama)=1.6576e-10 | K=8 tau=2.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 3.3KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt, gist_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
‚úÖ Saved latest checkpoint to runs/llama_single_20250926_170438/ckpt/stageB
üìù Saved LoRA adapters for Llama
üìù Saved Prefix-Tuning adapters for Llama
üìù Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0001094164326787, 'rms_mean_cal': 0.01057116800075164, 'embed_rms': 0.010569693520665169, 'count': 384}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/llama_single_20250926_170438/ckpt/stageB/training_stats.json
Encoder input alignment: mode=neutral_chat | strip_anchor=yes | samples=200
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 2597.49it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:01<00:04,  1.60s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:04<00:01,  1.50s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.01s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:04<00:00,  1.18s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
‚úì Loaded deep prefix generator for llama
trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605
‚úì Loaded LoRA adapters for llama
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)

‚Äî Text baseline summary:
llama: EM=0.000 F1=0.000
‚úì Loaded Prefix-Tuning adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 246.0 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 13107200 bytes (fp32); fp16 reference: 6553600 bytes; fp32 reference: 13107200 bytes
latent/text bytes (one-copy, fp16): n/a

‚Äî Baseline: Text prompting
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 9.251552021152039
Wall clock: 9.10s

‚Äî Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 9.156116559662246
       First-token acc: top1=0.025  top5=0.080
Wall clock: 1.88s

‚Äî Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Wall clock: 3.73s

‚Äî 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 246.03
  },
  "compression": {
    "llama": 3.84421875
  },
  "payload_bytes": 13107200,
  "payload_bytes_detail": {
    "fp32": 13107200,
    "fp16": 6553600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251558
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 13107200,
      "fp16": 6553600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 9.251552021152039
    },
    "wall_clock_sec": 9.099906206130981
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 9.156116559662246,
      "first_token_top1": 0.025,
      "first_token_top5": 0.08,
      "nll_token": 9.156116559662246
    },
    "wall_clock_sec": 1.8772342205047607
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 3.726504325866699
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
