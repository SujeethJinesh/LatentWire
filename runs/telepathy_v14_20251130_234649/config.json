{
    "run_id": "telepathy_v14_20251130_234649",
    "phase": 14,
    "key_fix": "Hybrid Conditioning (Global Pooling + Cross-Attention)",
    "source_model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "target_model": "mistralai/Mistral-7B-Instruct-v0.3",
    "source_layer": 16,
    "soft_tokens": 128,
    "depth": 6,
    "internal_dim": 1024,
    "heads": 16,
    "steps": 10000,
    "batch_size": 8,
    "lr": "3e-4",
    "warmup_steps": 1000,
    "ema_decay": 0.999,
    "diffusion_steps": 10,
    "num_gpus": 4,
    "v13_problem": "Pure cross-attention too weak -> conditioning collapse",
    "v14_solution": [
        "Attention-based global pooling (guide rail)",
        "Cross-attention (entity details)",
        "Larger dim (1024 vs 512)",
        "More heads (16 vs 8)",
        "Longer training (10k steps)"
    ]
}
