
=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3351.42it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.37s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.45s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.02it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.16s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
âš ï¸  No valid checkpoint found to resume; starting fresh.
Epoch 1/4
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/40 | grad_norm=325.32 | sec/step~1.96 | keep=0.70 | K=4 | first_w=2.00 | llama(L): tf=11.6341 first=18.8435 kCE=9.9332 KD=7.2675 state=26.1357 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=117.78 | sec/step~2.24 | keep=0.70 | K=4 | first_w=2.00 | llama(L): tf=10.7965 first=15.5284 kCE=8.8695 KD=9.1158 state=26.6979 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=434.87 | sec/step~2.24 | keep=0.71 | K=4 | first_w=2.00 | llama(L): tf=10.4265 first=15.1634 kCE=7.9801 KD=8.0233 state=26.5729 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=144.23 | sec/step~2.09 | keep=0.72 | K=4 | first_w=2.00 | llama(L): tf=10.4005 first=11.6876 kCE=8.6610 KD=9.4641 state=25.5762 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 2/4
  step  10/40 | grad_norm=52.85 | sec/step~2.22 | keep=0.73 | K=4 | first_w=2.00 | llama(L): tf=9.9489 first=10.5254 kCE=8.7059 KD=9.3455 state=25.3241 align=0.0000 | scale_pen(llama)=0.0000e+00 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=17.96 | sec/step~2.76 | keep=0.74 | K=4 | first_w=1.98 | llama(L): tf=10.0626 first=9.9013 kCE=9.3812 KD=8.4866 state=26.7609 align=0.0000 | scale_pen(llama)=2.2204e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=51.11 | sec/step~2.49 | keep=0.76 | K=4 | first_w=1.92 | llama(L): tf=10.2042 first=9.3629 kCE=9.3212 KD=9.2650 state=25.7609 align=0.0000 | scale_pen(llama)=2.2204e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=24.40 | sec/step~1.99 | keep=0.77 | K=4 | first_w=1.82 | llama(L): tf=10.0133 first=9.4995 kCE=9.5071 KD=9.6544 state=23.2000 align=0.0000 | scale_pen(llama)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 3/4
  step  10/40 | grad_norm=31.18 | sec/step~2.36 | keep=0.79 | K=4 | first_w=1.70 | llama(L): tf=9.8769 first=8.8979 kCE=8.8530 KD=9.3329 state=24.3847 align=0.0000 | scale_pen(llama)=3.5527e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=9.16 | sec/step~2.40 | keep=0.82 | K=4 | first_w=1.57 | llama(L): tf=9.7240 first=8.4174 kCE=8.9053 KD=9.6072 state=23.6344 align=0.0000 | scale_pen(llama)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=21.85 | sec/step~2.26 | keep=0.84 | K=4 | first_w=1.43 | llama(L): tf=10.1739 first=8.5622 kCE=8.7293 KD=9.0181 state=23.4477 align=0.0000 | scale_pen(llama)=1.2790e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=9.95 | sec/step~2.75 | keep=0.87 | K=4 | first_w=1.30 | llama(L): tf=9.4118 first=9.1539 kCE=8.1145 KD=7.6908 state=24.5082 align=0.0000 | scale_pen(llama)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
Epoch 4/4
  step  10/40 | grad_norm=12.77 | sec/step~1.99 | keep=0.90 | K=4 | first_w=1.18 | llama(L): tf=9.3274 first=8.0336 kCE=8.3097 KD=8.6870 state=21.3220 align=0.0000 | scale_pen(llama)=1.4211e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  20/40 | grad_norm=5.03 | sec/step~2.15 | keep=0.93 | K=4 | first_w=1.08 | llama(L): tf=9.6429 first=8.0801 kCE=8.6394 KD=9.2038 state=20.0721 align=0.0000 | scale_pen(llama)=2.8777e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  30/40 | grad_norm=16.03 | sec/step~2.01 | keep=0.96 | K=4 | first_w=1.02 | llama(L): tf=9.7123 first=8.3126 kCE=9.0756 KD=9.3487 state=20.0718 align=0.0000 | scale_pen(llama)=2.8777e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  step  40/40 | grad_norm=8.19 | sec/step~2.21 | keep=1.00 | K=4 | first_w=1.00 | llama(L): tf=9.9490 first=8.0041 kCE=8.2558 KD=8.4403 state=20.6020 align=0.0000 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
âœ… Saved latest checkpoint to runs/llama_single_20250924_213658/ckpt/stageA
ðŸ“ Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0000061437487602, 'rms_mean_cal': 0.010571220784913748, 'embed_rms': 0.01057521253824234, 'count': 160}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3086.32it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:05,  1.87s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:03<00:03,  1.51s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.21s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
trainable params: 272,723,968 || all params: 8,302,985,216 || trainable%: 3.2846
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
âª Resuming from: runs/llama_single_20250924_213658/ckpt/stageA/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 240 steps
Epoch 1/6
[warmup] step=0 mode=text (warm-up)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=9.3322 | latent_scale=0.00
[warmup] step=1 mode=text (warm-up)
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=9.9616 | latent_scale=0.00
[warmup] step=2 mode=text (warm-up)
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=9.6052 | latent_scale=0.01
[warmup] step=3 mode=text (warm-up)
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=9.6944 | latent_scale=0.01
[warmup] step=4 mode=text (warm-up)
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=11.1511 | latent_scale=0.02
[warmup] step=5 mode=text (warm-up)
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=9.9139 | latent_scale=0.02
[warmup] step=6 mode=text (warm-up)
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=10.7789 | latent_scale=0.03
[warmup] step=7 mode=text (warm-up)
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=10.8019 | latent_scale=0.03
[warmup] step=8 mode=text (warm-up)
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=11.1129 | latent_scale=0.03
[warmup] step=9 mode=text (warm-up)
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=9.4162 | latent_scale=0.04
  step  10/80 | grad_norm=6.69 | sec/step~3.52 | keep=0.50 | K=4 | first_w=1.50 | llama(T): tf=0.4017 first=0.4800 kCE=0.3638 KD=0.1011 state=0.9519 align=0.0003 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=9.9096 | latent_scale=0.04
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=10.1798 | latent_scale=0.05
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=8.9452 | latent_scale=0.05
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=10.9025 | latent_scale=0.05
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=10.3443 | latent_scale=0.06
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=9.8095 | latent_scale=0.06
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=7.1955 | latent_scale=0.07
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=8.1553 | latent_scale=0.07
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=7.0932 | latent_scale=0.07
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=7.2906 | latent_scale=0.08
  step  20/80 | grad_norm=0.69 | sec/step~3.10 | keep=0.50 | K=4 | first_w=1.50 | llama(T): tf=0.7522 first=0.7876 kCE=0.8174 KD=0.1150 state=1.8661 align=0.0003 | scale_pen(llama)=2.9878e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=8.1762 | latent_scale=0.08
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=7.6286 | latent_scale=0.09
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=8.5455 | latent_scale=0.09
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=8.2355 | latent_scale=0.10
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=9.0080 | latent_scale=0.10
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=8.2525 | latent_scale=0.10
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=8.4346 | latent_scale=0.11
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=7.8987 | latent_scale=0.11
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=8.1642 | latent_scale=0.12
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=8.2378 | latent_scale=0.12
  step  30/80 | grad_norm=2.58 | sec/step~3.51 | keep=0.50 | K=4 | first_w=1.50 | llama(T): tf=1.2311 first=1.1834 kCE=1.2411 KD=0.1382 state=2.8784 align=0.0003 | scale_pen(llama)=2.9878e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=8.2370 | latent_scale=0.12
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=8.0783 | latent_scale=0.13
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=7.3767 | latent_scale=0.13
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=6.7344 | latent_scale=0.14
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=7.0103 | latent_scale=0.14
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=7.8904 | latent_scale=0.15
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=7.1177 | latent_scale=0.15
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=8.0963 | latent_scale=0.15
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=7.7679 | latent_scale=0.16
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=6.4190 | latent_scale=0.16
  step  40/80 | grad_norm=1.76 | sec/step~2.70 | keep=0.50 | K=4 | first_w=1.50 | llama(T): tf=1.6163 first=1.4078 kCE=1.7958 KD=0.2647 state=3.2518 align=0.0003 | scale_pen(llama)=3.3266e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=7.0932 | latent_scale=0.17
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=7.5598 | latent_scale=0.17
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=7.4450 | latent_scale=0.17
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=7.6713 | latent_scale=0.18
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=6.7167 | latent_scale=0.18
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=6.4812 | latent_scale=0.19
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=7.1961 | latent_scale=0.19
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=6.9462 | latent_scale=0.20
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=6.4216 | latent_scale=0.20
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=5.4399 | latent_scale=0.20
  step  50/80 | grad_norm=1.50 | sec/step~2.91 | keep=0.51 | K=4 | first_w=1.50 | llama(T): tf=2.0121 first=1.9157 kCE=2.1072 KD=0.5879 state=2.8960 align=0.0003 | scale_pen(llama)=4.7072e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=6.1399 | latent_scale=0.21
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=7.1631 | latent_scale=0.21
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=5.8386 | latent_scale=0.22
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=6.3008 | latent_scale=0.22
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=7.8286 | latent_scale=0.23
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=6.9243 | latent_scale=0.23
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=6.0851 | latent_scale=0.23
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=6.2577 | latent_scale=0.24
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=6.9135 | latent_scale=0.24
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=6.3861 | latent_scale=0.25
  step  60/80 | grad_norm=15.93 | sec/step~2.88 | keep=0.51 | K=4 | first_w=1.50 | llama(T): tf=2.5901 first=2.4838 kCE=2.7016 KD=0.7205 state=3.3410 align=0.0003 | scale_pen(llama)=4.7072e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=6.5604 | latent_scale=0.25
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=5.7570 | latent_scale=0.25
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=6.1697 | latent_scale=0.26
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=6.4248 | latent_scale=0.26
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=5.8205 | latent_scale=0.27
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=6.3182 | latent_scale=0.27
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=5.5504 | latent_scale=0.28
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=6.3433 | latent_scale=0.28
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=7.5696 | latent_scale=0.28
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=7.1730 | latent_scale=0.29
  step  70/80 | grad_norm=14.08 | sec/step~3.32 | keep=0.51 | K=4 | first_w=1.50 | llama(T): tf=3.3995 first=2.6141 kCE=3.2504 KD=1.3002 state=1.0409 align=0.0003 | scale_pen(llama)=4.0588e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=6.2556 | latent_scale=0.29
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=6.9575 | latent_scale=0.30
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=6.4049 | latent_scale=0.30
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=6.2055 | latent_scale=0.30
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=5.6545 | latent_scale=0.31
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=5.4549 | latent_scale=0.31
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=5.4207 | latent_scale=0.32
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=6.7777 | latent_scale=0.32
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=6.1993 | latent_scale=0.33
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=6.4662 | latent_scale=0.33
  step  80/80 | grad_norm=37.89 | sec/step~3.32 | keep=0.51 | K=4 | first_w=1.50 | llama(T): tf=3.8843 first=2.8301 kCE=3.4843 KD=1.5205 state=1.1792 align=0.0003 | scale_pen(llama)=2.8253e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 2/6
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=6.2945 | latent_scale=0.33
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=6.8310 | latent_scale=0.34
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=5.9117 | latent_scale=0.34
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=5.6659 | latent_scale=0.35
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=5.4940 | latent_scale=0.35
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=4.8497 | latent_scale=0.35
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=6.4058 | latent_scale=0.36
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=5.9308 | latent_scale=0.36
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=5.8660 | latent_scale=0.37
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=5.0767 | latent_scale=0.37
  step  10/80 | grad_norm=26.35 | sec/step~3.21 | keep=0.52 | K=4 | first_w=1.50 | llama(T): tf=3.3781 first=3.2320 kCE=4.1547 KD=1.2479 state=7.7447 align=0.0003 | scale_pen(llama)=2.8253e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=5.6167 | latent_scale=0.38
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=5.3896 | latent_scale=0.38
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=4.8919 | latent_scale=0.38
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=6.0551 | latent_scale=0.39
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=6.4980 | latent_scale=0.39
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=5.7375 | latent_scale=0.40
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=5.6962 | latent_scale=0.40
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=5.3688 | latent_scale=0.40
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=4.7892 | latent_scale=0.41
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=4.8097 | latent_scale=0.41
  step  20/80 | grad_norm=19.27 | sec/step~3.96 | keep=0.52 | K=4 | first_w=1.50 | llama(T): tf=3.7122 first=3.4230 kCE=3.9293 KD=1.2599 state=9.3615 align=0.0003 | scale_pen(llama)=1.5370e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=5.9334 | latent_scale=0.42
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=5.9490 | latent_scale=0.42
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=4.8352 | latent_scale=0.42
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=5.4929 | latent_scale=0.43
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=5.5113 | latent_scale=0.43
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=5.7558 | latent_scale=0.44
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=6.3185 | latent_scale=0.44
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=5.4148 | latent_scale=0.45
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=5.1369 | latent_scale=0.45
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=5.7929 | latent_scale=0.45
  step  30/80 | grad_norm=86.50 | sec/step~3.21 | keep=0.53 | K=4 | first_w=1.50 | llama(T): tf=4.4223 first=4.3339 kCE=4.7201 KD=1.3673 state=9.8814 align=0.0003 | scale_pen(llama)=1.5370e-10 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=5.6708 | latent_scale=0.46
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=5.9274 | latent_scale=0.46
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=6.4579 | latent_scale=0.47
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=5.8266 | latent_scale=0.47
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=5.0574 | latent_scale=0.47
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=5.4592 | latent_scale=0.48
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=4.9569 | latent_scale=0.48
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=4.9722 | latent_scale=0.49
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=5.2225 | latent_scale=0.49
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=5.0761 | latent_scale=0.50
  step  40/80 | grad_norm=5.86 | sec/step~3.20 | keep=0.53 | K=4 | first_w=1.50 | llama(T): tf=4.7171 first=4.4707 kCE=5.1961 KD=1.5245 state=6.3254 align=0.0003 | scale_pen(llama)=6.5711e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=4.5639 | latent_scale=0.50
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=5.3794 | latent_scale=0.50
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=5.8443 | latent_scale=0.51
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=5.2123 | latent_scale=0.51
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=5.4484 | latent_scale=0.52
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=5.5509 | latent_scale=0.52
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=5.4834 | latent_scale=0.53
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=5.4671 | latent_scale=0.53
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=4.7034 | latent_scale=0.53
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=6.0062 | latent_scale=0.54
  step  50/80 | grad_norm=4.40 | sec/step~3.42 | keep=0.54 | K=4 | first_w=1.50 | llama(T): tf=4.8076 first=4.5963 kCE=5.2478 KD=1.5549 state=7.0927 align=0.0003 | scale_pen(llama)=3.6380e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=4.7979 | latent_scale=0.54
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=6.6413 | latent_scale=0.55
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=5.3860 | latent_scale=0.55
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=4.5688 | latent_scale=0.55
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=5.3807 | latent_scale=0.56
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=5.3767 | latent_scale=0.56
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=4.6522 | latent_scale=0.57
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=5.5710 | latent_scale=0.57
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=5.0160 | latent_scale=0.57
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=4.6696 | latent_scale=0.58
  step  60/80 | grad_norm=11.71 | sec/step~2.76 | keep=0.54 | K=4 | first_w=1.50 | llama(T): tf=5.0549 first=4.1802 kCE=5.4306 KD=1.7817 state=6.1954 align=0.0003 | scale_pen(llama)=3.6380e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=5.3668 | latent_scale=0.58
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=5.3341 | latent_scale=0.59
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=5.4895 | latent_scale=0.59
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=4.4266 | latent_scale=0.60
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=5.7956 | latent_scale=0.60
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=3.5807 | latent_scale=0.60
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=3.8508 | latent_scale=0.61
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=3.9402 | latent_scale=0.61
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=4.5727 | latent_scale=0.62
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=4.0634 | latent_scale=0.62
  step  70/80 | grad_norm=3.61 | sec/step~2.75 | keep=0.55 | K=4 | first_w=1.50 | llama(T): tf=5.7942 first=4.3010 kCE=6.5279 KD=2.0061 state=7.5308 align=0.0003 | scale_pen(llama)=1.8932e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=5.5693 | latent_scale=0.62
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=3.7539 | latent_scale=0.63
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=4.3765 | latent_scale=0.63
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=4.9544 | latent_scale=0.64
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=4.9893 | latent_scale=0.64
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=4.7915 | latent_scale=0.65
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=4.2382 | latent_scale=0.65
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=5.1264 | latent_scale=0.65
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=4.5419 | latent_scale=0.66
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=4.9179 | latent_scale=0.66
  step  80/80 | grad_norm=10.54 | sec/step~3.53 | keep=0.56 | K=4 | first_w=1.50 | llama(T): tf=5.5944 first=5.3505 kCE=7.2302 KD=1.9700 state=8.6497 align=0.0003 | scale_pen(llama)=7.4696e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
Epoch 3/6
  step  1/80 | (warm-up text) | align=0.0003 | text_tf=5.0270 | latent_scale=0.67
  step  2/80 | (warm-up text) | align=0.0003 | text_tf=4.1295 | latent_scale=0.67
  step  3/80 | (warm-up text) | align=0.0003 | text_tf=4.0959 | latent_scale=0.68
  step  4/80 | (warm-up text) | align=0.0003 | text_tf=4.5513 | latent_scale=0.68
  step  5/80 | (warm-up text) | align=0.0003 | text_tf=3.8906 | latent_scale=0.68
  step  6/80 | (warm-up text) | align=0.0003 | text_tf=3.7648 | latent_scale=0.69
  step  7/80 | (warm-up text) | align=0.0003 | text_tf=5.8756 | latent_scale=0.69
  step  8/80 | (warm-up text) | align=0.0003 | text_tf=4.3602 | latent_scale=0.70
  step  9/80 | (warm-up text) | align=0.0003 | text_tf=4.1307 | latent_scale=0.70
  step  10/80 | (warm-up text) | align=0.0003 | text_tf=4.3370 | latent_scale=0.70
  step  10/80 | grad_norm=4.91 | sec/step~2.90 | keep=0.56 | K=4 | first_w=1.50 | llama(T): tf=6.1251 first=5.9638 kCE=7.6288 KD=2.2413 state=8.6052 align=0.0003 | scale_pen(llama)=7.4696e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (warm-up text) | align=0.0003 | text_tf=3.9194 | latent_scale=0.71
  step  12/80 | (warm-up text) | align=0.0003 | text_tf=4.5846 | latent_scale=0.71
  step  13/80 | (warm-up text) | align=0.0003 | text_tf=5.4299 | latent_scale=0.72
  step  14/80 | (warm-up text) | align=0.0003 | text_tf=4.7291 | latent_scale=0.72
  step  15/80 | (warm-up text) | align=0.0003 | text_tf=4.1679 | latent_scale=0.72
  step  16/80 | (warm-up text) | align=0.0003 | text_tf=5.4191 | latent_scale=0.73
  step  17/80 | (warm-up text) | align=0.0003 | text_tf=4.8773 | latent_scale=0.73
  step  18/80 | (warm-up text) | align=0.0003 | text_tf=4.2380 | latent_scale=0.74
  step  19/80 | (warm-up text) | align=0.0003 | text_tf=4.3235 | latent_scale=0.74
  step  20/80 | (warm-up text) | align=0.0003 | text_tf=3.9138 | latent_scale=0.75
  step  20/80 | grad_norm=2.35 | sec/step~2.60 | keep=0.57 | K=4 | first_w=1.50 | llama(T): tf=6.2244 first=5.8096 kCE=6.8232 KD=2.4625 state=9.1032 align=0.0003 | scale_pen(llama)=9.2090e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (warm-up text) | align=0.0003 | text_tf=4.1482 | latent_scale=0.75
  step  22/80 | (warm-up text) | align=0.0003 | text_tf=4.3635 | latent_scale=0.75
  step  23/80 | (warm-up text) | align=0.0003 | text_tf=4.5023 | latent_scale=0.76
  step  24/80 | (warm-up text) | align=0.0003 | text_tf=4.5470 | latent_scale=0.76
  step  25/80 | (warm-up text) | align=0.0003 | text_tf=4.7928 | latent_scale=0.77
  step  26/80 | (warm-up text) | align=0.0003 | text_tf=4.3690 | latent_scale=0.77
  step  27/80 | (warm-up text) | align=0.0003 | text_tf=4.4321 | latent_scale=0.78
  step  28/80 | (warm-up text) | align=0.0003 | text_tf=4.9339 | latent_scale=0.78
  step  29/80 | (warm-up text) | align=0.0003 | text_tf=4.4011 | latent_scale=0.78
  step  30/80 | (warm-up text) | align=0.0003 | text_tf=4.5336 | latent_scale=0.79
  step  30/80 | grad_norm=8.50 | sec/step~3.56 | keep=0.58 | K=4 | first_w=1.50 | llama(T): tf=6.5821 first=5.8291 kCE=7.5629 KD=2.3812 state=11.6283 align=0.0003 | scale_pen(llama)=9.2090e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (warm-up text) | align=0.0003 | text_tf=4.7527 | latent_scale=0.79
  step  32/80 | (warm-up text) | align=0.0003 | text_tf=5.4477 | latent_scale=0.80
  step  33/80 | (warm-up text) | align=0.0003 | text_tf=5.1407 | latent_scale=0.80
  step  34/80 | (warm-up text) | align=0.0003 | text_tf=5.0621 | latent_scale=0.80
  step  35/80 | (warm-up text) | align=0.0003 | text_tf=4.7221 | latent_scale=0.81
  step  36/80 | (warm-up text) | align=0.0003 | text_tf=4.6169 | latent_scale=0.81
  step  37/80 | (warm-up text) | align=0.0003 | text_tf=3.9016 | latent_scale=0.82
  step  38/80 | (warm-up text) | align=0.0003 | text_tf=4.4694 | latent_scale=0.82
  step  39/80 | (warm-up text) | align=0.0003 | text_tf=4.7855 | latent_scale=0.82
  step  40/80 | (warm-up text) | align=0.0003 | text_tf=3.8674 | latent_scale=0.83
  step  40/80 | grad_norm=2.72 | sec/step~3.60 | keep=0.59 | K=4 | first_w=1.49 | llama(T): tf=6.8958 first=6.5278 kCE=7.0895 KD=2.5779 state=13.3388 align=0.0003 | scale_pen(llama)=5.2015e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01057]
  step  41/80 | (warm-up text) | align=0.0003 | text_tf=6.0640 | latent_scale=0.83
  step  42/80 | (warm-up text) | align=0.0003 | text_tf=3.9397 | latent_scale=0.84
  step  43/80 | (warm-up text) | align=0.0003 | text_tf=3.8311 | latent_scale=0.84
  step  44/80 | (warm-up text) | align=0.0003 | text_tf=4.5539 | latent_scale=0.85
  step  45/80 | (warm-up text) | align=0.0003 | text_tf=5.7135 | latent_scale=0.85
  step  46/80 | (warm-up text) | align=0.0003 | text_tf=4.6718 | latent_scale=0.85
  step  47/80 | (warm-up text) | align=0.0003 | text_tf=4.1799 | latent_scale=0.86
  step  48/80 | (warm-up text) | align=0.0003 | text_tf=3.8441 | latent_scale=0.86
  step  49/80 | (warm-up text) | align=0.0003 | text_tf=4.0379 | latent_scale=0.87
  step  50/80 | (warm-up text) | align=0.0003 | text_tf=4.2628 | latent_scale=0.87
  step  50/80 | grad_norm=2.08 | sec/step~3.43 | keep=0.60 | K=4 | first_w=1.49 | llama(T): tf=8.0051 first=7.0912 kCE=7.2991 KD=2.7314 state=15.1113 align=0.0003 | scale_pen(llama)=1.7195e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (warm-up text) | align=0.0003 | text_tf=4.7976 | latent_scale=0.88
  step  52/80 | (warm-up text) | align=0.0003 | text_tf=3.1635 | latent_scale=0.88
  step  53/80 | (warm-up text) | align=0.0003 | text_tf=4.5138 | latent_scale=0.88
  step  54/80 | (warm-up text) | align=0.0003 | text_tf=3.6891 | latent_scale=0.89
  step  55/80 | (warm-up text) | align=0.0003 | text_tf=5.5309 | latent_scale=0.89
  step  56/80 | (warm-up text) | align=0.0003 | text_tf=4.9595 | latent_scale=0.90
  step  57/80 | (warm-up text) | align=0.0003 | text_tf=5.4125 | latent_scale=0.90
  step  58/80 | (warm-up text) | align=0.0003 | text_tf=4.5611 | latent_scale=0.90
  step  59/80 | (warm-up text) | align=0.0003 | text_tf=4.1126 | latent_scale=0.91
  step  60/80 | (warm-up text) | align=0.0003 | text_tf=4.7181 | latent_scale=0.91
  step  60/80 | grad_norm=11.01 | sec/step~3.06 | keep=0.60 | K=4 | first_w=1.48 | llama(T): tf=7.9780 first=7.0690 kCE=7.4689 KD=2.8495 state=15.1931 align=0.0003 | scale_pen(llama)=1.7195e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (warm-up text) | align=0.0003 | text_tf=4.2467 | latent_scale=0.92
  step  62/80 | (warm-up text) | align=0.0003 | text_tf=4.1688 | latent_scale=0.92
  step  63/80 | (warm-up text) | align=0.0003 | text_tf=4.4176 | latent_scale=0.93
  step  64/80 | (warm-up text) | align=0.0003 | text_tf=3.4547 | latent_scale=0.93
  step  65/80 | (warm-up text) | align=0.0003 | text_tf=4.0944 | latent_scale=0.93
  step  66/80 | (warm-up text) | align=0.0003 | text_tf=5.0467 | latent_scale=0.94
  step  67/80 | (warm-up text) | align=0.0003 | text_tf=4.1239 | latent_scale=0.94
  step  68/80 | (warm-up text) | align=0.0003 | text_tf=3.8983 | latent_scale=0.95
  step  69/80 | (warm-up text) | align=0.0003 | text_tf=3.8185 | latent_scale=0.95
  step  70/80 | (warm-up text) | align=0.0003 | text_tf=3.7779 | latent_scale=0.95
  step  70/80 | grad_norm=2.09 | sec/step~2.80 | keep=0.61 | K=4 | first_w=1.47 | llama(T): tf=8.4526 first=7.4312 kCE=6.8406 KD=3.1755 state=13.1455 align=0.0003 | scale_pen(llama)=1.8417e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (warm-up text) | align=0.0003 | text_tf=4.4013 | latent_scale=0.96
  step  72/80 | (warm-up text) | align=0.0003 | text_tf=3.4976 | latent_scale=0.96
  step  73/80 | (warm-up text) | align=0.0003 | text_tf=4.3633 | latent_scale=0.97
  step  74/80 | (warm-up text) | align=0.0003 | text_tf=3.7034 | latent_scale=0.97
  step  75/80 | (warm-up text) | align=0.0003 | text_tf=4.1038 | latent_scale=0.97
  step  76/80 | (warm-up text) | align=0.0003 | text_tf=4.3480 | latent_scale=0.98
  step  77/80 | (warm-up text) | align=0.0003 | text_tf=3.7309 | latent_scale=0.98
  step  78/80 | (warm-up text) | align=0.0003 | text_tf=4.8719 | latent_scale=0.99
  step  79/80 | (warm-up text) | align=0.0003 | text_tf=4.0368 | latent_scale=0.99
  step  80/80 | (warm-up text) | align=0.0003 | text_tf=3.8896 | latent_scale=1.00
  step  80/80 | grad_norm=5.21 | sec/step~3.47 | keep=0.62 | K=4 | first_w=1.46 | llama(T): tf=8.5123 first=7.3462 kCE=6.8122 KD=2.9815 state=15.7562 align=0.0003 | scale_pen(llama)=4.4565e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
Epoch 4/6
[warmup] step=241 mode=text (tail)
  step  2/80 | (tail text) | align=0.0003 | text_tf=3.5138 | latent_scale=1.00
[warmup] step=242 mode=text (tail)
  step  3/80 | (tail text) | align=0.0003 | text_tf=3.4158 | latent_scale=1.00
[warmup] step=243 mode=text (tail)
  step  4/80 | (tail text) | align=0.0003 | text_tf=3.9894 | latent_scale=1.00
[warmup] step=247 mode=text (tail)
  step  8/80 | (tail text) | align=0.0003 | text_tf=4.2398 | latent_scale=1.00
[warmup] step=248 mode=text (tail)
  step  9/80 | (tail text) | align=0.0003 | text_tf=3.8425 | latent_scale=1.00
[warmup] step=249 mode=text (tail)
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.7504 | latent_scale=1.00
  step  10/80 | grad_norm=3.09 | sec/step~3.19 | keep=0.64 | K=4 | first_w=1.45 | llama(T): tf=8.2092 first=7.3503 kCE=6.6386 KD=3.3433 state=15.1350 align=0.0003 | scale_pen(llama)=4.4565e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=250 mode=text (tail)
  step  11/80 | (tail text) | align=0.0003 | text_tf=4.9597 | latent_scale=1.00
[warmup] step=252 mode=text (tail)
  step  13/80 | (tail text) | align=0.0003 | text_tf=4.3888 | latent_scale=1.00
[warmup] step=253 mode=text (tail)
  step  14/80 | (tail text) | align=0.0003 | text_tf=4.2634 | latent_scale=1.00
[warmup] step=256 mode=text (tail)
  step  17/80 | (tail text) | align=0.0003 | text_tf=3.3834 | latent_scale=1.00
[warmup] step=259 mode=text (tail)
  step  20/80 | (tail text) | align=0.0003 | text_tf=3.5083 | latent_scale=1.00
  step  20/80 | grad_norm=4.32 | sec/step~4.07 | keep=0.65 | K=4 | first_w=1.44 | llama(T): tf=7.8731 first=7.1424 kCE=5.7041 KD=3.4255 state=15.7596 align=0.0003 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=262 mode=text (tail)
  step  23/80 | (tail text) | align=0.0003 | text_tf=3.7476 | latent_scale=1.00
[warmup] step=263 mode=text (tail)
  step  24/80 | (tail text) | align=0.0003 | text_tf=4.8031 | latent_scale=1.00
[warmup] step=265 mode=text (tail)
  step  26/80 | (tail text) | align=0.0003 | text_tf=4.5033 | latent_scale=1.00
[warmup] step=266 mode=text (tail)
  step  27/80 | (tail text) | align=0.0003 | text_tf=4.1163 | latent_scale=1.00
[warmup] step=267 mode=text (tail)
  step  28/80 | (tail text) | align=0.0003 | text_tf=5.3078 | latent_scale=1.00
  step  30/80 | grad_norm=14.25 | sec/step~2.74 | keep=0.66 | K=4 | first_w=1.43 | llama(L): tf=8.6350 first=6.9142 kCE=6.4863 KD=3.4938 state=16.1200 align=0.0000 | scale_pen(llama)=2.1615e-11 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=274 mode=text (tail)
  step  35/80 | (tail text) | align=0.0003 | text_tf=3.2268 | latent_scale=1.00
  step  40/80 | grad_norm=10.85 | sec/step~2.86 | keep=0.67 | K=4 | first_w=1.42 | llama(L): tf=8.7411 first=8.0841 kCE=5.6494 KD=3.6355 state=14.7610 align=0.0000 | scale_pen(llama)=3.6380e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[warmup] step=281 mode=text (tail)
  step  42/80 | (tail text) | align=0.0003 | text_tf=4.6694 | latent_scale=1.00
[warmup] step=282 mode=text (tail)
  step  43/80 | (tail text) | align=0.0003 | text_tf=3.4909 | latent_scale=1.00
[warmup] step=283 mode=text (tail)
  step  44/80 | (tail text) | align=0.0003 | text_tf=3.1381 | latent_scale=1.00
[warmup] step=284 mode=text (tail)
  step  45/80 | (tail text) | align=0.0003 | text_tf=4.4346 | latent_scale=1.00
[warmup] step=285 mode=text (tail)
  step  46/80 | (tail text) | align=0.0003 | text_tf=3.6138 | latent_scale=1.00
[warmup] step=286 mode=text (tail)
  step  47/80 | (tail text) | align=0.0003 | text_tf=4.0907 | latent_scale=1.00
[warmup] step=287 mode=text (tail)
  step  48/80 | (tail text) | align=0.0003 | text_tf=4.1886 | latent_scale=1.00
[warmup] step=289 mode=text (tail)
  step  50/80 | (tail text) | align=0.0003 | text_tf=3.4889 | latent_scale=1.00
  step  50/80 | grad_norm=0.70 | sec/step~3.34 | keep=0.68 | K=4 | first_w=1.40 | llama(T): tf=8.7813 first=7.6426 kCE=5.7229 KD=3.9443 state=14.3852 align=0.0003 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (tail text) | align=0.0003 | text_tf=4.6983 | latent_scale=1.00
  step  52/80 | (tail text) | align=0.0003 | text_tf=4.2971 | latent_scale=1.00
  step  53/80 | (tail text) | align=0.0003 | text_tf=4.1070 | latent_scale=1.00
  step  57/80 | (tail text) | align=0.0003 | text_tf=4.5762 | latent_scale=1.00
  step  59/80 | (tail text) | align=0.0003 | text_tf=4.7660 | latent_scale=1.00
  step  60/80 | (tail text) | align=0.0003 | text_tf=3.8155 | latent_scale=1.00
  step  60/80 | grad_norm=4.45 | sec/step~3.03 | keep=0.69 | K=4 | first_w=1.39 | llama(T): tf=8.3186 first=7.2131 kCE=5.9241 KD=4.2850 state=13.5664 align=0.0003 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  67/80 | (tail text) | align=0.0003 | text_tf=4.7030 | latent_scale=1.00
  step  68/80 | (tail text) | align=0.0003 | text_tf=3.9895 | latent_scale=1.00
  step  69/80 | (tail text) | align=0.0003 | text_tf=3.5817 | latent_scale=1.00
  step  70/80 | (tail text) | align=0.0003 | text_tf=4.4373 | latent_scale=1.00
  step  70/80 | grad_norm=1.58 | sec/step~3.52 | keep=0.71 | K=4 | first_w=1.37 | llama(T): tf=8.7194 first=6.9016 kCE=5.4602 KD=3.9762 state=15.0261 align=0.0003 | scale_pen(llama)=8.5301e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (tail text) | align=0.0003 | text_tf=4.5468 | latent_scale=1.00
  step  74/80 | (tail text) | align=0.0003 | text_tf=3.5313 | latent_scale=1.00
  step  76/80 | (tail text) | align=0.0003 | text_tf=3.3324 | latent_scale=1.00
  step  78/80 | (tail text) | align=0.0003 | text_tf=3.5503 | latent_scale=1.00
  step  79/80 | (tail text) | align=0.0003 | text_tf=5.1108 | latent_scale=1.00
  step  80/80 | (tail text) | align=0.0003 | text_tf=3.7649 | latent_scale=1.00
  step  80/80 | grad_norm=3.86 | sec/step~3.21 | keep=0.72 | K=4 | first_w=1.36 | llama(T): tf=8.4217 first=7.3437 kCE=5.5979 KD=3.7507 state=14.7763 align=0.0003 | scale_pen(llama)=5.9721e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
Epoch 5/6
  step  2/80 | (tail text) | align=0.0003 | text_tf=3.1552 | latent_scale=1.00
  step  5/80 | (tail text) | align=0.0003 | text_tf=3.4178 | latent_scale=1.00
  step  6/80 | (tail text) | align=0.0003 | text_tf=4.8489 | latent_scale=1.00
  step  9/80 | (tail text) | align=0.0003 | text_tf=3.3487 | latent_scale=1.00
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.7228 | latent_scale=1.00
  step  10/80 | grad_norm=7.02 | sec/step~3.16 | keep=0.74 | K=4 | first_w=1.34 | llama(T): tf=8.2505 first=7.7283 kCE=5.3079 KD=3.7202 state=14.5572 align=0.0003 | scale_pen(llama)=5.9721e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  11/80 | (tail text) | align=0.0003 | text_tf=4.4219 | latent_scale=1.00
  step  14/80 | (tail text) | align=0.0003 | text_tf=3.8108 | latent_scale=1.00
  step  15/80 | (tail text) | align=0.0003 | text_tf=3.2520 | latent_scale=1.00
  step  16/80 | (tail text) | align=0.0003 | text_tf=3.6617 | latent_scale=1.00
  step  20/80 | grad_norm=1.61 | sec/step~3.20 | keep=0.75 | K=4 | first_w=1.33 | llama(L): tf=8.0583 first=6.7237 kCE=5.4952 KD=3.5217 state=16.3999 align=0.0000 | scale_pen(llama)=2.2204e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  21/80 | (tail text) | align=0.0003 | text_tf=4.8338 | latent_scale=1.00
  step  25/80 | (tail text) | align=0.0003 | text_tf=3.5988 | latent_scale=1.00
  step  27/80 | (tail text) | align=0.0003 | text_tf=3.1861 | latent_scale=1.00
  step  28/80 | (tail text) | align=0.0003 | text_tf=3.7871 | latent_scale=1.00
  step  29/80 | (tail text) | align=0.0003 | text_tf=4.5838 | latent_scale=1.00
  step  30/80 | grad_norm=5.23 | sec/step~3.43 | keep=0.77 | K=4 | first_w=1.31 | llama(L): tf=7.9639 first=7.5878 kCE=5.4626 KD=3.8062 state=16.4466 align=0.0000 | scale_pen(llama)=2.2204e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  32/80 | (tail text) | align=0.0003 | text_tf=3.0626 | latent_scale=1.00
  step  34/80 | (tail text) | align=0.0003 | text_tf=3.8648 | latent_scale=1.00
  step  37/80 | (tail text) | align=0.0003 | text_tf=3.1060 | latent_scale=1.00
  step  40/80 | (tail text) | align=0.0003 | text_tf=4.0678 | latent_scale=1.00
  step  40/80 | grad_norm=2.11 | sec/step~3.25 | keep=0.78 | K=4 | first_w=1.30 | llama(T): tf=8.1611 first=7.0474 kCE=5.1969 KD=3.5643 state=12.9869 align=0.0003 | scale_pen(llama)=1.4211e-14 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  45/80 | (tail text) | align=0.0003 | text_tf=5.2584 | latent_scale=1.00
  step  46/80 | (tail text) | align=0.0003 | text_tf=4.2078 | latent_scale=1.00
  step  47/80 | (tail text) | align=0.0003 | text_tf=4.1910 | latent_scale=1.00
  step  50/80 | grad_norm=0.45 | sec/step~2.56 | keep=0.80 | K=4 | first_w=1.28 | llama(L): tf=8.5596 first=6.8673 kCE=5.4809 KD=4.2106 state=12.0382 align=0.0000 | scale_pen(llama)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (tail text) | align=0.0003 | text_tf=4.6082 | latent_scale=1.00
  step  52/80 | (tail text) | align=0.0003 | text_tf=3.1771 | latent_scale=1.00
  step  55/80 | (tail text) | align=0.0003 | text_tf=3.8803 | latent_scale=1.00
  step  56/80 | (tail text) | align=0.0003 | text_tf=4.0674 | latent_scale=1.00
  step  57/80 | (tail text) | align=0.0003 | text_tf=3.7826 | latent_scale=1.00
  step  60/80 | (tail text) | align=0.0003 | text_tf=4.7551 | latent_scale=1.00
  step  60/80 | grad_norm=4.83 | sec/step~3.40 | keep=0.81 | K=4 | first_w=1.27 | llama(T): tf=8.1630 first=7.5187 kCE=4.9023 KD=3.6646 state=12.8104 align=0.0003 | scale_pen(llama)=2.4016e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  61/80 | (tail text) | align=0.0003 | text_tf=4.6201 | latent_scale=1.00
  step  63/80 | (tail text) | align=0.0003 | text_tf=3.3405 | latent_scale=1.00
  step  65/80 | (tail text) | align=0.0003 | text_tf=4.6065 | latent_scale=1.00
  step  66/80 | (tail text) | align=0.0003 | text_tf=3.9340 | latent_scale=1.00
  step  69/80 | (tail text) | align=0.0003 | text_tf=3.8964 | latent_scale=1.00
  step  70/80 | (tail text) | align=0.0003 | text_tf=4.3614 | latent_scale=1.00
  step  70/80 | grad_norm=67.35 | sec/step~3.49 | keep=0.83 | K=4 | first_w=1.26 | llama(T): tf=8.6742 first=7.4956 kCE=5.3783 KD=4.3816 state=13.3427 align=0.0003 | scale_pen(llama)=3.6380e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  73/80 | (tail text) | align=0.0003 | text_tf=3.8678 | latent_scale=1.00
  step  75/80 | (tail text) | align=0.0003 | text_tf=4.2306 | latent_scale=1.00
  step  76/80 | (tail text) | align=0.0003 | text_tf=3.7909 | latent_scale=1.00
  step  77/80 | (tail text) | align=0.0003 | text_tf=3.4999 | latent_scale=1.00
  step  79/80 | (tail text) | align=0.0003 | text_tf=2.7463 | latent_scale=1.00
  step  80/80 | (tail text) | align=0.0003 | text_tf=3.0558 | latent_scale=1.00
  step  80/80 | grad_norm=171.62 | sec/step~3.21 | keep=0.85 | K=4 | first_w=1.25 | llama(T): tf=8.0339 first=7.1286 kCE=5.4816 KD=4.3561 state=11.6039 align=0.0003 | scale_pen(llama)=5.1301e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
Epoch 6/6
  step  1/80 | (tail text) | align=0.0003 | text_tf=3.3988 | latent_scale=1.00
  step  3/80 | (tail text) | align=0.0003 | text_tf=3.5250 | latent_scale=1.00
  step  6/80 | (tail text) | align=0.0003 | text_tf=3.6997 | latent_scale=1.00
  step  7/80 | (tail text) | align=0.0003 | text_tf=4.0754 | latent_scale=1.00
  step  9/80 | (tail text) | align=0.0003 | text_tf=4.4265 | latent_scale=1.00
  step  10/80 | (tail text) | align=0.0003 | text_tf=3.4788 | latent_scale=1.00
  step  10/80 | grad_norm=2.85 | sec/step~3.42 | keep=0.86 | K=4 | first_w=1.24 | llama(T): tf=8.4443 first=7.6064 kCE=5.0933 KD=3.8907 state=13.4588 align=0.0003 | scale_pen(llama)=5.1301e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  13/80 | (tail text) | align=0.0003 | text_tf=4.1124 | latent_scale=1.00
  step  16/80 | (tail text) | align=0.0003 | text_tf=4.2057 | latent_scale=1.00
  step  17/80 | (tail text) | align=0.0003 | text_tf=4.6227 | latent_scale=1.00
  step  18/80 | (tail text) | align=0.0003 | text_tf=2.6679 | latent_scale=1.00
  step  19/80 | (tail text) | align=0.0003 | text_tf=3.4112 | latent_scale=1.00
  step  20/80 | (tail text) | align=0.0003 | text_tf=3.6138 | latent_scale=1.00
  step  20/80 | grad_norm=1.57 | sec/step~2.70 | keep=0.88 | K=4 | first_w=1.23 | llama(T): tf=8.0810 first=7.4013 kCE=5.4610 KD=4.3933 state=12.6525 align=0.0003 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  24/80 | (tail text) | align=0.0003 | text_tf=3.7544 | latent_scale=1.00
  step  25/80 | (tail text) | align=0.0003 | text_tf=3.1717 | latent_scale=1.00
  step  26/80 | (tail text) | align=0.0003 | text_tf=3.2729 | latent_scale=1.00
  step  28/80 | (tail text) | align=0.0003 | text_tf=4.0510 | latent_scale=1.00
  step  29/80 | (tail text) | align=0.0003 | text_tf=3.5554 | latent_scale=1.00
  step  30/80 | (tail text) | align=0.0003 | text_tf=3.7196 | latent_scale=1.00
  step  30/80 | grad_norm=4.08 | sec/step~3.70 | keep=0.90 | K=4 | first_w=1.22 | llama(T): tf=7.7092 first=6.9127 kCE=5.2852 KD=3.5348 state=14.4238 align=0.0003 | scale_pen(llama)=6.9633e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  31/80 | (tail text) | align=0.0003 | text_tf=2.8786 | latent_scale=1.00
  step  32/80 | (tail text) | align=0.0003 | text_tf=4.5117 | latent_scale=1.00
  step  33/80 | (tail text) | align=0.0003 | text_tf=3.2802 | latent_scale=1.00
  step  35/80 | (tail text) | align=0.0003 | text_tf=4.0151 | latent_scale=1.00
  step  38/80 | (tail text) | align=0.0003 | text_tf=4.0387 | latent_scale=1.00
  step  40/80 | grad_norm=2.03 | sec/step~2.80 | keep=0.92 | K=4 | first_w=1.21 | llama(L): tf=8.2071 first=7.6144 kCE=5.2044 KD=3.7818 state=13.9331 align=0.0000 | scale_pen(llama)=1.2825e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  44/80 | (tail text) | align=0.0003 | text_tf=3.5760 | latent_scale=1.00
  step  45/80 | (tail text) | align=0.0003 | text_tf=3.3507 | latent_scale=1.00
  step  46/80 | (tail text) | align=0.0003 | text_tf=3.1412 | latent_scale=1.00
  step  47/80 | (tail text) | align=0.0003 | text_tf=3.3961 | latent_scale=1.00
  step  48/80 | (tail text) | align=0.0003 | text_tf=3.4599 | latent_scale=1.00
  step  49/80 | (tail text) | align=0.0003 | text_tf=3.9063 | latent_scale=1.00
  step  50/80 | grad_norm=0.70 | sec/step~2.73 | keep=0.94 | K=4 | first_w=1.21 | llama(L): tf=7.7490 first=7.4062 kCE=5.1862 KD=4.0244 state=14.5735 align=0.0000 | scale_pen(llama)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  51/80 | (tail text) | align=0.0003 | text_tf=4.6761 | latent_scale=1.00
  step  53/80 | (tail text) | align=0.0003 | text_tf=3.5815 | latent_scale=1.00
  step  54/80 | (tail text) | align=0.0003 | text_tf=3.3743 | latent_scale=1.00
  step  59/80 | (tail text) | align=0.0003 | text_tf=2.8422 | latent_scale=1.00
  step  60/80 | (tail text) | align=0.0003 | text_tf=2.6795 | latent_scale=1.00
  step  60/80 | grad_norm=3.07 | sec/step~2.88 | keep=0.96 | K=4 | first_w=1.20 | llama(T): tf=7.7583 first=6.4706 kCE=5.2600 KD=3.6751 state=13.0366 align=0.0003 | scale_pen(llama)=4.6043e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  65/80 | (tail text) | align=0.0003 | text_tf=3.3496 | latent_scale=1.00
  step  69/80 | (tail text) | align=0.0003 | text_tf=3.8341 | latent_scale=1.00
  step  70/80 | grad_norm=1.58 | sec/step~2.26 | keep=0.98 | K=4 | first_w=1.20 | llama(L): tf=8.4783 first=7.6930 kCE=4.8572 KD=4.2563 state=12.6621 align=0.0000 | scale_pen(llama)=1.5667e-12 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
  step  71/80 | (tail text) | align=0.0003 | text_tf=3.2960 | latent_scale=1.00
  step  72/80 | (tail text) | align=0.0003 | text_tf=3.8588 | latent_scale=1.00
  step  75/80 | (tail text) | align=0.0003 | text_tf=3.6820 | latent_scale=1.00
  step  76/80 | (tail text) | align=0.0003 | text_tf=3.1307 | latent_scale=1.00
  step  78/80 | (tail text) | align=0.0003 | text_tf=4.0198 | latent_scale=1.00
  step  80/80 | grad_norm=3.74 | sec/step~2.55 | keep=1.00 | K=4 | first_w=1.20 | llama(L): tf=8.0461 first=7.1512 kCE=5.0748 KD=4.2253 state=13.6920 align=0.0000 | scale_pen(llama)=5.1159e-13 | K=4 tau=1.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01057]
[checkpoint] Freed 2.4KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt
[checkpoint] Freed 0.0B after save (non-canonical).
âœ… Saved latest checkpoint to runs/llama_single_20250924_213658/ckpt/stageB
ðŸ“ Saved Prefix-Tuning adapters for Llama
ðŸ“ Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0002362541854382, 'rms_mean_cal': 0.01057151168351993, 'embed_rms': 0.01056710910052061, 'count': 480}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/llama_single_20250924_213658/ckpt/stageB/training_stats.json
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 2869.37it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.44s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:04<00:01,  1.34s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.08s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

â€” Text baseline summary:
llama: EM=0.590 F1=0.796
âœ“ Loaded Prefix-Tuning adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 246.0 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 13107200 bytes (fp32); fp16 reference: 6553600 bytes; fp32 reference: 13107200 bytes
latent/text bytes (one-copy, fp16): n/a

â€” Baseline: Text prompting
Llama  EM: 0.590  F1: 0.796  |  NLL/token (gold): 13.675748455854526
Wall clock: 6.82s

â€” Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 8.280786253697748
       First-token acc: top1=0.000  top5=0.045
Wall clock: 1.82s

â€” Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Wall clock: 2.23s

â€” 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 246.03
  },
  "compression": {
    "llama": 3.84421875
  },
  "payload_bytes": 13107200,
  "payload_bytes_detail": {
    "fp32": 13107200,
    "fp16": 6553600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251558
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 13107200,
      "fp16": 6553600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.7961021295152534,
      "nll_token": 13.675748455854526
    },
    "wall_clock_sec": 6.816869497299194
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 8.280786253697748,
      "first_token_top1": 0.0,
      "first_token_top5": 0.045,
      "nll_token": 8.280786253697748
    },
    "wall_clock_sec": 1.8205687999725342
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 2.2328813076019287
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {},
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
