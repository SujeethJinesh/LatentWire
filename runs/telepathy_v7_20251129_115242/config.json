{
    "run_id": "telepathy_v7_20251129_115242",
    "phase": 7,
    "key_fixes": [
        "Output scaling (tanh + learnable scale)",
        "Reverted to Answer anchoring (V5 logic)"
    ],
    "source_model": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "target_model": "mistralai/Mistral-7B-Instruct-v0.3",
    "source_layer": 16,
    "soft_tokens": 256,
    "depth": 4,
    "heads": 8,
    "steps": 2500,
    "batch_size": 4,
    "lr": 1e-4,
    "anchor_weight": 2.0,
    "contrastive_weight": 0.1,
    "contrastive_temp": 0.07,
    "num_gpus": 4
}
