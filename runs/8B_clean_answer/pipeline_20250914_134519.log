
=========================================
Starting pipeline at Sun Sep 14 13:45:19 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 16 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_answer/ckpt


=========================================
EPOCH 1/16
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 707.90it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3240.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.49it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
  step 10/685 | loss_L=3.2697 | loss_Q=9.4794 | scale_pen(L)= 1.4154e-07 | scale_pen(Q)= 7.7819e-09 | grad_norm=0.73 | sec/step~1.20 | rms_L~0.5615 rms_Q~0.5584
  step 20/685 | loss_L=2.7314 | loss_Q=5.7021 | scale_pen(L)= 4.5727e-07 | scale_pen(Q)= 2.6276e-11 | grad_norm=0.67 | sec/step~1.05 | rms_L~0.5615 rms_Q~0.5590
  step 30/685 | loss_L=2.5314 | loss_Q=3.9623 | scale_pen(L)= 5.4988e-07 | scale_pen(Q)= 1.1524e-08 | grad_norm=0.68 | sec/step~1.11 | rms_L~0.5615 rms_Q~0.5595
  step 40/685 | loss_L=2.2152 | loss_Q=2.6161 | scale_pen(L)= 2.3407e-07 | scale_pen(Q)= 1.5772e-08 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.5619 rms_Q~0.5602
  step 50/685 | loss_L=1.9055 | loss_Q=2.0959 | scale_pen(L)= 6.9038e-09 | scale_pen(Q)= 2.3411e-08 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.5624 rms_Q~0.5609
  step 60/685 | loss_L=1.9450 | loss_Q=2.2074 | scale_pen(L)= 2.3853e-07 | scale_pen(Q)= 1.4930e-08 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.5633 rms_Q~0.5618
  step 70/685 | loss_L=1.9087 | loss_Q=2.1027 | scale_pen(L)= 1.3626e-06 | scale_pen(Q)= 1.4959e-08 | grad_norm=0.67 | sec/step~1.15 | rms_L~0.5645 rms_Q~0.5627
  step 80/685 | loss_L=1.6716 | loss_Q=1.7701 | scale_pen(L)= 3.4393e-06 | scale_pen(Q)= 2.8662e-07 | grad_norm=0.67 | sec/step~1.06 | rms_L~0.5658 rms_Q~0.5637
  step 90/685 | loss_L=1.5416 | loss_Q=1.5768 | scale_pen(L)= 6.2819e-06 | scale_pen(Q)= 7.8388e-07 | grad_norm=0.68 | sec/step~1.17 | rms_L~0.5674 rms_Q~0.5646
  step 100/685 | loss_L=1.5143 | loss_Q=1.5940 | scale_pen(L)= 9.8340e-06 | scale_pen(Q)= 8.9410e-07 | grad_norm=0.63 | sec/step~1.20 | rms_L~0.5689 rms_Q~0.5655
  step 110/685 | loss_L=1.5811 | loss_Q=1.7110 | scale_pen(L)= 1.4193e-05 | scale_pen(Q)= 8.9839e-07 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5705 rms_Q~0.5664
  step 120/685 | loss_L=1.5888 | loss_Q=1.7087 | scale_pen(L)= 1.8984e-05 | scale_pen(Q)= 9.8537e-07 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.5721 rms_Q~0.5672
  step 130/685 | loss_L=1.6731 | loss_Q=1.7986 | scale_pen(L)= 2.4251e-05 | scale_pen(Q)= 1.1188e-06 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.5737 rms_Q~0.5680
  step 140/685 | loss_L=1.4710 | loss_Q=1.6179 | scale_pen(L)= 2.8615e-05 | scale_pen(Q)= 1.3695e-06 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5752 rms_Q~0.5687
  step 150/685 | loss_L=1.5547 | loss_Q=1.6296 | scale_pen(L)= 3.3577e-05 | scale_pen(Q)= 1.7361e-06 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5767 rms_Q~0.5693
  step 160/685 | loss_L=1.6113 | loss_Q=1.7570 | scale_pen(L)= 3.9142e-05 | scale_pen(Q)= 2.1423e-06 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.5781 rms_Q~0.5700
  step 170/685 | loss_L=1.4194 | loss_Q=1.5121 | scale_pen(L)= 4.5537e-05 | scale_pen(Q)= 2.7205e-06 | grad_norm=0.61 | sec/step~1.14 | rms_L~0.5795 rms_Q~0.5706
  step 180/685 | loss_L=1.6471 | loss_Q=1.7130 | scale_pen(L)= 5.1413e-05 | scale_pen(Q)= 3.3711e-06 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.5809 rms_Q~0.5712
  step 190/685 | loss_L=1.4798 | loss_Q=1.5359 | scale_pen(L)= 5.7488e-05 | scale_pen(Q)= 4.3700e-06 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.5822 rms_Q~0.5718
  step 200/685 | loss_L=1.4482 | loss_Q=1.4698 | scale_pen(L)= 6.4235e-05 | scale_pen(Q)= 4.8921e-06 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.5835 rms_Q~0.5724
  step 210/685 | loss_L=1.5089 | loss_Q=1.5749 | scale_pen(L)= 7.0275e-05 | scale_pen(Q)= 5.8856e-06 | grad_norm=0.55 | sec/step~1.13 | rms_L~0.5848 rms_Q~0.5729
  step 220/685 | loss_L=1.4245 | loss_Q=1.4828 | scale_pen(L)= 7.6865e-05 | scale_pen(Q)= 6.7517e-06 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.5860 rms_Q~0.5735
  step 230/685 | loss_L=1.5150 | loss_Q=1.5295 | scale_pen(L)= 8.3431e-05 | scale_pen(Q)= 7.5260e-06 | grad_norm=0.57 | sec/step~1.10 | rms_L~0.5872 rms_Q~0.5741
  step 240/685 | loss_L=1.6047 | loss_Q=1.6503 | scale_pen(L)= 8.7740e-05 | scale_pen(Q)= 8.0354e-06 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.5884 rms_Q~0.5746
  step 250/685 | loss_L=1.5823 | loss_Q=1.6268 | scale_pen(L)= 9.2954e-05 | scale_pen(Q)= 8.7762e-06 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.5896 rms_Q~0.5751
  step 260/685 | loss_L=1.3996 | loss_Q=1.3989 | scale_pen(L)= 9.9108e-05 | scale_pen(Q)= 8.9138e-06 | grad_norm=0.60 | sec/step~1.17 | rms_L~0.5906 rms_Q~0.5756
  step 270/685 | loss_L=1.3895 | loss_Q=1.3932 | scale_pen(L)= 1.0555e-04 | scale_pen(Q)= 1.0262e-05 | grad_norm=0.57 | sec/step~1.13 | rms_L~0.5917 rms_Q~0.5761
  step 280/685 | loss_L=1.4500 | loss_Q=1.4717 | scale_pen(L)= 1.1216e-04 | scale_pen(Q)= 1.2027e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.5928 rms_Q~0.5766
  step 290/685 | loss_L=1.4650 | loss_Q=1.4801 | scale_pen(L)= 1.2023e-04 | scale_pen(Q)= 1.4083e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5938 rms_Q~0.5771
  step 300/685 | loss_L=1.5664 | loss_Q=1.5564 | scale_pen(L)= 1.2805e-04 | scale_pen(Q)= 1.6129e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.5948 rms_Q~0.5776
  step 310/685 | loss_L=1.3062 | loss_Q=1.2962 | scale_pen(L)= 1.3631e-04 | scale_pen(Q)= 1.8150e-05 | grad_norm=0.54 | sec/step~1.05 | rms_L~0.5958 rms_Q~0.5781
  step 320/685 | loss_L=1.4048 | loss_Q=1.4113 | scale_pen(L)= 1.4286e-04 | scale_pen(Q)= 2.0261e-05 | grad_norm=0.58 | sec/step~1.09 | rms_L~0.5968 rms_Q~0.5785
  step 330/685 | loss_L=1.2523 | loss_Q=1.2550 | scale_pen(L)= 1.4724e-04 | scale_pen(Q)= 2.1940e-05 | grad_norm=0.55 | sec/step~1.05 | rms_L~0.5978 rms_Q~0.5790
  step 340/685 | loss_L=1.5353 | loss_Q=1.5351 | scale_pen(L)= 1.5331e-04 | scale_pen(Q)= 2.3798e-05 | grad_norm=0.57 | sec/step~1.12 | rms_L~0.5987 rms_Q~0.5795
  step 350/685 | loss_L=1.3615 | loss_Q=1.3839 | scale_pen(L)= 1.5928e-04 | scale_pen(Q)= 2.5481e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5996 rms_Q~0.5799
  step 360/685 | loss_L=1.1969 | loss_Q=1.1405 | scale_pen(L)= 1.6810e-04 | scale_pen(Q)= 2.6657e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6005 rms_Q~0.5804
  step 370/685 | loss_L=1.2376 | loss_Q=1.2047 | scale_pen(L)= 1.7443e-04 | scale_pen(Q)= 2.8415e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.6014 rms_Q~0.5808
  step 380/685 | loss_L=1.3508 | loss_Q=1.3437 | scale_pen(L)= 1.8077e-04 | scale_pen(Q)= 2.9350e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.6023 rms_Q~0.5812
  step 390/685 | loss_L=1.5087 | loss_Q=1.4698 | scale_pen(L)= 1.8907e-04 | scale_pen(Q)= 3.1837e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.6031 rms_Q~0.5817
  step 400/685 | loss_L=1.2898 | loss_Q=1.2512 | scale_pen(L)= 1.9580e-04 | scale_pen(Q)= 3.4535e-05 | grad_norm=0.56 | sec/step~1.05 | rms_L~0.6040 rms_Q~0.5821
  step 410/685 | loss_L=1.2423 | loss_Q=1.2207 | scale_pen(L)= 2.0257e-04 | scale_pen(Q)= 3.7049e-05 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.6048 rms_Q~0.5825
  step 420/685 | loss_L=1.4158 | loss_Q=1.3793 | scale_pen(L)= 2.1036e-04 | scale_pen(Q)= 3.8447e-05 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.6056 rms_Q~0.5829
  step 430/685 | loss_L=1.4859 | loss_Q=1.4817 | scale_pen(L)= 2.1794e-04 | scale_pen(Q)= 4.0004e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.6064 rms_Q~0.5833
  step 440/685 | loss_L=1.2841 | loss_Q=1.2524 | scale_pen(L)= 2.2525e-04 | scale_pen(Q)= 4.2332e-05 | grad_norm=0.52 | sec/step~1.12 | rms_L~0.6072 rms_Q~0.5837
  step 450/685 | loss_L=1.3562 | loss_Q=1.3314 | scale_pen(L)= 2.3002e-04 | scale_pen(Q)= 4.2450e-05 | grad_norm=0.56 | sec/step~1.11 | rms_L~0.6079 rms_Q~0.5841
  step 460/685 | loss_L=1.5087 | loss_Q=1.4824 | scale_pen(L)= 2.3745e-04 | scale_pen(Q)= 4.3254e-05 | grad_norm=0.55 | sec/step~1.19 | rms_L~0.6087 rms_Q~0.5844
  step 470/685 | loss_L=1.2829 | loss_Q=1.2299 | scale_pen(L)= 2.4533e-04 | scale_pen(Q)= 4.4182e-05 | grad_norm=0.53 | sec/step~1.11 | rms_L~0.6094 rms_Q~0.5848
  step 480/685 | loss_L=1.3223 | loss_Q=1.3035 | scale_pen(L)= 2.5222e-04 | scale_pen(Q)= 4.5108e-05 | grad_norm=0.58 | sec/step~1.10 | rms_L~0.6102 rms_Q~0.5852
  step 490/685 | loss_L=1.2911 | loss_Q=1.2609 | scale_pen(L)= 2.5877e-04 | scale_pen(Q)= 4.7171e-05 | grad_norm=0.57 | sec/step~1.11 | rms_L~0.6109 rms_Q~0.5855
  step 500/685 | loss_L=1.2686 | loss_Q=1.2312 | scale_pen(L)= 2.6561e-04 | scale_pen(Q)= 4.8711e-05 | grad_norm=0.59 | sec/step~1.17 | rms_L~0.6116 rms_Q~0.5859
  step 510/685 | loss_L=1.3622 | loss_Q=1.3092 | scale_pen(L)= 2.7412e-04 | scale_pen(Q)= 4.9599e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6124 rms_Q~0.5862
  step 520/685 | loss_L=1.5473 | loss_Q=1.5317 | scale_pen(L)= 2.8168e-04 | scale_pen(Q)= 5.1318e-05 | grad_norm=0.55 | sec/step~1.11 | rms_L~0.6131 rms_Q~0.5866
  step 530/685 | loss_L=1.3920 | loss_Q=1.3578 | scale_pen(L)= 2.8846e-04 | scale_pen(Q)= 5.2086e-05 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.6138 rms_Q~0.5869
  step 540/685 | loss_L=1.2953 | loss_Q=1.2641 | scale_pen(L)= 2.9490e-04 | scale_pen(Q)= 5.3339e-05 | grad_norm=0.58 | sec/step~1.11 | rms_L~0.6144 rms_Q~0.5872
  step 550/685 | loss_L=1.4649 | loss_Q=1.4338 | scale_pen(L)= 3.0548e-04 | scale_pen(Q)= 5.4433e-05 | grad_norm=0.54 | sec/step~1.06 | rms_L~0.6151 rms_Q~0.5875
  step 560/685 | loss_L=1.2543 | loss_Q=1.2064 | scale_pen(L)= 3.1180e-04 | scale_pen(Q)= 5.5079e-05 | grad_norm=0.52 | sec/step~1.09 | rms_L~0.6158 rms_Q~0.5879
  step 570/685 | loss_L=1.3072 | loss_Q=1.2770 | scale_pen(L)= 3.1871e-04 | scale_pen(Q)= 5.5671e-05 | grad_norm=0.55 | sec/step~1.06 | rms_L~0.6165 rms_Q~0.5882
  step 580/685 | loss_L=1.5042 | loss_Q=1.5065 | scale_pen(L)= 3.2562e-04 | scale_pen(Q)= 5.6941e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.6171 rms_Q~0.5885
  step 590/685 | loss_L=1.4401 | loss_Q=1.3939 | scale_pen(L)= 3.3487e-04 | scale_pen(Q)= 5.8260e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6178 rms_Q~0.5888
  step 600/685 | loss_L=1.4143 | loss_Q=1.4019 | scale_pen(L)= 3.4260e-04 | scale_pen(Q)= 5.9422e-05 | grad_norm=0.57 | sec/step~1.05 | rms_L~0.6184 rms_Q~0.5891
  step 610/685 | loss_L=1.4324 | loss_Q=1.4123 | scale_pen(L)= 3.5354e-04 | scale_pen(Q)= 6.0989e-05 | grad_norm=0.58 | sec/step~1.12 | rms_L~0.6191 rms_Q~0.5894
  step 620/685 | loss_L=1.1834 | loss_Q=1.1613 | scale_pen(L)= 3.5889e-04 | scale_pen(Q)= 6.3446e-05 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.6197 rms_Q~0.5897
  step 630/685 | loss_L=1.2952 | loss_Q=1.2659 | scale_pen(L)= 3.6940e-04 | scale_pen(Q)= 6.5381e-05 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.6203 rms_Q~0.5899
  step 640/685 | loss_L=1.1739 | loss_Q=1.1628 | scale_pen(L)= 3.7869e-04 | scale_pen(Q)= 6.6909e-05 | grad_norm=0.55 | sec/step~1.05 | rms_L~0.6209 rms_Q~0.5902
  step 650/685 | loss_L=1.2607 | loss_Q=1.2260 | scale_pen(L)= 3.8768e-04 | scale_pen(Q)= 6.8155e-05 | grad_norm=0.57 | sec/step~1.12 | rms_L~0.6216 rms_Q~0.5905
  step 660/685 | loss_L=1.2984 | loss_Q=1.2931 | scale_pen(L)= 3.9365e-04 | scale_pen(Q)= 6.8480e-05 | grad_norm=0.55 | sec/step~1.07 | rms_L~0.6222 rms_Q~0.5908
  step 670/685 | loss_L=1.3730 | loss_Q=1.3286 | scale_pen(L)= 4.0100e-04 | scale_pen(Q)= 6.6578e-05 | grad_norm=0.57 | sec/step~1.05 | rms_L~0.6228 rms_Q~0.5911
  step 680/685 | loss_L=1.3205 | loss_Q=1.2960 | scale_pen(L)= 4.0817e-04 | scale_pen(Q)= 6.8861e-05 | grad_norm=0.54 | sec/step~1.06 | rms_L~0.6234 rms_Q~0.5913
  step 685/685 | loss_L=1.2313 | loss_Q=1.2080 | scale_pen(L)= 4.1152e-04 | scale_pen(Q)= 6.8295e-05 | grad_norm=0.61 | sec/step~0.46 | rms_L~0.6237 rms_Q~0.5915
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.6237111437929808, 'count': 685}, 'qwen': {'rms_mean': 0.5914779565630168, 'count': 685}}

Evaluating epoch 1 checkpoint...
Evaluating: runs/8B_clean_answer/epoch1 -> runs/8B_clean_answer/eval_epoch1
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch1/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3569.62it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 9.79802307358134, 'neutral_chat': 9.693238764393087, 'llama_chat': 9.766721776823879} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch1/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.66368 -> target=0.01057
[debug:llama] adapter.scale=1.0203 | Z.std=1.0015 Z.mean||=16.0243 | prefix.std=0.0106 prefix.mean||=0.6761 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the of the the of the the of the the'
  1: '1. 2. 3. 4'
  2: '2000, 1,000'
  3: '1. 2. 3. 4'
  4: '1. 2. 3. 4'
Saved Llama results to runs/8B_clean_answer/eval_epoch1/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3575.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.35it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 9.530080047234026, 'neutral_chat': 9.595448964802676, 'qwen_chat': 9.53020258271505} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch1/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.60294 -> target=0.01363
[debug:qwen] adapter.scale=1.0083 | Z.std=1.0011 Z.mean||=16.0168 | prefix.std=0.0136 prefix.mean||=0.8158 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: ']  ]  ]  ]  ]  ]'
  1: ''
  2: '\n\n \n\n'
  3: '111111111111'
  4: '[__]  [__]  [__]'
Saved Qwen results to runs/8B_clean_answer/eval_epoch1/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3113.23it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6087.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.44it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.91s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.007  |  NLL/token (gold): 9.694262968439634
Qwen   EM: 0.000   F1: 0.002  |  NLL/token (gold): 9.537757828122093
Wall clock: 13.19s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.72s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.009

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.907333850860596,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.189993619918823,
    "llama": {
      "em": 0.0,
      "f1": 0.007413059163059165,
      "nll_token": 9.694262968439634
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0018288367978770458,
      "nll_token": 9.537757828122093
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.718528509140015,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004904233623273872,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0202858448028564,
      "Z_std": 1.0015217065811157,
      "Z_mean_norm": 16.024335861206055,
      "prefix_std": 0.010572589933872223,
      "prefix_mean_norm": 0.6760960221290588,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0082640647888184,
      "Z_std": 1.0010535717010498,
      "Z_mean_norm": 16.016845703125,
      "prefix_std": 0.013631654903292656,
      "prefix_mean_norm": 0.8157997727394104,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.009241895960936209
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch1/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch1/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.007 | Qwen 0.002


=========================================
EPOCH 2/16
=========================================

Training epoch 2...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7272.31it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3986.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=1, global_step=685
Epoch 2/1
  step 10/685 | loss_L=1.2233 | loss_Q=1.2157 | scale_pen(L)= 4.2040e-04 | scale_pen(Q)= 6.8210e-05 | grad_norm=0.59 | sec/step~1.08 | rms_L~0.6652 rms_Q~0.6098
  step 20/685 | loss_L=1.3594 | loss_Q=1.3343 | scale_pen(L)= 4.2785e-04 | scale_pen(Q)= 6.9986e-05 | grad_norm=0.57 | sec/step~1.13 | rms_L~0.6656 rms_Q~0.6099
  step 30/685 | loss_L=1.4161 | loss_Q=1.3962 | scale_pen(L)= 4.3487e-04 | scale_pen(Q)= 6.8923e-05 | grad_norm=0.51 | sec/step~1.15 | rms_L~0.6660 rms_Q~0.6100
  step 40/685 | loss_L=1.3645 | loss_Q=1.3311 | scale_pen(L)= 4.4258e-04 | scale_pen(Q)= 6.9321e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6664 rms_Q~0.6101
  step 50/685 | loss_L=1.3010 | loss_Q=1.2855 | scale_pen(L)= 4.5049e-04 | scale_pen(Q)= 7.0076e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.6668 rms_Q~0.6103
  step 60/685 | loss_L=1.2835 | loss_Q=1.2604 | scale_pen(L)= 4.5624e-04 | scale_pen(Q)= 6.8974e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6672 rms_Q~0.6104
  step 70/685 | loss_L=1.4801 | loss_Q=1.4811 | scale_pen(L)= 4.6612e-04 | scale_pen(Q)= 6.9123e-05 | grad_norm=0.52 | sec/step~1.13 | rms_L~0.6676 rms_Q~0.6105
  step 80/685 | loss_L=1.4461 | loss_Q=1.4553 | scale_pen(L)= 4.7076e-04 | scale_pen(Q)= 6.9916e-05 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.6681 rms_Q~0.6107
  step 90/685 | loss_L=1.3264 | loss_Q=1.3195 | scale_pen(L)= 4.8133e-04 | scale_pen(Q)= 7.2321e-05 | grad_norm=0.56 | sec/step~1.11 | rms_L~0.6685 rms_Q~0.6108
  step 100/685 | loss_L=1.2789 | loss_Q=1.2742 | scale_pen(L)= 4.8530e-04 | scale_pen(Q)= 7.0648e-05 | grad_norm=0.56 | sec/step~1.12 | rms_L~0.6689 rms_Q~0.6109
  step 110/685 | loss_L=1.3694 | loss_Q=1.3571 | scale_pen(L)= 4.9198e-04 | scale_pen(Q)= 6.9966e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.6693 rms_Q~0.6111
  step 120/685 | loss_L=1.3569 | loss_Q=1.3435 | scale_pen(L)= 4.9870e-04 | scale_pen(Q)= 7.0880e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.6697 rms_Q~0.6112
  step 130/685 | loss_L=1.2592 | loss_Q=1.2565 | scale_pen(L)= 5.0499e-04 | scale_pen(Q)= 7.1139e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.6701 rms_Q~0.6113
  step 140/685 | loss_L=1.1554 | loss_Q=1.1302 | scale_pen(L)= 5.1360e-04 | scale_pen(Q)= 6.9355e-05 | grad_norm=0.55 | sec/step~1.11 | rms_L~0.6704 rms_Q~0.6114
  step 150/685 | loss_L=1.3158 | loss_Q=1.3144 | scale_pen(L)= 5.2004e-04 | scale_pen(Q)= 6.9986e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6708 rms_Q~0.6115
  step 160/685 | loss_L=1.2787 | loss_Q=1.2641 | scale_pen(L)= 5.2537e-04 | scale_pen(Q)= 7.0802e-05 | grad_norm=0.58 | sec/step~1.09 | rms_L~0.6712 rms_Q~0.6116
  step 170/685 | loss_L=1.2842 | loss_Q=1.2406 | scale_pen(L)= 5.3351e-04 | scale_pen(Q)= 7.1286e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.6715 rms_Q~0.6118
  step 180/685 | loss_L=1.2684 | loss_Q=1.2595 | scale_pen(L)= 5.4229e-04 | scale_pen(Q)= 7.1748e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.6719 rms_Q~0.6119
  step 190/685 | loss_L=1.3804 | loss_Q=1.3778 | scale_pen(L)= 5.5040e-04 | scale_pen(Q)= 7.2231e-05 | grad_norm=0.57 | sec/step~1.13 | rms_L~0.6723 rms_Q~0.6120
  step 200/685 | loss_L=1.3251 | loss_Q=1.3181 | scale_pen(L)= 5.5473e-04 | scale_pen(Q)= 7.1883e-05 | grad_norm=0.55 | sec/step~1.05 | rms_L~0.6726 rms_Q~0.6121
  step 210/685 | loss_L=1.3386 | loss_Q=1.3206 | scale_pen(L)= 5.6279e-04 | scale_pen(Q)= 7.0999e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.6730 rms_Q~0.6122
  step 220/685 | loss_L=1.2672 | loss_Q=1.2709 | scale_pen(L)= 5.7127e-04 | scale_pen(Q)= 7.1188e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.6733 rms_Q~0.6123
  step 230/685 | loss_L=1.3733 | loss_Q=1.3558 | scale_pen(L)= 5.7929e-04 | scale_pen(Q)= 6.9918e-05 | grad_norm=0.58 | sec/step~1.12 | rms_L~0.6737 rms_Q~0.6124
  step 240/685 | loss_L=1.4482 | loss_Q=1.4435 | scale_pen(L)= 5.8617e-04 | scale_pen(Q)= 6.8729e-05 | grad_norm=0.57 | sec/step~1.05 | rms_L~0.6740 rms_Q~0.6125
  step 250/685 | loss_L=1.1808 | loss_Q=1.1701 | scale_pen(L)= 5.9196e-04 | scale_pen(Q)= 6.8305e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.6744 rms_Q~0.6126
  step 260/685 | loss_L=1.2040 | loss_Q=1.1901 | scale_pen(L)= 5.9830e-04 | scale_pen(Q)= 6.8194e-05 | grad_norm=0.55 | sec/step~1.06 | rms_L~0.6747 rms_Q~0.6127
  step 270/685 | loss_L=1.2115 | loss_Q=1.2054 | scale_pen(L)= 6.0547e-04 | scale_pen(Q)= 6.8644e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.6751 rms_Q~0.6128
  step 280/685 | loss_L=1.1187 | loss_Q=1.1016 | scale_pen(L)= 6.1245e-04 | scale_pen(Q)= 6.9707e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.6754 rms_Q~0.6129
  step 290/685 | loss_L=1.2366 | loss_Q=1.2087 | scale_pen(L)= 6.1582e-04 | scale_pen(Q)= 6.8996e-05 | grad_norm=0.52 | sec/step~1.06 | rms_L~0.6758 rms_Q~0.6130
  step 300/685 | loss_L=1.3864 | loss_Q=1.3762 | scale_pen(L)= 6.2231e-04 | scale_pen(Q)= 6.7284e-05 | grad_norm=0.57 | sec/step~1.09 | rms_L~0.6761 rms_Q~0.6131
  step 310/685 | loss_L=1.2774 | loss_Q=1.2854 | scale_pen(L)= 6.2930e-04 | scale_pen(Q)= 6.8794e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.6765 rms_Q~0.6132
  step 320/685 | loss_L=1.3064 | loss_Q=1.3224 | scale_pen(L)= 6.3727e-04 | scale_pen(Q)= 6.7709e-05 | grad_norm=0.54 | sec/step~1.10 | rms_L~0.6768 rms_Q~0.6133
  step 330/685 | loss_L=1.2393 | loss_Q=1.2188 | scale_pen(L)= 6.4410e-04 | scale_pen(Q)= 6.7644e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.6771 rms_Q~0.6134
  step 340/685 | loss_L=1.4211 | loss_Q=1.4322 | scale_pen(L)= 6.5332e-04 | scale_pen(Q)= 6.7627e-05 | grad_norm=0.62 | sec/step~1.09 | rms_L~0.6775 rms_Q~0.6135
  step 350/685 | loss_L=1.1157 | loss_Q=1.0933 | scale_pen(L)= 6.6597e-04 | scale_pen(Q)= 6.8307e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.6778 rms_Q~0.6136
  step 360/685 | loss_L=1.2244 | loss_Q=1.2157 | scale_pen(L)= 6.7323e-04 | scale_pen(Q)= 6.7789e-05 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.6782 rms_Q~0.6137
  step 370/685 | loss_L=1.2958 | loss_Q=1.3183 | scale_pen(L)= 6.7904e-04 | scale_pen(Q)= 6.8660e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.6785 rms_Q~0.6138
  step 380/685 | loss_L=1.2697 | loss_Q=1.2605 | scale_pen(L)= 6.8417e-04 | scale_pen(Q)= 6.8822e-05 | grad_norm=0.57 | sec/step~1.07 | rms_L~0.6789 rms_Q~0.6139
  step 390/685 | loss_L=1.2588 | loss_Q=1.2486 | scale_pen(L)= 6.9129e-04 | scale_pen(Q)= 6.7495e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.6792 rms_Q~0.6140
  step 400/685 | loss_L=1.2446 | loss_Q=1.2471 | scale_pen(L)= 6.9669e-04 | scale_pen(Q)= 6.6255e-05 | grad_norm=0.57 | sec/step~1.10 | rms_L~0.6795 rms_Q~0.6141
  step 410/685 | loss_L=1.2370 | loss_Q=1.2590 | scale_pen(L)= 6.9980e-04 | scale_pen(Q)= 6.6895e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.6799 rms_Q~0.6142
  step 420/685 | loss_L=1.1460 | loss_Q=1.1592 | scale_pen(L)= 7.0239e-04 | scale_pen(Q)= 6.7139e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.6802 rms_Q~0.6143
  step 430/685 | loss_L=1.0880 | loss_Q=1.0731 | scale_pen(L)= 7.0657e-04 | scale_pen(Q)= 6.6872e-05 | grad_norm=0.57 | sec/step~1.12 | rms_L~0.6805 rms_Q~0.6144
  step 440/685 | loss_L=1.2797 | loss_Q=1.2994 | scale_pen(L)= 7.1089e-04 | scale_pen(Q)= 6.4579e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.6809 rms_Q~0.6145
  step 450/685 | loss_L=1.1636 | loss_Q=1.1588 | scale_pen(L)= 7.1953e-04 | scale_pen(Q)= 6.4459e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.6812 rms_Q~0.6145
  step 460/685 | loss_L=1.2698 | loss_Q=1.3039 | scale_pen(L)= 7.2682e-04 | scale_pen(Q)= 6.5342e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.6815 rms_Q~0.6146
  step 470/685 | loss_L=1.2885 | loss_Q=1.3055 | scale_pen(L)= 7.3429e-04 | scale_pen(Q)= 6.4008e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.6818 rms_Q~0.6147
  step 480/685 | loss_L=1.3560 | loss_Q=1.3384 | scale_pen(L)= 7.3880e-04 | scale_pen(Q)= 6.4356e-05 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.6821 rms_Q~0.6148
  step 490/685 | loss_L=1.1823 | loss_Q=1.1726 | scale_pen(L)= 7.4476e-04 | scale_pen(Q)= 6.5260e-05 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.6824 rms_Q~0.6149
  step 500/685 | loss_L=1.2561 | loss_Q=1.2523 | scale_pen(L)= 7.5546e-04 | scale_pen(Q)= 6.5537e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.6827 rms_Q~0.6150
  step 510/685 | loss_L=1.2870 | loss_Q=1.2739 | scale_pen(L)= 7.5768e-04 | scale_pen(Q)= 6.4633e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.6830 rms_Q~0.6151
  step 520/685 | loss_L=1.2104 | loss_Q=1.2314 | scale_pen(L)= 7.6347e-04 | scale_pen(Q)= 6.4996e-05 | grad_norm=0.53 | sec/step~1.11 | rms_L~0.6833 rms_Q~0.6151
  step 530/685 | loss_L=1.1866 | loss_Q=1.2017 | scale_pen(L)= 7.6866e-04 | scale_pen(Q)= 6.2969e-05 | grad_norm=0.53 | sec/step~1.05 | rms_L~0.6836 rms_Q~0.6152
  step 540/685 | loss_L=1.2772 | loss_Q=1.2858 | scale_pen(L)= 7.7127e-04 | scale_pen(Q)= 6.2865e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.6839 rms_Q~0.6153
  step 550/685 | loss_L=1.1884 | loss_Q=1.1997 | scale_pen(L)= 7.7144e-04 | scale_pen(Q)= 6.2729e-05 | grad_norm=0.57 | sec/step~1.12 | rms_L~0.6842 rms_Q~0.6154
  step 560/685 | loss_L=1.2256 | loss_Q=1.2359 | scale_pen(L)= 7.7790e-04 | scale_pen(Q)= 6.2697e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.6845 rms_Q~0.6155
  step 570/685 | loss_L=1.2401 | loss_Q=1.2341 | scale_pen(L)= 7.8298e-04 | scale_pen(Q)= 6.1233e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.6848 rms_Q~0.6156
  step 580/685 | loss_L=1.1868 | loss_Q=1.2115 | scale_pen(L)= 7.8892e-04 | scale_pen(Q)= 6.1097e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.6851 rms_Q~0.6156
  step 590/685 | loss_L=1.1219 | loss_Q=1.1111 | scale_pen(L)= 7.9828e-04 | scale_pen(Q)= 6.1171e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.6854 rms_Q~0.6157
  step 600/685 | loss_L=1.0594 | loss_Q=1.0508 | scale_pen(L)= 8.0549e-04 | scale_pen(Q)= 6.1132e-05 | grad_norm=0.61 | sec/step~1.18 | rms_L~0.6857 rms_Q~0.6158
  step 610/685 | loss_L=1.2077 | loss_Q=1.2016 | scale_pen(L)= 8.1125e-04 | scale_pen(Q)= 6.1728e-05 | grad_norm=0.55 | sec/step~1.06 | rms_L~0.6860 rms_Q~0.6159
  step 620/685 | loss_L=1.3691 | loss_Q=1.3794 | scale_pen(L)= 8.1715e-04 | scale_pen(Q)= 6.2948e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.6863 rms_Q~0.6160
  step 630/685 | loss_L=1.1735 | loss_Q=1.1614 | scale_pen(L)= 8.1837e-04 | scale_pen(Q)= 6.2259e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.6866 rms_Q~0.6161
  step 640/685 | loss_L=1.3251 | loss_Q=1.3444 | scale_pen(L)= 8.2120e-04 | scale_pen(Q)= 6.2514e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.6868 rms_Q~0.6162
  step 650/685 | loss_L=1.2451 | loss_Q=1.2266 | scale_pen(L)= 8.2940e-04 | scale_pen(Q)= 6.1317e-05 | grad_norm=0.57 | sec/step~1.14 | rms_L~0.6871 rms_Q~0.6162
  step 660/685 | loss_L=1.4094 | loss_Q=1.4194 | scale_pen(L)= 8.3694e-04 | scale_pen(Q)= 6.1030e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.6874 rms_Q~0.6163
  step 670/685 | loss_L=1.1030 | loss_Q=1.1163 | scale_pen(L)= 8.4298e-04 | scale_pen(Q)= 6.1429e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.6877 rms_Q~0.6164
  step 680/685 | loss_L=1.1836 | loss_Q=1.1760 | scale_pen(L)= 8.5077e-04 | scale_pen(Q)= 6.0684e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.6880 rms_Q~0.6165
  step 685/685 | loss_L=1.1282 | loss_Q=1.1408 | scale_pen(L)= 8.5575e-04 | scale_pen(Q)= 6.0672e-05 | grad_norm=0.63 | sec/step~0.43 | rms_L~0.6881 rms_Q~0.6165
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.6881121497954765, 'count': 685}, 'qwen': {'rms_mean': 0.6165356554254128, 'count': 685}}

Evaluating epoch 2 checkpoint...
Evaluating: runs/8B_clean_answer/epoch2 -> runs/8B_clean_answer/eval_epoch2
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch2/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch2/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3019.11it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 9.397949814526132, 'neutral_chat': 9.392868074430089, 'llama_chat': 9.492557533744241} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch2/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.70608 -> target=0.01057
[debug:llama] adapter.scale=1.0293 | Z.std=1.0019 Z.mean||=16.0310 | prefix.std=0.0106 prefix.mean||=0.6759 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '1. the of the of the  the of the of'
  1: '1. and of, and, and, and'
  2: '1. 2. 3. 4'
  3: '2, 3, 4, 5'
  4: '1-2-3-4-5-6'
Saved Llama results to runs/8B_clean_answer/eval_epoch2/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3689.73it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.15it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 9.193598835556596, 'neutral_chat': 9.124025371024217, 'qwen_chat': 9.041261359181984} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch2/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.62142 -> target=0.01363
[debug:qwen] adapter.scale=1.0078 | Z.std=1.0019 Z.mean||=16.0302 | prefix.std=0.0136 prefix.mean||=0.8157 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'of  of  of  of  of  of'
  1: ''
  2: '1990s1990s'
  3: 'to the of the of the of the of the of the'
  4: 'of  of  of  of  of  of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch2/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3093.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.42it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5976.92it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.34it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.18s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.019  |  NLL/token (gold): 9.390165537663329
Qwen   EM: 0.000   F1: 0.019  |  NLL/token (gold): 9.033910992915038
Wall clock: 12.83s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.46s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.019
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.026

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.184317111968994,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.825080156326294,
    "llama": {
      "em": 0.0,
      "f1": 0.018571248196248197,
      "nll_token": 9.390165537663329
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.019156453024100083,
      "nll_token": 9.033910992915038
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.463441133499146,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.019295341912988973,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0292532444000244,
      "Z_std": 1.0019396543502808,
      "Z_mean_norm": 16.03101921081543,
      "prefix_std": 0.010572744533419609,
      "prefix_mean_norm": 0.6759084463119507,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0077892541885376,
      "Z_std": 1.0018854141235352,
      "Z_mean_norm": 16.030153274536133,
      "prefix_std": 0.013631699606776237,
      "prefix_mean_norm": 0.8157073855400085,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.026260063465945813
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch2/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch2/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.019 | Qwen 0.019


=========================================
EPOCH 3/16
=========================================

Training epoch 3...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6814.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3594.86it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.28it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=2, global_step=1370
Epoch 3/1
  step 10/685 | loss_L=1.3162 | loss_Q=1.3038 | scale_pen(L)= 8.5980e-04 | scale_pen(Q)= 5.9799e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7077 rms_Q~0.6224
  step 20/685 | loss_L=1.2245 | loss_Q=1.2173 | scale_pen(L)= 8.6530e-04 | scale_pen(Q)= 6.0879e-05 | grad_norm=0.61 | sec/step~1.23 | rms_L~0.7080 rms_Q~0.6225
  step 30/685 | loss_L=1.2176 | loss_Q=1.2124 | scale_pen(L)= 8.7008e-04 | scale_pen(Q)= 6.0777e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7083 rms_Q~0.6226
  step 40/685 | loss_L=1.2036 | loss_Q=1.2339 | scale_pen(L)= 8.7419e-04 | scale_pen(Q)= 6.1363e-05 | grad_norm=0.60 | sec/step~1.12 | rms_L~0.7086 rms_Q~0.6227
  step 50/685 | loss_L=1.3415 | loss_Q=1.3546 | scale_pen(L)= 8.7679e-04 | scale_pen(Q)= 6.1313e-05 | grad_norm=0.56 | sec/step~1.05 | rms_L~0.7088 rms_Q~0.6227
  step 60/685 | loss_L=1.2665 | loss_Q=1.2631 | scale_pen(L)= 8.7971e-04 | scale_pen(Q)= 6.1311e-05 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.7090 rms_Q~0.6228
  step 70/685 | loss_L=1.2165 | loss_Q=1.2034 | scale_pen(L)= 8.8397e-04 | scale_pen(Q)= 6.3168e-05 | grad_norm=0.53 | sec/step~1.21 | rms_L~0.7092 rms_Q~0.6229
  step 80/685 | loss_L=1.1884 | loss_Q=1.1811 | scale_pen(L)= 8.9143e-04 | scale_pen(Q)= 6.2372e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7094 rms_Q~0.6230
  step 90/685 | loss_L=1.3308 | loss_Q=1.3419 | scale_pen(L)= 8.9573e-04 | scale_pen(Q)= 6.1317e-05 | grad_norm=0.56 | sec/step~1.08 | rms_L~0.7095 rms_Q~0.6230
  step 100/685 | loss_L=1.2208 | loss_Q=1.2038 | scale_pen(L)= 8.9914e-04 | scale_pen(Q)= 6.1216e-05 | grad_norm=0.55 | sec/step~1.09 | rms_L~0.7097 rms_Q~0.6231
  step 110/685 | loss_L=1.2547 | loss_Q=1.2617 | scale_pen(L)= 9.0676e-04 | scale_pen(Q)= 6.2759e-05 | grad_norm=0.58 | sec/step~1.10 | rms_L~0.7099 rms_Q~0.6232
  step 120/685 | loss_L=1.3435 | loss_Q=1.3637 | scale_pen(L)= 9.0557e-04 | scale_pen(Q)= 6.2152e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7101 rms_Q~0.6232
  step 130/685 | loss_L=1.1797 | loss_Q=1.1823 | scale_pen(L)= 9.1280e-04 | scale_pen(Q)= 6.2090e-05 | grad_norm=0.52 | sec/step~1.10 | rms_L~0.7102 rms_Q~0.6233
  step 140/685 | loss_L=1.2776 | loss_Q=1.2876 | scale_pen(L)= 9.1424e-04 | scale_pen(Q)= 6.0961e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7104 rms_Q~0.6233
  step 150/685 | loss_L=1.1057 | loss_Q=1.1157 | scale_pen(L)= 9.1946e-04 | scale_pen(Q)= 6.0080e-05 | grad_norm=0.57 | sec/step~1.13 | rms_L~0.7106 rms_Q~0.6234
  step 160/685 | loss_L=1.1378 | loss_Q=1.1504 | scale_pen(L)= 9.2629e-04 | scale_pen(Q)= 5.9884e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7107 rms_Q~0.6234
  step 170/685 | loss_L=1.1595 | loss_Q=1.1590 | scale_pen(L)= 9.3080e-04 | scale_pen(Q)= 6.1438e-05 | grad_norm=0.56 | sec/step~1.09 | rms_L~0.7109 rms_Q~0.6235
  step 180/685 | loss_L=1.2517 | loss_Q=1.2753 | scale_pen(L)= 9.3768e-04 | scale_pen(Q)= 6.2367e-05 | grad_norm=0.61 | sec/step~1.22 | rms_L~0.7110 rms_Q~0.6235
  step 190/685 | loss_L=1.2153 | loss_Q=1.2168 | scale_pen(L)= 9.4089e-04 | scale_pen(Q)= 6.3481e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.7112 rms_Q~0.6236
  step 200/685 | loss_L=1.2397 | loss_Q=1.2414 | scale_pen(L)= 9.4362e-04 | scale_pen(Q)= 6.4794e-05 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.7114 rms_Q~0.6236
  step 210/685 | loss_L=1.1586 | loss_Q=1.1555 | scale_pen(L)= 9.4567e-04 | scale_pen(Q)= 6.4679e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7116 rms_Q~0.6237
  step 220/685 | loss_L=1.1126 | loss_Q=1.1188 | scale_pen(L)= 9.5201e-04 | scale_pen(Q)= 6.3951e-05 | grad_norm=0.61 | sec/step~1.16 | rms_L~0.7117 rms_Q~0.6237
  step 230/685 | loss_L=1.2232 | loss_Q=1.2384 | scale_pen(L)= 9.5505e-04 | scale_pen(Q)= 6.4069e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7119 rms_Q~0.6238
  step 240/685 | loss_L=1.1607 | loss_Q=1.1610 | scale_pen(L)= 9.5879e-04 | scale_pen(Q)= 6.1158e-05 | grad_norm=0.55 | sec/step~1.13 | rms_L~0.7121 rms_Q~0.6238
  step 250/685 | loss_L=1.2127 | loss_Q=1.1902 | scale_pen(L)= 9.6287e-04 | scale_pen(Q)= 6.0000e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7123 rms_Q~0.6239
  step 260/685 | loss_L=1.1837 | loss_Q=1.2060 | scale_pen(L)= 9.7043e-04 | scale_pen(Q)= 5.9960e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7124 rms_Q~0.6239
  step 270/685 | loss_L=1.2308 | loss_Q=1.2298 | scale_pen(L)= 9.7047e-04 | scale_pen(Q)= 5.9878e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7126 rms_Q~0.6240
  step 280/685 | loss_L=1.1720 | loss_Q=1.1719 | scale_pen(L)= 9.7577e-04 | scale_pen(Q)= 6.0303e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7128 rms_Q~0.6240
  step 290/685 | loss_L=1.1642 | loss_Q=1.1634 | scale_pen(L)= 9.8113e-04 | scale_pen(Q)= 6.0645e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7130 rms_Q~0.6241
  step 300/685 | loss_L=1.2462 | loss_Q=1.2597 | scale_pen(L)= 9.8534e-04 | scale_pen(Q)= 6.1582e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7132 rms_Q~0.6241
  step 310/685 | loss_L=1.3743 | loss_Q=1.4016 | scale_pen(L)= 9.9226e-04 | scale_pen(Q)= 6.1934e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.7133 rms_Q~0.6242
  step 320/685 | loss_L=1.2297 | loss_Q=1.2151 | scale_pen(L)= 9.9546e-04 | scale_pen(Q)= 6.0205e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7135 rms_Q~0.6242
  step 330/685 | loss_L=1.1874 | loss_Q=1.1867 | scale_pen(L)= 9.9982e-04 | scale_pen(Q)= 5.9753e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7137 rms_Q~0.6243
  step 340/685 | loss_L=1.3620 | loss_Q=1.3843 | scale_pen(L)= 1.0005e-03 | scale_pen(Q)= 5.9808e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7139 rms_Q~0.6243
  step 350/685 | loss_L=1.1447 | loss_Q=1.1580 | scale_pen(L)= 1.0059e-03 | scale_pen(Q)= 5.9407e-05 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.7141 rms_Q~0.6244
  step 360/685 | loss_L=1.3735 | loss_Q=1.3952 | scale_pen(L)= 1.0123e-03 | scale_pen(Q)= 5.7669e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7143 rms_Q~0.6245
  step 370/685 | loss_L=1.2036 | loss_Q=1.2133 | scale_pen(L)= 1.0137e-03 | scale_pen(Q)= 5.9110e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7144 rms_Q~0.6245
  step 380/685 | loss_L=1.0137 | loss_Q=1.0029 | scale_pen(L)= 1.0165e-03 | scale_pen(Q)= 5.8831e-05 | grad_norm=0.54 | sec/step~1.10 | rms_L~0.7146 rms_Q~0.6246
  step 390/685 | loss_L=1.2191 | loss_Q=1.2307 | scale_pen(L)= 1.0208e-03 | scale_pen(Q)= 5.8672e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7148 rms_Q~0.6247
  step 400/685 | loss_L=1.1917 | loss_Q=1.1994 | scale_pen(L)= 1.0233e-03 | scale_pen(Q)= 5.8694e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7150 rms_Q~0.6247
  step 410/685 | loss_L=1.1347 | loss_Q=1.1359 | scale_pen(L)= 1.0291e-03 | scale_pen(Q)= 5.9565e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7151 rms_Q~0.6248
  step 420/685 | loss_L=1.2975 | loss_Q=1.3104 | scale_pen(L)= 1.0307e-03 | scale_pen(Q)= 6.0148e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7153 rms_Q~0.6248
  step 430/685 | loss_L=1.2471 | loss_Q=1.2605 | scale_pen(L)= 1.0360e-03 | scale_pen(Q)= 6.0333e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7155 rms_Q~0.6249
  step 440/685 | loss_L=1.3042 | loss_Q=1.3208 | scale_pen(L)= 1.0398e-03 | scale_pen(Q)= 6.1760e-05 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.7157 rms_Q~0.6249
  step 450/685 | loss_L=1.2343 | loss_Q=1.2430 | scale_pen(L)= 1.0483e-03 | scale_pen(Q)= 6.0398e-05 | grad_norm=0.58 | sec/step~1.15 | rms_L~0.7158 rms_Q~0.6250
  step 460/685 | loss_L=1.2507 | loss_Q=1.2396 | scale_pen(L)= 1.0525e-03 | scale_pen(Q)= 5.9458e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.7160 rms_Q~0.6251
  step 470/685 | loss_L=1.1144 | loss_Q=1.1320 | scale_pen(L)= 1.0563e-03 | scale_pen(Q)= 5.8857e-05 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.7162 rms_Q~0.6251
  step 480/685 | loss_L=1.2379 | loss_Q=1.2547 | scale_pen(L)= 1.0599e-03 | scale_pen(Q)= 5.8104e-05 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.7164 rms_Q~0.6252
  step 490/685 | loss_L=1.1600 | loss_Q=1.1567 | scale_pen(L)= 1.0685e-03 | scale_pen(Q)= 5.8215e-05 | grad_norm=0.58 | sec/step~1.09 | rms_L~0.7165 rms_Q~0.6252
  step 500/685 | loss_L=1.2564 | loss_Q=1.2400 | scale_pen(L)= 1.0701e-03 | scale_pen(Q)= 5.5978e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7167 rms_Q~0.6253
  step 510/685 | loss_L=1.1495 | loss_Q=1.1581 | scale_pen(L)= 1.0715e-03 | scale_pen(Q)= 5.6342e-05 | grad_norm=0.58 | sec/step~1.14 | rms_L~0.7169 rms_Q~0.6253
  step 520/685 | loss_L=1.1563 | loss_Q=1.1708 | scale_pen(L)= 1.0757e-03 | scale_pen(Q)= 5.6618e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.7170 rms_Q~0.6254
  step 530/685 | loss_L=1.2147 | loss_Q=1.2072 | scale_pen(L)= 1.0782e-03 | scale_pen(Q)= 5.7471e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7172 rms_Q~0.6254
  step 540/685 | loss_L=1.2200 | loss_Q=1.2282 | scale_pen(L)= 1.0850e-03 | scale_pen(Q)= 5.9853e-05 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.7174 rms_Q~0.6255
  step 550/685 | loss_L=1.2829 | loss_Q=1.2944 | scale_pen(L)= 1.0873e-03 | scale_pen(Q)= 5.8444e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7176 rms_Q~0.6255
  step 560/685 | loss_L=1.2419 | loss_Q=1.2707 | scale_pen(L)= 1.0950e-03 | scale_pen(Q)= 5.9035e-05 | grad_norm=0.61 | sec/step~1.14 | rms_L~0.7177 rms_Q~0.6256
  step 570/685 | loss_L=1.1247 | loss_Q=1.1069 | scale_pen(L)= 1.0986e-03 | scale_pen(Q)= 5.9187e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.7179 rms_Q~0.6256
  step 580/685 | loss_L=1.2194 | loss_Q=1.2285 | scale_pen(L)= 1.1085e-03 | scale_pen(Q)= 5.8619e-05 | grad_norm=0.55 | sec/step~1.06 | rms_L~0.7181 rms_Q~0.6257
  step 590/685 | loss_L=1.1828 | loss_Q=1.1961 | scale_pen(L)= 1.1122e-03 | scale_pen(Q)= 5.7986e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.7183 rms_Q~0.6257
  step 600/685 | loss_L=1.1720 | loss_Q=1.1585 | scale_pen(L)= 1.1197e-03 | scale_pen(Q)= 5.7806e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7184 rms_Q~0.6258
  step 610/685 | loss_L=1.0799 | loss_Q=1.0855 | scale_pen(L)= 1.1219e-03 | scale_pen(Q)= 5.8077e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7186 rms_Q~0.6258
  step 620/685 | loss_L=1.2441 | loss_Q=1.2596 | scale_pen(L)= 1.1311e-03 | scale_pen(Q)= 5.9368e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.7188 rms_Q~0.6259
  step 630/685 | loss_L=1.2130 | loss_Q=1.2258 | scale_pen(L)= 1.1342e-03 | scale_pen(Q)= 5.9453e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7190 rms_Q~0.6259
  step 640/685 | loss_L=1.2264 | loss_Q=1.2466 | scale_pen(L)= 1.1369e-03 | scale_pen(Q)= 6.0491e-05 | grad_norm=0.58 | sec/step~1.05 | rms_L~0.7191 rms_Q~0.6260
  step 650/685 | loss_L=1.2599 | loss_Q=1.2799 | scale_pen(L)= 1.1392e-03 | scale_pen(Q)= 5.9913e-05 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.7193 rms_Q~0.6260
  step 660/685 | loss_L=1.2514 | loss_Q=1.2531 | scale_pen(L)= 1.1455e-03 | scale_pen(Q)= 5.9359e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7195 rms_Q~0.6261
  step 670/685 | loss_L=1.3029 | loss_Q=1.3091 | scale_pen(L)= 1.1500e-03 | scale_pen(Q)= 6.0072e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.7197 rms_Q~0.6261
  step 680/685 | loss_L=1.0896 | loss_Q=1.0811 | scale_pen(L)= 1.1543e-03 | scale_pen(Q)= 6.0322e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7198 rms_Q~0.6262
  step 685/685 | loss_L=1.2553 | loss_Q=1.2740 | scale_pen(L)= 1.1563e-03 | scale_pen(Q)= 6.0608e-05 | grad_norm=0.62 | sec/step~0.43 | rms_L~0.7199 rms_Q~0.6262
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.7199383024751705, 'count': 685}, 'qwen': {'rms_mean': 0.626219842294707, 'count': 685}}

Evaluating epoch 3 checkpoint...
Evaluating: runs/8B_clean_answer/epoch3 -> runs/8B_clean_answer/eval_epoch3
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch3/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch3/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2965.22it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 9.223453883411121, 'neutral_chat': 9.220614157025777, 'llama_chat': 9.319409928354276} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch3/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.73043 -> target=0.01057
[debug:llama] adapter.scale=1.0340 | Z.std=1.0019 Z.mean||=16.0306 | prefix.std=0.0106 prefix.mean||=0.6758 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the of the the of the of the the of'
  1: '1. insects,, and,, and'
  2: 'the of the of the the of the of the of the'
  3: 'the of the of the of the of the of the of'
  4: '2-4 bonds per molecule'
Saved Llama results to runs/8B_clean_answer/eval_epoch3/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2961.03it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 9.033615114512267, 'neutral_chat': 9.009099378156915, 'qwen_chat': 8.90768673432567} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch3/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.62905 -> target=0.01363
[debug:qwen] adapter.scale=1.0078 | Z.std=1.0019 Z.mean||=16.0297 | prefix.std=0.0136 prefix.mean||=0.8156 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'of  of  of  of  of  of'
  1: 'of the and and and and and and and and and and'
  2: '1990s the of and in the of and'
  3: 'to the of the of the of the of the of the'
  4: 'of  of  of  of  of  of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch3/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3061.54it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.32s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6652.35it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.17s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.032  |  NLL/token (gold): 9.227866110077251
Qwen   EM: 0.000   F1: 0.018  |  NLL/token (gold): 8.893579340801038
Wall clock: 12.83s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.67s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.021
Inter-model agreement (normalized): 0.010
Oracle upper bound:  EM 0.000  F1 0.038

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.166240215301514,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.825681447982788,
    "llama": {
      "em": 0.0,
      "f1": 0.032080086580086584,
      "nll_token": 9.227866110077251
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.017591197691197692,
      "nll_token": 8.893579340801038
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.670462131500244,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0207502886002886,
    "agreement": 0.01,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0340040922164917,
      "Z_std": 1.0019116401672363,
      "Z_mean_norm": 16.03057289123535,
      "prefix_std": 0.010572756640613079,
      "prefix_mean_norm": 0.675762414932251,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0077850818634033,
      "Z_std": 1.0018573999404907,
      "Z_mean_norm": 16.029705047607422,
      "prefix_std": 0.01363172847777605,
      "prefix_mean_norm": 0.8156195878982544,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03777290764790765
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch3/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch3/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.032 | Qwen 0.018


=========================================
EPOCH 4/16
=========================================

Training epoch 4...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7958.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.02s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4147.64it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.47it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=3, global_step=2055
Epoch 4/1
  step 10/685 | loss_L=1.3076 | loss_Q=1.3102 | scale_pen(L)= 1.1575e-03 | scale_pen(Q)= 6.0788e-05 | grad_norm=0.55 | sec/step~1.14 | rms_L~0.7316 rms_Q~0.6301
  step 20/685 | loss_L=1.3652 | loss_Q=1.3974 | scale_pen(L)= 1.1632e-03 | scale_pen(Q)= 6.0541e-05 | grad_norm=0.57 | sec/step~1.07 | rms_L~0.7319 rms_Q~0.6302
  step 30/685 | loss_L=1.1850 | loss_Q=1.1920 | scale_pen(L)= 1.1642e-03 | scale_pen(Q)= 6.0194e-05 | grad_norm=0.58 | sec/step~1.10 | rms_L~0.7321 rms_Q~0.6303
  step 40/685 | loss_L=1.1667 | loss_Q=1.1718 | scale_pen(L)= 1.1715e-03 | scale_pen(Q)= 6.0054e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7323 rms_Q~0.6304
  step 50/685 | loss_L=1.1061 | loss_Q=1.1102 | scale_pen(L)= 1.1701e-03 | scale_pen(Q)= 5.9963e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7325 rms_Q~0.6305
  step 60/685 | loss_L=1.2173 | loss_Q=1.2232 | scale_pen(L)= 1.1750e-03 | scale_pen(Q)= 6.0098e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7326 rms_Q~0.6305
  step 70/685 | loss_L=1.2408 | loss_Q=1.2393 | scale_pen(L)= 1.1775e-03 | scale_pen(Q)= 6.0355e-05 | grad_norm=0.56 | sec/step~1.05 | rms_L~0.7328 rms_Q~0.6306
  step 80/685 | loss_L=1.3197 | loss_Q=1.3344 | scale_pen(L)= 1.1819e-03 | scale_pen(Q)= 6.0159e-05 | grad_norm=0.61 | sec/step~1.16 | rms_L~0.7329 rms_Q~0.6307
  step 90/685 | loss_L=1.3483 | loss_Q=1.3633 | scale_pen(L)= 1.1861e-03 | scale_pen(Q)= 6.0307e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7330 rms_Q~0.6307
  step 100/685 | loss_L=1.1778 | loss_Q=1.1938 | scale_pen(L)= 1.1832e-03 | scale_pen(Q)= 6.1401e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7331 rms_Q~0.6308
  step 110/685 | loss_L=1.1927 | loss_Q=1.2306 | scale_pen(L)= 1.1833e-03 | scale_pen(Q)= 6.0072e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7332 rms_Q~0.6308
  step 120/685 | loss_L=1.1433 | loss_Q=1.1635 | scale_pen(L)= 1.1891e-03 | scale_pen(Q)= 5.9060e-05 | grad_norm=0.58 | sec/step~1.12 | rms_L~0.7333 rms_Q~0.6308
  step 130/685 | loss_L=1.1960 | loss_Q=1.1850 | scale_pen(L)= 1.1920e-03 | scale_pen(Q)= 5.9451e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.7335 rms_Q~0.6309
  step 140/685 | loss_L=1.2188 | loss_Q=1.2135 | scale_pen(L)= 1.1974e-03 | scale_pen(Q)= 6.0715e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7335 rms_Q~0.6309
  step 150/685 | loss_L=1.1759 | loss_Q=1.1764 | scale_pen(L)= 1.1989e-03 | scale_pen(Q)= 6.1689e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7336 rms_Q~0.6309
  step 160/685 | loss_L=1.1170 | loss_Q=1.0958 | scale_pen(L)= 1.2071e-03 | scale_pen(Q)= 6.1134e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7338 rms_Q~0.6310
  step 170/685 | loss_L=1.1834 | loss_Q=1.1797 | scale_pen(L)= 1.2112e-03 | scale_pen(Q)= 5.9602e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7339 rms_Q~0.6310
  step 180/685 | loss_L=1.1988 | loss_Q=1.2000 | scale_pen(L)= 1.2159e-03 | scale_pen(Q)= 5.9337e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7341 rms_Q~0.6311
  step 190/685 | loss_L=1.3003 | loss_Q=1.2915 | scale_pen(L)= 1.2196e-03 | scale_pen(Q)= 5.9009e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7342 rms_Q~0.6311
  step 200/685 | loss_L=1.3052 | loss_Q=1.2982 | scale_pen(L)= 1.2238e-03 | scale_pen(Q)= 5.9847e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7343 rms_Q~0.6312
  step 210/685 | loss_L=1.2380 | loss_Q=1.2394 | scale_pen(L)= 1.2233e-03 | scale_pen(Q)= 6.0468e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7344 rms_Q~0.6312
  step 220/685 | loss_L=1.2745 | loss_Q=1.2847 | scale_pen(L)= 1.2251e-03 | scale_pen(Q)= 6.0843e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7346 rms_Q~0.6312
  step 230/685 | loss_L=1.3375 | loss_Q=1.3462 | scale_pen(L)= 1.2257e-03 | scale_pen(Q)= 6.2227e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.7347 rms_Q~0.6313
  step 240/685 | loss_L=1.1650 | loss_Q=1.1550 | scale_pen(L)= 1.2350e-03 | scale_pen(Q)= 6.2171e-05 | grad_norm=0.55 | sec/step~1.13 | rms_L~0.7349 rms_Q~0.6313
  step 250/685 | loss_L=1.1466 | loss_Q=1.1617 | scale_pen(L)= 1.2336e-03 | scale_pen(Q)= 6.0342e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.7350 rms_Q~0.6314
  step 260/685 | loss_L=1.2401 | loss_Q=1.2360 | scale_pen(L)= 1.2393e-03 | scale_pen(Q)= 6.1419e-05 | grad_norm=0.58 | sec/step~1.12 | rms_L~0.7351 rms_Q~0.6314
  step 270/685 | loss_L=1.2202 | loss_Q=1.2231 | scale_pen(L)= 1.2407e-03 | scale_pen(Q)= 6.1233e-05 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.7353 rms_Q~0.6315
  step 280/685 | loss_L=1.1251 | loss_Q=1.1075 | scale_pen(L)= 1.2440e-03 | scale_pen(Q)= 6.1123e-05 | grad_norm=0.57 | sec/step~1.10 | rms_L~0.7354 rms_Q~0.6315
  step 290/685 | loss_L=1.1114 | loss_Q=1.1143 | scale_pen(L)= 1.2450e-03 | scale_pen(Q)= 6.1741e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7356 rms_Q~0.6316
  step 300/685 | loss_L=1.1823 | loss_Q=1.1833 | scale_pen(L)= 1.2467e-03 | scale_pen(Q)= 6.3278e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.7357 rms_Q~0.6316
  step 310/685 | loss_L=1.1616 | loss_Q=1.1619 | scale_pen(L)= 1.2507e-03 | scale_pen(Q)= 6.1571e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7358 rms_Q~0.6317
  step 320/685 | loss_L=1.1545 | loss_Q=1.1560 | scale_pen(L)= 1.2490e-03 | scale_pen(Q)= 6.2088e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7360 rms_Q~0.6317
  step 330/685 | loss_L=1.1364 | loss_Q=1.1112 | scale_pen(L)= 1.2533e-03 | scale_pen(Q)= 6.2301e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7361 rms_Q~0.6318
  step 340/685 | loss_L=1.1672 | loss_Q=1.1632 | scale_pen(L)= 1.2644e-03 | scale_pen(Q)= 6.3073e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7362 rms_Q~0.6318
  step 350/685 | loss_L=1.3854 | loss_Q=1.3750 | scale_pen(L)= 1.2682e-03 | scale_pen(Q)= 6.3075e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.7364 rms_Q~0.6319
  step 360/685 | loss_L=1.2524 | loss_Q=1.2651 | scale_pen(L)= 1.2737e-03 | scale_pen(Q)= 6.2570e-05 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.7365 rms_Q~0.6319
  step 370/685 | loss_L=1.2643 | loss_Q=1.2895 | scale_pen(L)= 1.2780e-03 | scale_pen(Q)= 6.2389e-05 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.7367 rms_Q~0.6320
  step 380/685 | loss_L=1.1053 | loss_Q=1.0988 | scale_pen(L)= 1.2842e-03 | scale_pen(Q)= 6.4497e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7368 rms_Q~0.6321
  step 390/685 | loss_L=1.3541 | loss_Q=1.3689 | scale_pen(L)= 1.2834e-03 | scale_pen(Q)= 6.3169e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7370 rms_Q~0.6321
  step 400/685 | loss_L=1.1311 | loss_Q=1.1176 | scale_pen(L)= 1.2836e-03 | scale_pen(Q)= 6.2621e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7371 rms_Q~0.6322
  step 410/685 | loss_L=1.1753 | loss_Q=1.1882 | scale_pen(L)= 1.2880e-03 | scale_pen(Q)= 6.2797e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7372 rms_Q~0.6322
  step 420/685 | loss_L=1.1333 | loss_Q=1.1214 | scale_pen(L)= 1.2928e-03 | scale_pen(Q)= 6.2931e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7374 rms_Q~0.6323
  step 430/685 | loss_L=1.2327 | loss_Q=1.2226 | scale_pen(L)= 1.2973e-03 | scale_pen(Q)= 6.2619e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7375 rms_Q~0.6323
  step 440/685 | loss_L=1.1868 | loss_Q=1.2189 | scale_pen(L)= 1.3038e-03 | scale_pen(Q)= 6.2376e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.7377 rms_Q~0.6324
  step 450/685 | loss_L=1.0492 | loss_Q=1.0463 | scale_pen(L)= 1.3088e-03 | scale_pen(Q)= 6.4712e-05 | grad_norm=0.55 | sec/step~1.06 | rms_L~0.7378 rms_Q~0.6324
  step 460/685 | loss_L=1.3087 | loss_Q=1.3187 | scale_pen(L)= 1.3114e-03 | scale_pen(Q)= 6.4635e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.7380 rms_Q~0.6325
  step 470/685 | loss_L=1.2726 | loss_Q=1.2655 | scale_pen(L)= 1.3142e-03 | scale_pen(Q)= 6.3796e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7381 rms_Q~0.6325
  step 480/685 | loss_L=1.0757 | loss_Q=1.0850 | scale_pen(L)= 1.3210e-03 | scale_pen(Q)= 6.3760e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7383 rms_Q~0.6326
  step 490/685 | loss_L=1.1716 | loss_Q=1.1722 | scale_pen(L)= 1.3247e-03 | scale_pen(Q)= 6.3359e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7384 rms_Q~0.6327
  step 500/685 | loss_L=1.1903 | loss_Q=1.2213 | scale_pen(L)= 1.3312e-03 | scale_pen(Q)= 6.4317e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.7386 rms_Q~0.6327
  step 510/685 | loss_L=1.1803 | loss_Q=1.1833 | scale_pen(L)= 1.3340e-03 | scale_pen(Q)= 6.3899e-05 | grad_norm=0.56 | sec/step~1.11 | rms_L~0.7387 rms_Q~0.6328
  step 520/685 | loss_L=1.0390 | loss_Q=1.0351 | scale_pen(L)= 1.3336e-03 | scale_pen(Q)= 6.3350e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7389 rms_Q~0.6328
  step 530/685 | loss_L=1.1379 | loss_Q=1.1475 | scale_pen(L)= 1.3377e-03 | scale_pen(Q)= 6.3865e-05 | grad_norm=0.66 | sec/step~1.15 | rms_L~0.7390 rms_Q~0.6329
  step 540/685 | loss_L=1.0137 | loss_Q=0.9991 | scale_pen(L)= 1.3425e-03 | scale_pen(Q)= 6.4735e-05 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.7392 rms_Q~0.6329
  step 550/685 | loss_L=1.2256 | loss_Q=1.2429 | scale_pen(L)= 1.3470e-03 | scale_pen(Q)= 6.5288e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7393 rms_Q~0.6330
  step 560/685 | loss_L=1.1398 | loss_Q=1.1481 | scale_pen(L)= 1.3451e-03 | scale_pen(Q)= 6.5271e-05 | grad_norm=0.57 | sec/step~1.09 | rms_L~0.7394 rms_Q~0.6330
  step 570/685 | loss_L=1.1665 | loss_Q=1.1740 | scale_pen(L)= 1.3499e-03 | scale_pen(Q)= 6.5367e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7396 rms_Q~0.6331
  step 580/685 | loss_L=1.2667 | loss_Q=1.2808 | scale_pen(L)= 1.3547e-03 | scale_pen(Q)= 6.6383e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.7397 rms_Q~0.6331
  step 590/685 | loss_L=1.2469 | loss_Q=1.2465 | scale_pen(L)= 1.3568e-03 | scale_pen(Q)= 6.6125e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7399 rms_Q~0.6332
  step 600/685 | loss_L=1.1972 | loss_Q=1.2081 | scale_pen(L)= 1.3613e-03 | scale_pen(Q)= 6.6178e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7400 rms_Q~0.6332
  step 610/685 | loss_L=1.2006 | loss_Q=1.2227 | scale_pen(L)= 1.3639e-03 | scale_pen(Q)= 6.5468e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7402 rms_Q~0.6333
  step 620/685 | loss_L=1.1710 | loss_Q=1.1797 | scale_pen(L)= 1.3630e-03 | scale_pen(Q)= 6.6714e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.7403 rms_Q~0.6333
  step 630/685 | loss_L=1.1083 | loss_Q=1.1088 | scale_pen(L)= 1.3641e-03 | scale_pen(Q)= 6.5668e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7404 rms_Q~0.6334
  step 640/685 | loss_L=1.0979 | loss_Q=1.1159 | scale_pen(L)= 1.3710e-03 | scale_pen(Q)= 6.7215e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7406 rms_Q~0.6335
  step 650/685 | loss_L=1.1975 | loss_Q=1.2163 | scale_pen(L)= 1.3735e-03 | scale_pen(Q)= 6.5943e-05 | grad_norm=0.58 | sec/step~1.18 | rms_L~0.7407 rms_Q~0.6335
  step 660/685 | loss_L=1.2454 | loss_Q=1.2651 | scale_pen(L)= 1.3792e-03 | scale_pen(Q)= 6.5939e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7409 rms_Q~0.6336
  step 670/685 | loss_L=1.1547 | loss_Q=1.1752 | scale_pen(L)= 1.3845e-03 | scale_pen(Q)= 6.4627e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7410 rms_Q~0.6336
  step 680/685 | loss_L=1.1389 | loss_Q=1.1209 | scale_pen(L)= 1.3870e-03 | scale_pen(Q)= 6.4342e-05 | grad_norm=0.63 | sec/step~1.08 | rms_L~0.7411 rms_Q~0.6337
  step 685/685 | loss_L=1.5051 | loss_Q=1.4946 | scale_pen(L)= 1.3890e-03 | scale_pen(Q)= 6.3407e-05 | grad_norm=0.58 | sec/step~0.43 | rms_L~0.7412 rms_Q~0.6337
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.741214579648345, 'count': 685}, 'qwen': {'rms_mean': 0.6336935089452423, 'count': 685}}

Evaluating epoch 4 checkpoint...
Evaluating: runs/8B_clean_answer/epoch4 -> runs/8B_clean_answer/eval_epoch4
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch4/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch4/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3129.49it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 9.059059139281985, 'neutral_chat': 9.084058712399194, 'llama_chat': 9.152675224269599} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch4/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.73654 -> target=0.01057
[debug:llama] adapter.scale=1.0373 | Z.std=1.0014 Z.mean||=16.0227 | prefix.std=0.0106 prefix.mean||=0.6755 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the is the of the of the. the of'
  1: '2,000,000,000,000'
  2: 'the of the of the and the of the of the'
  3: 'the of the of the of the of the of the of'
  4: '3-4-5-6-7-8'
Saved Llama results to runs/8B_clean_answer/eval_epoch4/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3149.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.45it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.631740021957922, 'neutral_chat': 8.666031656441865, 'qwen_chat': 8.521564984132374} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch4/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.63600 -> target=0.01363
[debug:qwen] adapter.scale=1.0080 | Z.std=1.0017 Z.mean||=16.0276 | prefix.std=0.0136 prefix.mean||=0.8155 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '1950s, the of the and the of'
  1: 'of ands ands ands ands ands and'
  2: '1990s the of the in the of the'
  3: 'of the and and and and and and and and and and'
  4: 'of the and and and and and and and and and and'
Saved Qwen results to runs/8B_clean_answer/eval_epoch4/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3068.25it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6260.16it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.27it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.01s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.027  |  NLL/token (gold): 9.062140493976827
Qwen   EM: 0.000   F1: 0.025  |  NLL/token (gold): 8.512811786283262
Wall clock: 12.84s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.61s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.027
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.039

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.01315951347351,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.841646909713745,
    "llama": {
      "em": 0.0,
      "f1": 0.026780154486036847,
      "nll_token": 9.062140493976827
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.025396302227184583,
      "nll_token": 8.512811786283262
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.613834142684937,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.026908095988978344,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0372693538665771,
      "Z_std": 1.0014199018478394,
      "Z_mean_norm": 16.022703170776367,
      "prefix_std": 0.01057275291532278,
      "prefix_mean_norm": 0.6755250692367554,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0079628229141235,
      "Z_std": 1.0017262697219849,
      "Z_mean_norm": 16.027605056762695,
      "prefix_std": 0.01363171637058258,
      "prefix_mean_norm": 0.8155412077903748,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03887235803412274
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch4/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch4/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.027 | Qwen 0.025


=========================================
EPOCH 5/16
=========================================

Training epoch 5...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7256.58it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4045.63it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=4, global_step=2740
Epoch 5/1
  step 10/685 | loss_L=1.3338 | loss_Q=1.3387 | scale_pen(L)= 1.3915e-03 | scale_pen(Q)= 6.3483e-05 | grad_norm=0.56 | sec/step~1.12 | rms_L~0.7507 rms_Q~0.6371
  step 20/685 | loss_L=1.0620 | loss_Q=1.0535 | scale_pen(L)= 1.3917e-03 | scale_pen(Q)= 6.2872e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7509 rms_Q~0.6372
  step 30/685 | loss_L=1.1698 | loss_Q=1.1851 | scale_pen(L)= 1.3928e-03 | scale_pen(Q)= 6.3325e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7511 rms_Q~0.6373
  step 40/685 | loss_L=1.2562 | loss_Q=1.2474 | scale_pen(L)= 1.3929e-03 | scale_pen(Q)= 6.4773e-05 | grad_norm=0.57 | sec/step~1.16 | rms_L~0.7511 rms_Q~0.6373
  step 50/685 | loss_L=1.2605 | loss_Q=1.2818 | scale_pen(L)= 1.4013e-03 | scale_pen(Q)= 6.5038e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7511 rms_Q~0.6374
  step 60/685 | loss_L=1.1202 | loss_Q=1.1209 | scale_pen(L)= 1.4031e-03 | scale_pen(Q)= 6.6597e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7513 rms_Q~0.6374
  step 70/685 | loss_L=1.1428 | loss_Q=1.1230 | scale_pen(L)= 1.4064e-03 | scale_pen(Q)= 6.7642e-05 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.7514 rms_Q~0.6375
  step 80/685 | loss_L=1.0141 | loss_Q=1.0216 | scale_pen(L)= 1.4076e-03 | scale_pen(Q)= 6.8297e-05 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.7515 rms_Q~0.6375
  step 90/685 | loss_L=1.2039 | loss_Q=1.2039 | scale_pen(L)= 1.4147e-03 | scale_pen(Q)= 7.0201e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7516 rms_Q~0.6376
  step 100/685 | loss_L=1.1362 | loss_Q=1.1300 | scale_pen(L)= 1.4171e-03 | scale_pen(Q)= 7.0058e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.7517 rms_Q~0.6377
  step 110/685 | loss_L=1.0036 | loss_Q=1.0318 | scale_pen(L)= 1.4223e-03 | scale_pen(Q)= 6.8905e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.7518 rms_Q~0.6377
  step 120/685 | loss_L=1.1522 | loss_Q=1.1577 | scale_pen(L)= 1.4242e-03 | scale_pen(Q)= 6.9581e-05 | grad_norm=0.57 | sec/step~1.07 | rms_L~0.7519 rms_Q~0.6378
  step 130/685 | loss_L=1.1071 | loss_Q=1.0920 | scale_pen(L)= 1.4290e-03 | scale_pen(Q)= 6.9401e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.7520 rms_Q~0.6378
  step 140/685 | loss_L=1.1375 | loss_Q=1.1441 | scale_pen(L)= 1.4282e-03 | scale_pen(Q)= 6.9713e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7521 rms_Q~0.6378
  step 150/685 | loss_L=1.3195 | loss_Q=1.3157 | scale_pen(L)= 1.4301e-03 | scale_pen(Q)= 6.9024e-05 | grad_norm=0.60 | sec/step~1.12 | rms_L~0.7522 rms_Q~0.6379
  step 160/685 | loss_L=1.1784 | loss_Q=1.1976 | scale_pen(L)= 1.4319e-03 | scale_pen(Q)= 6.8161e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.7523 rms_Q~0.6379
  step 170/685 | loss_L=1.1525 | loss_Q=1.1554 | scale_pen(L)= 1.4400e-03 | scale_pen(Q)= 6.7736e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7525 rms_Q~0.6380
  step 180/685 | loss_L=1.0795 | loss_Q=1.0758 | scale_pen(L)= 1.4420e-03 | scale_pen(Q)= 6.5810e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7526 rms_Q~0.6380
  step 190/685 | loss_L=1.1520 | loss_Q=1.1592 | scale_pen(L)= 1.4454e-03 | scale_pen(Q)= 6.7407e-05 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.7527 rms_Q~0.6381
  step 200/685 | loss_L=1.3377 | loss_Q=1.3665 | scale_pen(L)= 1.4484e-03 | scale_pen(Q)= 6.7417e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7529 rms_Q~0.6381
  step 210/685 | loss_L=1.1397 | loss_Q=1.1425 | scale_pen(L)= 1.4489e-03 | scale_pen(Q)= 6.7016e-05 | grad_norm=0.57 | sec/step~1.11 | rms_L~0.7530 rms_Q~0.6382
  step 220/685 | loss_L=1.3145 | loss_Q=1.3041 | scale_pen(L)= 1.4518e-03 | scale_pen(Q)= 6.8399e-05 | grad_norm=0.60 | sec/step~1.12 | rms_L~0.7531 rms_Q~0.6383
  step 230/685 | loss_L=1.1222 | loss_Q=1.1451 | scale_pen(L)= 1.4548e-03 | scale_pen(Q)= 6.8299e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7532 rms_Q~0.6383
  step 240/685 | loss_L=1.1551 | loss_Q=1.1812 | scale_pen(L)= 1.4598e-03 | scale_pen(Q)= 6.9004e-05 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.7533 rms_Q~0.6384
  step 250/685 | loss_L=1.1480 | loss_Q=1.1383 | scale_pen(L)= 1.4592e-03 | scale_pen(Q)= 6.9341e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7534 rms_Q~0.6384
  step 260/685 | loss_L=1.1978 | loss_Q=1.1994 | scale_pen(L)= 1.4671e-03 | scale_pen(Q)= 7.0305e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7535 rms_Q~0.6385
  step 270/685 | loss_L=1.1936 | loss_Q=1.1972 | scale_pen(L)= 1.4646e-03 | scale_pen(Q)= 7.0632e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7536 rms_Q~0.6385
  step 280/685 | loss_L=1.0941 | loss_Q=1.1085 | scale_pen(L)= 1.4682e-03 | scale_pen(Q)= 7.2078e-05 | grad_norm=0.59 | sec/step~1.11 | rms_L~0.7537 rms_Q~0.6386
  step 290/685 | loss_L=1.1361 | loss_Q=1.1316 | scale_pen(L)= 1.4694e-03 | scale_pen(Q)= 7.1019e-05 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.7538 rms_Q~0.6386
  step 300/685 | loss_L=1.2784 | loss_Q=1.2783 | scale_pen(L)= 1.4686e-03 | scale_pen(Q)= 7.0237e-05 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.7539 rms_Q~0.6387
  step 310/685 | loss_L=1.1433 | loss_Q=1.0967 | scale_pen(L)= 1.4729e-03 | scale_pen(Q)= 7.0545e-05 | grad_norm=0.58 | sec/step~1.12 | rms_L~0.7540 rms_Q~0.6387
  step 320/685 | loss_L=1.1222 | loss_Q=1.1490 | scale_pen(L)= 1.4779e-03 | scale_pen(Q)= 7.1439e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7542 rms_Q~0.6388
  step 330/685 | loss_L=1.2696 | loss_Q=1.2699 | scale_pen(L)= 1.4829e-03 | scale_pen(Q)= 7.1244e-05 | grad_norm=0.63 | sec/step~1.22 | rms_L~0.7543 rms_Q~0.6389
  step 340/685 | loss_L=1.1224 | loss_Q=1.1300 | scale_pen(L)= 1.4881e-03 | scale_pen(Q)= 7.0958e-05 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.7544 rms_Q~0.6389
  step 350/685 | loss_L=1.1334 | loss_Q=1.1388 | scale_pen(L)= 1.4889e-03 | scale_pen(Q)= 6.9661e-05 | grad_norm=0.63 | sec/step~1.17 | rms_L~0.7545 rms_Q~0.6390
  step 360/685 | loss_L=1.1436 | loss_Q=1.1341 | scale_pen(L)= 1.4919e-03 | scale_pen(Q)= 7.0591e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7546 rms_Q~0.6390
  step 370/685 | loss_L=1.0012 | loss_Q=1.0217 | scale_pen(L)= 1.4983e-03 | scale_pen(Q)= 7.0451e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7548 rms_Q~0.6391
  step 380/685 | loss_L=1.0180 | loss_Q=0.9977 | scale_pen(L)= 1.5002e-03 | scale_pen(Q)= 7.0507e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7549 rms_Q~0.6391
  step 390/685 | loss_L=1.1831 | loss_Q=1.1783 | scale_pen(L)= 1.5046e-03 | scale_pen(Q)= 7.0056e-05 | grad_norm=0.59 | sec/step~1.14 | rms_L~0.7550 rms_Q~0.6392
  step 400/685 | loss_L=1.1481 | loss_Q=1.1284 | scale_pen(L)= 1.5102e-03 | scale_pen(Q)= 6.7874e-05 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.7551 rms_Q~0.6392
  step 410/685 | loss_L=1.2364 | loss_Q=1.2524 | scale_pen(L)= 1.5086e-03 | scale_pen(Q)= 6.7591e-05 | grad_norm=0.57 | sec/step~1.07 | rms_L~0.7552 rms_Q~0.6392
  step 420/685 | loss_L=1.1053 | loss_Q=1.1081 | scale_pen(L)= 1.5126e-03 | scale_pen(Q)= 6.6821e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.7553 rms_Q~0.6393
  step 430/685 | loss_L=1.2263 | loss_Q=1.2248 | scale_pen(L)= 1.5131e-03 | scale_pen(Q)= 6.5551e-05 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.7554 rms_Q~0.6393
  step 440/685 | loss_L=1.0456 | loss_Q=1.0314 | scale_pen(L)= 1.5162e-03 | scale_pen(Q)= 6.5833e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.7556 rms_Q~0.6394
  step 450/685 | loss_L=1.0476 | loss_Q=1.0444 | scale_pen(L)= 1.5191e-03 | scale_pen(Q)= 6.6685e-05 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.7557 rms_Q~0.6394
  step 460/685 | loss_L=1.0787 | loss_Q=1.0963 | scale_pen(L)= 1.5220e-03 | scale_pen(Q)= 6.7627e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7558 rms_Q~0.6394
  step 470/685 | loss_L=1.0981 | loss_Q=1.0789 | scale_pen(L)= 1.5223e-03 | scale_pen(Q)= 6.6630e-05 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.7559 rms_Q~0.6395
  step 480/685 | loss_L=1.3158 | loss_Q=1.3285 | scale_pen(L)= 1.5277e-03 | scale_pen(Q)= 6.6591e-05 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.7560 rms_Q~0.6395
  step 490/685 | loss_L=1.1937 | loss_Q=1.2186 | scale_pen(L)= 1.5320e-03 | scale_pen(Q)= 6.7628e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7561 rms_Q~0.6396
  step 500/685 | loss_L=1.1261 | loss_Q=1.1192 | scale_pen(L)= 1.5326e-03 | scale_pen(Q)= 6.7356e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7562 rms_Q~0.6396
  step 510/685 | loss_L=1.3040 | loss_Q=1.3062 | scale_pen(L)= 1.5405e-03 | scale_pen(Q)= 6.7548e-05 | grad_norm=0.63 | sec/step~1.16 | rms_L~0.7563 rms_Q~0.6396
  step 520/685 | loss_L=1.1553 | loss_Q=1.1551 | scale_pen(L)= 1.5465e-03 | scale_pen(Q)= 6.7317e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.7564 rms_Q~0.6397
  step 530/685 | loss_L=0.9965 | loss_Q=0.9957 | scale_pen(L)= 1.5447e-03 | scale_pen(Q)= 6.7233e-05 | grad_norm=0.56 | sec/step~1.09 | rms_L~0.7565 rms_Q~0.6397
  step 540/685 | loss_L=1.1168 | loss_Q=1.1283 | scale_pen(L)= 1.5475e-03 | scale_pen(Q)= 6.7225e-05 | grad_norm=0.62 | sec/step~1.10 | rms_L~0.7566 rms_Q~0.6398
  step 550/685 | loss_L=1.0588 | loss_Q=1.0557 | scale_pen(L)= 1.5501e-03 | scale_pen(Q)= 6.7059e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7567 rms_Q~0.6398
  step 560/685 | loss_L=1.1034 | loss_Q=1.0897 | scale_pen(L)= 1.5502e-03 | scale_pen(Q)= 6.7280e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7568 rms_Q~0.6398
  step 570/685 | loss_L=1.1804 | loss_Q=1.2089 | scale_pen(L)= 1.5512e-03 | scale_pen(Q)= 6.8563e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7570 rms_Q~0.6399
  step 580/685 | loss_L=1.1292 | loss_Q=1.1222 | scale_pen(L)= 1.5564e-03 | scale_pen(Q)= 7.0393e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7571 rms_Q~0.6399
  step 590/685 | loss_L=1.0429 | loss_Q=1.0305 | scale_pen(L)= 1.5563e-03 | scale_pen(Q)= 7.1101e-05 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.7572 rms_Q~0.6400
  step 600/685 | loss_L=1.2712 | loss_Q=1.2894 | scale_pen(L)= 1.5662e-03 | scale_pen(Q)= 6.9508e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.7573 rms_Q~0.6400
  step 610/685 | loss_L=1.0794 | loss_Q=1.0918 | scale_pen(L)= 1.5697e-03 | scale_pen(Q)= 6.9260e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7574 rms_Q~0.6401
  step 620/685 | loss_L=1.1771 | loss_Q=1.1801 | scale_pen(L)= 1.5748e-03 | scale_pen(Q)= 7.0084e-05 | grad_norm=0.62 | sec/step~1.10 | rms_L~0.7575 rms_Q~0.6401
  step 630/685 | loss_L=1.1849 | loss_Q=1.1759 | scale_pen(L)= 1.5767e-03 | scale_pen(Q)= 7.2037e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7576 rms_Q~0.6401
  step 640/685 | loss_L=1.2345 | loss_Q=1.2470 | scale_pen(L)= 1.5845e-03 | scale_pen(Q)= 7.1425e-05 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.7577 rms_Q~0.6402
  step 650/685 | loss_L=1.0706 | loss_Q=1.0745 | scale_pen(L)= 1.5839e-03 | scale_pen(Q)= 7.1137e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7578 rms_Q~0.6402
  step 660/685 | loss_L=1.1316 | loss_Q=1.1420 | scale_pen(L)= 1.5914e-03 | scale_pen(Q)= 7.0147e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.7579 rms_Q~0.6403
  step 670/685 | loss_L=1.1492 | loss_Q=1.1449 | scale_pen(L)= 1.5905e-03 | scale_pen(Q)= 6.9755e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7581 rms_Q~0.6403
  step 680/685 | loss_L=1.0687 | loss_Q=1.0837 | scale_pen(L)= 1.5918e-03 | scale_pen(Q)= 6.9464e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7582 rms_Q~0.6403
  step 685/685 | loss_L=1.2733 | loss_Q=1.2912 | scale_pen(L)= 1.5927e-03 | scale_pen(Q)= 6.8569e-05 | grad_norm=0.65 | sec/step~0.43 | rms_L~0.7582 rms_Q~0.6404
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.7582206758269429, 'count': 685}, 'qwen': {'rms_mean': 0.6403694667955384, 'count': 685}}

Evaluating epoch 5 checkpoint...
Evaluating: runs/8B_clean_answer/epoch5 -> runs/8B_clean_answer/eval_epoch5
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch5/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch5/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2946.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.90512121399514, 'neutral_chat': 8.953570374555868, 'llama_chat': 9.028184326327576} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch5/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.75230 -> target=0.01057
[debug:llama] adapter.scale=1.0399 | Z.std=1.0012 Z.mean||=16.0195 | prefix.std=0.0106 prefix.mean||=0.6754 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the is the of the of the. the of'
  1: '2-3 of the of the of the of'
  2: '1980s the of the economy was in decline the of'
  3: 'the of the of the of the of the of the of'
  4: '3-4-5 3-4-6'
Saved Llama results to runs/8B_clean_answer/eval_epoch5/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3532.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.24it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.641281285298565, 'neutral_chat': 8.700915515422821, 'qwen_chat': 8.568228509375658} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch5/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.64225 -> target=0.01363
[debug:qwen] adapter.scale=1.0083 | Z.std=1.0015 Z.mean||=16.0239 | prefix.std=0.0136 prefix.mean||=0.8155 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and, and, and, and, and, and'
  1: 'of ands ands ands ands ands and'
  2: '\n\n \n\n'
  3: 'of the and and and and and and and and and and'
  4: '1950s the of and and of and of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch5/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3279.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7917.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.03s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.022  |  NLL/token (gold): 8.911036262706835
Qwen   EM: 0.000   F1: 0.022  |  NLL/token (gold): 8.567819045018897
Wall clock: 12.97s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.64s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.022
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.032

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.026613473892212,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.966899633407593,
    "llama": {
      "em": 0.0,
      "f1": 0.021884262796027506,
      "nll_token": 8.911036262706835
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.02192134221987163,
      "nll_token": 8.567819045018897
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.636526823043823,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.022010744647509357,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0399081707000732,
      "Z_std": 1.0012201070785522,
      "Z_mean_norm": 16.019508361816406,
      "prefix_std": 0.010572751052677631,
      "prefix_mean_norm": 0.6754205226898193,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.008280634880066,
      "Z_std": 1.0014946460723877,
      "Z_mean_norm": 16.02389907836914,
      "prefix_std": 0.013631680980324745,
      "prefix_mean_norm": 0.8154643774032593,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03245009973686444
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch5/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch5/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.022 | Qwen 0.022


=========================================
EPOCH 6/16
=========================================

Training epoch 6...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7303.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3969.06it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.17it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=5, global_step=3425
Epoch 6/1
  step 10/685 | loss_L=1.3138 | loss_Q=1.3408 | scale_pen(L)= 1.5992e-03 | scale_pen(Q)= 6.6519e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7658 rms_Q~0.6433
  step 20/685 | loss_L=1.0755 | loss_Q=1.0812 | scale_pen(L)= 1.6006e-03 | scale_pen(Q)= 6.5725e-05 | grad_norm=0.59 | sec/step~1.15 | rms_L~0.7661 rms_Q~0.6434
  step 30/685 | loss_L=1.2213 | loss_Q=1.2339 | scale_pen(L)= 1.6011e-03 | scale_pen(Q)= 6.6009e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.7662 rms_Q~0.6434
  step 40/685 | loss_L=1.0931 | loss_Q=1.1071 | scale_pen(L)= 1.6030e-03 | scale_pen(Q)= 6.5589e-05 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.7663 rms_Q~0.6435
  step 50/685 | loss_L=1.0850 | loss_Q=1.0785 | scale_pen(L)= 1.6129e-03 | scale_pen(Q)= 6.6685e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7664 rms_Q~0.6436
  step 60/685 | loss_L=1.1643 | loss_Q=1.1682 | scale_pen(L)= 1.6148e-03 | scale_pen(Q)= 6.7200e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7665 rms_Q~0.6436
  step 70/685 | loss_L=1.3009 | loss_Q=1.2794 | scale_pen(L)= 1.6164e-03 | scale_pen(Q)= 6.7333e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.7666 rms_Q~0.6436
  step 80/685 | loss_L=1.1737 | loss_Q=1.1753 | scale_pen(L)= 1.6169e-03 | scale_pen(Q)= 6.7311e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.7666 rms_Q~0.6437
  step 90/685 | loss_L=1.1403 | loss_Q=1.1481 | scale_pen(L)= 1.6185e-03 | scale_pen(Q)= 6.8120e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7667 rms_Q~0.6437
  step 100/685 | loss_L=1.3414 | loss_Q=1.3673 | scale_pen(L)= 1.6201e-03 | scale_pen(Q)= 6.8873e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7668 rms_Q~0.6438
  step 110/685 | loss_L=1.1647 | loss_Q=1.1591 | scale_pen(L)= 1.6230e-03 | scale_pen(Q)= 6.9117e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7668 rms_Q~0.6438
  step 120/685 | loss_L=1.2077 | loss_Q=1.2156 | scale_pen(L)= 1.6290e-03 | scale_pen(Q)= 6.9607e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7669 rms_Q~0.6438
  step 130/685 | loss_L=1.1574 | loss_Q=1.1556 | scale_pen(L)= 1.6303e-03 | scale_pen(Q)= 6.9798e-05 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.7670 rms_Q~0.6439
  step 140/685 | loss_L=1.2174 | loss_Q=1.2290 | scale_pen(L)= 1.6315e-03 | scale_pen(Q)= 7.1588e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.7671 rms_Q~0.6439
  step 150/685 | loss_L=1.2208 | loss_Q=1.2247 | scale_pen(L)= 1.6318e-03 | scale_pen(Q)= 6.9753e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.7671 rms_Q~0.6440
  step 160/685 | loss_L=1.1770 | loss_Q=1.1910 | scale_pen(L)= 1.6345e-03 | scale_pen(Q)= 6.9874e-05 | grad_norm=0.65 | sec/step~1.11 | rms_L~0.7672 rms_Q~0.6440
  step 170/685 | loss_L=1.1477 | loss_Q=1.1477 | scale_pen(L)= 1.6401e-03 | scale_pen(Q)= 6.9966e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7673 rms_Q~0.6441
  step 180/685 | loss_L=1.0618 | loss_Q=1.0478 | scale_pen(L)= 1.6383e-03 | scale_pen(Q)= 7.0060e-05 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.7673 rms_Q~0.6441
  step 190/685 | loss_L=1.2163 | loss_Q=1.1991 | scale_pen(L)= 1.6398e-03 | scale_pen(Q)= 6.9916e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7674 rms_Q~0.6441
  step 200/685 | loss_L=1.1546 | loss_Q=1.1272 | scale_pen(L)= 1.6492e-03 | scale_pen(Q)= 7.1262e-05 | grad_norm=0.56 | sec/step~1.13 | rms_L~0.7675 rms_Q~0.6442
  step 210/685 | loss_L=1.0877 | loss_Q=1.1053 | scale_pen(L)= 1.6527e-03 | scale_pen(Q)= 7.3976e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7677 rms_Q~0.6442
  step 220/685 | loss_L=1.0585 | loss_Q=1.0529 | scale_pen(L)= 1.6517e-03 | scale_pen(Q)= 7.4515e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7678 rms_Q~0.6443
  step 230/685 | loss_L=1.0579 | loss_Q=1.0478 | scale_pen(L)= 1.6554e-03 | scale_pen(Q)= 7.3436e-05 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.7679 rms_Q~0.6443
  step 240/685 | loss_L=1.2481 | loss_Q=1.2470 | scale_pen(L)= 1.6478e-03 | scale_pen(Q)= 7.3837e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7680 rms_Q~0.6444
  step 250/685 | loss_L=1.1541 | loss_Q=1.1685 | scale_pen(L)= 1.6546e-03 | scale_pen(Q)= 7.4696e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.7680 rms_Q~0.6444
  step 260/685 | loss_L=1.1963 | loss_Q=1.2092 | scale_pen(L)= 1.6567e-03 | scale_pen(Q)= 7.4210e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7681 rms_Q~0.6444
  step 270/685 | loss_L=1.0515 | loss_Q=1.0603 | scale_pen(L)= 1.6588e-03 | scale_pen(Q)= 7.3802e-05 | grad_norm=0.59 | sec/step~1.14 | rms_L~0.7682 rms_Q~0.6445
  step 280/685 | loss_L=1.2143 | loss_Q=1.2048 | scale_pen(L)= 1.6627e-03 | scale_pen(Q)= 7.2971e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7683 rms_Q~0.6445
  step 290/685 | loss_L=1.1493 | loss_Q=1.1332 | scale_pen(L)= 1.6598e-03 | scale_pen(Q)= 7.3006e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7684 rms_Q~0.6446
  step 300/685 | loss_L=1.3049 | loss_Q=1.3233 | scale_pen(L)= 1.6658e-03 | scale_pen(Q)= 7.2430e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7685 rms_Q~0.6446
  step 310/685 | loss_L=1.1221 | loss_Q=1.1458 | scale_pen(L)= 1.6671e-03 | scale_pen(Q)= 7.3262e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.7686 rms_Q~0.6446
  step 320/685 | loss_L=1.1732 | loss_Q=1.2097 | scale_pen(L)= 1.6678e-03 | scale_pen(Q)= 7.2503e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7687 rms_Q~0.6446
  step 330/685 | loss_L=1.1071 | loss_Q=1.1164 | scale_pen(L)= 1.6682e-03 | scale_pen(Q)= 7.3690e-05 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.7688 rms_Q~0.6447
  step 340/685 | loss_L=1.0410 | loss_Q=1.0288 | scale_pen(L)= 1.6749e-03 | scale_pen(Q)= 7.2082e-05 | grad_norm=0.64 | sec/step~1.15 | rms_L~0.7689 rms_Q~0.6447
  step 350/685 | loss_L=1.0722 | loss_Q=1.0813 | scale_pen(L)= 1.6749e-03 | scale_pen(Q)= 7.2698e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.7690 rms_Q~0.6448
  step 360/685 | loss_L=1.3505 | loss_Q=1.3443 | scale_pen(L)= 1.6768e-03 | scale_pen(Q)= 7.2104e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7691 rms_Q~0.6448
  step 370/685 | loss_L=1.2272 | loss_Q=1.2368 | scale_pen(L)= 1.6813e-03 | scale_pen(Q)= 7.2154e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7692 rms_Q~0.6449
  step 380/685 | loss_L=1.2854 | loss_Q=1.3173 | scale_pen(L)= 1.6834e-03 | scale_pen(Q)= 7.2165e-05 | grad_norm=0.59 | sec/step~1.23 | rms_L~0.7693 rms_Q~0.6449
  step 390/685 | loss_L=1.2752 | loss_Q=1.3047 | scale_pen(L)= 1.6871e-03 | scale_pen(Q)= 7.2118e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7694 rms_Q~0.6449
  step 400/685 | loss_L=1.1028 | loss_Q=1.1063 | scale_pen(L)= 1.6924e-03 | scale_pen(Q)= 7.1720e-05 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.7695 rms_Q~0.6450
  step 410/685 | loss_L=1.2256 | loss_Q=1.2360 | scale_pen(L)= 1.6932e-03 | scale_pen(Q)= 7.2812e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.7696 rms_Q~0.6450
  step 420/685 | loss_L=1.1421 | loss_Q=1.1589 | scale_pen(L)= 1.6943e-03 | scale_pen(Q)= 7.2798e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7697 rms_Q~0.6451
  step 430/685 | loss_L=1.1470 | loss_Q=1.1266 | scale_pen(L)= 1.6930e-03 | scale_pen(Q)= 7.2477e-05 | grad_norm=0.56 | sec/step~1.15 | rms_L~0.7698 rms_Q~0.6451
  step 440/685 | loss_L=1.2170 | loss_Q=1.2376 | scale_pen(L)= 1.6976e-03 | scale_pen(Q)= 7.2546e-05 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.7699 rms_Q~0.6451
  step 450/685 | loss_L=1.0387 | loss_Q=1.0432 | scale_pen(L)= 1.6973e-03 | scale_pen(Q)= 7.2719e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.7700 rms_Q~0.6452
  step 460/685 | loss_L=1.2359 | loss_Q=1.2664 | scale_pen(L)= 1.7047e-03 | scale_pen(Q)= 7.1292e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7701 rms_Q~0.6452
  step 470/685 | loss_L=1.1300 | loss_Q=1.1231 | scale_pen(L)= 1.7020e-03 | scale_pen(Q)= 7.1258e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.7702 rms_Q~0.6452
  step 480/685 | loss_L=1.2026 | loss_Q=1.2178 | scale_pen(L)= 1.7107e-03 | scale_pen(Q)= 7.1463e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7703 rms_Q~0.6453
  step 490/685 | loss_L=1.0189 | loss_Q=1.0439 | scale_pen(L)= 1.7095e-03 | scale_pen(Q)= 7.1133e-05 | grad_norm=0.57 | sec/step~1.05 | rms_L~0.7704 rms_Q~0.6453
  step 500/685 | loss_L=1.1679 | loss_Q=1.1631 | scale_pen(L)= 1.7128e-03 | scale_pen(Q)= 7.1391e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.7705 rms_Q~0.6454
  step 510/685 | loss_L=1.0752 | loss_Q=1.0807 | scale_pen(L)= 1.7195e-03 | scale_pen(Q)= 6.9329e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7705 rms_Q~0.6454
  step 520/685 | loss_L=1.2392 | loss_Q=1.2653 | scale_pen(L)= 1.7188e-03 | scale_pen(Q)= 6.7485e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7706 rms_Q~0.6454
  step 530/685 | loss_L=1.2016 | loss_Q=1.1966 | scale_pen(L)= 1.7215e-03 | scale_pen(Q)= 6.7376e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7707 rms_Q~0.6454
  step 540/685 | loss_L=1.2382 | loss_Q=1.2617 | scale_pen(L)= 1.7220e-03 | scale_pen(Q)= 6.7530e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7708 rms_Q~0.6455
  step 550/685 | loss_L=1.1579 | loss_Q=1.1725 | scale_pen(L)= 1.7270e-03 | scale_pen(Q)= 6.8145e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7709 rms_Q~0.6455
  step 560/685 | loss_L=1.1233 | loss_Q=1.1135 | scale_pen(L)= 1.7340e-03 | scale_pen(Q)= 6.6741e-05 | grad_norm=0.63 | sec/step~1.10 | rms_L~0.7710 rms_Q~0.6455
  step 570/685 | loss_L=1.3136 | loss_Q=1.3109 | scale_pen(L)= 1.7314e-03 | scale_pen(Q)= 6.5337e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7711 rms_Q~0.6456
  step 580/685 | loss_L=1.3740 | loss_Q=1.4008 | scale_pen(L)= 1.7368e-03 | scale_pen(Q)= 6.6096e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7712 rms_Q~0.6456
  step 590/685 | loss_L=1.1092 | loss_Q=1.1150 | scale_pen(L)= 1.7370e-03 | scale_pen(Q)= 6.6712e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.7713 rms_Q~0.6456
  step 600/685 | loss_L=1.1210 | loss_Q=1.1248 | scale_pen(L)= 1.7392e-03 | scale_pen(Q)= 6.8224e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7714 rms_Q~0.6457
  step 610/685 | loss_L=0.9899 | loss_Q=0.9988 | scale_pen(L)= 1.7433e-03 | scale_pen(Q)= 6.8820e-05 | grad_norm=0.57 | sec/step~1.12 | rms_L~0.7714 rms_Q~0.6457
  step 620/685 | loss_L=1.2572 | loss_Q=1.2532 | scale_pen(L)= 1.7466e-03 | scale_pen(Q)= 6.8232e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7715 rms_Q~0.6457
  step 630/685 | loss_L=1.1343 | loss_Q=1.1460 | scale_pen(L)= 1.7456e-03 | scale_pen(Q)= 6.8514e-05 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.7716 rms_Q~0.6458
  step 640/685 | loss_L=1.1132 | loss_Q=1.1257 | scale_pen(L)= 1.7479e-03 | scale_pen(Q)= 6.8646e-05 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.7717 rms_Q~0.6458
  step 650/685 | loss_L=1.1339 | loss_Q=1.1413 | scale_pen(L)= 1.7493e-03 | scale_pen(Q)= 6.9460e-05 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.7718 rms_Q~0.6458
  step 660/685 | loss_L=1.1887 | loss_Q=1.1829 | scale_pen(L)= 1.7529e-03 | scale_pen(Q)= 7.0024e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7719 rms_Q~0.6459
  step 670/685 | loss_L=1.2490 | loss_Q=1.2579 | scale_pen(L)= 1.7556e-03 | scale_pen(Q)= 7.2031e-05 | grad_norm=0.58 | sec/step~1.05 | rms_L~0.7720 rms_Q~0.6459
  step 680/685 | loss_L=1.1669 | loss_Q=1.1792 | scale_pen(L)= 1.7616e-03 | scale_pen(Q)= 7.1738e-05 | grad_norm=0.58 | sec/step~1.11 | rms_L~0.7721 rms_Q~0.6460
  step 685/685 | loss_L=1.0146 | loss_Q=0.9965 | scale_pen(L)= 1.7638e-03 | scale_pen(Q)= 7.2187e-05 | grad_norm=0.63 | sec/step~0.50 | rms_L~0.7721 rms_Q~0.6460
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.7721265929458785, 'count': 685}, 'qwen': {'rms_mean': 0.645969764420586, 'count': 685}}

Evaluating epoch 6 checkpoint...
Evaluating: runs/8B_clean_answer/epoch6 -> runs/8B_clean_answer/eval_epoch6
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch6/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch6/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3097.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.873123933939166, 'neutral_chat': 8.918318746311595, 'llama_chat': 9.013289469980599} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch6/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.76561 -> target=0.01057
[debug:llama] adapter.scale=1.0420 | Z.std=1.0010 Z.mean||=16.0155 | prefix.std=0.0106 prefix.mean||=0.6753 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the is the of the of the. the of'
  1: '2. insects,, and,, and'
  2: '198 recession the of the and the of the economy the of'
  3: 'the of the of. the of the of'
  4: '3-4-5 3-4-6'
Saved Llama results to runs/8B_clean_answer/eval_epoch6/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3508.41it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.24it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.685277173128078, 'neutral_chat': 8.750139047859838, 'qwen_chat': 8.646790914434604} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch6/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.64747 -> target=0.01363
[debug:qwen] adapter.scale=1.0085 | Z.std=1.0012 Z.mean||=16.0194 | prefix.std=0.0136 prefix.mean||=0.8154 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '\n", "en": "The of and the of'
  1: ''
  2: '\n\n \n\n'
  3: '\n", "en": "The Park of the and'
  4: '\n\n\n\n\n'
Saved Qwen results to runs/8B_clean_answer/eval_epoch6/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2599.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.11s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5901.24it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.97s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.026  |  NLL/token (gold): 8.87147299448649
Qwen   EM: 0.000   F1: 0.017  |  NLL/token (gold): 8.634079721869615
Wall clock: 13.91s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.82s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.022
Inter-model agreement (normalized): 0.005
Oracle upper bound:  EM 0.000  F1 0.033

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.968859434127808,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.914730548858643,
    "llama": {
      "em": 0.0,
      "f1": 0.026356314437196788,
      "nll_token": 8.87147299448649
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.01703456510809452,
      "nll_token": 8.634079721869615
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.81860876083374,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.02162955345308287,
    "agreement": 0.005,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0419981479644775,
      "Z_std": 1.0009690523147583,
      "Z_mean_norm": 16.01548957824707,
      "prefix_std": 0.010572755709290504,
      "prefix_mean_norm": 0.6753281950950623,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0084962844848633,
      "Z_std": 1.0012123584747314,
      "Z_mean_norm": 16.01938247680664,
      "prefix_std": 0.013631640933454037,
      "prefix_mean_norm": 0.8153812885284424,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03280915163268105
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch6/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch6/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.026 | Qwen 0.017


=========================================
EPOCH 7/16
=========================================

Training epoch 7...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6460.23it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.01it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4554.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=6, global_step=4110
Epoch 7/1
  step 10/685 | loss_L=1.1832 | loss_Q=1.1670 | scale_pen(L)= 1.7664e-03 | scale_pen(Q)= 7.2936e-05 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.7785 rms_Q~0.6487
  step 20/685 | loss_L=1.0444 | loss_Q=1.0451 | scale_pen(L)= 1.7755e-03 | scale_pen(Q)= 7.1345e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7786 rms_Q~0.6487
  step 30/685 | loss_L=1.0521 | loss_Q=1.0688 | scale_pen(L)= 1.7755e-03 | scale_pen(Q)= 7.3710e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7788 rms_Q~0.6488
  step 40/685 | loss_L=1.1063 | loss_Q=1.1322 | scale_pen(L)= 1.7798e-03 | scale_pen(Q)= 7.4408e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.7789 rms_Q~0.6489
  step 50/685 | loss_L=1.3223 | loss_Q=1.3526 | scale_pen(L)= 1.7848e-03 | scale_pen(Q)= 7.4408e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7789 rms_Q~0.6490
  step 60/685 | loss_L=1.1751 | loss_Q=1.1835 | scale_pen(L)= 1.7866e-03 | scale_pen(Q)= 7.4578e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.7790 rms_Q~0.6490
  step 70/685 | loss_L=1.1155 | loss_Q=1.0952 | scale_pen(L)= 1.7929e-03 | scale_pen(Q)= 7.4642e-05 | grad_norm=0.61 | sec/step~1.18 | rms_L~0.7791 rms_Q~0.6491
  step 80/685 | loss_L=1.1249 | loss_Q=1.1373 | scale_pen(L)= 1.7966e-03 | scale_pen(Q)= 7.3426e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7792 rms_Q~0.6491
  step 90/685 | loss_L=1.1605 | loss_Q=1.1915 | scale_pen(L)= 1.7924e-03 | scale_pen(Q)= 7.2138e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7793 rms_Q~0.6492
  step 100/685 | loss_L=1.2575 | loss_Q=1.2619 | scale_pen(L)= 1.7912e-03 | scale_pen(Q)= 7.1744e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7793 rms_Q~0.6492
  step 110/685 | loss_L=1.0859 | loss_Q=1.1053 | scale_pen(L)= 1.7953e-03 | scale_pen(Q)= 7.1508e-05 | grad_norm=0.61 | sec/step~1.14 | rms_L~0.7794 rms_Q~0.6492
  step 120/685 | loss_L=1.0613 | loss_Q=1.0470 | scale_pen(L)= 1.7974e-03 | scale_pen(Q)= 7.2173e-05 | grad_norm=0.55 | sec/step~1.07 | rms_L~0.7795 rms_Q~0.6493
  step 130/685 | loss_L=1.2314 | loss_Q=1.2167 | scale_pen(L)= 1.7986e-03 | scale_pen(Q)= 7.2562e-05 | grad_norm=0.62 | sec/step~1.23 | rms_L~0.7795 rms_Q~0.6493
  step 140/685 | loss_L=1.0306 | loss_Q=1.0125 | scale_pen(L)= 1.8016e-03 | scale_pen(Q)= 7.2810e-05 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.7796 rms_Q~0.6493
  step 150/685 | loss_L=1.0980 | loss_Q=1.1122 | scale_pen(L)= 1.8110e-03 | scale_pen(Q)= 7.3446e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.7797 rms_Q~0.6493
  step 160/685 | loss_L=1.0712 | loss_Q=1.0818 | scale_pen(L)= 1.8111e-03 | scale_pen(Q)= 7.2971e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7797 rms_Q~0.6493
  step 170/685 | loss_L=1.1347 | loss_Q=1.1297 | scale_pen(L)= 1.8086e-03 | scale_pen(Q)= 7.2832e-05 | grad_norm=0.59 | sec/step~1.08 | rms_L~0.7798 rms_Q~0.6494
  step 180/685 | loss_L=1.2610 | loss_Q=1.2572 | scale_pen(L)= 1.8081e-03 | scale_pen(Q)= 7.2664e-05 | grad_norm=0.59 | sec/step~1.15 | rms_L~0.7799 rms_Q~0.6494
  step 190/685 | loss_L=1.1640 | loss_Q=1.1563 | scale_pen(L)= 1.8076e-03 | scale_pen(Q)= 7.2306e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7799 rms_Q~0.6494
  step 200/685 | loss_L=1.0455 | loss_Q=1.0286 | scale_pen(L)= 1.8110e-03 | scale_pen(Q)= 7.0928e-05 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.7800 rms_Q~0.6494
  step 210/685 | loss_L=0.9308 | loss_Q=0.9536 | scale_pen(L)= 1.8138e-03 | scale_pen(Q)= 7.1562e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7801 rms_Q~0.6495
  step 220/685 | loss_L=1.2002 | loss_Q=1.2150 | scale_pen(L)= 1.8152e-03 | scale_pen(Q)= 7.1800e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7801 rms_Q~0.6495
  step 230/685 | loss_L=1.1160 | loss_Q=1.1304 | scale_pen(L)= 1.8171e-03 | scale_pen(Q)= 7.1075e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.7802 rms_Q~0.6495
  step 240/685 | loss_L=1.1658 | loss_Q=1.1980 | scale_pen(L)= 1.8153e-03 | scale_pen(Q)= 7.2152e-05 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.7803 rms_Q~0.6496
  step 250/685 | loss_L=1.0970 | loss_Q=1.0993 | scale_pen(L)= 1.8203e-03 | scale_pen(Q)= 7.2355e-05 | grad_norm=0.57 | sec/step~1.13 | rms_L~0.7804 rms_Q~0.6496
  step 260/685 | loss_L=1.1064 | loss_Q=1.1232 | scale_pen(L)= 1.8219e-03 | scale_pen(Q)= 7.3792e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7804 rms_Q~0.6497
  step 270/685 | loss_L=1.1921 | loss_Q=1.1743 | scale_pen(L)= 1.8272e-03 | scale_pen(Q)= 7.4478e-05 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.7805 rms_Q~0.6497
  step 280/685 | loss_L=1.1005 | loss_Q=1.1245 | scale_pen(L)= 1.8270e-03 | scale_pen(Q)= 7.4525e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7806 rms_Q~0.6497
  step 290/685 | loss_L=1.2250 | loss_Q=1.2392 | scale_pen(L)= 1.8322e-03 | scale_pen(Q)= 7.5100e-05 | grad_norm=0.56 | sec/step~1.05 | rms_L~0.7806 rms_Q~0.6498
  step 300/685 | loss_L=1.1153 | loss_Q=1.1193 | scale_pen(L)= 1.8303e-03 | scale_pen(Q)= 7.4807e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7807 rms_Q~0.6498
  step 310/685 | loss_L=1.0511 | loss_Q=1.0493 | scale_pen(L)= 1.8328e-03 | scale_pen(Q)= 7.4791e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.7808 rms_Q~0.6498
  step 320/685 | loss_L=1.1461 | loss_Q=1.1254 | scale_pen(L)= 1.8372e-03 | scale_pen(Q)= 7.4192e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7808 rms_Q~0.6499
  step 330/685 | loss_L=1.0925 | loss_Q=1.1051 | scale_pen(L)= 1.8366e-03 | scale_pen(Q)= 7.3608e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.7809 rms_Q~0.6499
  step 340/685 | loss_L=1.2667 | loss_Q=1.2680 | scale_pen(L)= 1.8420e-03 | scale_pen(Q)= 7.3528e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7810 rms_Q~0.6499
  step 350/685 | loss_L=1.1421 | loss_Q=1.1351 | scale_pen(L)= 1.8468e-03 | scale_pen(Q)= 7.3804e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7811 rms_Q~0.6500
  step 360/685 | loss_L=1.1336 | loss_Q=1.1334 | scale_pen(L)= 1.8413e-03 | scale_pen(Q)= 7.3507e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7811 rms_Q~0.6500
  step 370/685 | loss_L=1.1678 | loss_Q=1.1786 | scale_pen(L)= 1.8452e-03 | scale_pen(Q)= 7.4188e-05 | grad_norm=0.60 | sec/step~1.15 | rms_L~0.7812 rms_Q~0.6501
  step 380/685 | loss_L=1.0840 | loss_Q=1.0891 | scale_pen(L)= 1.8509e-03 | scale_pen(Q)= 7.3628e-05 | grad_norm=0.58 | sec/step~1.09 | rms_L~0.7813 rms_Q~0.6501
  step 390/685 | loss_L=1.0293 | loss_Q=1.0254 | scale_pen(L)= 1.8563e-03 | scale_pen(Q)= 7.2457e-05 | grad_norm=0.65 | sec/step~1.08 | rms_L~0.7814 rms_Q~0.6501
  step 400/685 | loss_L=1.1307 | loss_Q=1.1540 | scale_pen(L)= 1.8587e-03 | scale_pen(Q)= 7.1049e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7814 rms_Q~0.6502
  step 410/685 | loss_L=1.0977 | loss_Q=1.1141 | scale_pen(L)= 1.8572e-03 | scale_pen(Q)= 7.0914e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7815 rms_Q~0.6502
  step 420/685 | loss_L=1.1568 | loss_Q=1.1639 | scale_pen(L)= 1.8622e-03 | scale_pen(Q)= 7.1359e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.7816 rms_Q~0.6502
  step 430/685 | loss_L=1.1352 | loss_Q=1.1280 | scale_pen(L)= 1.8642e-03 | scale_pen(Q)= 7.1760e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7817 rms_Q~0.6502
  step 440/685 | loss_L=1.1156 | loss_Q=1.1073 | scale_pen(L)= 1.8654e-03 | scale_pen(Q)= 7.2790e-05 | grad_norm=0.59 | sec/step~1.08 | rms_L~0.7817 rms_Q~0.6503
  step 450/685 | loss_L=1.0545 | loss_Q=1.0643 | scale_pen(L)= 1.8656e-03 | scale_pen(Q)= 7.5703e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7818 rms_Q~0.6503
  step 460/685 | loss_L=1.1179 | loss_Q=1.1031 | scale_pen(L)= 1.8653e-03 | scale_pen(Q)= 7.5802e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7819 rms_Q~0.6504
  step 470/685 | loss_L=1.1109 | loss_Q=1.0815 | scale_pen(L)= 1.8710e-03 | scale_pen(Q)= 7.5862e-05 | grad_norm=0.63 | sec/step~1.17 | rms_L~0.7820 rms_Q~0.6504
  step 480/685 | loss_L=1.2456 | loss_Q=1.2513 | scale_pen(L)= 1.8716e-03 | scale_pen(Q)= 7.5526e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7821 rms_Q~0.6504
  step 490/685 | loss_L=1.1372 | loss_Q=1.1358 | scale_pen(L)= 1.8756e-03 | scale_pen(Q)= 7.5253e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7822 rms_Q~0.6505
  step 500/685 | loss_L=1.0218 | loss_Q=1.0256 | scale_pen(L)= 1.8799e-03 | scale_pen(Q)= 7.5340e-05 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.7822 rms_Q~0.6505
  step 510/685 | loss_L=1.1322 | loss_Q=1.1591 | scale_pen(L)= 1.8798e-03 | scale_pen(Q)= 7.4399e-05 | grad_norm=0.62 | sec/step~1.18 | rms_L~0.7823 rms_Q~0.6505
  step 520/685 | loss_L=1.1363 | loss_Q=1.1206 | scale_pen(L)= 1.8810e-03 | scale_pen(Q)= 7.4147e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7824 rms_Q~0.6506
  step 530/685 | loss_L=1.0497 | loss_Q=1.0531 | scale_pen(L)= 1.8880e-03 | scale_pen(Q)= 7.3516e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.7825 rms_Q~0.6506
  step 540/685 | loss_L=1.1023 | loss_Q=1.1099 | scale_pen(L)= 1.8890e-03 | scale_pen(Q)= 7.3481e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.7825 rms_Q~0.6506
  step 550/685 | loss_L=1.1483 | loss_Q=1.1785 | scale_pen(L)= 1.8923e-03 | scale_pen(Q)= 7.3507e-05 | grad_norm=0.62 | sec/step~1.09 | rms_L~0.7826 rms_Q~0.6507
  step 560/685 | loss_L=1.1374 | loss_Q=1.1335 | scale_pen(L)= 1.8883e-03 | scale_pen(Q)= 7.3839e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.7827 rms_Q~0.6507
  step 570/685 | loss_L=1.1828 | loss_Q=1.1707 | scale_pen(L)= 1.8856e-03 | scale_pen(Q)= 7.5030e-05 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.7828 rms_Q~0.6507
  step 580/685 | loss_L=1.0519 | loss_Q=1.0540 | scale_pen(L)= 1.8901e-03 | scale_pen(Q)= 7.4498e-05 | grad_norm=0.61 | sec/step~1.21 | rms_L~0.7829 rms_Q~0.6508
  step 590/685 | loss_L=1.0540 | loss_Q=1.0400 | scale_pen(L)= 1.8934e-03 | scale_pen(Q)= 7.5220e-05 | grad_norm=0.58 | sec/step~1.11 | rms_L~0.7829 rms_Q~0.6508
  step 600/685 | loss_L=1.0771 | loss_Q=1.0618 | scale_pen(L)= 1.8948e-03 | scale_pen(Q)= 7.5558e-05 | grad_norm=0.60 | sec/step~1.18 | rms_L~0.7830 rms_Q~0.6508
  step 610/685 | loss_L=1.0407 | loss_Q=1.0277 | scale_pen(L)= 1.8972e-03 | scale_pen(Q)= 7.4989e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.7831 rms_Q~0.6508
  step 620/685 | loss_L=1.1275 | loss_Q=1.1103 | scale_pen(L)= 1.8983e-03 | scale_pen(Q)= 7.6756e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.7832 rms_Q~0.6509
  step 630/685 | loss_L=1.2651 | loss_Q=1.2882 | scale_pen(L)= 1.8995e-03 | scale_pen(Q)= 7.7392e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7833 rms_Q~0.6509
  step 640/685 | loss_L=1.1492 | loss_Q=1.1670 | scale_pen(L)= 1.9036e-03 | scale_pen(Q)= 7.4836e-05 | grad_norm=0.62 | sec/step~1.15 | rms_L~0.7833 rms_Q~0.6509
  step 650/685 | loss_L=1.2154 | loss_Q=1.2102 | scale_pen(L)= 1.9080e-03 | scale_pen(Q)= 7.5233e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7834 rms_Q~0.6510
  step 660/685 | loss_L=1.0850 | loss_Q=1.1028 | scale_pen(L)= 1.9097e-03 | scale_pen(Q)= 7.4155e-05 | grad_norm=0.59 | sec/step~1.16 | rms_L~0.7835 rms_Q~0.6510
  step 670/685 | loss_L=1.0541 | loss_Q=1.0522 | scale_pen(L)= 1.9135e-03 | scale_pen(Q)= 7.4387e-05 | grad_norm=0.57 | sec/step~1.08 | rms_L~0.7836 rms_Q~0.6510
  step 680/685 | loss_L=1.1154 | loss_Q=1.1010 | scale_pen(L)= 1.9191e-03 | scale_pen(Q)= 7.5104e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.7836 rms_Q~0.6511
  step 685/685 | loss_L=1.1315 | loss_Q=1.1103 | scale_pen(L)= 1.9195e-03 | scale_pen(Q)= 7.4615e-05 | grad_norm=0.65 | sec/step~0.43 | rms_L~0.7837 rms_Q~0.6511
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.7836888744883294, 'count': 685}, 'qwen': {'rms_mean': 0.6510674400921286, 'count': 685}}

Evaluating epoch 7 checkpoint...
Evaluating: runs/8B_clean_answer/epoch7 -> runs/8B_clean_answer/eval_epoch7
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch7/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch7/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2992.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.760928663807391, 'neutral_chat': 8.80826473560463, 'llama_chat': 8.894494966044178} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch7/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.77755 -> target=0.01057
[debug:llama] adapter.scale=1.0438 | Z.std=1.0007 Z.mean||=16.0105 | prefix.std=0.0106 prefix.mean||=0.6752 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the is a of the of the'
  1: '2. insects,, and insects,, and'
  2: '198 recession the of the  recession the of the  recession'
  3: 'the of the of the of'
  4: '2,3,4,5,6,7'
Saved Llama results to runs/8B_clean_answer/eval_epoch7/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2833.99it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.38it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.470003703283885, 'neutral_chat': 8.551352133195866, 'qwen_chat': 8.423715255247853} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch7/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.65203 -> target=0.01363
[debug:qwen] adapter.scale=1.0086 | Z.std=1.0009 Z.mean||=16.0140 | prefix.std=0.0136 prefix.mean||=0.8153 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: ''
  2: 'to1980s1990s2'
  3: 'of the and and and and and and and and and and'
  4: 'and of the of and of of of of of of of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch7/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3067.69it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5399.81it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.00it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.18s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.024  |  NLL/token (gold): 8.76029359510426
Qwen   EM: 0.000   F1: 0.024  |  NLL/token (gold): 8.422177261461027
Wall clock: 14.61s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 12.74s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.027
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.036

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 17.180128574371338,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 14.611945152282715,
    "llama": {
      "em": 0.0,
      "f1": 0.024258449883449888,
      "nll_token": 8.76029359510426
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.02424750086514793,
      "nll_token": 8.422177261461027
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 12.735944032669067,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.027446613027495383,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.04381263256073,
      "Z_std": 1.0006574392318726,
      "Z_mean_norm": 16.01050567626953,
      "prefix_std": 0.010572738945484161,
      "prefix_mean_norm": 0.6752429008483887,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0086380243301392,
      "Z_std": 1.0008751153945923,
      "Z_mean_norm": 16.013988494873047,
      "prefix_std": 0.013631599955260754,
      "prefix_mean_norm": 0.8152983784675598,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.035559745809745816
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch7/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch7/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.024 | Qwen 0.024


=========================================
EPOCH 8/16
=========================================

Training epoch 8...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6267.17it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3982.25it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.12it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=7, global_step=4795
Epoch 8/1
  step 10/685 | loss_L=1.1424 | loss_Q=1.1396 | scale_pen(L)= 1.9207e-03 | scale_pen(Q)= 7.4140e-05 | grad_norm=0.59 | sec/step~1.11 | rms_L~0.7893 rms_Q~0.6530
  step 20/685 | loss_L=1.0812 | loss_Q=1.1054 | scale_pen(L)= 1.9249e-03 | scale_pen(Q)= 7.6639e-05 | grad_norm=0.62 | sec/step~1.23 | rms_L~0.7894 rms_Q~0.6531
  step 30/685 | loss_L=1.1664 | loss_Q=1.1620 | scale_pen(L)= 1.9258e-03 | scale_pen(Q)= 7.6366e-05 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.7893 rms_Q~0.6531
  step 40/685 | loss_L=1.2031 | loss_Q=1.2110 | scale_pen(L)= 1.9293e-03 | scale_pen(Q)= 7.7707e-05 | grad_norm=0.57 | sec/step~1.08 | rms_L~0.7894 rms_Q~0.6532
  step 50/685 | loss_L=1.0327 | loss_Q=1.0584 | scale_pen(L)= 1.9313e-03 | scale_pen(Q)= 7.7166e-05 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.7894 rms_Q~0.6532
  step 60/685 | loss_L=1.1682 | loss_Q=1.1746 | scale_pen(L)= 1.9373e-03 | scale_pen(Q)= 7.7722e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7895 rms_Q~0.6533
  step 70/685 | loss_L=1.1895 | loss_Q=1.2037 | scale_pen(L)= 1.9321e-03 | scale_pen(Q)= 7.6416e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.7895 rms_Q~0.6533
  step 80/685 | loss_L=1.2015 | loss_Q=1.2438 | scale_pen(L)= 1.9271e-03 | scale_pen(Q)= 7.8662e-05 | grad_norm=0.63 | sec/step~1.23 | rms_L~0.7896 rms_Q~0.6533
  step 90/685 | loss_L=1.0451 | loss_Q=1.0417 | scale_pen(L)= 1.9274e-03 | scale_pen(Q)= 7.8464e-05 | grad_norm=0.55 | sec/step~1.08 | rms_L~0.7896 rms_Q~0.6533
  step 100/685 | loss_L=1.0397 | loss_Q=1.0422 | scale_pen(L)= 1.9259e-03 | scale_pen(Q)= 7.8288e-05 | grad_norm=0.62 | sec/step~1.20 | rms_L~0.7896 rms_Q~0.6533
  step 110/685 | loss_L=1.1047 | loss_Q=1.1209 | scale_pen(L)= 1.9257e-03 | scale_pen(Q)= 7.9406e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.7896 rms_Q~0.6533
  step 120/685 | loss_L=1.1560 | loss_Q=1.1486 | scale_pen(L)= 1.9332e-03 | scale_pen(Q)= 7.8704e-05 | grad_norm=0.62 | sec/step~1.19 | rms_L~0.7896 rms_Q~0.6533
  step 130/685 | loss_L=1.1585 | loss_Q=1.1923 | scale_pen(L)= 1.9369e-03 | scale_pen(Q)= 7.8762e-05 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.7897 rms_Q~0.6533
  step 140/685 | loss_L=1.1301 | loss_Q=1.1361 | scale_pen(L)= 1.9363e-03 | scale_pen(Q)= 7.9033e-05 | grad_norm=0.60 | sec/step~1.16 | rms_L~0.7897 rms_Q~0.6533
  step 150/685 | loss_L=1.1157 | loss_Q=1.1209 | scale_pen(L)= 1.9366e-03 | scale_pen(Q)= 7.8341e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7897 rms_Q~0.6534
  step 160/685 | loss_L=1.0823 | loss_Q=1.0775 | scale_pen(L)= 1.9374e-03 | scale_pen(Q)= 7.8065e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.7898 rms_Q~0.6534
  step 170/685 | loss_L=1.0658 | loss_Q=1.0520 | scale_pen(L)= 1.9419e-03 | scale_pen(Q)= 7.8210e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7898 rms_Q~0.6534
  step 180/685 | loss_L=1.0502 | loss_Q=1.0689 | scale_pen(L)= 1.9399e-03 | scale_pen(Q)= 7.8221e-05 | grad_norm=0.58 | sec/step~1.10 | rms_L~0.7898 rms_Q~0.6534
  step 190/685 | loss_L=1.0428 | loss_Q=1.0233 | scale_pen(L)= 1.9449e-03 | scale_pen(Q)= 7.9058e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.7899 rms_Q~0.6535
  step 200/685 | loss_L=1.3657 | loss_Q=1.3604 | scale_pen(L)= 1.9433e-03 | scale_pen(Q)= 7.7558e-05 | grad_norm=0.59 | sec/step~1.09 | rms_L~0.7900 rms_Q~0.6535
  step 210/685 | loss_L=1.1604 | loss_Q=1.1601 | scale_pen(L)= 1.9472e-03 | scale_pen(Q)= 7.7487e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7900 rms_Q~0.6536
  step 220/685 | loss_L=1.1357 | loss_Q=1.1277 | scale_pen(L)= 1.9496e-03 | scale_pen(Q)= 7.5464e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7901 rms_Q~0.6536
  step 230/685 | loss_L=1.1880 | loss_Q=1.1885 | scale_pen(L)= 1.9491e-03 | scale_pen(Q)= 7.4597e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7902 rms_Q~0.6536
  step 240/685 | loss_L=1.0660 | loss_Q=1.0822 | scale_pen(L)= 1.9545e-03 | scale_pen(Q)= 7.5919e-05 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.7903 rms_Q~0.6537
  step 250/685 | loss_L=1.1982 | loss_Q=1.1975 | scale_pen(L)= 1.9530e-03 | scale_pen(Q)= 7.7166e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7903 rms_Q~0.6537
  step 260/685 | loss_L=1.1317 | loss_Q=1.1578 | scale_pen(L)= 1.9550e-03 | scale_pen(Q)= 7.5804e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7904 rms_Q~0.6537
  step 270/685 | loss_L=1.1460 | loss_Q=1.1430 | scale_pen(L)= 1.9570e-03 | scale_pen(Q)= 7.5765e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.7904 rms_Q~0.6538
  step 280/685 | loss_L=1.1515 | loss_Q=1.1676 | scale_pen(L)= 1.9643e-03 | scale_pen(Q)= 7.6443e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.7905 rms_Q~0.6538
  step 290/685 | loss_L=1.0258 | loss_Q=1.0429 | scale_pen(L)= 1.9656e-03 | scale_pen(Q)= 7.5964e-05 | grad_norm=0.63 | sec/step~1.18 | rms_L~0.7906 rms_Q~0.6538
  step 300/685 | loss_L=1.1518 | loss_Q=1.1534 | scale_pen(L)= 1.9626e-03 | scale_pen(Q)= 7.6748e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7906 rms_Q~0.6539
  step 310/685 | loss_L=1.2953 | loss_Q=1.2881 | scale_pen(L)= 1.9659e-03 | scale_pen(Q)= 7.7126e-05 | grad_norm=0.64 | sec/step~1.14 | rms_L~0.7907 rms_Q~0.6539
  step 320/685 | loss_L=1.2481 | loss_Q=1.2732 | scale_pen(L)= 1.9673e-03 | scale_pen(Q)= 7.9011e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7908 rms_Q~0.6539
  step 330/685 | loss_L=1.0907 | loss_Q=1.1101 | scale_pen(L)= 1.9682e-03 | scale_pen(Q)= 7.9517e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7908 rms_Q~0.6540
  step 340/685 | loss_L=1.2484 | loss_Q=1.2512 | scale_pen(L)= 1.9671e-03 | scale_pen(Q)= 7.6389e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7909 rms_Q~0.6540
  step 350/685 | loss_L=1.1641 | loss_Q=1.1659 | scale_pen(L)= 1.9630e-03 | scale_pen(Q)= 7.6606e-05 | grad_norm=0.65 | sec/step~1.16 | rms_L~0.7909 rms_Q~0.6540
  step 360/685 | loss_L=1.2105 | loss_Q=1.2273 | scale_pen(L)= 1.9707e-03 | scale_pen(Q)= 7.8080e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7910 rms_Q~0.6541
  step 370/685 | loss_L=1.0864 | loss_Q=1.1092 | scale_pen(L)= 1.9712e-03 | scale_pen(Q)= 7.8975e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.7910 rms_Q~0.6541
  step 380/685 | loss_L=1.1558 | loss_Q=1.1607 | scale_pen(L)= 1.9741e-03 | scale_pen(Q)= 7.9304e-05 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.7911 rms_Q~0.6541
  step 390/685 | loss_L=1.0580 | loss_Q=1.0557 | scale_pen(L)= 1.9782e-03 | scale_pen(Q)= 8.1291e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7912 rms_Q~0.6542
  step 400/685 | loss_L=1.1462 | loss_Q=1.1502 | scale_pen(L)= 1.9795e-03 | scale_pen(Q)= 8.1330e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7912 rms_Q~0.6542
  step 410/685 | loss_L=1.1276 | loss_Q=1.1553 | scale_pen(L)= 1.9785e-03 | scale_pen(Q)= 8.2378e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7913 rms_Q~0.6542
  step 420/685 | loss_L=1.1470 | loss_Q=1.1466 | scale_pen(L)= 1.9826e-03 | scale_pen(Q)= 8.5081e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7913 rms_Q~0.6543
  step 430/685 | loss_L=1.0892 | loss_Q=1.0787 | scale_pen(L)= 1.9952e-03 | scale_pen(Q)= 8.5841e-05 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.7914 rms_Q~0.6543
  step 440/685 | loss_L=1.0888 | loss_Q=1.0749 | scale_pen(L)= 1.9976e-03 | scale_pen(Q)= 8.4112e-05 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.7915 rms_Q~0.6543
  step 450/685 | loss_L=0.9510 | loss_Q=0.9508 | scale_pen(L)= 2.0021e-03 | scale_pen(Q)= 8.3113e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7916 rms_Q~0.6544
  step 460/685 | loss_L=1.0797 | loss_Q=1.1047 | scale_pen(L)= 2.0048e-03 | scale_pen(Q)= 8.2411e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7916 rms_Q~0.6544
  step 470/685 | loss_L=0.9694 | loss_Q=0.9677 | scale_pen(L)= 2.0085e-03 | scale_pen(Q)= 8.1851e-05 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.7917 rms_Q~0.6545
  step 480/685 | loss_L=1.0846 | loss_Q=1.0716 | scale_pen(L)= 2.0104e-03 | scale_pen(Q)= 8.0329e-05 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.7918 rms_Q~0.6545
  step 490/685 | loss_L=0.9618 | loss_Q=0.9483 | scale_pen(L)= 2.0066e-03 | scale_pen(Q)= 8.1472e-05 | grad_norm=0.59 | sec/step~1.11 | rms_L~0.7919 rms_Q~0.6545
  step 500/685 | loss_L=1.1697 | loss_Q=1.1951 | scale_pen(L)= 2.0070e-03 | scale_pen(Q)= 7.9028e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.7920 rms_Q~0.6546
  step 510/685 | loss_L=1.0238 | loss_Q=1.0437 | scale_pen(L)= 2.0110e-03 | scale_pen(Q)= 7.9753e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7920 rms_Q~0.6546
  step 520/685 | loss_L=1.1332 | loss_Q=1.1404 | scale_pen(L)= 2.0144e-03 | scale_pen(Q)= 7.9759e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.7921 rms_Q~0.6546
  step 530/685 | loss_L=1.0535 | loss_Q=1.0582 | scale_pen(L)= 2.0117e-03 | scale_pen(Q)= 7.7308e-05 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.7922 rms_Q~0.6547
  step 540/685 | loss_L=1.1085 | loss_Q=1.1180 | scale_pen(L)= 2.0116e-03 | scale_pen(Q)= 7.8183e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.7922 rms_Q~0.6547
  step 550/685 | loss_L=1.1044 | loss_Q=1.1138 | scale_pen(L)= 2.0126e-03 | scale_pen(Q)= 7.9383e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.7923 rms_Q~0.6547
  step 560/685 | loss_L=1.0456 | loss_Q=1.0373 | scale_pen(L)= 2.0139e-03 | scale_pen(Q)= 7.9512e-05 | grad_norm=0.61 | sec/step~1.17 | rms_L~0.7923 rms_Q~0.6547
  step 570/685 | loss_L=1.1467 | loss_Q=1.1666 | scale_pen(L)= 2.0179e-03 | scale_pen(Q)= 8.0549e-05 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.7924 rms_Q~0.6548
  step 580/685 | loss_L=1.0641 | loss_Q=1.0648 | scale_pen(L)= 2.0173e-03 | scale_pen(Q)= 8.0517e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7925 rms_Q~0.6548
  step 590/685 | loss_L=1.0421 | loss_Q=1.0587 | scale_pen(L)= 2.0241e-03 | scale_pen(Q)= 7.9968e-05 | grad_norm=0.64 | sec/step~1.22 | rms_L~0.7925 rms_Q~0.6548
  step 600/685 | loss_L=1.3229 | loss_Q=1.3593 | scale_pen(L)= 2.0194e-03 | scale_pen(Q)= 7.9096e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7926 rms_Q~0.6549
  step 610/685 | loss_L=1.0780 | loss_Q=1.0731 | scale_pen(L)= 2.0239e-03 | scale_pen(Q)= 7.9798e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.7927 rms_Q~0.6549
  step 620/685 | loss_L=1.0498 | loss_Q=1.0560 | scale_pen(L)= 2.0186e-03 | scale_pen(Q)= 8.0282e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.7927 rms_Q~0.6549
  step 630/685 | loss_L=1.2767 | loss_Q=1.2797 | scale_pen(L)= 2.0210e-03 | scale_pen(Q)= 7.8918e-05 | grad_norm=0.62 | sec/step~1.09 | rms_L~0.7928 rms_Q~0.6549
  step 640/685 | loss_L=1.1437 | loss_Q=1.1479 | scale_pen(L)= 2.0255e-03 | scale_pen(Q)= 7.8669e-05 | grad_norm=0.59 | sec/step~1.20 | rms_L~0.7928 rms_Q~0.6550
  step 650/685 | loss_L=1.1261 | loss_Q=1.1420 | scale_pen(L)= 2.0300e-03 | scale_pen(Q)= 8.0145e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.7929 rms_Q~0.6550
  step 660/685 | loss_L=1.1458 | loss_Q=1.1598 | scale_pen(L)= 2.0331e-03 | scale_pen(Q)= 8.2441e-05 | grad_norm=0.60 | sec/step~1.21 | rms_L~0.7929 rms_Q~0.6550
  step 670/685 | loss_L=1.1443 | loss_Q=1.1603 | scale_pen(L)= 2.0319e-03 | scale_pen(Q)= 8.2305e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7930 rms_Q~0.6550
  step 680/685 | loss_L=1.0846 | loss_Q=1.1069 | scale_pen(L)= 2.0382e-03 | scale_pen(Q)= 8.3255e-05 | grad_norm=0.62 | sec/step~1.25 | rms_L~0.7931 rms_Q~0.6551
  step 685/685 | loss_L=1.2019 | loss_Q=1.2012 | scale_pen(L)= 2.0394e-03 | scale_pen(Q)= 8.2987e-05 | grad_norm=0.63 | sec/step~0.43 | rms_L~0.7931 rms_Q~0.6551
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.7931038057717094, 'count': 685}, 'qwen': {'rms_mean': 0.6550806089909408, 'count': 685}}

Evaluating epoch 8 checkpoint...
Evaluating: runs/8B_clean_answer/epoch8 -> runs/8B_clean_answer/eval_epoch8
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch8/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch8/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3236.35it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.764648434256209, 'neutral_chat': 8.775281779620112, 'llama_chat': 8.868539097628085} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch8/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.78692 -> target=0.01057
[debug:llama] adapter.scale=1.0452 | Z.std=1.0003 Z.mean||=16.0049 | prefix.std=0.0106 prefix.mean||=0.6752 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the is a of the of the'
  1: '2 species species species species species species species species species'
  2: '198 housing the of the, the of the and the of'
  3: 'the of the, the of the and the of the'
  4: '3,4,5,6,7,8'
Saved Llama results to runs/8B_clean_answer/eval_epoch8/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3080.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.35it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.37it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.424558255722914, 'neutral_chat': 8.463317002253557, 'qwen_chat': 8.362802903803567} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch8/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.65594 -> target=0.01363
[debug:qwen] adapter.scale=1.0091 | Z.std=1.0005 Z.mean||=16.0080 | prefix.std=0.0136 prefix.mean||=0.8152 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '1960s the of and and and and and'
  1: ''
  2: 'and of the 1990s the of the'
  3: 'and the of the and the of the and the of the'
  4: 'and of the of the of the of the of the of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch8/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3377.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.09it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5853.88it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.33it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.17it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 17.55s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.033  |  NLL/token (gold): 8.769114638131763
Qwen   EM: 0.000   F1: 0.019  |  NLL/token (gold): 8.362961448374248
Wall clock: 16.19s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 13.95s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.024
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.039

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 17.547767639160156,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 16.187195539474487,
    "llama": {
      "em": 0.0,
      "f1": 0.033436553642436,
      "nll_token": 8.769114638131763
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.018833650826297885,
      "nll_token": 8.362961448374248
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 13.945641040802002,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.02421295779384015,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0451595783233643,
      "Z_std": 1.0003076791763306,
      "Z_mean_norm": 16.004907608032227,
      "prefix_std": 0.010572732426226139,
      "prefix_mean_norm": 0.6751546859741211,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0091097354888916,
      "Z_std": 1.000500202178955,
      "Z_mean_norm": 16.00798797607422,
      "prefix_std": 0.013631533831357956,
      "prefix_mean_norm": 0.8152260184288025,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03946036316624552
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch8/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch8/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.033 | Qwen 0.019


=========================================
EPOCH 9/16
=========================================

Training epoch 9...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4748.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3245.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.22it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=8, global_step=5480
Epoch 9/1
  step 10/685 | loss_L=1.1300 | loss_Q=1.1401 | scale_pen(L)= 2.0394e-03 | scale_pen(Q)= 8.1840e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7976 rms_Q~0.6569
  step 20/685 | loss_L=1.0995 | loss_Q=1.0996 | scale_pen(L)= 2.0376e-03 | scale_pen(Q)= 8.2385e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.7976 rms_Q~0.6569
  step 30/685 | loss_L=0.9946 | loss_Q=1.0091 | scale_pen(L)= 2.0434e-03 | scale_pen(Q)= 8.1786e-05 | grad_norm=0.57 | sec/step~1.08 | rms_L~0.7976 rms_Q~0.6569
  step 40/685 | loss_L=1.0521 | loss_Q=1.0472 | scale_pen(L)= 2.0495e-03 | scale_pen(Q)= 8.4379e-05 | grad_norm=0.63 | sec/step~1.19 | rms_L~0.7976 rms_Q~0.6570
  step 50/685 | loss_L=1.1007 | loss_Q=1.1111 | scale_pen(L)= 2.0522e-03 | scale_pen(Q)= 8.3185e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.7977 rms_Q~0.6570
  step 60/685 | loss_L=1.0710 | loss_Q=1.0743 | scale_pen(L)= 2.0573e-03 | scale_pen(Q)= 8.4611e-05 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.7977 rms_Q~0.6570
  step 70/685 | loss_L=0.9878 | loss_Q=1.0007 | scale_pen(L)= 2.0541e-03 | scale_pen(Q)= 8.6040e-05 | grad_norm=0.56 | sec/step~1.19 | rms_L~0.7979 rms_Q~0.6571
  step 80/685 | loss_L=1.0981 | loss_Q=1.1019 | scale_pen(L)= 2.0547e-03 | scale_pen(Q)= 8.5372e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.7979 rms_Q~0.6571
  step 90/685 | loss_L=1.2179 | loss_Q=1.2289 | scale_pen(L)= 2.0595e-03 | scale_pen(Q)= 8.5532e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.7980 rms_Q~0.6572
  step 100/685 | loss_L=1.3341 | loss_Q=1.3453 | scale_pen(L)= 2.0621e-03 | scale_pen(Q)= 8.5828e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7981 rms_Q~0.6572
  step 110/685 | loss_L=1.0340 | loss_Q=1.0436 | scale_pen(L)= 2.0637e-03 | scale_pen(Q)= 8.3832e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7982 rms_Q~0.6573
  step 120/685 | loss_L=1.1407 | loss_Q=1.1427 | scale_pen(L)= 2.0667e-03 | scale_pen(Q)= 8.3016e-05 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.7983 rms_Q~0.6573
  step 130/685 | loss_L=1.1356 | loss_Q=1.1272 | scale_pen(L)= 2.0699e-03 | scale_pen(Q)= 8.1918e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.7984 rms_Q~0.6574
  step 140/685 | loss_L=1.2018 | loss_Q=1.2117 | scale_pen(L)= 2.0676e-03 | scale_pen(Q)= 8.3185e-05 | grad_norm=0.64 | sec/step~1.14 | rms_L~0.7984 rms_Q~0.6574
  step 150/685 | loss_L=1.1942 | loss_Q=1.2121 | scale_pen(L)= 2.0674e-03 | scale_pen(Q)= 8.1108e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.7985 rms_Q~0.6574
  step 160/685 | loss_L=1.2069 | loss_Q=1.2132 | scale_pen(L)= 2.0669e-03 | scale_pen(Q)= 8.1083e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.7986 rms_Q~0.6574
  step 170/685 | loss_L=1.1930 | loss_Q=1.2022 | scale_pen(L)= 2.0696e-03 | scale_pen(Q)= 8.0607e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.7986 rms_Q~0.6574
  step 180/685 | loss_L=0.9813 | loss_Q=0.9766 | scale_pen(L)= 2.0765e-03 | scale_pen(Q)= 7.9970e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7987 rms_Q~0.6575
  step 190/685 | loss_L=1.2562 | loss_Q=1.2762 | scale_pen(L)= 2.0743e-03 | scale_pen(Q)= 8.0825e-05 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.7988 rms_Q~0.6575
  step 200/685 | loss_L=1.1328 | loss_Q=1.1518 | scale_pen(L)= 2.0768e-03 | scale_pen(Q)= 8.1336e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.7988 rms_Q~0.6575
  step 210/685 | loss_L=1.1940 | loss_Q=1.1833 | scale_pen(L)= 2.0774e-03 | scale_pen(Q)= 8.1849e-05 | grad_norm=0.62 | sec/step~1.17 | rms_L~0.7989 rms_Q~0.6576
  step 220/685 | loss_L=1.1604 | loss_Q=1.1562 | scale_pen(L)= 2.0816e-03 | scale_pen(Q)= 8.2021e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7990 rms_Q~0.6576
  step 230/685 | loss_L=1.0614 | loss_Q=1.0681 | scale_pen(L)= 2.0860e-03 | scale_pen(Q)= 8.0855e-05 | grad_norm=0.64 | sec/step~1.21 | rms_L~0.7990 rms_Q~0.6576
  step 240/685 | loss_L=1.1041 | loss_Q=1.1110 | scale_pen(L)= 2.0855e-03 | scale_pen(Q)= 8.0924e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.7991 rms_Q~0.6576
  step 250/685 | loss_L=1.1647 | loss_Q=1.1595 | scale_pen(L)= 2.0880e-03 | scale_pen(Q)= 8.0928e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.7992 rms_Q~0.6577
  step 260/685 | loss_L=1.0701 | loss_Q=1.1026 | scale_pen(L)= 2.0803e-03 | scale_pen(Q)= 8.0455e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.7992 rms_Q~0.6577
  step 270/685 | loss_L=1.0560 | loss_Q=1.0802 | scale_pen(L)= 2.0840e-03 | scale_pen(Q)= 8.1457e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.7993 rms_Q~0.6577
  step 280/685 | loss_L=1.0374 | loss_Q=1.0379 | scale_pen(L)= 2.0778e-03 | scale_pen(Q)= 8.0607e-05 | grad_norm=0.59 | sec/step~1.05 | rms_L~0.7994 rms_Q~0.6578
  step 290/685 | loss_L=1.1663 | loss_Q=1.1952 | scale_pen(L)= 2.0846e-03 | scale_pen(Q)= 8.0866e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.7994 rms_Q~0.6578
  step 300/685 | loss_L=0.9896 | loss_Q=0.9911 | scale_pen(L)= 2.0904e-03 | scale_pen(Q)= 8.1961e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.7995 rms_Q~0.6578
  step 310/685 | loss_L=1.0904 | loss_Q=1.0867 | scale_pen(L)= 2.0919e-03 | scale_pen(Q)= 8.1205e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.7996 rms_Q~0.6579
  step 320/685 | loss_L=1.1165 | loss_Q=1.1285 | scale_pen(L)= 2.0906e-03 | scale_pen(Q)= 8.0731e-05 | grad_norm=0.63 | sec/step~1.08 | rms_L~0.7996 rms_Q~0.6579
  step 330/685 | loss_L=1.1369 | loss_Q=1.1402 | scale_pen(L)= 2.0955e-03 | scale_pen(Q)= 7.8652e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.7997 rms_Q~0.6579
  step 340/685 | loss_L=1.0247 | loss_Q=1.0394 | scale_pen(L)= 2.1011e-03 | scale_pen(Q)= 7.9315e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.7998 rms_Q~0.6580
  step 350/685 | loss_L=1.0977 | loss_Q=1.0934 | scale_pen(L)= 2.0970e-03 | scale_pen(Q)= 8.0577e-05 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.7998 rms_Q~0.6580
  step 360/685 | loss_L=1.1048 | loss_Q=1.1217 | scale_pen(L)= 2.0995e-03 | scale_pen(Q)= 7.8700e-05 | grad_norm=0.64 | sec/step~1.14 | rms_L~0.7999 rms_Q~0.6580
  step 370/685 | loss_L=1.0509 | loss_Q=1.0636 | scale_pen(L)= 2.0970e-03 | scale_pen(Q)= 8.2114e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.7999 rms_Q~0.6581
  step 380/685 | loss_L=1.0964 | loss_Q=1.0864 | scale_pen(L)= 2.0966e-03 | scale_pen(Q)= 8.3981e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.8000 rms_Q~0.6581
  step 390/685 | loss_L=1.0281 | loss_Q=1.0192 | scale_pen(L)= 2.1005e-03 | scale_pen(Q)= 8.4943e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8001 rms_Q~0.6581
  step 400/685 | loss_L=1.1296 | loss_Q=1.1331 | scale_pen(L)= 2.0980e-03 | scale_pen(Q)= 8.5992e-05 | grad_norm=0.63 | sec/step~1.14 | rms_L~0.8001 rms_Q~0.6581
  step 410/685 | loss_L=1.2424 | loss_Q=1.2435 | scale_pen(L)= 2.0974e-03 | scale_pen(Q)= 8.6417e-05 | grad_norm=0.56 | sec/step~1.15 | rms_L~0.8002 rms_Q~0.6582
  step 420/685 | loss_L=1.0204 | loss_Q=1.0273 | scale_pen(L)= 2.1003e-03 | scale_pen(Q)= 8.5738e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8002 rms_Q~0.6582
  step 430/685 | loss_L=1.0387 | loss_Q=1.0390 | scale_pen(L)= 2.0967e-03 | scale_pen(Q)= 8.5429e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8003 rms_Q~0.6582
  step 440/685 | loss_L=1.0903 | loss_Q=1.1146 | scale_pen(L)= 2.0997e-03 | scale_pen(Q)= 8.6599e-05 | grad_norm=0.65 | sec/step~1.22 | rms_L~0.8003 rms_Q~0.6583
  step 450/685 | loss_L=1.0575 | loss_Q=1.0705 | scale_pen(L)= 2.0947e-03 | scale_pen(Q)= 8.5532e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8004 rms_Q~0.6583
  step 460/685 | loss_L=1.1465 | loss_Q=1.1506 | scale_pen(L)= 2.0926e-03 | scale_pen(Q)= 8.5231e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8004 rms_Q~0.6583
  step 470/685 | loss_L=1.0833 | loss_Q=1.0940 | scale_pen(L)= 2.1011e-03 | scale_pen(Q)= 8.4910e-05 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.8005 rms_Q~0.6584
  step 480/685 | loss_L=1.1124 | loss_Q=1.0975 | scale_pen(L)= 2.1045e-03 | scale_pen(Q)= 8.5923e-05 | grad_norm=0.60 | sec/step~1.14 | rms_L~0.8005 rms_Q~0.6584
  step 490/685 | loss_L=1.1882 | loss_Q=1.2101 | scale_pen(L)= 2.1109e-03 | scale_pen(Q)= 8.5513e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8006 rms_Q~0.6584
  step 500/685 | loss_L=1.0071 | loss_Q=1.0158 | scale_pen(L)= 2.1091e-03 | scale_pen(Q)= 8.5603e-05 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8006 rms_Q~0.6584
  step 510/685 | loss_L=1.1116 | loss_Q=1.1247 | scale_pen(L)= 2.1081e-03 | scale_pen(Q)= 8.4243e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8007 rms_Q~0.6585
  step 520/685 | loss_L=1.0715 | loss_Q=1.0661 | scale_pen(L)= 2.1115e-03 | scale_pen(Q)= 8.5689e-05 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.8007 rms_Q~0.6585
  step 530/685 | loss_L=0.9968 | loss_Q=1.0200 | scale_pen(L)= 2.1181e-03 | scale_pen(Q)= 8.4528e-05 | grad_norm=0.58 | sec/step~1.14 | rms_L~0.8008 rms_Q~0.6585
  step 540/685 | loss_L=0.9724 | loss_Q=0.9752 | scale_pen(L)= 2.1258e-03 | scale_pen(Q)= 8.5281e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8009 rms_Q~0.6586
  step 550/685 | loss_L=1.0802 | loss_Q=1.0975 | scale_pen(L)= 2.1222e-03 | scale_pen(Q)= 8.6905e-05 | grad_norm=0.62 | sec/step~1.15 | rms_L~0.8009 rms_Q~0.6586
  step 560/685 | loss_L=1.1557 | loss_Q=1.1460 | scale_pen(L)= 2.1254e-03 | scale_pen(Q)= 8.8121e-05 | grad_norm=0.57 | sec/step~1.07 | rms_L~0.8010 rms_Q~0.6586
  step 570/685 | loss_L=1.1945 | loss_Q=1.2132 | scale_pen(L)= 2.1268e-03 | scale_pen(Q)= 8.8818e-05 | grad_norm=0.62 | sec/step~1.22 | rms_L~0.8010 rms_Q~0.6587
  step 580/685 | loss_L=1.1239 | loss_Q=1.1103 | scale_pen(L)= 2.1273e-03 | scale_pen(Q)= 8.8324e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8011 rms_Q~0.6587
  step 590/685 | loss_L=1.0518 | loss_Q=1.0130 | scale_pen(L)= 2.1242e-03 | scale_pen(Q)= 8.7272e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8012 rms_Q~0.6587
  step 600/685 | loss_L=1.1033 | loss_Q=1.0878 | scale_pen(L)= 2.1204e-03 | scale_pen(Q)= 8.7368e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8012 rms_Q~0.6588
  step 610/685 | loss_L=1.1100 | loss_Q=1.1066 | scale_pen(L)= 2.1267e-03 | scale_pen(Q)= 8.5466e-05 | grad_norm=0.57 | sec/step~1.14 | rms_L~0.8013 rms_Q~0.6588
  step 620/685 | loss_L=1.0767 | loss_Q=1.0784 | scale_pen(L)= 2.1279e-03 | scale_pen(Q)= 8.5753e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8014 rms_Q~0.6588
  step 630/685 | loss_L=1.0520 | loss_Q=1.0752 | scale_pen(L)= 2.1314e-03 | scale_pen(Q)= 8.7551e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8014 rms_Q~0.6589
  step 640/685 | loss_L=0.9981 | loss_Q=1.0052 | scale_pen(L)= 2.1302e-03 | scale_pen(Q)= 8.7216e-05 | grad_norm=0.59 | sec/step~1.26 | rms_L~0.8015 rms_Q~0.6589
  step 650/685 | loss_L=1.0929 | loss_Q=1.1015 | scale_pen(L)= 2.1327e-03 | scale_pen(Q)= 8.6676e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.8015 rms_Q~0.6589
  step 660/685 | loss_L=1.1863 | loss_Q=1.1959 | scale_pen(L)= 2.1303e-03 | scale_pen(Q)= 8.7698e-05 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8016 rms_Q~0.6590
  step 670/685 | loss_L=1.0588 | loss_Q=1.0340 | scale_pen(L)= 2.1326e-03 | scale_pen(Q)= 9.0806e-05 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.8017 rms_Q~0.6590
  step 680/685 | loss_L=1.1767 | loss_Q=1.2029 | scale_pen(L)= 2.1355e-03 | scale_pen(Q)= 8.9372e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8017 rms_Q~0.6590
  step 685/685 | loss_L=1.1549 | loss_Q=1.1634 | scale_pen(L)= 2.1325e-03 | scale_pen(Q)= 8.9757e-05 | grad_norm=0.65 | sec/step~0.43 | rms_L~0.8017 rms_Q~0.6590
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8017428330261341, 'count': 685}, 'qwen': {'rms_mean': 0.6590477317789175, 'count': 685}}

Evaluating epoch 9 checkpoint...
Evaluating: runs/8B_clean_answer/epoch9 -> runs/8B_clean_answer/eval_epoch9
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch9/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch9/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3262.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.847066911169732, 'neutral_chat': 8.841229123052827, 'llama_chat': 8.960519896072595} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch9/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.80488 -> target=0.01057
[debug:llama] adapter.scale=1.0462 | Z.std=1.0002 Z.mean||=16.0030 | prefix.std=0.0106 prefix.mean||=0.6752 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the is the of the, the of'
  1: '2 species of and species of, and species of'
  2: '2. housing costs housing costs housing housing housing housing'
  3: '2. the of the, a of the and'
  4: '2,3,4,5,6,7'
Saved Llama results to runs/8B_clean_answer/eval_epoch9/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3300.00it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.41it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.39it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.351349129563285, 'neutral_chat': 8.408664250499987, 'qwen_chat': 8.316538513652862} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch9/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.66042 -> target=0.01363
[debug:qwen] adapter.scale=1.0095 | Z.std=1.0002 Z.mean||=16.0025 | prefix.std=0.0136 prefix.mean||=0.8152 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: ''
  2: 'and of the 1990s the of the'
  3: 'and of the city, the and of the city, the'
  4: 'and of the of and of of and of of of of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch9/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2875.77it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7087.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.37it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.09it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.47s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.030  |  NLL/token (gold): 8.840766210404654
Qwen   EM: 0.000   F1: 0.019  |  NLL/token (gold): 8.30532878794998
Wall clock: 13.56s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.72s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.022
Inter-model agreement (normalized): 0.005
Oracle upper bound:  EM 0.000  F1 0.035

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.46709394454956,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.558192729949951,
    "llama": {
      "em": 0.0,
      "f1": 0.029643550893550894,
      "nll_token": 8.840766210404654
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.019081751581751585,
      "nll_token": 8.30532878794998
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.721620798110962,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.022419053169053172,
    "agreement": 0.005,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.046178936958313,
      "Z_std": 1.000191330909729,
      "Z_mean_norm": 16.0030460357666,
      "prefix_std": 0.010572722181677818,
      "prefix_mean_norm": 0.6751720905303955,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0094740390777588,
      "Z_std": 1.000157117843628,
      "Z_mean_norm": 16.002500534057617,
      "prefix_std": 0.013631480745971203,
      "prefix_mean_norm": 0.8151574730873108,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.035405455655455664
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch9/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch9/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.019


=========================================
EPOCH 10/16
=========================================

Training epoch 10...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6136.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3824.30it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.43it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=9, global_step=6165
Epoch 10/1
  step 10/685 | loss_L=1.0590 | loss_Q=1.0649 | scale_pen(L)= 2.1360e-03 | scale_pen(Q)= 9.0464e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8058 rms_Q~0.6614
  step 20/685 | loss_L=1.1242 | loss_Q=1.1092 | scale_pen(L)= 2.1402e-03 | scale_pen(Q)= 8.9396e-05 | grad_norm=0.65 | sec/step~1.13 | rms_L~0.8058 rms_Q~0.6614
  step 30/685 | loss_L=1.1311 | loss_Q=1.1432 | scale_pen(L)= 2.1389e-03 | scale_pen(Q)= 8.7513e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8057 rms_Q~0.6614
  step 40/685 | loss_L=1.1563 | loss_Q=1.1631 | scale_pen(L)= 2.1465e-03 | scale_pen(Q)= 8.6501e-05 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.8058 rms_Q~0.6615
  step 50/685 | loss_L=1.0255 | loss_Q=1.0183 | scale_pen(L)= 2.1474e-03 | scale_pen(Q)= 8.6579e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8058 rms_Q~0.6615
  step 60/685 | loss_L=1.0530 | loss_Q=1.0685 | scale_pen(L)= 2.1460e-03 | scale_pen(Q)= 8.6282e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8059 rms_Q~0.6615
  step 70/685 | loss_L=1.1395 | loss_Q=1.1514 | scale_pen(L)= 2.1465e-03 | scale_pen(Q)= 8.6914e-05 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.8060 rms_Q~0.6615
  step 80/685 | loss_L=1.2240 | loss_Q=1.2277 | scale_pen(L)= 2.1457e-03 | scale_pen(Q)= 8.5367e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.8060 rms_Q~0.6616
  step 90/685 | loss_L=1.1657 | loss_Q=1.1793 | scale_pen(L)= 2.1489e-03 | scale_pen(Q)= 8.4885e-05 | grad_norm=0.63 | sec/step~1.17 | rms_L~0.8061 rms_Q~0.6616
  step 100/685 | loss_L=1.0990 | loss_Q=1.1087 | scale_pen(L)= 2.1475e-03 | scale_pen(Q)= 8.5345e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8062 rms_Q~0.6616
  step 110/685 | loss_L=1.1176 | loss_Q=1.1419 | scale_pen(L)= 2.1488e-03 | scale_pen(Q)= 8.3194e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.8062 rms_Q~0.6617
  step 120/685 | loss_L=1.0450 | loss_Q=1.0633 | scale_pen(L)= 2.1564e-03 | scale_pen(Q)= 8.3281e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8063 rms_Q~0.6617
  step 130/685 | loss_L=1.1199 | loss_Q=1.1023 | scale_pen(L)= 2.1590e-03 | scale_pen(Q)= 8.3963e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8064 rms_Q~0.6617
  step 140/685 | loss_L=1.0745 | loss_Q=1.0872 | scale_pen(L)= 2.1611e-03 | scale_pen(Q)= 8.2723e-05 | grad_norm=0.63 | sec/step~1.16 | rms_L~0.8065 rms_Q~0.6618
  step 150/685 | loss_L=1.2331 | loss_Q=1.2551 | scale_pen(L)= 2.1649e-03 | scale_pen(Q)= 8.3037e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8065 rms_Q~0.6618
  step 160/685 | loss_L=1.0938 | loss_Q=1.0934 | scale_pen(L)= 2.1633e-03 | scale_pen(Q)= 8.2957e-05 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8066 rms_Q~0.6619
  step 170/685 | loss_L=1.0922 | loss_Q=1.0976 | scale_pen(L)= 2.1660e-03 | scale_pen(Q)= 8.4291e-05 | grad_norm=0.64 | sec/step~1.09 | rms_L~0.8066 rms_Q~0.6619
  step 180/685 | loss_L=1.0311 | loss_Q=1.0222 | scale_pen(L)= 2.1614e-03 | scale_pen(Q)= 8.3978e-05 | grad_norm=0.64 | sec/step~1.15 | rms_L~0.8067 rms_Q~0.6619
  step 190/685 | loss_L=1.0697 | loss_Q=1.0807 | scale_pen(L)= 2.1624e-03 | scale_pen(Q)= 8.3266e-05 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.8068 rms_Q~0.6620
  step 200/685 | loss_L=1.0123 | loss_Q=0.9917 | scale_pen(L)= 2.1680e-03 | scale_pen(Q)= 8.2790e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8068 rms_Q~0.6620
  step 210/685 | loss_L=1.2037 | loss_Q=1.2152 | scale_pen(L)= 2.1659e-03 | scale_pen(Q)= 8.3057e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8069 rms_Q~0.6620
  step 220/685 | loss_L=1.1640 | loss_Q=1.1799 | scale_pen(L)= 2.1627e-03 | scale_pen(Q)= 8.2777e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8069 rms_Q~0.6621
  step 230/685 | loss_L=1.1147 | loss_Q=1.1245 | scale_pen(L)= 2.1665e-03 | scale_pen(Q)= 8.3289e-05 | grad_norm=0.63 | sec/step~1.15 | rms_L~0.8069 rms_Q~0.6621
  step 240/685 | loss_L=1.0082 | loss_Q=1.0055 | scale_pen(L)= 2.1699e-03 | scale_pen(Q)= 8.3494e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8070 rms_Q~0.6621
  step 250/685 | loss_L=1.3441 | loss_Q=1.3753 | scale_pen(L)= 2.1732e-03 | scale_pen(Q)= 8.4180e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8070 rms_Q~0.6621
  step 260/685 | loss_L=1.3408 | loss_Q=1.3494 | scale_pen(L)= 2.1690e-03 | scale_pen(Q)= 8.4263e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8071 rms_Q~0.6622
  step 270/685 | loss_L=1.0467 | loss_Q=1.0488 | scale_pen(L)= 2.1655e-03 | scale_pen(Q)= 8.3712e-05 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8071 rms_Q~0.6622
  step 280/685 | loss_L=1.0117 | loss_Q=1.0220 | scale_pen(L)= 2.1608e-03 | scale_pen(Q)= 8.3363e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8072 rms_Q~0.6622
  step 290/685 | loss_L=1.0459 | loss_Q=1.0492 | scale_pen(L)= 2.1701e-03 | scale_pen(Q)= 8.4215e-05 | grad_norm=0.65 | sec/step~1.13 | rms_L~0.8072 rms_Q~0.6622
  step 300/685 | loss_L=1.1342 | loss_Q=1.1503 | scale_pen(L)= 2.1672e-03 | scale_pen(Q)= 8.5480e-05 | grad_norm=0.60 | sec/step~1.18 | rms_L~0.8073 rms_Q~0.6623
  step 310/685 | loss_L=1.1941 | loss_Q=1.1959 | scale_pen(L)= 2.1721e-03 | scale_pen(Q)= 8.5475e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8073 rms_Q~0.6623
  step 320/685 | loss_L=1.1401 | loss_Q=1.1548 | scale_pen(L)= 2.1624e-03 | scale_pen(Q)= 8.5398e-05 | grad_norm=0.61 | sec/step~1.14 | rms_L~0.8074 rms_Q~0.6623
  step 330/685 | loss_L=1.1345 | loss_Q=1.1349 | scale_pen(L)= 2.1627e-03 | scale_pen(Q)= 8.6118e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8074 rms_Q~0.6624
  step 340/685 | loss_L=1.0552 | loss_Q=1.0534 | scale_pen(L)= 2.1661e-03 | scale_pen(Q)= 8.7611e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8074 rms_Q~0.6624
  step 350/685 | loss_L=0.9749 | loss_Q=0.9636 | scale_pen(L)= 2.1736e-03 | scale_pen(Q)= 8.8062e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.8075 rms_Q~0.6624
  step 360/685 | loss_L=1.2259 | loss_Q=1.2570 | scale_pen(L)= 2.1763e-03 | scale_pen(Q)= 8.7745e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8075 rms_Q~0.6624
  step 370/685 | loss_L=1.0882 | loss_Q=1.0939 | scale_pen(L)= 2.1781e-03 | scale_pen(Q)= 8.9514e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8075 rms_Q~0.6625
  step 380/685 | loss_L=0.9963 | loss_Q=1.0066 | scale_pen(L)= 2.1793e-03 | scale_pen(Q)= 9.0106e-05 | grad_norm=0.64 | sec/step~1.17 | rms_L~0.8076 rms_Q~0.6625
  step 390/685 | loss_L=1.1042 | loss_Q=1.1120 | scale_pen(L)= 2.1753e-03 | scale_pen(Q)= 9.2670e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8076 rms_Q~0.6625
  step 400/685 | loss_L=1.0191 | loss_Q=1.0299 | scale_pen(L)= 2.1813e-03 | scale_pen(Q)= 9.4371e-05 | grad_norm=0.59 | sec/step~1.16 | rms_L~0.8077 rms_Q~0.6626
  step 410/685 | loss_L=1.0958 | loss_Q=1.0948 | scale_pen(L)= 2.1818e-03 | scale_pen(Q)= 9.3777e-05 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.8077 rms_Q~0.6626
  step 420/685 | loss_L=1.1107 | loss_Q=1.1110 | scale_pen(L)= 2.1839e-03 | scale_pen(Q)= 9.5384e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8078 rms_Q~0.6626
  step 430/685 | loss_L=1.1753 | loss_Q=1.1639 | scale_pen(L)= 2.1874e-03 | scale_pen(Q)= 9.2998e-05 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8078 rms_Q~0.6627
  step 440/685 | loss_L=1.2137 | loss_Q=1.2351 | scale_pen(L)= 2.1848e-03 | scale_pen(Q)= 9.1925e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8079 rms_Q~0.6627
  step 450/685 | loss_L=1.0721 | loss_Q=1.0850 | scale_pen(L)= 2.1915e-03 | scale_pen(Q)= 9.3795e-05 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8079 rms_Q~0.6627
  step 460/685 | loss_L=1.1728 | loss_Q=1.1950 | scale_pen(L)= 2.1908e-03 | scale_pen(Q)= 9.1898e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8080 rms_Q~0.6627
  step 470/685 | loss_L=1.1947 | loss_Q=1.2011 | scale_pen(L)= 2.1860e-03 | scale_pen(Q)= 9.1368e-05 | grad_norm=0.62 | sec/step~1.22 | rms_L~0.8080 rms_Q~0.6628
  step 480/685 | loss_L=1.0726 | loss_Q=1.0758 | scale_pen(L)= 2.1881e-03 | scale_pen(Q)= 9.1836e-05 | grad_norm=0.63 | sec/step~1.08 | rms_L~0.8081 rms_Q~0.6628
  step 490/685 | loss_L=1.0206 | loss_Q=1.0345 | scale_pen(L)= 2.1883e-03 | scale_pen(Q)= 9.2463e-05 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8081 rms_Q~0.6628
  step 500/685 | loss_L=1.0204 | loss_Q=1.0307 | scale_pen(L)= 2.1915e-03 | scale_pen(Q)= 9.4200e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8082 rms_Q~0.6629
  step 510/685 | loss_L=1.0962 | loss_Q=1.0925 | scale_pen(L)= 2.1891e-03 | scale_pen(Q)= 9.4332e-05 | grad_norm=0.61 | sec/step~1.16 | rms_L~0.8082 rms_Q~0.6629
  step 520/685 | loss_L=1.0289 | loss_Q=1.0311 | scale_pen(L)= 2.1899e-03 | scale_pen(Q)= 9.3786e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8082 rms_Q~0.6629
  step 530/685 | loss_L=1.0668 | loss_Q=1.0928 | scale_pen(L)= 2.1959e-03 | scale_pen(Q)= 9.4642e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8083 rms_Q~0.6629
  step 540/685 | loss_L=1.2293 | loss_Q=1.2306 | scale_pen(L)= 2.1916e-03 | scale_pen(Q)= 9.5890e-05 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8083 rms_Q~0.6630
  step 550/685 | loss_L=1.1129 | loss_Q=1.1323 | scale_pen(L)= 2.1889e-03 | scale_pen(Q)= 9.5775e-05 | grad_norm=0.59 | sec/step~1.14 | rms_L~0.8084 rms_Q~0.6630
  step 560/685 | loss_L=1.1220 | loss_Q=1.1389 | scale_pen(L)= 2.1961e-03 | scale_pen(Q)= 9.4575e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8084 rms_Q~0.6630
  step 570/685 | loss_L=1.1823 | loss_Q=1.2070 | scale_pen(L)= 2.2004e-03 | scale_pen(Q)= 9.4513e-05 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.8085 rms_Q~0.6630
  step 580/685 | loss_L=1.1664 | loss_Q=1.1597 | scale_pen(L)= 2.1979e-03 | scale_pen(Q)= 9.3237e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8085 rms_Q~0.6631
  step 590/685 | loss_L=1.0852 | loss_Q=1.0818 | scale_pen(L)= 2.1989e-03 | scale_pen(Q)= 9.3567e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8086 rms_Q~0.6631
  step 600/685 | loss_L=1.1212 | loss_Q=1.1373 | scale_pen(L)= 2.2042e-03 | scale_pen(Q)= 9.3260e-05 | grad_norm=0.62 | sec/step~1.17 | rms_L~0.8086 rms_Q~0.6631
  step 610/685 | loss_L=1.1881 | loss_Q=1.2033 | scale_pen(L)= 2.2068e-03 | scale_pen(Q)= 9.3274e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8087 rms_Q~0.6632
  step 620/685 | loss_L=1.1855 | loss_Q=1.1934 | scale_pen(L)= 2.2027e-03 | scale_pen(Q)= 9.3742e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8087 rms_Q~0.6632
  step 630/685 | loss_L=0.9134 | loss_Q=0.9202 | scale_pen(L)= 2.2062e-03 | scale_pen(Q)= 9.3925e-05 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8088 rms_Q~0.6632
  step 640/685 | loss_L=1.0002 | loss_Q=1.0219 | scale_pen(L)= 2.2092e-03 | scale_pen(Q)= 9.3613e-05 | grad_norm=0.64 | sec/step~1.09 | rms_L~0.8088 rms_Q~0.6633
  step 650/685 | loss_L=1.1386 | loss_Q=1.1639 | scale_pen(L)= 2.2132e-03 | scale_pen(Q)= 9.3332e-05 | grad_norm=0.63 | sec/step~1.19 | rms_L~0.8089 rms_Q~0.6633
  step 660/685 | loss_L=1.2106 | loss_Q=1.2181 | scale_pen(L)= 2.2150e-03 | scale_pen(Q)= 9.2495e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8089 rms_Q~0.6633
  step 670/685 | loss_L=1.0698 | loss_Q=1.0769 | scale_pen(L)= 2.2176e-03 | scale_pen(Q)= 9.3373e-05 | grad_norm=0.59 | sec/step~1.18 | rms_L~0.8090 rms_Q~0.6633
  step 680/685 | loss_L=1.1712 | loss_Q=1.1933 | scale_pen(L)= 2.2241e-03 | scale_pen(Q)= 9.3650e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8090 rms_Q~0.6634
  step 685/685 | loss_L=1.1600 | loss_Q=1.1543 | scale_pen(L)= 2.2199e-03 | scale_pen(Q)= 9.1779e-05 | grad_norm=0.64 | sec/step~0.43 | rms_L~0.8091 rms_Q~0.6634
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8090561118439166, 'count': 685}, 'qwen': {'rms_mean': 0.6633857819285706, 'count': 685}}

Evaluating epoch 10 checkpoint...
Evaluating: runs/8B_clean_answer/epoch10 -> runs/8B_clean_answer/eval_epoch10
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch10/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch10/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2117.80it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.734918772768813, 'neutral_chat': 8.761994727885101, 'llama_chat': 8.841790725314429} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch10/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.80329 -> target=0.01057
[debug:llama] adapter.scale=1.0471 | Z.std=0.9996 Z.mean||=15.9940 | prefix.std=0.0106 prefix.mean||=0.6750 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of bonds between atoms the of bonds between atoms'
  1: 's,,,,s, and'
  2: '198 housing prices fell 20 in 198. housing prices'
  3: '2,000.000 square in area'
  4: 'Al,,, (,),, (,)'
Saved Llama results to runs/8B_clean_answer/eval_epoch10/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2834.47it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.07it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.26it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.536717349890047, 'neutral_chat': 8.58706744764217, 'qwen_chat': 8.502498844313243} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch10/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.66427 -> target=0.01363
[debug:qwen] adapter.scale=1.0096 | Z.std=0.9998 Z.mean||=15.9967 | prefix.std=0.0136 prefix.mean||=0.8151 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: ''
  2: 'and of the 1990s the of the'
  3: 'and the station the and the station the and the station the'
  4: 'and of the of and of of of of of of of'
Saved Qwen results to runs/8B_clean_answer/eval_epoch10/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2740.03it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5307.57it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.74s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.033  |  NLL/token (gold): 8.73480723017738
Qwen   EM: 0.000   F1: 0.023  |  NLL/token (gold): 8.511022266572114
Wall clock: 13.28s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.64s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.024
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.044

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.741546630859375,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.277673244476318,
    "llama": {
      "em": 0.0,
      "f1": 0.03289668664668665,
      "nll_token": 8.73480723017738
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0227917939903234,
      "nll_token": 8.511022266572114
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.639484643936157,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.02433816020580726,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0471159219741821,
      "Z_std": 0.9996258020401001,
      "Z_mean_norm": 15.993997573852539,
      "prefix_std": 0.010572698898613453,
      "prefix_mean_norm": 0.6750058531761169,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.009580135345459,
      "Z_std": 0.9997972846031189,
      "Z_mean_norm": 15.996740341186523,
      "prefix_std": 0.013631466776132584,
      "prefix_mean_norm": 0.8150853514671326,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.043633582920347615
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch10/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch10/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.033 | Qwen 0.023


=========================================
EPOCH 11/16
=========================================

Training epoch 11...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6737.84it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3975.64it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=10, global_step=6850
Epoch 11/1
  step 10/685 | loss_L=1.1112 | loss_Q=1.1304 | scale_pen(L)= 2.2205e-03 | scale_pen(Q)= 9.1793e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8126 rms_Q~0.6653
  step 20/685 | loss_L=1.1822 | loss_Q=1.2053 | scale_pen(L)= 2.2170e-03 | scale_pen(Q)= 9.1596e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8126 rms_Q~0.6653
  step 30/685 | loss_L=1.1823 | loss_Q=1.2030 | scale_pen(L)= 2.2170e-03 | scale_pen(Q)= 9.3219e-05 | grad_norm=0.64 | sec/step~1.15 | rms_L~0.8126 rms_Q~0.6653
  step 40/685 | loss_L=1.0536 | loss_Q=1.0510 | scale_pen(L)= 2.2228e-03 | scale_pen(Q)= 9.4633e-05 | grad_norm=0.63 | sec/step~1.10 | rms_L~0.8127 rms_Q~0.6654
  step 50/685 | loss_L=1.1304 | loss_Q=1.1164 | scale_pen(L)= 2.2249e-03 | scale_pen(Q)= 9.3429e-05 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8127 rms_Q~0.6654
  step 60/685 | loss_L=1.0067 | loss_Q=1.0078 | scale_pen(L)= 2.2248e-03 | scale_pen(Q)= 9.4374e-05 | grad_norm=0.59 | sec/step~1.09 | rms_L~0.8127 rms_Q~0.6654
  step 70/685 | loss_L=1.0129 | loss_Q=1.0285 | scale_pen(L)= 2.2283e-03 | scale_pen(Q)= 9.3003e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8127 rms_Q~0.6655
  step 80/685 | loss_L=0.9655 | loss_Q=0.9672 | scale_pen(L)= 2.2293e-03 | scale_pen(Q)= 9.1642e-05 | grad_norm=0.63 | sec/step~1.14 | rms_L~0.8128 rms_Q~0.6655
  step 90/685 | loss_L=1.1582 | loss_Q=1.1296 | scale_pen(L)= 2.2287e-03 | scale_pen(Q)= 9.1320e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8128 rms_Q~0.6655
  step 100/685 | loss_L=0.9985 | loss_Q=1.0053 | scale_pen(L)= 2.2209e-03 | scale_pen(Q)= 9.2826e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.8128 rms_Q~0.6655
  step 110/685 | loss_L=1.1832 | loss_Q=1.1859 | scale_pen(L)= 2.2219e-03 | scale_pen(Q)= 9.1491e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8128 rms_Q~0.6655
  step 120/685 | loss_L=1.0682 | loss_Q=1.0811 | scale_pen(L)= 2.2234e-03 | scale_pen(Q)= 9.0818e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.8128 rms_Q~0.6655
  step 130/685 | loss_L=1.1060 | loss_Q=1.1331 | scale_pen(L)= 2.2240e-03 | scale_pen(Q)= 9.1234e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8128 rms_Q~0.6655
  step 140/685 | loss_L=0.9853 | loss_Q=0.9954 | scale_pen(L)= 2.2244e-03 | scale_pen(Q)= 9.0246e-05 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.8128 rms_Q~0.6655
  step 150/685 | loss_L=0.9739 | loss_Q=0.9694 | scale_pen(L)= 2.2283e-03 | scale_pen(Q)= 9.2427e-05 | grad_norm=0.65 | sec/step~1.14 | rms_L~0.8129 rms_Q~0.6656
  step 160/685 | loss_L=1.2030 | loss_Q=1.2184 | scale_pen(L)= 2.2344e-03 | scale_pen(Q)= 9.2477e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8130 rms_Q~0.6656
  step 170/685 | loss_L=1.1782 | loss_Q=1.2108 | scale_pen(L)= 2.2340e-03 | scale_pen(Q)= 9.3012e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8130 rms_Q~0.6656
  step 180/685 | loss_L=1.1187 | loss_Q=1.1145 | scale_pen(L)= 2.2345e-03 | scale_pen(Q)= 9.1245e-05 | grad_norm=0.65 | sec/step~1.21 | rms_L~0.8131 rms_Q~0.6657
  step 190/685 | loss_L=1.1677 | loss_Q=1.1832 | scale_pen(L)= 2.2378e-03 | scale_pen(Q)= 9.0212e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8131 rms_Q~0.6657
  step 200/685 | loss_L=1.0216 | loss_Q=1.0455 | scale_pen(L)= 2.2394e-03 | scale_pen(Q)= 9.0995e-05 | grad_norm=0.59 | sec/step~1.08 | rms_L~0.8132 rms_Q~0.6657
  step 210/685 | loss_L=1.0753 | loss_Q=1.1052 | scale_pen(L)= 2.2380e-03 | scale_pen(Q)= 9.0196e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8133 rms_Q~0.6658
  step 220/685 | loss_L=1.0744 | loss_Q=1.0926 | scale_pen(L)= 2.2377e-03 | scale_pen(Q)= 9.2360e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8133 rms_Q~0.6658
  step 230/685 | loss_L=1.0332 | loss_Q=1.0282 | scale_pen(L)= 2.2415e-03 | scale_pen(Q)= 9.0965e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8134 rms_Q~0.6658
  step 240/685 | loss_L=1.0914 | loss_Q=1.1100 | scale_pen(L)= 2.2437e-03 | scale_pen(Q)= 9.0287e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8134 rms_Q~0.6659
  step 250/685 | loss_L=1.0340 | loss_Q=1.0467 | scale_pen(L)= 2.2441e-03 | scale_pen(Q)= 9.0545e-05 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.8135 rms_Q~0.6659
  step 260/685 | loss_L=1.0358 | loss_Q=1.0340 | scale_pen(L)= 2.2457e-03 | scale_pen(Q)= 8.9938e-05 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8135 rms_Q~0.6659
  step 270/685 | loss_L=0.9758 | loss_Q=0.9766 | scale_pen(L)= 2.2480e-03 | scale_pen(Q)= 9.0829e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8136 rms_Q~0.6660
  step 280/685 | loss_L=1.0060 | loss_Q=0.9890 | scale_pen(L)= 2.2458e-03 | scale_pen(Q)= 9.0115e-05 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8136 rms_Q~0.6660
  step 290/685 | loss_L=1.2897 | loss_Q=1.2822 | scale_pen(L)= 2.2417e-03 | scale_pen(Q)= 9.2065e-05 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.8137 rms_Q~0.6660
  step 300/685 | loss_L=1.1498 | loss_Q=1.1467 | scale_pen(L)= 2.2443e-03 | scale_pen(Q)= 9.1934e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.8137 rms_Q~0.6660
  step 310/685 | loss_L=1.0985 | loss_Q=1.1127 | scale_pen(L)= 2.2470e-03 | scale_pen(Q)= 9.3115e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8138 rms_Q~0.6661
  step 320/685 | loss_L=1.0858 | loss_Q=1.0946 | scale_pen(L)= 2.2542e-03 | scale_pen(Q)= 9.3973e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8138 rms_Q~0.6661
  step 330/685 | loss_L=1.2070 | loss_Q=1.2172 | scale_pen(L)= 2.2521e-03 | scale_pen(Q)= 9.2624e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8139 rms_Q~0.6661
  step 340/685 | loss_L=1.0298 | loss_Q=1.0142 | scale_pen(L)= 2.2544e-03 | scale_pen(Q)= 9.2099e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8140 rms_Q~0.6662
  step 350/685 | loss_L=1.0062 | loss_Q=1.0133 | scale_pen(L)= 2.2557e-03 | scale_pen(Q)= 9.2459e-05 | grad_norm=0.61 | sec/step~1.14 | rms_L~0.8140 rms_Q~0.6662
  step 360/685 | loss_L=1.0901 | loss_Q=1.1208 | scale_pen(L)= 2.2520e-03 | scale_pen(Q)= 9.5244e-05 | grad_norm=0.66 | sec/step~1.07 | rms_L~0.8141 rms_Q~0.6662
  step 370/685 | loss_L=1.1390 | loss_Q=1.1724 | scale_pen(L)= 2.2553e-03 | scale_pen(Q)= 9.6112e-05 | grad_norm=0.62 | sec/step~1.15 | rms_L~0.8141 rms_Q~0.6663
  step 380/685 | loss_L=0.9924 | loss_Q=1.0048 | scale_pen(L)= 2.2567e-03 | scale_pen(Q)= 9.7693e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.8142 rms_Q~0.6663
  step 390/685 | loss_L=1.0884 | loss_Q=1.0860 | scale_pen(L)= 2.2547e-03 | scale_pen(Q)= 9.8428e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8142 rms_Q~0.6663
  step 400/685 | loss_L=0.9855 | loss_Q=0.9820 | scale_pen(L)= 2.2488e-03 | scale_pen(Q)= 9.8135e-05 | grad_norm=0.61 | sec/step~1.16 | rms_L~0.8143 rms_Q~0.6664
  step 410/685 | loss_L=1.0390 | loss_Q=1.0338 | scale_pen(L)= 2.2519e-03 | scale_pen(Q)= 9.8399e-05 | grad_norm=0.60 | sec/step~1.12 | rms_L~0.8143 rms_Q~0.6664
  step 420/685 | loss_L=1.0727 | loss_Q=1.0912 | scale_pen(L)= 2.2537e-03 | scale_pen(Q)= 9.8009e-05 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8144 rms_Q~0.6664
  step 430/685 | loss_L=0.9941 | loss_Q=1.0101 | scale_pen(L)= 2.2622e-03 | scale_pen(Q)= 9.7769e-05 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8144 rms_Q~0.6665
  step 440/685 | loss_L=0.9529 | loss_Q=0.9774 | scale_pen(L)= 2.2552e-03 | scale_pen(Q)= 9.9219e-05 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8145 rms_Q~0.6665
  step 450/685 | loss_L=1.0626 | loss_Q=1.0457 | scale_pen(L)= 2.2568e-03 | scale_pen(Q)= 9.9390e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8146 rms_Q~0.6666
  step 460/685 | loss_L=1.1118 | loss_Q=1.0994 | scale_pen(L)= 2.2600e-03 | scale_pen(Q)= 9.9993e-05 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8146 rms_Q~0.6666
  step 470/685 | loss_L=1.1939 | loss_Q=1.2040 | scale_pen(L)= 2.2579e-03 | scale_pen(Q)= 9.9909e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8147 rms_Q~0.6666
  step 480/685 | loss_L=0.9618 | loss_Q=0.9704 | scale_pen(L)= 2.2578e-03 | scale_pen(Q)= 9.9112e-05 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8147 rms_Q~0.6667
  step 490/685 | loss_L=1.0605 | loss_Q=1.0670 | scale_pen(L)= 2.2558e-03 | scale_pen(Q)= 9.8726e-05 | grad_norm=0.57 | sec/step~1.08 | rms_L~0.8148 rms_Q~0.6667
  step 500/685 | loss_L=1.0768 | loss_Q=1.0813 | scale_pen(L)= 2.2596e-03 | scale_pen(Q)= 9.8788e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8148 rms_Q~0.6667
  step 510/685 | loss_L=1.0928 | loss_Q=1.1198 | scale_pen(L)= 2.2611e-03 | scale_pen(Q)= 9.8180e-05 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8149 rms_Q~0.6668
  step 520/685 | loss_L=0.9468 | loss_Q=0.9476 | scale_pen(L)= 2.2661e-03 | scale_pen(Q)= 9.8475e-05 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8150 rms_Q~0.6668
  step 530/685 | loss_L=1.1149 | loss_Q=1.1341 | scale_pen(L)= 2.2645e-03 | scale_pen(Q)= 9.8165e-05 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8150 rms_Q~0.6668
  step 540/685 | loss_L=1.1632 | loss_Q=1.1527 | scale_pen(L)= 2.2648e-03 | scale_pen(Q)= 9.6399e-05 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8151 rms_Q~0.6669
  step 550/685 | loss_L=1.1110 | loss_Q=1.1268 | scale_pen(L)= 2.2614e-03 | scale_pen(Q)= 9.6953e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8151 rms_Q~0.6669
  step 560/685 | loss_L=1.1595 | loss_Q=1.1578 | scale_pen(L)= 2.2677e-03 | scale_pen(Q)= 9.7554e-05 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8151 rms_Q~0.6669
  step 570/685 | loss_L=1.0601 | loss_Q=1.0703 | scale_pen(L)= 2.2714e-03 | scale_pen(Q)= 9.7569e-05 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8152 rms_Q~0.6670
  step 580/685 | loss_L=0.9929 | loss_Q=1.0070 | scale_pen(L)= 2.2682e-03 | scale_pen(Q)= 9.5722e-05 | grad_norm=0.65 | sec/step~1.19 | rms_L~0.8152 rms_Q~0.6670
  step 590/685 | loss_L=0.9954 | loss_Q=0.9995 | scale_pen(L)= 2.2682e-03 | scale_pen(Q)= 9.4128e-05 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8153 rms_Q~0.6670
  step 600/685 | loss_L=1.0816 | loss_Q=1.1084 | scale_pen(L)= 2.2698e-03 | scale_pen(Q)= 9.5253e-05 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8153 rms_Q~0.6670
  step 610/685 | loss_L=1.1516 | loss_Q=1.1644 | scale_pen(L)= 2.2682e-03 | scale_pen(Q)= 9.7251e-05 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8154 rms_Q~0.6671
  step 620/685 | loss_L=0.9787 | loss_Q=0.9860 | scale_pen(L)= 2.2765e-03 | scale_pen(Q)= 9.6899e-05 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8154 rms_Q~0.6671
  step 630/685 | loss_L=1.2078 | loss_Q=1.2294 | scale_pen(L)= 2.2766e-03 | scale_pen(Q)= 9.6523e-05 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.8155 rms_Q~0.6671
  step 640/685 | loss_L=1.1642 | loss_Q=1.1919 | scale_pen(L)= 2.2785e-03 | scale_pen(Q)= 9.7042e-05 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8155 rms_Q~0.6672
  step 650/685 | loss_L=1.1709 | loss_Q=1.1814 | scale_pen(L)= 2.2759e-03 | scale_pen(Q)= 9.9554e-05 | grad_norm=0.62 | sec/step~1.26 | rms_L~0.8156 rms_Q~0.6672
  step 660/685 | loss_L=1.1402 | loss_Q=1.1594 | scale_pen(L)= 2.2770e-03 | scale_pen(Q)= 9.9900e-05 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8156 rms_Q~0.6672
  step 670/685 | loss_L=1.1513 | loss_Q=1.1600 | scale_pen(L)= 2.2794e-03 | scale_pen(Q)= 1.0156e-04 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8157 rms_Q~0.6673
  step 680/685 | loss_L=1.2090 | loss_Q=1.2459 | scale_pen(L)= 2.2766e-03 | scale_pen(Q)= 1.0094e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8157 rms_Q~0.6673
  step 685/685 | loss_L=1.4266 | loss_Q=1.4650 | scale_pen(L)= 2.2755e-03 | scale_pen(Q)= 1.0115e-04 | grad_norm=0.65 | sec/step~0.43 | rms_L~0.8158 rms_Q~0.6673
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8157548968809365, 'count': 685}, 'qwen': {'rms_mean': 0.6673075740354775, 'count': 685}}

Evaluating epoch 11 checkpoint...
Evaluating: runs/8B_clean_answer/epoch11 -> runs/8B_clean_answer/eval_epoch11
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch11/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch11/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3166.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.15s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.610065324506522, 'neutral_chat': 8.632408578920256, 'llama_chat': 8.749437281874572} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch11/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.80987 -> target=0.01057
[debug:llama] adapter.scale=1.0477 | Z.std=0.9993 Z.mean||=15.9883 | prefix.std=0.0106 prefix.mean||=0.6749 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the molecule the of the molecule the of'
  1: '2. insects, and insects, and insects'
  2: '198 housing prices fell 20 in 198. housing prices'
  3: 'the of the, the of the and the of the'
  4: 'Al,,, (,),, (,)'
Saved Llama results to runs/8B_clean_answer/eval_epoch11/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3332.12it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.32798204062477, 'neutral_chat': 8.420464403414853, 'qwen_chat': 8.349250780842292} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch11/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.66583 -> target=0.01363
[debug:qwen] adapter.scale=1.0101 | Z.std=0.9993 Z.mean||=15.9883 | prefix.std=0.0136 prefix.mean||=0.8150 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: 'ands, ands, ands, ands'
  2: '1920s and 30s the of'
  3: ''
  4: '100000000000'
Saved Qwen results to runs/8B_clean_answer/eval_epoch11/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3363.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8363.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.42it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.45it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.95s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.032  |  NLL/token (gold): 8.606112452591358
Qwen   EM: 0.000   F1: 0.019  |  NLL/token (gold): 8.348007526977984
Wall clock: 12.99s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.33s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.024
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.040

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.948981285095215,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.986897468566895,
    "llama": {
      "em": 0.0,
      "f1": 0.03167944555444555,
      "nll_token": 8.606112452591358
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.01895719803072744,
      "nll_token": 8.348007526977984
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.328083515167236,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.02419966389819331,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0477019548416138,
      "Z_std": 0.9992673993110657,
      "Z_mean_norm": 15.988264083862305,
      "prefix_std": 0.010572700761258602,
      "prefix_mean_norm": 0.6749294996261597,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0100575685501099,
      "Z_std": 0.9992673993110657,
      "Z_mean_norm": 15.988264083862305,
      "prefix_std": 0.013631434179842472,
      "prefix_mean_norm": 0.815021812915802,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.039613083811613235
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch11/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch11/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.032 | Qwen 0.019


=========================================
EPOCH 12/16
=========================================

Training epoch 12...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7112.00it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.46it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4130.28it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.31it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.41it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=11, global_step=7535
Epoch 12/1
  step 10/685 | loss_L=1.1993 | loss_Q=1.2162 | scale_pen(L)= 2.2766e-03 | scale_pen(Q)= 1.0299e-04 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.8188 rms_Q~0.6694
  step 20/685 | loss_L=1.1801 | loss_Q=1.2026 | scale_pen(L)= 2.2789e-03 | scale_pen(Q)= 1.0369e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8188 rms_Q~0.6694
  step 30/685 | loss_L=1.0349 | loss_Q=1.0600 | scale_pen(L)= 2.2750e-03 | scale_pen(Q)= 1.0271e-04 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.8189 rms_Q~0.6695
  step 40/685 | loss_L=1.0787 | loss_Q=1.0589 | scale_pen(L)= 2.2752e-03 | scale_pen(Q)= 1.0151e-04 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8189 rms_Q~0.6695
  step 50/685 | loss_L=1.0817 | loss_Q=1.0891 | scale_pen(L)= 2.2767e-03 | scale_pen(Q)= 1.0120e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8188 rms_Q~0.6694
  step 60/685 | loss_L=1.0572 | loss_Q=1.0432 | scale_pen(L)= 2.2774e-03 | scale_pen(Q)= 1.0306e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8188 rms_Q~0.6694
  step 70/685 | loss_L=1.0580 | loss_Q=1.0795 | scale_pen(L)= 2.2760e-03 | scale_pen(Q)= 1.0326e-04 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8189 rms_Q~0.6694
  step 80/685 | loss_L=1.0584 | loss_Q=1.0716 | scale_pen(L)= 2.2773e-03 | scale_pen(Q)= 1.0477e-04 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8189 rms_Q~0.6695
  step 90/685 | loss_L=1.2444 | loss_Q=1.2510 | scale_pen(L)= 2.2764e-03 | scale_pen(Q)= 1.0421e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8189 rms_Q~0.6695
  step 100/685 | loss_L=1.1201 | loss_Q=1.1465 | scale_pen(L)= 2.2733e-03 | scale_pen(Q)= 1.0551e-04 | grad_norm=0.66 | sec/step~1.11 | rms_L~0.8190 rms_Q~0.6695
  step 110/685 | loss_L=1.0126 | loss_Q=1.0218 | scale_pen(L)= 2.2735e-03 | scale_pen(Q)= 1.0466e-04 | grad_norm=0.62 | sec/step~1.10 | rms_L~0.8190 rms_Q~0.6696
  step 120/685 | loss_L=1.0272 | loss_Q=1.0639 | scale_pen(L)= 2.2767e-03 | scale_pen(Q)= 1.0507e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8190 rms_Q~0.6696
  step 130/685 | loss_L=1.0571 | loss_Q=1.0660 | scale_pen(L)= 2.2664e-03 | scale_pen(Q)= 1.0586e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8191 rms_Q~0.6697
  step 140/685 | loss_L=1.0122 | loss_Q=1.0267 | scale_pen(L)= 2.2711e-03 | scale_pen(Q)= 1.0561e-04 | grad_norm=0.57 | sec/step~1.10 | rms_L~0.8191 rms_Q~0.6697
  step 150/685 | loss_L=1.0862 | loss_Q=1.0799 | scale_pen(L)= 2.2766e-03 | scale_pen(Q)= 1.0385e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8191 rms_Q~0.6697
  step 160/685 | loss_L=0.9801 | loss_Q=0.9837 | scale_pen(L)= 2.2856e-03 | scale_pen(Q)= 1.0496e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8192 rms_Q~0.6698
  step 170/685 | loss_L=0.9742 | loss_Q=0.9935 | scale_pen(L)= 2.2824e-03 | scale_pen(Q)= 1.0689e-04 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.8192 rms_Q~0.6698
  step 180/685 | loss_L=1.1047 | loss_Q=1.1204 | scale_pen(L)= 2.2848e-03 | scale_pen(Q)= 1.0615e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8193 rms_Q~0.6699
  step 190/685 | loss_L=1.0619 | loss_Q=1.0508 | scale_pen(L)= 2.2815e-03 | scale_pen(Q)= 1.0547e-04 | grad_norm=0.58 | sec/step~1.11 | rms_L~0.8193 rms_Q~0.6699
  step 200/685 | loss_L=0.9476 | loss_Q=0.9518 | scale_pen(L)= 2.2830e-03 | scale_pen(Q)= 1.0511e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8193 rms_Q~0.6699
  step 210/685 | loss_L=1.0220 | loss_Q=1.0237 | scale_pen(L)= 2.2804e-03 | scale_pen(Q)= 1.0358e-04 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.8194 rms_Q~0.6700
  step 220/685 | loss_L=1.0407 | loss_Q=1.0537 | scale_pen(L)= 2.2835e-03 | scale_pen(Q)= 1.0501e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8194 rms_Q~0.6700
  step 230/685 | loss_L=1.0462 | loss_Q=1.0733 | scale_pen(L)= 2.2854e-03 | scale_pen(Q)= 1.0495e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8194 rms_Q~0.6700
  step 240/685 | loss_L=1.0101 | loss_Q=0.9934 | scale_pen(L)= 2.2919e-03 | scale_pen(Q)= 1.0434e-04 | grad_norm=0.64 | sec/step~1.10 | rms_L~0.8194 rms_Q~0.6700
  step 250/685 | loss_L=1.0062 | loss_Q=1.0225 | scale_pen(L)= 2.2900e-03 | scale_pen(Q)= 1.0349e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8195 rms_Q~0.6701
  step 260/685 | loss_L=1.1334 | loss_Q=1.1543 | scale_pen(L)= 2.2840e-03 | scale_pen(Q)= 1.0257e-04 | grad_norm=0.65 | sec/step~1.14 | rms_L~0.8195 rms_Q~0.6701
  step 270/685 | loss_L=0.9976 | loss_Q=1.0064 | scale_pen(L)= 2.2856e-03 | scale_pen(Q)= 1.0287e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8195 rms_Q~0.6701
  step 280/685 | loss_L=1.0813 | loss_Q=1.0888 | scale_pen(L)= 2.2876e-03 | scale_pen(Q)= 1.0434e-04 | grad_norm=0.66 | sec/step~1.11 | rms_L~0.8196 rms_Q~0.6702
  step 290/685 | loss_L=1.1683 | loss_Q=1.1823 | scale_pen(L)= 2.2910e-03 | scale_pen(Q)= 1.0528e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8196 rms_Q~0.6702
  step 300/685 | loss_L=1.0924 | loss_Q=1.0970 | scale_pen(L)= 2.2903e-03 | scale_pen(Q)= 1.0520e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8197 rms_Q~0.6702
  step 310/685 | loss_L=1.2500 | loss_Q=1.2727 | scale_pen(L)= 2.2941e-03 | scale_pen(Q)= 1.0479e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8197 rms_Q~0.6702
  step 320/685 | loss_L=1.0372 | loss_Q=1.0479 | scale_pen(L)= 2.2988e-03 | scale_pen(Q)= 1.0454e-04 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.8197 rms_Q~0.6702
  step 330/685 | loss_L=1.1406 | loss_Q=1.1605 | scale_pen(L)= 2.2996e-03 | scale_pen(Q)= 1.0629e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8198 rms_Q~0.6703
  step 340/685 | loss_L=1.0599 | loss_Q=1.0735 | scale_pen(L)= 2.2981e-03 | scale_pen(Q)= 1.0607e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8198 rms_Q~0.6703
  step 350/685 | loss_L=1.1382 | loss_Q=1.1324 | scale_pen(L)= 2.2982e-03 | scale_pen(Q)= 1.0562e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8199 rms_Q~0.6703
  step 360/685 | loss_L=1.2424 | loss_Q=1.2595 | scale_pen(L)= 2.3035e-03 | scale_pen(Q)= 1.0518e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8199 rms_Q~0.6703
  step 370/685 | loss_L=1.1280 | loss_Q=1.1291 | scale_pen(L)= 2.3008e-03 | scale_pen(Q)= 1.0385e-04 | grad_norm=0.66 | sec/step~1.10 | rms_L~0.8199 rms_Q~0.6704
  step 380/685 | loss_L=0.9798 | loss_Q=1.0060 | scale_pen(L)= 2.3104e-03 | scale_pen(Q)= 1.0555e-04 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8200 rms_Q~0.6704
  step 390/685 | loss_L=1.0643 | loss_Q=1.0828 | scale_pen(L)= 2.3089e-03 | scale_pen(Q)= 1.0487e-04 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.8200 rms_Q~0.6704
  step 400/685 | loss_L=0.9990 | loss_Q=1.0128 | scale_pen(L)= 2.3096e-03 | scale_pen(Q)= 1.0439e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8201 rms_Q~0.6705
  step 410/685 | loss_L=1.0689 | loss_Q=1.0891 | scale_pen(L)= 2.3101e-03 | scale_pen(Q)= 1.0500e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8201 rms_Q~0.6705
  step 420/685 | loss_L=1.1056 | loss_Q=1.1016 | scale_pen(L)= 2.3118e-03 | scale_pen(Q)= 1.0561e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8202 rms_Q~0.6705
  step 430/685 | loss_L=0.9347 | loss_Q=0.9524 | scale_pen(L)= 2.3098e-03 | scale_pen(Q)= 1.0578e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8202 rms_Q~0.6706
  step 440/685 | loss_L=1.1112 | loss_Q=1.1389 | scale_pen(L)= 2.3144e-03 | scale_pen(Q)= 1.0657e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8203 rms_Q~0.6706
  step 450/685 | loss_L=1.1469 | loss_Q=1.1711 | scale_pen(L)= 2.3154e-03 | scale_pen(Q)= 1.0632e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8204 rms_Q~0.6706
  step 460/685 | loss_L=1.0918 | loss_Q=1.1194 | scale_pen(L)= 2.3167e-03 | scale_pen(Q)= 1.0799e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8204 rms_Q~0.6707
  step 470/685 | loss_L=1.1362 | loss_Q=1.1658 | scale_pen(L)= 2.3160e-03 | scale_pen(Q)= 1.0633e-04 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.8205 rms_Q~0.6707
  step 480/685 | loss_L=1.1430 | loss_Q=1.1584 | scale_pen(L)= 2.3191e-03 | scale_pen(Q)= 1.0757e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8205 rms_Q~0.6707
  step 490/685 | loss_L=1.1845 | loss_Q=1.2005 | scale_pen(L)= 2.3154e-03 | scale_pen(Q)= 1.0806e-04 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8206 rms_Q~0.6708
  step 500/685 | loss_L=1.0598 | loss_Q=1.0580 | scale_pen(L)= 2.3152e-03 | scale_pen(Q)= 1.0665e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8206 rms_Q~0.6708
  step 510/685 | loss_L=1.1365 | loss_Q=1.1514 | scale_pen(L)= 2.3246e-03 | scale_pen(Q)= 1.0735e-04 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8207 rms_Q~0.6708
  step 520/685 | loss_L=1.0345 | loss_Q=1.0412 | scale_pen(L)= 2.3223e-03 | scale_pen(Q)= 1.0659e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8207 rms_Q~0.6709
  step 530/685 | loss_L=1.0904 | loss_Q=1.0982 | scale_pen(L)= 2.3222e-03 | scale_pen(Q)= 1.0679e-04 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8208 rms_Q~0.6709
  step 540/685 | loss_L=1.1155 | loss_Q=1.1236 | scale_pen(L)= 2.3260e-03 | scale_pen(Q)= 1.0474e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8208 rms_Q~0.6709
  step 550/685 | loss_L=1.1063 | loss_Q=1.1135 | scale_pen(L)= 2.3257e-03 | scale_pen(Q)= 1.0574e-04 | grad_norm=0.65 | sec/step~1.09 | rms_L~0.8209 rms_Q~0.6710
  step 560/685 | loss_L=1.2284 | loss_Q=1.2457 | scale_pen(L)= 2.3296e-03 | scale_pen(Q)= 1.0587e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8209 rms_Q~0.6710
  step 570/685 | loss_L=1.1131 | loss_Q=1.1367 | scale_pen(L)= 2.3301e-03 | scale_pen(Q)= 1.0649e-04 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8210 rms_Q~0.6710
  step 580/685 | loss_L=1.0376 | loss_Q=1.0372 | scale_pen(L)= 2.3330e-03 | scale_pen(Q)= 1.0724e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8210 rms_Q~0.6711
  step 590/685 | loss_L=1.2317 | loss_Q=1.2351 | scale_pen(L)= 2.3348e-03 | scale_pen(Q)= 1.0550e-04 | grad_norm=0.68 | sec/step~1.06 | rms_L~0.8211 rms_Q~0.6711
  step 600/685 | loss_L=1.2160 | loss_Q=1.2387 | scale_pen(L)= 2.3323e-03 | scale_pen(Q)= 1.0489e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8211 rms_Q~0.6711
  step 610/685 | loss_L=1.1026 | loss_Q=1.1217 | scale_pen(L)= 2.3238e-03 | scale_pen(Q)= 1.0584e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8211 rms_Q~0.6711
  step 620/685 | loss_L=0.8599 | loss_Q=0.8781 | scale_pen(L)= 2.3242e-03 | scale_pen(Q)= 1.0762e-04 | grad_norm=0.64 | sec/step~1.16 | rms_L~0.8212 rms_Q~0.6712
  step 630/685 | loss_L=1.0669 | loss_Q=1.0956 | scale_pen(L)= 2.3211e-03 | scale_pen(Q)= 1.0715e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8212 rms_Q~0.6712
  step 640/685 | loss_L=1.0620 | loss_Q=1.0496 | scale_pen(L)= 2.3154e-03 | scale_pen(Q)= 1.0675e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8213 rms_Q~0.6712
  step 650/685 | loss_L=1.2395 | loss_Q=1.2542 | scale_pen(L)= 2.3175e-03 | scale_pen(Q)= 1.0755e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8213 rms_Q~0.6713
  step 660/685 | loss_L=1.0734 | loss_Q=1.1007 | scale_pen(L)= 2.3181e-03 | scale_pen(Q)= 1.0633e-04 | grad_norm=0.66 | sec/step~1.11 | rms_L~0.8213 rms_Q~0.6713
  step 670/685 | loss_L=1.0620 | loss_Q=1.0698 | scale_pen(L)= 2.3184e-03 | scale_pen(Q)= 1.0611e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8214 rms_Q~0.6713
  step 680/685 | loss_L=1.0822 | loss_Q=1.0996 | scale_pen(L)= 2.3237e-03 | scale_pen(Q)= 1.0694e-04 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8214 rms_Q~0.6713
  step 685/685 | loss_L=0.9017 | loss_Q=0.9018 | scale_pen(L)= 2.3269e-03 | scale_pen(Q)= 1.0600e-04 | grad_norm=0.61 | sec/step~0.49 | rms_L~0.8214 rms_Q~0.6714
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8214279692538463, 'count': 685}, 'qwen': {'rms_mean': 0.6713525543247696, 'count': 685}}

Evaluating epoch 12 checkpoint...
Evaluating: runs/8B_clean_answer/epoch12 -> runs/8B_clean_answer/eval_epoch12
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch12/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch12/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2895.62it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.04it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.46it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.748828442458933, 'neutral_chat': 8.752431052071708, 'llama_chat': 8.876425791219248} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch12/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.81589 -> target=0.01057
[debug:llama] adapter.scale=1.0482 | Z.std=0.9989 Z.mean||=15.9823 | prefix.std=0.0106 prefix.mean||=0.6749 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 3 4 5 6 7'
  1: '2. the of the, which the of the'
  2: '1967 the of the housing market the of the housing the'
  3: 'Angeles Los Angeles Angeles Angeles Angeles Angeles Angeles Angeles Angeles Angeles Angeles'
  4: '2,3,4,5,6'
Saved Llama results to runs/8B_clean_answer/eval_epoch12/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3326.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.43601972337753, 'neutral_chat': 8.532566577669174, 'qwen_chat': 8.45733241997068} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch12/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.66971 -> target=0.01363
[debug:qwen] adapter.scale=1.0103 | Z.std=0.9989 Z.mean||=15.9823 | prefix.std=0.0136 prefix.mean||=0.8150 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '1960s the of and and and and and'
  1: 'ands of and and and and and and and and and'
  2: '1920s the of and in were in the'
  3: '\n", "location": {"latitude": 39'
  4: '123456789101'
Saved Qwen results to runs/8B_clean_answer/eval_epoch12/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3301.30it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.20s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7713.66it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.25it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.20it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.37it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.02s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.030  |  NLL/token (gold): 8.755467157515268
Qwen   EM: 0.000   F1: 0.021  |  NLL/token (gold): 8.438078822282256
Wall clock: 13.13s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.63s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.021
Inter-model agreement (normalized): 0.005
Oracle upper bound:  EM 0.000  F1 0.040

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.01729917526245,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.129518508911133,
    "llama": {
      "em": 0.0,
      "f1": 0.030393578643578646,
      "nll_token": 8.755467157515268
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.021471980030483864,
      "nll_token": 8.438078822282256
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.634447813034058,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.02074699728376199,
    "agreement": 0.005,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0482382774353027,
      "Z_std": 0.9988926649093628,
      "Z_mean_norm": 15.982266426086426,
      "prefix_std": 0.01057269424200058,
      "prefix_mean_norm": 0.6748616099357605,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0102957487106323,
      "Z_std": 0.9988926649093628,
      "Z_mean_norm": 15.982266426086426,
      "prefix_std": 0.013631420210003853,
      "prefix_mean_norm": 0.81496262550354,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03988750078423992
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch12/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch12/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.021


=========================================
EPOCH 13/16
=========================================

Training epoch 13...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5251.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.09it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3744.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.42it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.10it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=12, global_step=8220
Epoch 13/1
  step 10/685 | loss_L=1.0605 | loss_Q=1.0658 | scale_pen(L)= 2.3245e-03 | scale_pen(Q)= 1.0774e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8239 rms_Q~0.6730
  step 20/685 | loss_L=1.1039 | loss_Q=1.1149 | scale_pen(L)= 2.3279e-03 | scale_pen(Q)= 1.0869e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8240 rms_Q~0.6731
  step 30/685 | loss_L=1.1741 | loss_Q=1.1843 | scale_pen(L)= 2.3275e-03 | scale_pen(Q)= 1.1028e-04 | grad_norm=0.64 | sec/step~1.10 | rms_L~0.8240 rms_Q~0.6731
  step 40/685 | loss_L=1.0269 | loss_Q=1.0387 | scale_pen(L)= 2.3322e-03 | scale_pen(Q)= 1.0991e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8240 rms_Q~0.6731
  step 50/685 | loss_L=1.1773 | loss_Q=1.1833 | scale_pen(L)= 2.3320e-03 | scale_pen(Q)= 1.1021e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8239 rms_Q~0.6731
  step 60/685 | loss_L=1.0676 | loss_Q=1.0820 | scale_pen(L)= 2.3307e-03 | scale_pen(Q)= 1.1087e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8239 rms_Q~0.6731
  step 70/685 | loss_L=1.0556 | loss_Q=1.0707 | scale_pen(L)= 2.3327e-03 | scale_pen(Q)= 1.1230e-04 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.8240 rms_Q~0.6731
  step 80/685 | loss_L=1.0356 | loss_Q=1.0384 | scale_pen(L)= 2.3315e-03 | scale_pen(Q)= 1.1464e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8240 rms_Q~0.6732
  step 90/685 | loss_L=1.0260 | loss_Q=1.0402 | scale_pen(L)= 2.3283e-03 | scale_pen(Q)= 1.1418e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8241 rms_Q~0.6732
  step 100/685 | loss_L=1.0472 | loss_Q=1.0675 | scale_pen(L)= 2.3194e-03 | scale_pen(Q)= 1.1264e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8241 rms_Q~0.6732
  step 110/685 | loss_L=1.1218 | loss_Q=1.1158 | scale_pen(L)= 2.3297e-03 | scale_pen(Q)= 1.1215e-04 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.8241 rms_Q~0.6732
  step 120/685 | loss_L=1.1539 | loss_Q=1.1623 | scale_pen(L)= 2.3273e-03 | scale_pen(Q)= 1.1160e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8242 rms_Q~0.6732
  step 130/685 | loss_L=1.1224 | loss_Q=1.1444 | scale_pen(L)= 2.3324e-03 | scale_pen(Q)= 1.1052e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8242 rms_Q~0.6733
  step 140/685 | loss_L=1.0907 | loss_Q=1.1002 | scale_pen(L)= 2.3352e-03 | scale_pen(Q)= 1.1042e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8242 rms_Q~0.6733
  step 150/685 | loss_L=1.0601 | loss_Q=1.0925 | scale_pen(L)= 2.3354e-03 | scale_pen(Q)= 1.0854e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8243 rms_Q~0.6733
  step 160/685 | loss_L=0.9632 | loss_Q=0.9652 | scale_pen(L)= 2.3343e-03 | scale_pen(Q)= 1.0934e-04 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8243 rms_Q~0.6733
  step 170/685 | loss_L=1.0512 | loss_Q=1.0553 | scale_pen(L)= 2.3293e-03 | scale_pen(Q)= 1.0774e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8243 rms_Q~0.6733
  step 180/685 | loss_L=1.0405 | loss_Q=1.0577 | scale_pen(L)= 2.3335e-03 | scale_pen(Q)= 1.0795e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8244 rms_Q~0.6734
  step 190/685 | loss_L=1.1015 | loss_Q=1.1365 | scale_pen(L)= 2.3279e-03 | scale_pen(Q)= 1.0812e-04 | grad_norm=0.67 | sec/step~1.05 | rms_L~0.8244 rms_Q~0.6734
  step 200/685 | loss_L=1.0555 | loss_Q=1.0481 | scale_pen(L)= 2.3268e-03 | scale_pen(Q)= 1.0967e-04 | grad_norm=0.63 | sec/step~1.20 | rms_L~0.8244 rms_Q~0.6734
  step 210/685 | loss_L=1.0423 | loss_Q=1.0495 | scale_pen(L)= 2.3261e-03 | scale_pen(Q)= 1.0773e-04 | grad_norm=0.60 | sec/step~1.05 | rms_L~0.8245 rms_Q~0.6734
  step 220/685 | loss_L=1.1613 | loss_Q=1.1673 | scale_pen(L)= 2.3328e-03 | scale_pen(Q)= 1.0661e-04 | grad_norm=0.65 | sec/step~1.10 | rms_L~0.8245 rms_Q~0.6735
  step 230/685 | loss_L=1.0310 | loss_Q=1.0422 | scale_pen(L)= 2.3337e-03 | scale_pen(Q)= 1.0630e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8245 rms_Q~0.6735
  step 240/685 | loss_L=1.1381 | loss_Q=1.1478 | scale_pen(L)= 2.3304e-03 | scale_pen(Q)= 1.0643e-04 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8246 rms_Q~0.6735
  step 250/685 | loss_L=1.1836 | loss_Q=1.2071 | scale_pen(L)= 2.3286e-03 | scale_pen(Q)= 1.0651e-04 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.8246 rms_Q~0.6735
  step 260/685 | loss_L=1.1520 | loss_Q=1.1544 | scale_pen(L)= 2.3253e-03 | scale_pen(Q)= 1.0688e-04 | grad_norm=0.65 | sec/step~1.12 | rms_L~0.8246 rms_Q~0.6735
  step 270/685 | loss_L=1.1294 | loss_Q=1.1179 | scale_pen(L)= 2.3247e-03 | scale_pen(Q)= 1.0472e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8247 rms_Q~0.6735
  step 280/685 | loss_L=1.2151 | loss_Q=1.2022 | scale_pen(L)= 2.3291e-03 | scale_pen(Q)= 1.0592e-04 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.8247 rms_Q~0.6735
  step 290/685 | loss_L=1.0938 | loss_Q=1.0910 | scale_pen(L)= 2.3329e-03 | scale_pen(Q)= 1.0560e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8247 rms_Q~0.6736
  step 300/685 | loss_L=1.1456 | loss_Q=1.1491 | scale_pen(L)= 2.3361e-03 | scale_pen(Q)= 1.0281e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8248 rms_Q~0.6736
  step 310/685 | loss_L=1.1590 | loss_Q=1.1333 | scale_pen(L)= 2.3344e-03 | scale_pen(Q)= 1.0296e-04 | grad_norm=0.63 | sec/step~1.10 | rms_L~0.8248 rms_Q~0.6736
  step 320/685 | loss_L=1.0335 | loss_Q=1.0431 | scale_pen(L)= 2.3373e-03 | scale_pen(Q)= 1.0364e-04 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8248 rms_Q~0.6736
  step 330/685 | loss_L=1.0113 | loss_Q=1.0127 | scale_pen(L)= 2.3420e-03 | scale_pen(Q)= 1.0320e-04 | grad_norm=0.64 | sec/step~1.09 | rms_L~0.8249 rms_Q~0.6736
  step 340/685 | loss_L=1.1203 | loss_Q=1.1097 | scale_pen(L)= 2.3442e-03 | scale_pen(Q)= 1.0155e-04 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8249 rms_Q~0.6737
  step 350/685 | loss_L=1.0738 | loss_Q=1.0749 | scale_pen(L)= 2.3485e-03 | scale_pen(Q)= 1.0188e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8250 rms_Q~0.6737
  step 360/685 | loss_L=1.2172 | loss_Q=1.2272 | scale_pen(L)= 2.3480e-03 | scale_pen(Q)= 1.0297e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8250 rms_Q~0.6737
  step 370/685 | loss_L=1.1398 | loss_Q=1.1699 | scale_pen(L)= 2.3422e-03 | scale_pen(Q)= 1.0202e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8250 rms_Q~0.6737
  step 380/685 | loss_L=1.1147 | loss_Q=1.1149 | scale_pen(L)= 2.3365e-03 | scale_pen(Q)= 1.0371e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8251 rms_Q~0.6737
  step 390/685 | loss_L=0.9494 | loss_Q=0.9447 | scale_pen(L)= 2.3418e-03 | scale_pen(Q)= 1.0524e-04 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.8251 rms_Q~0.6737
  step 400/685 | loss_L=1.0252 | loss_Q=1.0385 | scale_pen(L)= 2.3437e-03 | scale_pen(Q)= 1.0711e-04 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8251 rms_Q~0.6738
  step 410/685 | loss_L=0.9819 | loss_Q=0.9978 | scale_pen(L)= 2.3490e-03 | scale_pen(Q)= 1.0724e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8252 rms_Q~0.6738
  step 420/685 | loss_L=0.9651 | loss_Q=0.9803 | scale_pen(L)= 2.3545e-03 | scale_pen(Q)= 1.0887e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8252 rms_Q~0.6738
  step 430/685 | loss_L=1.0756 | loss_Q=1.0846 | scale_pen(L)= 2.3538e-03 | scale_pen(Q)= 1.0827e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8252 rms_Q~0.6738
  step 440/685 | loss_L=1.2373 | loss_Q=1.2602 | scale_pen(L)= 2.3542e-03 | scale_pen(Q)= 1.0696e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8253 rms_Q~0.6738
  step 450/685 | loss_L=1.0563 | loss_Q=1.0655 | scale_pen(L)= 2.3513e-03 | scale_pen(Q)= 1.0575e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8253 rms_Q~0.6738
  step 460/685 | loss_L=1.1976 | loss_Q=1.2091 | scale_pen(L)= 2.3582e-03 | scale_pen(Q)= 1.0462e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8253 rms_Q~0.6739
  step 470/685 | loss_L=1.0579 | loss_Q=1.0611 | scale_pen(L)= 2.3565e-03 | scale_pen(Q)= 1.0491e-04 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.8253 rms_Q~0.6739
  step 480/685 | loss_L=1.1191 | loss_Q=1.1482 | scale_pen(L)= 2.3556e-03 | scale_pen(Q)= 1.0406e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8254 rms_Q~0.6739
  step 490/685 | loss_L=1.0891 | loss_Q=1.1000 | scale_pen(L)= 2.3529e-03 | scale_pen(Q)= 1.0658e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8254 rms_Q~0.6739
  step 500/685 | loss_L=1.0695 | loss_Q=1.0942 | scale_pen(L)= 2.3513e-03 | scale_pen(Q)= 1.0783e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8255 rms_Q~0.6739
  step 510/685 | loss_L=1.1377 | loss_Q=1.1476 | scale_pen(L)= 2.3532e-03 | scale_pen(Q)= 1.0978e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8255 rms_Q~0.6739
  step 520/685 | loss_L=1.0140 | loss_Q=1.0345 | scale_pen(L)= 2.3515e-03 | scale_pen(Q)= 1.0951e-04 | grad_norm=0.60 | sec/step~1.18 | rms_L~0.8255 rms_Q~0.6740
  step 530/685 | loss_L=1.1434 | loss_Q=1.1792 | scale_pen(L)= 2.3484e-03 | scale_pen(Q)= 1.0932e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8256 rms_Q~0.6740
  step 540/685 | loss_L=1.1760 | loss_Q=1.1856 | scale_pen(L)= 2.3498e-03 | scale_pen(Q)= 1.0808e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8256 rms_Q~0.6740
  step 550/685 | loss_L=0.9587 | loss_Q=0.9641 | scale_pen(L)= 2.3506e-03 | scale_pen(Q)= 1.0906e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8257 rms_Q~0.6740
  step 560/685 | loss_L=1.0338 | loss_Q=1.0419 | scale_pen(L)= 2.3523e-03 | scale_pen(Q)= 1.0893e-04 | grad_norm=0.63 | sec/step~1.10 | rms_L~0.8257 rms_Q~0.6741
  step 570/685 | loss_L=1.0401 | loss_Q=1.0690 | scale_pen(L)= 2.3509e-03 | scale_pen(Q)= 1.0845e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8257 rms_Q~0.6741
  step 580/685 | loss_L=0.9988 | loss_Q=1.0075 | scale_pen(L)= 2.3477e-03 | scale_pen(Q)= 1.0808e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8258 rms_Q~0.6741
  step 590/685 | loss_L=1.0697 | loss_Q=1.1253 | scale_pen(L)= 2.3501e-03 | scale_pen(Q)= 1.1018e-04 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8258 rms_Q~0.6741
  step 600/685 | loss_L=1.2144 | loss_Q=1.2198 | scale_pen(L)= 2.3520e-03 | scale_pen(Q)= 1.1206e-04 | grad_norm=0.67 | sec/step~1.05 | rms_L~0.8259 rms_Q~0.6741
  step 610/685 | loss_L=1.0451 | loss_Q=1.0399 | scale_pen(L)= 2.3526e-03 | scale_pen(Q)= 1.1419e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8259 rms_Q~0.6742
  step 620/685 | loss_L=1.0555 | loss_Q=1.0620 | scale_pen(L)= 2.3522e-03 | scale_pen(Q)= 1.1240e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8259 rms_Q~0.6742
  step 630/685 | loss_L=1.0405 | loss_Q=1.0480 | scale_pen(L)= 2.3530e-03 | scale_pen(Q)= 1.1228e-04 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8260 rms_Q~0.6742
  step 640/685 | loss_L=1.3165 | loss_Q=1.3258 | scale_pen(L)= 2.3495e-03 | scale_pen(Q)= 1.1153e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8260 rms_Q~0.6743
  step 650/685 | loss_L=0.9811 | loss_Q=0.9938 | scale_pen(L)= 2.3566e-03 | scale_pen(Q)= 1.1242e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8261 rms_Q~0.6743
  step 660/685 | loss_L=1.0996 | loss_Q=1.1253 | scale_pen(L)= 2.3650e-03 | scale_pen(Q)= 1.1264e-04 | grad_norm=0.63 | sec/step~1.14 | rms_L~0.8261 rms_Q~0.6743
  step 670/685 | loss_L=1.2079 | loss_Q=1.2148 | scale_pen(L)= 2.3635e-03 | scale_pen(Q)= 1.1193e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8261 rms_Q~0.6743
  step 680/685 | loss_L=0.9775 | loss_Q=0.9837 | scale_pen(L)= 2.3576e-03 | scale_pen(Q)= 1.1029e-04 | grad_norm=0.65 | sec/step~1.15 | rms_L~0.8262 rms_Q~0.6744
  step 685/685 | loss_L=1.1804 | loss_Q=1.1967 | scale_pen(L)= 2.3599e-03 | scale_pen(Q)= 1.1047e-04 | grad_norm=0.64 | sec/step~0.43 | rms_L~0.8262 rms_Q~0.6744
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8261841518165421, 'count': 685}, 'qwen': {'rms_mean': 0.6743629559983302, 'count': 685}}

Evaluating epoch 13 checkpoint...
Evaluating: runs/8B_clean_answer/epoch13 -> runs/8B_clean_answer/eval_epoch13
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch13/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch13/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2842.63it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.14s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.573225985849255, 'neutral_chat': 8.583062347100705, 'llama_chat': 8.668592626545705} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch13/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.82113 -> target=0.01057
[debug:llama] adapter.scale=1.0486 | Z.std=0.9985 Z.mean||=15.9759 | prefix.std=0.0106 prefix.mean||=0.6748 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the is not the of the but the'
  1: 's, and of the and of the'
  2: '2. housing costs housing and costs of housing housing'
  3: '2. the of the, the of the and'
  4: 'ation of and of elements the element is in'
Saved Llama results to runs/8B_clean_answer/eval_epoch13/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3508.41it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.43it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.37720328821707, 'neutral_chat': 8.443531967975476, 'qwen_chat': 8.337794871872696} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch13/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.67488 -> target=0.01363
[debug:qwen] adapter.scale=1.0105 | Z.std=0.9986 Z.mean||=15.9781 | prefix.std=0.0136 prefix.mean||=0.8149 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and are the of and the of and the of and the'
  1: ''
  2: 'and of the 1990s the of the'
  3: 'and the of the and the of the and the of the'
  4: '\n\n \n\n'
Saved Qwen results to runs/8B_clean_answer/eval_epoch13/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2905.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5827.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.73s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.028  |  NLL/token (gold): 8.572221201833955
Qwen   EM: 0.000   F1: 0.022  |  NLL/token (gold): 8.335433462309458
Wall clock: 12.92s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.76s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.023
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.037

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.732406854629517,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.92491626739502,
    "llama": {
      "em": 0.0,
      "f1": 0.028429176817334714,
      "nll_token": 8.572221201833955
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.022321204938851993,
      "nll_token": 8.335433462309458
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.761043310165405,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.022510950435873037,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0485783815383911,
      "Z_std": 0.9984936714172363,
      "Z_mean_norm": 15.975883483886719,
      "prefix_std": 0.010572678409516811,
      "prefix_mean_norm": 0.6747868061065674,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0105104446411133,
      "Z_std": 0.9986310601234436,
      "Z_mean_norm": 15.978080749511719,
      "prefix_std": 0.013631384819746017,
      "prefix_mean_norm": 0.8148948550224304,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.037449132566779626
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch13/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch13/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.028 | Qwen 0.022


=========================================
EPOCH 14/16
=========================================

Training epoch 14...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7869.24it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.14s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4370.20it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=13, global_step=8905
Epoch 14/1
  step 10/685 | loss_L=1.0000 | loss_Q=1.0032 | scale_pen(L)= 2.3566e-03 | scale_pen(Q)= 1.0752e-04 | grad_norm=0.59 | sec/step~1.10 | rms_L~0.8287 rms_Q~0.6761
  step 20/685 | loss_L=1.0578 | loss_Q=1.0527 | scale_pen(L)= 2.3579e-03 | scale_pen(Q)= 1.0687e-04 | grad_norm=0.61 | sec/step~1.11 | rms_L~0.8286 rms_Q~0.6760
  step 30/685 | loss_L=1.0512 | loss_Q=1.0442 | scale_pen(L)= 2.3567e-03 | scale_pen(Q)= 1.0730e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8288 rms_Q~0.6761
  step 40/685 | loss_L=1.0943 | loss_Q=1.1148 | scale_pen(L)= 2.3613e-03 | scale_pen(Q)= 1.0562e-04 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.8288 rms_Q~0.6761
  step 50/685 | loss_L=1.0688 | loss_Q=1.0569 | scale_pen(L)= 2.3593e-03 | scale_pen(Q)= 1.0537e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8289 rms_Q~0.6761
  step 60/685 | loss_L=1.0456 | loss_Q=1.0443 | scale_pen(L)= 2.3596e-03 | scale_pen(Q)= 1.0685e-04 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.8289 rms_Q~0.6761
  step 70/685 | loss_L=1.1696 | loss_Q=1.1825 | scale_pen(L)= 2.3551e-03 | scale_pen(Q)= 1.0615e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8290 rms_Q~0.6761
  step 80/685 | loss_L=1.1841 | loss_Q=1.1984 | scale_pen(L)= 2.3588e-03 | scale_pen(Q)= 1.0406e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8290 rms_Q~0.6762
  step 90/685 | loss_L=1.0659 | loss_Q=1.0947 | scale_pen(L)= 2.3553e-03 | scale_pen(Q)= 1.0439e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8290 rms_Q~0.6762
  step 100/685 | loss_L=1.0943 | loss_Q=1.1347 | scale_pen(L)= 2.3545e-03 | scale_pen(Q)= 1.0381e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8290 rms_Q~0.6762
  step 110/685 | loss_L=1.1232 | loss_Q=1.1299 | scale_pen(L)= 2.3552e-03 | scale_pen(Q)= 1.0587e-04 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.8290 rms_Q~0.6762
  step 120/685 | loss_L=1.2692 | loss_Q=1.2902 | scale_pen(L)= 2.3566e-03 | scale_pen(Q)= 1.0622e-04 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8289 rms_Q~0.6762
  step 130/685 | loss_L=0.9578 | loss_Q=0.9739 | scale_pen(L)= 2.3551e-03 | scale_pen(Q)= 1.0652e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8289 rms_Q~0.6762
  step 140/685 | loss_L=1.0079 | loss_Q=1.0041 | scale_pen(L)= 2.3526e-03 | scale_pen(Q)= 1.0633e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8290 rms_Q~0.6762
  step 150/685 | loss_L=0.9926 | loss_Q=1.0265 | scale_pen(L)= 2.3604e-03 | scale_pen(Q)= 1.0712e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8290 rms_Q~0.6762
  step 160/685 | loss_L=1.1236 | loss_Q=1.1454 | scale_pen(L)= 2.3574e-03 | scale_pen(Q)= 1.0444e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8290 rms_Q~0.6762
  step 170/685 | loss_L=1.0474 | loss_Q=1.0448 | scale_pen(L)= 2.3615e-03 | scale_pen(Q)= 1.0455e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8290 rms_Q~0.6762
  step 180/685 | loss_L=1.0129 | loss_Q=1.0150 | scale_pen(L)= 2.3667e-03 | scale_pen(Q)= 1.0573e-04 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.8291 rms_Q~0.6762
  step 190/685 | loss_L=1.0177 | loss_Q=1.0258 | scale_pen(L)= 2.3623e-03 | scale_pen(Q)= 1.0702e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8291 rms_Q~0.6763
  step 200/685 | loss_L=1.1388 | loss_Q=1.1607 | scale_pen(L)= 2.3615e-03 | scale_pen(Q)= 1.0553e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8292 rms_Q~0.6763
  step 210/685 | loss_L=1.2246 | loss_Q=1.2333 | scale_pen(L)= 2.3650e-03 | scale_pen(Q)= 1.0497e-04 | grad_norm=0.65 | sec/step~1.12 | rms_L~0.8292 rms_Q~0.6763
  step 220/685 | loss_L=1.0664 | loss_Q=1.0783 | scale_pen(L)= 2.3653e-03 | scale_pen(Q)= 1.0395e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8293 rms_Q~0.6763
  step 230/685 | loss_L=1.1597 | loss_Q=1.1805 | scale_pen(L)= 2.3644e-03 | scale_pen(Q)= 1.0295e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8293 rms_Q~0.6764
  step 240/685 | loss_L=1.0215 | loss_Q=1.0230 | scale_pen(L)= 2.3674e-03 | scale_pen(Q)= 1.0337e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8293 rms_Q~0.6764
  step 250/685 | loss_L=1.1182 | loss_Q=1.1322 | scale_pen(L)= 2.3611e-03 | scale_pen(Q)= 1.0402e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8294 rms_Q~0.6764
  step 260/685 | loss_L=1.1182 | loss_Q=1.1046 | scale_pen(L)= 2.3677e-03 | scale_pen(Q)= 1.0244e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8294 rms_Q~0.6764
  step 270/685 | loss_L=1.0774 | loss_Q=1.0844 | scale_pen(L)= 2.3719e-03 | scale_pen(Q)= 1.0262e-04 | grad_norm=0.60 | sec/step~1.23 | rms_L~0.8294 rms_Q~0.6764
  step 280/685 | loss_L=1.1134 | loss_Q=1.1266 | scale_pen(L)= 2.3698e-03 | scale_pen(Q)= 1.0199e-04 | grad_norm=0.62 | sec/step~1.05 | rms_L~0.8295 rms_Q~0.6764
  step 290/685 | loss_L=1.0244 | loss_Q=1.0016 | scale_pen(L)= 2.3727e-03 | scale_pen(Q)= 1.0174e-04 | grad_norm=0.60 | sec/step~1.18 | rms_L~0.8295 rms_Q~0.6765
  step 300/685 | loss_L=1.0583 | loss_Q=1.0642 | scale_pen(L)= 2.3687e-03 | scale_pen(Q)= 1.0332e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8295 rms_Q~0.6765
  step 310/685 | loss_L=1.1192 | loss_Q=1.1186 | scale_pen(L)= 2.3728e-03 | scale_pen(Q)= 1.0291e-04 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8296 rms_Q~0.6765
  step 320/685 | loss_L=1.0575 | loss_Q=1.0736 | scale_pen(L)= 2.3737e-03 | scale_pen(Q)= 1.0280e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8296 rms_Q~0.6765
  step 330/685 | loss_L=1.2075 | loss_Q=1.2353 | scale_pen(L)= 2.3761e-03 | scale_pen(Q)= 1.0362e-04 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.8296 rms_Q~0.6765
  step 340/685 | loss_L=1.1516 | loss_Q=1.1572 | scale_pen(L)= 2.3780e-03 | scale_pen(Q)= 1.0545e-04 | grad_norm=0.63 | sec/step~1.27 | rms_L~0.8297 rms_Q~0.6766
  step 350/685 | loss_L=1.2480 | loss_Q=1.2521 | scale_pen(L)= 2.3887e-03 | scale_pen(Q)= 1.0749e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8297 rms_Q~0.6766
  step 360/685 | loss_L=1.2033 | loss_Q=1.2488 | scale_pen(L)= 2.3866e-03 | scale_pen(Q)= 1.0806e-04 | grad_norm=0.61 | sec/step~1.17 | rms_L~0.8298 rms_Q~0.6766
  step 370/685 | loss_L=1.2010 | loss_Q=1.2264 | scale_pen(L)= 2.3808e-03 | scale_pen(Q)= 1.1007e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8298 rms_Q~0.6766
  step 380/685 | loss_L=1.0262 | loss_Q=1.0385 | scale_pen(L)= 2.3723e-03 | scale_pen(Q)= 1.1041e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8298 rms_Q~0.6767
  step 390/685 | loss_L=1.0452 | loss_Q=1.0280 | scale_pen(L)= 2.3683e-03 | scale_pen(Q)= 1.1241e-04 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.8299 rms_Q~0.6767
  step 400/685 | loss_L=1.1808 | loss_Q=1.1943 | scale_pen(L)= 2.3611e-03 | scale_pen(Q)= 1.1376e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8299 rms_Q~0.6767
  step 410/685 | loss_L=1.1479 | loss_Q=1.1531 | scale_pen(L)= 2.3632e-03 | scale_pen(Q)= 1.1291e-04 | grad_norm=0.65 | sec/step~1.13 | rms_L~0.8299 rms_Q~0.6767
  step 420/685 | loss_L=1.0405 | loss_Q=1.0543 | scale_pen(L)= 2.3621e-03 | scale_pen(Q)= 1.1332e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8299 rms_Q~0.6768
  step 430/685 | loss_L=1.0486 | loss_Q=1.0615 | scale_pen(L)= 2.3624e-03 | scale_pen(Q)= 1.1461e-04 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.8300 rms_Q~0.6768
  step 440/685 | loss_L=1.1143 | loss_Q=1.1262 | scale_pen(L)= 2.3585e-03 | scale_pen(Q)= 1.1539e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8300 rms_Q~0.6768
  step 450/685 | loss_L=1.0021 | loss_Q=1.0166 | scale_pen(L)= 2.3617e-03 | scale_pen(Q)= 1.1524e-04 | grad_norm=0.65 | sec/step~1.08 | rms_L~0.8300 rms_Q~0.6769
  step 460/685 | loss_L=1.2075 | loss_Q=1.2058 | scale_pen(L)= 2.3625e-03 | scale_pen(Q)= 1.1422e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8301 rms_Q~0.6769
  step 470/685 | loss_L=1.1980 | loss_Q=1.2145 | scale_pen(L)= 2.3666e-03 | scale_pen(Q)= 1.1496e-04 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8301 rms_Q~0.6769
  step 480/685 | loss_L=1.2432 | loss_Q=1.2368 | scale_pen(L)= 2.3672e-03 | scale_pen(Q)= 1.1291e-04 | grad_norm=0.60 | sec/step~1.18 | rms_L~0.8301 rms_Q~0.6769
  step 490/685 | loss_L=1.0905 | loss_Q=1.0869 | scale_pen(L)= 2.3707e-03 | scale_pen(Q)= 1.1291e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8302 rms_Q~0.6770
  step 500/685 | loss_L=1.0200 | loss_Q=1.0464 | scale_pen(L)= 2.3700e-03 | scale_pen(Q)= 1.1318e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8302 rms_Q~0.6770
  step 510/685 | loss_L=1.1748 | loss_Q=1.1864 | scale_pen(L)= 2.3766e-03 | scale_pen(Q)= 1.1371e-04 | grad_norm=0.59 | sec/step~1.06 | rms_L~0.8302 rms_Q~0.6770
  step 520/685 | loss_L=1.1692 | loss_Q=1.1639 | scale_pen(L)= 2.3830e-03 | scale_pen(Q)= 1.1302e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8303 rms_Q~0.6771
  step 530/685 | loss_L=1.0934 | loss_Q=1.1047 | scale_pen(L)= 2.3823e-03 | scale_pen(Q)= 1.1271e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8303 rms_Q~0.6771
  step 540/685 | loss_L=1.0662 | loss_Q=1.0639 | scale_pen(L)= 2.3826e-03 | scale_pen(Q)= 1.0935e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8303 rms_Q~0.6771
  step 550/685 | loss_L=1.1113 | loss_Q=1.1438 | scale_pen(L)= 2.3876e-03 | scale_pen(Q)= 1.0880e-04 | grad_norm=0.64 | sec/step~1.19 | rms_L~0.8304 rms_Q~0.6772
  step 560/685 | loss_L=1.1088 | loss_Q=1.1057 | scale_pen(L)= 2.3935e-03 | scale_pen(Q)= 1.0843e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8304 rms_Q~0.6772
  step 570/685 | loss_L=1.0791 | loss_Q=1.0914 | scale_pen(L)= 2.3944e-03 | scale_pen(Q)= 1.0969e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8304 rms_Q~0.6772
  step 580/685 | loss_L=1.1047 | loss_Q=1.0972 | scale_pen(L)= 2.3904e-03 | scale_pen(Q)= 1.1024e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8305 rms_Q~0.6772
  step 590/685 | loss_L=1.0299 | loss_Q=1.0385 | scale_pen(L)= 2.3923e-03 | scale_pen(Q)= 1.1092e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8305 rms_Q~0.6773
  step 600/685 | loss_L=1.0916 | loss_Q=1.0984 | scale_pen(L)= 2.3885e-03 | scale_pen(Q)= 1.0947e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8306 rms_Q~0.6773
  step 610/685 | loss_L=1.1377 | loss_Q=1.1357 | scale_pen(L)= 2.3938e-03 | scale_pen(Q)= 1.0825e-04 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.8306 rms_Q~0.6773
  step 620/685 | loss_L=1.0595 | loss_Q=1.0710 | scale_pen(L)= 2.3925e-03 | scale_pen(Q)= 1.0780e-04 | grad_norm=0.59 | sec/step~1.13 | rms_L~0.8306 rms_Q~0.6773
  step 630/685 | loss_L=1.1000 | loss_Q=1.0977 | scale_pen(L)= 2.3987e-03 | scale_pen(Q)= 1.0746e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8307 rms_Q~0.6774
  step 640/685 | loss_L=0.9920 | loss_Q=1.0289 | scale_pen(L)= 2.3927e-03 | scale_pen(Q)= 1.0891e-04 | grad_norm=0.62 | sec/step~1.16 | rms_L~0.8307 rms_Q~0.6774
  step 650/685 | loss_L=1.0669 | loss_Q=1.0708 | scale_pen(L)= 2.4006e-03 | scale_pen(Q)= 1.1025e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8308 rms_Q~0.6774
  step 660/685 | loss_L=0.9579 | loss_Q=0.9614 | scale_pen(L)= 2.3986e-03 | scale_pen(Q)= 1.0872e-04 | grad_norm=0.65 | sec/step~1.11 | rms_L~0.8308 rms_Q~0.6775
  step 670/685 | loss_L=0.9895 | loss_Q=0.9967 | scale_pen(L)= 2.4026e-03 | scale_pen(Q)= 1.0976e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8308 rms_Q~0.6775
  step 680/685 | loss_L=1.0485 | loss_Q=1.0568 | scale_pen(L)= 2.4036e-03 | scale_pen(Q)= 1.1305e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8309 rms_Q~0.6775
  step 685/685 | loss_L=1.1886 | loss_Q=1.1951 | scale_pen(L)= 2.4032e-03 | scale_pen(Q)= 1.1436e-04 | grad_norm=0.64 | sec/step~0.52 | rms_L~0.8309 rms_Q~0.6775
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8308929502528949, 'count': 685}, 'qwen': {'rms_mean': 0.6775280117118445, 'count': 685}}

Evaluating epoch 14 checkpoint...
Evaluating: runs/8B_clean_answer/epoch14 -> runs/8B_clean_answer/eval_epoch14
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch14/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch14/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3300.65it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.6393034041874, 'neutral_chat': 8.66287830627424, 'llama_chat': 8.785072532100202} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch14/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.82637 -> target=0.01057
[debug:llama] adapter.scale=1.0490 | Z.std=0.9981 Z.mean||=15.9699 | prefix.std=0.0106 prefix.mean||=0.6747 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the molecule is the of the molecule'
  1: '2,000 to,000 individuals the of the'
  2: '2 the of the 20s the of the'
  3: 'Angeles,, the of Angeles, the of Angeles, Angeles'
  4: '2,3,4,5,6'
Saved Llama results to runs/8B_clean_answer/eval_epoch14/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3288.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.41it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.33it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.416114013346414, 'neutral_chat': 8.509295601693411, 'qwen_chat': 8.42124580706238} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch14/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.67649 -> target=0.01363
[debug:qwen] adapter.scale=1.0107 | Z.std=0.9981 Z.mean||=15.9699 | prefix.std=0.0136 prefix.mean||=0.8148 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: 'ands of ands ands ands ands and'
  2: '1920s the of and and and and and'
  3: 'and the of the and the of the and the of the'
  4: '100000000000'
Saved Qwen results to runs/8B_clean_answer/eval_epoch14/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3257.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6369.48it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.42it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.43it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.19s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.030  |  NLL/token (gold): 8.635882891765258
Qwen   EM: 0.000   F1: 0.026  |  NLL/token (gold): 8.407447650319053
Wall clock: 13.09s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.53s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.024
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.043

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.191887855529785,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.088459730148315,
    "llama": {
      "em": 0.0,
      "f1": 0.030051619132501487,
      "nll_token": 8.635882891765258
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.026349588533092362,
      "nll_token": 8.407447650319053
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.534106254577637,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0238320649938297,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0490227937698364,
      "Z_std": 0.9981169104576111,
      "Z_mean_norm": 15.969854354858398,
      "prefix_std": 0.010572683066129684,
      "prefix_mean_norm": 0.6747214198112488,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0106937885284424,
      "Z_std": 0.9981169104576111,
      "Z_mean_norm": 15.969854354858398,
      "prefix_std": 0.013631409965455532,
      "prefix_mean_norm": 0.8148394823074341,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.04291636489398637
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch14/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch14/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.026


=========================================
EPOCH 15/16
=========================================

Training epoch 15...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7516.67it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.09s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.12s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4568.96it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.44it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=14, global_step=9590
Epoch 15/1
  step 10/685 | loss_L=1.2447 | loss_Q=1.2798 | scale_pen(L)= 2.4022e-03 | scale_pen(Q)= 1.1651e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8337 rms_Q~0.6798
  step 20/685 | loss_L=1.1666 | loss_Q=1.1656 | scale_pen(L)= 2.3988e-03 | scale_pen(Q)= 1.1630e-04 | grad_norm=0.63 | sec/step~1.15 | rms_L~0.8335 rms_Q~0.6797
  step 30/685 | loss_L=0.9751 | loss_Q=0.9942 | scale_pen(L)= 2.3973e-03 | scale_pen(Q)= 1.1552e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8334 rms_Q~0.6797
  step 40/685 | loss_L=1.0765 | loss_Q=1.0725 | scale_pen(L)= 2.4011e-03 | scale_pen(Q)= 1.1493e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8334 rms_Q~0.6796
  step 50/685 | loss_L=1.1476 | loss_Q=1.1670 | scale_pen(L)= 2.4098e-03 | scale_pen(Q)= 1.1326e-04 | grad_norm=0.64 | sec/step~1.11 | rms_L~0.8334 rms_Q~0.6796
  step 60/685 | loss_L=1.2690 | loss_Q=1.2975 | scale_pen(L)= 2.4042e-03 | scale_pen(Q)= 1.1417e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8334 rms_Q~0.6796
  step 70/685 | loss_L=1.0107 | loss_Q=1.0202 | scale_pen(L)= 2.4026e-03 | scale_pen(Q)= 1.1510e-04 | grad_norm=0.64 | sec/step~1.10 | rms_L~0.8335 rms_Q~0.6797
  step 80/685 | loss_L=1.0547 | loss_Q=1.0739 | scale_pen(L)= 2.4046e-03 | scale_pen(Q)= 1.1514e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8335 rms_Q~0.6797
  step 90/685 | loss_L=1.0445 | loss_Q=1.0422 | scale_pen(L)= 2.4032e-03 | scale_pen(Q)= 1.1469e-04 | grad_norm=0.64 | sec/step~1.15 | rms_L~0.8335 rms_Q~0.6797
  step 100/685 | loss_L=1.1086 | loss_Q=1.1360 | scale_pen(L)= 2.4009e-03 | scale_pen(Q)= 1.1489e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8335 rms_Q~0.6797
  step 110/685 | loss_L=1.1162 | loss_Q=1.1322 | scale_pen(L)= 2.4110e-03 | scale_pen(Q)= 1.1386e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8335 rms_Q~0.6798
  step 120/685 | loss_L=1.0463 | loss_Q=1.0521 | scale_pen(L)= 2.4042e-03 | scale_pen(Q)= 1.1504e-04 | grad_norm=0.60 | sec/step~1.09 | rms_L~0.8336 rms_Q~0.6798
  step 130/685 | loss_L=1.0325 | loss_Q=1.0643 | scale_pen(L)= 2.4086e-03 | scale_pen(Q)= 1.1673e-04 | grad_norm=0.66 | sec/step~1.05 | rms_L~0.8336 rms_Q~0.6798
  step 140/685 | loss_L=1.1249 | loss_Q=1.1268 | scale_pen(L)= 2.4081e-03 | scale_pen(Q)= 1.1639e-04 | grad_norm=0.63 | sec/step~1.18 | rms_L~0.8336 rms_Q~0.6799
  step 150/685 | loss_L=1.1872 | loss_Q=1.2326 | scale_pen(L)= 2.4026e-03 | scale_pen(Q)= 1.1645e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8336 rms_Q~0.6799
  step 160/685 | loss_L=1.0176 | loss_Q=1.0214 | scale_pen(L)= 2.3995e-03 | scale_pen(Q)= 1.1589e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8337 rms_Q~0.6799
  step 170/685 | loss_L=1.0716 | loss_Q=1.0695 | scale_pen(L)= 2.4048e-03 | scale_pen(Q)= 1.1532e-04 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8337 rms_Q~0.6799
  step 180/685 | loss_L=1.0785 | loss_Q=1.1017 | scale_pen(L)= 2.4074e-03 | scale_pen(Q)= 1.1476e-04 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.8337 rms_Q~0.6799
  step 190/685 | loss_L=1.0428 | loss_Q=1.0445 | scale_pen(L)= 2.4089e-03 | scale_pen(Q)= 1.1444e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8337 rms_Q~0.6800
  step 200/685 | loss_L=1.1998 | loss_Q=1.2109 | scale_pen(L)= 2.4092e-03 | scale_pen(Q)= 1.1544e-04 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8337 rms_Q~0.6800
  step 210/685 | loss_L=1.1225 | loss_Q=1.1474 | scale_pen(L)= 2.4091e-03 | scale_pen(Q)= 1.1238e-04 | grad_norm=0.61 | sec/step~1.16 | rms_L~0.8338 rms_Q~0.6800
  step 220/685 | loss_L=1.0440 | loss_Q=1.0681 | scale_pen(L)= 2.4034e-03 | scale_pen(Q)= 1.1267e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8338 rms_Q~0.6800
  step 230/685 | loss_L=1.1196 | loss_Q=1.1185 | scale_pen(L)= 2.4027e-03 | scale_pen(Q)= 1.1409e-04 | grad_norm=0.64 | sec/step~1.10 | rms_L~0.8338 rms_Q~0.6801
  step 240/685 | loss_L=1.1678 | loss_Q=1.1700 | scale_pen(L)= 2.4046e-03 | scale_pen(Q)= 1.1159e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8339 rms_Q~0.6801
  step 250/685 | loss_L=1.1666 | loss_Q=1.1897 | scale_pen(L)= 2.4018e-03 | scale_pen(Q)= 1.1106e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8339 rms_Q~0.6801
  step 260/685 | loss_L=1.1045 | loss_Q=1.1183 | scale_pen(L)= 2.3979e-03 | scale_pen(Q)= 1.1227e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8339 rms_Q~0.6801
  step 270/685 | loss_L=1.0207 | loss_Q=1.0419 | scale_pen(L)= 2.3943e-03 | scale_pen(Q)= 1.1284e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8339 rms_Q~0.6801
  step 280/685 | loss_L=0.9905 | loss_Q=0.9945 | scale_pen(L)= 2.3983e-03 | scale_pen(Q)= 1.1375e-04 | grad_norm=0.62 | sec/step~1.20 | rms_L~0.8340 rms_Q~0.6802
  step 290/685 | loss_L=1.0245 | loss_Q=1.0410 | scale_pen(L)= 2.3994e-03 | scale_pen(Q)= 1.1425e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8340 rms_Q~0.6802
  step 300/685 | loss_L=1.0142 | loss_Q=1.0259 | scale_pen(L)= 2.3932e-03 | scale_pen(Q)= 1.1379e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8340 rms_Q~0.6802
  step 310/685 | loss_L=1.1504 | loss_Q=1.1659 | scale_pen(L)= 2.3932e-03 | scale_pen(Q)= 1.1373e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8340 rms_Q~0.6802
  step 320/685 | loss_L=1.0966 | loss_Q=1.0975 | scale_pen(L)= 2.3857e-03 | scale_pen(Q)= 1.1440e-04 | grad_norm=0.62 | sec/step~1.20 | rms_L~0.8340 rms_Q~0.6802
  step 330/685 | loss_L=1.0980 | loss_Q=1.1217 | scale_pen(L)= 2.3906e-03 | scale_pen(Q)= 1.1360e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8341 rms_Q~0.6803
  step 340/685 | loss_L=1.0917 | loss_Q=1.1062 | scale_pen(L)= 2.3943e-03 | scale_pen(Q)= 1.1234e-04 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.8341 rms_Q~0.6803
  step 350/685 | loss_L=0.9357 | loss_Q=0.9728 | scale_pen(L)= 2.3868e-03 | scale_pen(Q)= 1.1335e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8341 rms_Q~0.6803
  step 360/685 | loss_L=0.9773 | loss_Q=1.0073 | scale_pen(L)= 2.3826e-03 | scale_pen(Q)= 1.1185e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8341 rms_Q~0.6803
  step 370/685 | loss_L=1.1478 | loss_Q=1.1578 | scale_pen(L)= 2.3860e-03 | scale_pen(Q)= 1.1240e-04 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.8341 rms_Q~0.6804
  step 380/685 | loss_L=1.0756 | loss_Q=1.0993 | scale_pen(L)= 2.3828e-03 | scale_pen(Q)= 1.1437e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8342 rms_Q~0.6804
  step 390/685 | loss_L=0.9767 | loss_Q=1.0148 | scale_pen(L)= 2.3837e-03 | scale_pen(Q)= 1.1573e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8342 rms_Q~0.6804
  step 400/685 | loss_L=1.1298 | loss_Q=1.1649 | scale_pen(L)= 2.3782e-03 | scale_pen(Q)= 1.1233e-04 | grad_norm=0.64 | sec/step~1.05 | rms_L~0.8342 rms_Q~0.6804
  step 410/685 | loss_L=1.1470 | loss_Q=1.1406 | scale_pen(L)= 2.3831e-03 | scale_pen(Q)= 1.1243e-04 | grad_norm=0.61 | sec/step~1.05 | rms_L~0.8342 rms_Q~0.6804
  step 420/685 | loss_L=1.0412 | loss_Q=1.0608 | scale_pen(L)= 2.3846e-03 | scale_pen(Q)= 1.1205e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8342 rms_Q~0.6805
  step 430/685 | loss_L=1.0823 | loss_Q=1.0772 | scale_pen(L)= 2.3852e-03 | scale_pen(Q)= 1.1182e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8342 rms_Q~0.6805
  step 440/685 | loss_L=1.0665 | loss_Q=1.0829 | scale_pen(L)= 2.3858e-03 | scale_pen(Q)= 1.1265e-04 | grad_norm=0.63 | sec/step~1.05 | rms_L~0.8343 rms_Q~0.6805
  step 450/685 | loss_L=1.1415 | loss_Q=1.1392 | scale_pen(L)= 2.3879e-03 | scale_pen(Q)= 1.1028e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8343 rms_Q~0.6805
  step 460/685 | loss_L=1.0039 | loss_Q=1.0166 | scale_pen(L)= 2.3881e-03 | scale_pen(Q)= 1.1251e-04 | grad_norm=0.63 | sec/step~1.14 | rms_L~0.8343 rms_Q~0.6805
  step 470/685 | loss_L=1.2412 | loss_Q=1.2467 | scale_pen(L)= 2.3875e-03 | scale_pen(Q)= 1.1288e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8343 rms_Q~0.6806
  step 480/685 | loss_L=1.0514 | loss_Q=1.0633 | scale_pen(L)= 2.3865e-03 | scale_pen(Q)= 1.1307e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8344 rms_Q~0.6806
  step 490/685 | loss_L=1.1215 | loss_Q=1.1515 | scale_pen(L)= 2.3822e-03 | scale_pen(Q)= 1.1497e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8344 rms_Q~0.6806
  step 500/685 | loss_L=1.1810 | loss_Q=1.1868 | scale_pen(L)= 2.3847e-03 | scale_pen(Q)= 1.1542e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8344 rms_Q~0.6806
  step 510/685 | loss_L=1.0075 | loss_Q=1.0211 | scale_pen(L)= 2.3883e-03 | scale_pen(Q)= 1.1503e-04 | grad_norm=0.67 | sec/step~1.06 | rms_L~0.8344 rms_Q~0.6807
  step 520/685 | loss_L=1.0566 | loss_Q=1.0655 | scale_pen(L)= 2.3900e-03 | scale_pen(Q)= 1.1388e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8345 rms_Q~0.6807
  step 530/685 | loss_L=1.0271 | loss_Q=1.0354 | scale_pen(L)= 2.3915e-03 | scale_pen(Q)= 1.1428e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8345 rms_Q~0.6807
  step 540/685 | loss_L=1.0062 | loss_Q=1.0170 | scale_pen(L)= 2.3974e-03 | scale_pen(Q)= 1.1330e-04 | grad_norm=0.63 | sec/step~1.12 | rms_L~0.8345 rms_Q~0.6807
  step 550/685 | loss_L=0.9924 | loss_Q=1.0083 | scale_pen(L)= 2.4001e-03 | scale_pen(Q)= 1.1437e-04 | grad_norm=0.64 | sec/step~1.13 | rms_L~0.8346 rms_Q~0.6808
  step 560/685 | loss_L=1.1380 | loss_Q=1.1379 | scale_pen(L)= 2.4055e-03 | scale_pen(Q)= 1.1666e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8346 rms_Q~0.6808
  step 570/685 | loss_L=1.1570 | loss_Q=1.1655 | scale_pen(L)= 2.4016e-03 | scale_pen(Q)= 1.1718e-04 | grad_norm=0.63 | sec/step~1.08 | rms_L~0.8346 rms_Q~0.6808
  step 580/685 | loss_L=1.1208 | loss_Q=1.1299 | scale_pen(L)= 2.3910e-03 | scale_pen(Q)= 1.1657e-04 | grad_norm=0.65 | sec/step~1.10 | rms_L~0.8347 rms_Q~0.6808
  step 590/685 | loss_L=1.0083 | loss_Q=1.0288 | scale_pen(L)= 2.3900e-03 | scale_pen(Q)= 1.1847e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8347 rms_Q~0.6808
  step 600/685 | loss_L=1.0233 | loss_Q=1.0235 | scale_pen(L)= 2.3938e-03 | scale_pen(Q)= 1.1994e-04 | grad_norm=0.60 | sec/step~1.13 | rms_L~0.8347 rms_Q~0.6809
  step 610/685 | loss_L=0.9719 | loss_Q=0.9610 | scale_pen(L)= 2.3966e-03 | scale_pen(Q)= 1.1928e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8348 rms_Q~0.6809
  step 620/685 | loss_L=1.0766 | loss_Q=1.0941 | scale_pen(L)= 2.3984e-03 | scale_pen(Q)= 1.1899e-04 | grad_norm=0.67 | sec/step~1.09 | rms_L~0.8348 rms_Q~0.6809
  step 630/685 | loss_L=1.1798 | loss_Q=1.1909 | scale_pen(L)= 2.3994e-03 | scale_pen(Q)= 1.1851e-04 | grad_norm=0.59 | sec/step~1.12 | rms_L~0.8348 rms_Q~0.6809
  step 640/685 | loss_L=1.0982 | loss_Q=1.1142 | scale_pen(L)= 2.3973e-03 | scale_pen(Q)= 1.2007e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8349 rms_Q~0.6810
  step 650/685 | loss_L=1.0912 | loss_Q=1.0947 | scale_pen(L)= 2.3946e-03 | scale_pen(Q)= 1.2233e-04 | grad_norm=0.66 | sec/step~1.13 | rms_L~0.8349 rms_Q~0.6810
  step 660/685 | loss_L=0.9502 | loss_Q=0.9695 | scale_pen(L)= 2.3996e-03 | scale_pen(Q)= 1.1969e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8349 rms_Q~0.6810
  step 670/685 | loss_L=1.1106 | loss_Q=1.1273 | scale_pen(L)= 2.4016e-03 | scale_pen(Q)= 1.2056e-04 | grad_norm=0.64 | sec/step~1.16 | rms_L~0.8350 rms_Q~0.6810
  step 680/685 | loss_L=0.9543 | loss_Q=0.9641 | scale_pen(L)= 2.4086e-03 | scale_pen(Q)= 1.2256e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8350 rms_Q~0.6811
  step 685/685 | loss_L=1.0571 | loss_Q=1.0806 | scale_pen(L)= 2.4092e-03 | scale_pen(Q)= 1.2039e-04 | grad_norm=0.64 | sec/step~0.43 | rms_L~0.8350 rms_Q~0.6811
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8350168023666326, 'count': 685}, 'qwen': {'rms_mean': 0.6810845429879906, 'count': 685}}

Evaluating epoch 15 checkpoint...
Evaluating: runs/8B_clean_answer/epoch15 -> runs/8B_clean_answer/eval_epoch15
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch15/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch15/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3322.88it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.66044123426857, 'neutral_chat': 8.6582733930644, 'llama_chat': 8.772139330840165} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch15/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.83714 -> target=0.01057
[debug:llama] adapter.scale=1.0491 | Z.std=0.9979 Z.mean||=15.9663 | prefix.std=0.0106 prefix.mean||=0.6747 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of bonds between molecules the of bonds between atoms'
  1: 's, and some species of and, and some'
  2: '2 the of housing in the US the of housing'
  3: 'the of the, the of the and the of the'
  4: 'Al,,, (,), (,), ('
Saved Llama results to runs/8B_clean_answer/eval_epoch15/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3457.08it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.18it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.554396162903497, 'neutral_chat': 8.66707457972582, 'qwen_chat': 8.605839139885372} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch15/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.67967 -> target=0.01363
[debug:qwen] adapter.scale=1.0110 | Z.std=0.9977 Z.mean||=15.9637 | prefix.std=0.0136 prefix.mean||=0.8148 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: 'ands of, ands, ands, ands'
  2: '1920s the of and were in the of'
  3: '100,000-seat arena the of the'
  4: '122222222222'
Saved Qwen results to runs/8B_clean_answer/eval_epoch15/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3514.29it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6505.32it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.40it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.45it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.12s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.037  |  NLL/token (gold): 8.658454694985803
Qwen   EM: 0.000   F1: 0.020  |  NLL/token (gold): 8.568282164909222
Wall clock: 12.91s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.76s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.027
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.047

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.115791082382202,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 12.914813756942749,
    "llama": {
      "em": 0.0,
      "f1": 0.0367014097014097,
      "nll_token": 8.658454694985803
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.01964495226995227,
      "nll_token": 8.568282164909222
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.756850481033325,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.026825438450438447,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0490837097167969,
      "Z_std": 0.997895359992981,
      "Z_mean_norm": 15.96630859375,
      "prefix_std": 0.01057267002761364,
      "prefix_mean_norm": 0.6747087240219116,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0109721422195435,
      "Z_std": 0.9977326989173889,
      "Z_mean_norm": 15.963706016540527,
      "prefix_std": 0.013631422072649002,
      "prefix_mean_norm": 0.8147774934768677,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.047062340437340436
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch15/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch15/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.037 | Qwen 0.020


=========================================
EPOCH 16/16
=========================================

Training epoch 16...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6432.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.15s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4032.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.34it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=15, global_step=10275
Epoch 16/1
  step 10/685 | loss_L=1.0654 | loss_Q=1.0917 | scale_pen(L)= 2.4122e-03 | scale_pen(Q)= 1.2051e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8376 rms_Q~0.6828
  step 20/685 | loss_L=1.1903 | loss_Q=1.1914 | scale_pen(L)= 2.4158e-03 | scale_pen(Q)= 1.2028e-04 | grad_norm=0.66 | sec/step~1.10 | rms_L~0.8375 rms_Q~0.6826
  step 30/685 | loss_L=1.1358 | loss_Q=1.1389 | scale_pen(L)= 2.4129e-03 | scale_pen(Q)= 1.1988e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8374 rms_Q~0.6826
  step 40/685 | loss_L=1.1722 | loss_Q=1.1661 | scale_pen(L)= 2.4048e-03 | scale_pen(Q)= 1.1985e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8374 rms_Q~0.6826
  step 50/685 | loss_L=1.1114 | loss_Q=1.1495 | scale_pen(L)= 2.4091e-03 | scale_pen(Q)= 1.2107e-04 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8375 rms_Q~0.6827
  step 60/685 | loss_L=1.0103 | loss_Q=1.0225 | scale_pen(L)= 2.4103e-03 | scale_pen(Q)= 1.2175e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8374 rms_Q~0.6827
  step 70/685 | loss_L=1.0324 | loss_Q=1.0243 | scale_pen(L)= 2.4178e-03 | scale_pen(Q)= 1.2362e-04 | grad_norm=0.62 | sec/step~1.11 | rms_L~0.8375 rms_Q~0.6828
  step 80/685 | loss_L=0.9391 | loss_Q=0.9586 | scale_pen(L)= 2.4146e-03 | scale_pen(Q)= 1.2523e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8375 rms_Q~0.6828
  step 90/685 | loss_L=1.1675 | loss_Q=1.1589 | scale_pen(L)= 2.4141e-03 | scale_pen(Q)= 1.2353e-04 | grad_norm=0.65 | sec/step~1.16 | rms_L~0.8376 rms_Q~0.6829
  step 100/685 | loss_L=0.9713 | loss_Q=0.9998 | scale_pen(L)= 2.4180e-03 | scale_pen(Q)= 1.2419e-04 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8376 rms_Q~0.6829
  step 110/685 | loss_L=1.1185 | loss_Q=1.1363 | scale_pen(L)= 2.4213e-03 | scale_pen(Q)= 1.2456e-04 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.8376 rms_Q~0.6830
  step 120/685 | loss_L=1.1358 | loss_Q=1.1334 | scale_pen(L)= 2.4243e-03 | scale_pen(Q)= 1.2369e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8377 rms_Q~0.6830
  step 130/685 | loss_L=1.1408 | loss_Q=1.1700 | scale_pen(L)= 2.4200e-03 | scale_pen(Q)= 1.2248e-04 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.8377 rms_Q~0.6830
  step 140/685 | loss_L=1.0176 | loss_Q=1.0168 | scale_pen(L)= 2.4211e-03 | scale_pen(Q)= 1.2535e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8378 rms_Q~0.6831
  step 150/685 | loss_L=1.1201 | loss_Q=1.1631 | scale_pen(L)= 2.4184e-03 | scale_pen(Q)= 1.2316e-04 | grad_norm=0.64 | sec/step~1.09 | rms_L~0.8378 rms_Q~0.6831
  step 160/685 | loss_L=1.0631 | loss_Q=1.0811 | scale_pen(L)= 2.4247e-03 | scale_pen(Q)= 1.2183e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8378 rms_Q~0.6831
  step 170/685 | loss_L=0.9612 | loss_Q=0.9664 | scale_pen(L)= 2.4238e-03 | scale_pen(Q)= 1.2244e-04 | grad_norm=0.62 | sec/step~1.08 | rms_L~0.8378 rms_Q~0.6831
  step 180/685 | loss_L=0.9790 | loss_Q=1.0169 | scale_pen(L)= 2.4276e-03 | scale_pen(Q)= 1.2226e-04 | grad_norm=0.62 | sec/step~1.07 | rms_L~0.8379 rms_Q~0.6831
  step 190/685 | loss_L=1.0681 | loss_Q=1.0789 | scale_pen(L)= 2.4257e-03 | scale_pen(Q)= 1.2251e-04 | grad_norm=0.65 | sec/step~1.09 | rms_L~0.8379 rms_Q~0.6832
  step 200/685 | loss_L=1.1145 | loss_Q=1.1255 | scale_pen(L)= 2.4192e-03 | scale_pen(Q)= 1.2341e-04 | grad_norm=0.63 | sec/step~1.16 | rms_L~0.8379 rms_Q~0.6832
  step 210/685 | loss_L=0.9880 | loss_Q=1.0029 | scale_pen(L)= 2.4253e-03 | scale_pen(Q)= 1.2201e-04 | grad_norm=0.67 | sec/step~1.07 | rms_L~0.8379 rms_Q~0.6832
  step 220/685 | loss_L=1.0242 | loss_Q=1.0294 | scale_pen(L)= 2.4287e-03 | scale_pen(Q)= 1.2312e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8380 rms_Q~0.6832
  step 230/685 | loss_L=1.1079 | loss_Q=1.1259 | scale_pen(L)= 2.4287e-03 | scale_pen(Q)= 1.2236e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8380 rms_Q~0.6833
  step 240/685 | loss_L=1.1232 | loss_Q=1.1335 | scale_pen(L)= 2.4313e-03 | scale_pen(Q)= 1.2256e-04 | grad_norm=0.60 | sec/step~1.10 | rms_L~0.8380 rms_Q~0.6833
  step 250/685 | loss_L=1.0981 | loss_Q=1.1273 | scale_pen(L)= 2.4242e-03 | scale_pen(Q)= 1.2200e-04 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.8380 rms_Q~0.6833
  step 260/685 | loss_L=0.9783 | loss_Q=1.0123 | scale_pen(L)= 2.4218e-03 | scale_pen(Q)= 1.2342e-04 | grad_norm=0.64 | sec/step~1.09 | rms_L~0.8381 rms_Q~0.6833
  step 270/685 | loss_L=1.1470 | loss_Q=1.1857 | scale_pen(L)= 2.4165e-03 | scale_pen(Q)= 1.2602e-04 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.8381 rms_Q~0.6833
  step 280/685 | loss_L=1.0907 | loss_Q=1.1161 | scale_pen(L)= 2.4126e-03 | scale_pen(Q)= 1.2552e-04 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.8381 rms_Q~0.6834
  step 290/685 | loss_L=1.1590 | loss_Q=1.1742 | scale_pen(L)= 2.4022e-03 | scale_pen(Q)= 1.2531e-04 | grad_norm=0.65 | sec/step~1.13 | rms_L~0.8381 rms_Q~0.6834
  step 300/685 | loss_L=1.1034 | loss_Q=1.1232 | scale_pen(L)= 2.4079e-03 | scale_pen(Q)= 1.2579e-04 | grad_norm=0.65 | sec/step~1.05 | rms_L~0.8381 rms_Q~0.6834
  step 310/685 | loss_L=1.1261 | loss_Q=1.1579 | scale_pen(L)= 2.4165e-03 | scale_pen(Q)= 1.2531e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8381 rms_Q~0.6835
  step 320/685 | loss_L=1.1068 | loss_Q=1.1140 | scale_pen(L)= 2.4092e-03 | scale_pen(Q)= 1.2524e-04 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.8381 rms_Q~0.6835
  step 330/685 | loss_L=1.2251 | loss_Q=1.2672 | scale_pen(L)= 2.4113e-03 | scale_pen(Q)= 1.2672e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8381 rms_Q~0.6835
  step 340/685 | loss_L=0.9408 | loss_Q=0.9616 | scale_pen(L)= 2.4128e-03 | scale_pen(Q)= 1.2713e-04 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.8381 rms_Q~0.6835
  step 350/685 | loss_L=0.9994 | loss_Q=1.0343 | scale_pen(L)= 2.4081e-03 | scale_pen(Q)= 1.2703e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8382 rms_Q~0.6835
  step 360/685 | loss_L=1.2131 | loss_Q=1.2450 | scale_pen(L)= 2.4136e-03 | scale_pen(Q)= 1.2493e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8382 rms_Q~0.6836
  step 370/685 | loss_L=0.9334 | loss_Q=0.9473 | scale_pen(L)= 2.4172e-03 | scale_pen(Q)= 1.2485e-04 | grad_norm=0.64 | sec/step~1.10 | rms_L~0.8382 rms_Q~0.6836
  step 380/685 | loss_L=1.0635 | loss_Q=1.0848 | scale_pen(L)= 2.4176e-03 | scale_pen(Q)= 1.2793e-04 | grad_norm=0.63 | sec/step~1.08 | rms_L~0.8382 rms_Q~0.6836
  step 390/685 | loss_L=1.0732 | loss_Q=1.0973 | scale_pen(L)= 2.4173e-03 | scale_pen(Q)= 1.2761e-04 | grad_norm=0.64 | sec/step~1.08 | rms_L~0.8382 rms_Q~0.6836
  step 400/685 | loss_L=1.0152 | loss_Q=1.0158 | scale_pen(L)= 2.4137e-03 | scale_pen(Q)= 1.2732e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8382 rms_Q~0.6836
  step 410/685 | loss_L=1.1372 | loss_Q=1.1726 | scale_pen(L)= 2.4136e-03 | scale_pen(Q)= 1.2635e-04 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.8382 rms_Q~0.6836
  step 420/685 | loss_L=0.9927 | loss_Q=1.0031 | scale_pen(L)= 2.4100e-03 | scale_pen(Q)= 1.2478e-04 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8382 rms_Q~0.6837
  step 430/685 | loss_L=0.9235 | loss_Q=0.9541 | scale_pen(L)= 2.4205e-03 | scale_pen(Q)= 1.2454e-04 | grad_norm=0.65 | sec/step~1.11 | rms_L~0.8383 rms_Q~0.6837
  step 440/685 | loss_L=0.9866 | loss_Q=1.0011 | scale_pen(L)= 2.4155e-03 | scale_pen(Q)= 1.2237e-04 | grad_norm=0.65 | sec/step~1.13 | rms_L~0.8383 rms_Q~0.6837
  step 450/685 | loss_L=1.2310 | loss_Q=1.2620 | scale_pen(L)= 2.4194e-03 | scale_pen(Q)= 1.2007e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8383 rms_Q~0.6837
  step 460/685 | loss_L=1.1544 | loss_Q=1.1637 | scale_pen(L)= 2.4139e-03 | scale_pen(Q)= 1.1924e-04 | grad_norm=0.65 | sec/step~1.10 | rms_L~0.8383 rms_Q~0.6837
  step 470/685 | loss_L=1.0317 | loss_Q=1.0302 | scale_pen(L)= 2.4143e-03 | scale_pen(Q)= 1.1948e-04 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.8383 rms_Q~0.6838
  step 480/685 | loss_L=1.0509 | loss_Q=1.0538 | scale_pen(L)= 2.4147e-03 | scale_pen(Q)= 1.2116e-04 | grad_norm=0.64 | sec/step~1.12 | rms_L~0.8384 rms_Q~0.6838
  step 490/685 | loss_L=1.1089 | loss_Q=1.1157 | scale_pen(L)= 2.4127e-03 | scale_pen(Q)= 1.2253e-04 | grad_norm=0.66 | sec/step~1.13 | rms_L~0.8384 rms_Q~0.6838
  step 500/685 | loss_L=1.0133 | loss_Q=1.0429 | scale_pen(L)= 2.4110e-03 | scale_pen(Q)= 1.2516e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8384 rms_Q~0.6838
  step 510/685 | loss_L=0.9070 | loss_Q=0.9239 | scale_pen(L)= 2.4172e-03 | scale_pen(Q)= 1.2609e-04 | grad_norm=0.64 | sec/step~1.17 | rms_L~0.8384 rms_Q~0.6838
  step 520/685 | loss_L=1.0868 | loss_Q=1.1096 | scale_pen(L)= 2.4221e-03 | scale_pen(Q)= 1.2448e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8384 rms_Q~0.6838
  step 530/685 | loss_L=1.0199 | loss_Q=1.0366 | scale_pen(L)= 2.4229e-03 | scale_pen(Q)= 1.2327e-04 | grad_norm=0.62 | sec/step~1.13 | rms_L~0.8384 rms_Q~0.6839
  step 540/685 | loss_L=1.0243 | loss_Q=1.0298 | scale_pen(L)= 2.4222e-03 | scale_pen(Q)= 1.2321e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8385 rms_Q~0.6839
  step 550/685 | loss_L=0.9806 | loss_Q=1.0066 | scale_pen(L)= 2.4245e-03 | scale_pen(Q)= 1.2398e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8385 rms_Q~0.6839
  step 560/685 | loss_L=1.0368 | loss_Q=1.0407 | scale_pen(L)= 2.4203e-03 | scale_pen(Q)= 1.2401e-04 | grad_norm=0.61 | sec/step~1.06 | rms_L~0.8385 rms_Q~0.6839
  step 570/685 | loss_L=1.1167 | loss_Q=1.1274 | scale_pen(L)= 2.4246e-03 | scale_pen(Q)= 1.2358e-04 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8385 rms_Q~0.6839
  step 580/685 | loss_L=1.0725 | loss_Q=1.0869 | scale_pen(L)= 2.4169e-03 | scale_pen(Q)= 1.2343e-04 | grad_norm=0.63 | sec/step~1.13 | rms_L~0.8386 rms_Q~0.6840
  step 590/685 | loss_L=1.0445 | loss_Q=1.0717 | scale_pen(L)= 2.4213e-03 | scale_pen(Q)= 1.2410e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8386 rms_Q~0.6840
  step 600/685 | loss_L=0.9494 | loss_Q=0.9397 | scale_pen(L)= 2.4158e-03 | scale_pen(Q)= 1.2302e-04 | grad_norm=0.61 | sec/step~1.15 | rms_L~0.8386 rms_Q~0.6840
  step 610/685 | loss_L=1.1460 | loss_Q=1.1788 | scale_pen(L)= 2.4216e-03 | scale_pen(Q)= 1.2350e-04 | grad_norm=0.63 | sec/step~1.06 | rms_L~0.8386 rms_Q~0.6840
  step 620/685 | loss_L=1.1684 | loss_Q=1.1769 | scale_pen(L)= 2.4295e-03 | scale_pen(Q)= 1.2405e-04 | grad_norm=0.63 | sec/step~1.17 | rms_L~0.8387 rms_Q~0.6840
  step 630/685 | loss_L=1.1925 | loss_Q=1.1921 | scale_pen(L)= 2.4276e-03 | scale_pen(Q)= 1.2359e-04 | grad_norm=0.65 | sec/step~1.06 | rms_L~0.8387 rms_Q~0.6841
  step 640/685 | loss_L=1.1954 | loss_Q=1.2292 | scale_pen(L)= 2.4264e-03 | scale_pen(Q)= 1.2495e-04 | grad_norm=0.62 | sec/step~1.12 | rms_L~0.8387 rms_Q~0.6841
  step 650/685 | loss_L=1.1273 | loss_Q=1.1468 | scale_pen(L)= 2.4277e-03 | scale_pen(Q)= 1.2529e-04 | grad_norm=0.65 | sec/step~1.12 | rms_L~0.8388 rms_Q~0.6841
  step 660/685 | loss_L=1.0403 | loss_Q=1.0495 | scale_pen(L)= 2.4271e-03 | scale_pen(Q)= 1.2594e-04 | grad_norm=0.62 | sec/step~1.06 | rms_L~0.8388 rms_Q~0.6841
  step 670/685 | loss_L=1.0235 | loss_Q=1.0269 | scale_pen(L)= 2.4238e-03 | scale_pen(Q)= 1.2833e-04 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.8388 rms_Q~0.6841
  step 680/685 | loss_L=0.9975 | loss_Q=1.0064 | scale_pen(L)= 2.4279e-03 | scale_pen(Q)= 1.2640e-04 | grad_norm=0.59 | sec/step~1.16 | rms_L~0.8389 rms_Q~0.6842
  step 685/685 | loss_L=0.9880 | loss_Q=1.0106 | scale_pen(L)= 2.4295e-03 | scale_pen(Q)= 1.2553e-04 | grad_norm=0.64 | sec/step~0.53 | rms_L~0.8389 rms_Q~0.6842
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.8388804012841552, 'count': 685}, 'qwen': {'rms_mean': 0.6841729675766325, 'count': 685}}

Evaluating epoch 16 checkpoint...
Evaluating: runs/8B_clean_answer/epoch16 -> runs/8B_clean_answer/eval_epoch16
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch16/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch16/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3261.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.689752071622818, 'neutral_chat': 8.696313778559366, 'llama_chat': 8.815850695785212} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch16/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.83484 -> target=0.01057
[debug:llama] adapter.scale=1.0493 | Z.std=0.9973 Z.mean||=15.9573 | prefix.std=0.0106 prefix.mean||=0.6746 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the molecule the of the molecule the of'
  1: '2. the of the, which the of the'
  2: '2. housing prices were than in the 197'
  3: 'Angeles,, the of Angeles, the of Angeles the'
  4: '2,3,4,5,6'
Saved Llama results to runs/8B_clean_answer/eval_epoch16/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3309.12it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.45it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.44it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.41it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.390699953313858, 'neutral_chat': 8.467909522472866, 'qwen_chat': 8.438669340320365} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch16/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.68254 -> target=0.01363
[debug:qwen] adapter.scale=1.0112 | Z.std=0.9973 Z.mean||=15.9573 | prefix.std=0.0136 prefix.mean||=0.8147 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: 'ands of ands ands ands ands and'
  2: '1920s the of and and and and and'
  3: '100,00000000'
  4: '100000000000'
Saved Qwen results to runs/8B_clean_answer/eval_epoch16/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3287.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7591.50it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.12it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.35it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.91s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.030  |  NLL/token (gold): 8.691138099380632
Qwen   EM: 0.000   F1: 0.025  |  NLL/token (gold): 8.382622984351304
Wall clock: 13.19s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.62s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.023
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.043

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.913238525390625,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.18606972694397,
    "llama": {
      "em": 0.0,
      "f1": 0.0301461038961039,
      "nll_token": 8.691138099380632
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.024734106792610618,
      "nll_token": 8.382622984351304
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.61713695526123,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.023126867413632114,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0492894649505615,
      "Z_std": 0.9973323345184326,
      "Z_mean_norm": 15.95729923248291,
      "prefix_std": 0.010572670958936214,
      "prefix_mean_norm": 0.6745932102203369,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0112040042877197,
      "Z_std": 0.9973323345184326,
      "Z_mean_norm": 15.95729923248291,
      "prefix_std": 0.013631436042487621,
      "prefix_mean_norm": 0.8147056102752686,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.04290138395988778
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch16/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch16/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.025


=========================================
TRAINING COMPLETE - EPOCH SUMMARY
=========================================

Epoch 1 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch1/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.007 | Qwen 0.002

Epoch 2 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch2/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.019 | Qwen 0.019

Epoch 3 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch3/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.032 | Qwen 0.018

Epoch 4 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch4/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.027 | Qwen 0.025

Epoch 5 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch5/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.022 | Qwen 0.022

Epoch 6 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch6/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.026 | Qwen 0.017

Epoch 7 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch7/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.024 | Qwen 0.024

Epoch 8 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch8/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.033 | Qwen 0.019

Epoch 9 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch9/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.019

Epoch 10 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch10/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.033 | Qwen 0.023

Epoch 11 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch11/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.032 | Qwen 0.019

Epoch 12 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch12/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.021

Epoch 13 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch13/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.028 | Qwen 0.022

Epoch 14 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch14/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.026

Epoch 15 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch15/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.037 | Qwen 0.020

Epoch 16 results:
✓ Metrics from: runs/8B_clean_answer/eval_epoch16/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.025


=========================================
PHASE 2: FINAL FULL EVALUATION
=========================================

Best epoch: 14 (avg latent F1: 0.028200603832796926)
Running full evaluation on best checkpoint...
Evaluating: runs/8B_clean_answer/epoch14 -> runs/8B_clean_answer/eval_final_best
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch14/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_final_best/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9029.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.02it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.6393034041874, 'neutral_chat': 8.66287830627424, 'llama_chat': 8.785072532100202} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_final_best/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.82637 -> target=0.01057
[debug:llama] adapter.scale=1.0490 | Z.std=0.9981 Z.mean||=15.9699 | prefix.std=0.0106 prefix.mean||=0.6747 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '2 the of the molecule is the of the molecule'
  1: '2,000 to,000 individuals the of the'
  2: '2 the of the 20s the of the'
  3: 'Angeles,, the of Angeles, the of Angeles, Angeles'
  4: '2,3,4,5,6'
Saved Llama results to runs/8B_clean_answer/eval_final_best/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3303.25it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.416114013346414, 'neutral_chat': 8.509295601693411, 'qwen_chat': 8.42124580706238} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_final_best/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.67649 -> target=0.01363
[debug:qwen] adapter.scale=1.0107 | Z.std=0.9981 Z.mean||=15.9699 | prefix.std=0.0136 prefix.mean||=0.8148 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'and of the and of the and of the and of the'
  1: 'ands of ands ands ands ands and'
  2: '1920s the of and and and and and'
  3: 'and the of the and the of the and the of the'
  4: '100000000000'
Saved Qwen results to runs/8B_clean_answer/eval_final_best/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3264.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8004.40it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 15.73s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.030  |  NLL/token (gold): 8.635882891765258
Qwen   EM: 0.000   F1: 0.026  |  NLL/token (gold): 8.407447650319053
Wall clock: 13.17s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 11.64s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.024
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.043

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 15.729878425598145,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.173708200454712,
    "llama": {
      "em": 0.0,
      "f1": 0.030051619132501487,
      "nll_token": 8.635882891765258
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.026349588533092362,
      "nll_token": 8.407447650319053
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 11.635384798049927,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0238320649938297,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0490227937698364,
      "Z_std": 0.9981169104576111,
      "Z_mean_norm": 15.969854354858398,
      "prefix_std": 0.010572683066129684,
      "prefix_mean_norm": 0.6747214198112488,
      "embed_rms": 0.010578219778835773,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0106937885284424,
      "Z_std": 0.9981169104576111,
      "Z_mean_norm": 15.969854354858398,
      "prefix_std": 0.013631409965455532,
      "prefix_mean_norm": 0.8148394823074341,
      "embed_rms": 0.013649147935211658,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "auto",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "auto",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.04291636489398637
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_final_best/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_final_best/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.030 | Qwen 0.026

=========================================
PIPELINE SUMMARY
=========================================

Run ID: 8B_clean_answer
Completed: Sun Sep 14 17:49:01 PDT 2025

Outputs:
  Epoch 1: runs/8B_clean_answer/epoch1/
           Eval: runs/8B_clean_answer/eval_epoch1/
  Epoch 2: runs/8B_clean_answer/epoch2/
           Eval: runs/8B_clean_answer/eval_epoch2/
  Epoch 3: runs/8B_clean_answer/epoch3/
           Eval: runs/8B_clean_answer/eval_epoch3/
  Epoch 4: runs/8B_clean_answer/epoch4/
           Eval: runs/8B_clean_answer/eval_epoch4/
  Epoch 5: runs/8B_clean_answer/epoch5/
           Eval: runs/8B_clean_answer/eval_epoch5/
  Epoch 6: runs/8B_clean_answer/epoch6/
           Eval: runs/8B_clean_answer/eval_epoch6/
  Epoch 7: runs/8B_clean_answer/epoch7/
           Eval: runs/8B_clean_answer/eval_epoch7/
  Epoch 8: runs/8B_clean_answer/epoch8/
           Eval: runs/8B_clean_answer/eval_epoch8/
  Epoch 9: runs/8B_clean_answer/epoch9/
           Eval: runs/8B_clean_answer/eval_epoch9/
  Epoch 10: runs/8B_clean_answer/epoch10/
           Eval: runs/8B_clean_answer/eval_epoch10/
  Epoch 11: runs/8B_clean_answer/epoch11/
           Eval: runs/8B_clean_answer/eval_epoch11/
  Epoch 12: runs/8B_clean_answer/epoch12/
           Eval: runs/8B_clean_answer/eval_epoch12/
  Epoch 13: runs/8B_clean_answer/epoch13/
           Eval: runs/8B_clean_answer/eval_epoch13/
  Epoch 14: runs/8B_clean_answer/epoch14/
           Eval: runs/8B_clean_answer/eval_epoch14/
  Epoch 15: runs/8B_clean_answer/epoch15/
           Eval: runs/8B_clean_answer/eval_epoch15/
  Epoch 16: runs/8B_clean_answer/epoch16/
           Eval: runs/8B_clean_answer/eval_epoch16/
  Best checkpoint: runs/8B_clean_answer/epoch14/
  Final eval: runs/8B_clean_answer/eval_final_best/
