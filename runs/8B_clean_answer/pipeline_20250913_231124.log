
=========================================
Starting pipeline at Sat Sep 13 23:11:24 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 16 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_answer/ckpt


=========================================
EPOCH 1/16
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1907.59it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.05s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:10<00:10,  5.04s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:15<00:05,  5.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.50s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.07s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2059.31it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:11,  3.85s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:07,  3.84s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:11<00:03,  3.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.74s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
  step 10/685 | loss_L=4.7321 | loss_Q=11.3319 | scale_pen(L)= 2.2257e-07 | scale_pen(Q)= 4.6428e-08 | grad_norm=0.71 | sec/step~1.24 | rms_L~0.5574 rms_Q~0.5582
  step 20/685 | loss_L=2.5559 | loss_Q=9.7760 | scale_pen(L)= 1.6067e-07 | scale_pen(Q)= 1.8254e-07 | grad_norm=0.66 | sec/step~1.07 | rms_L~0.5582 rms_Q~0.5584
  step 30/685 | loss_L=2.4822 | loss_Q=9.2697 | scale_pen(L)= 6.2969e-08 | scale_pen(Q)= 3.6486e-07 | grad_norm=0.71 | sec/step~1.12 | rms_L~0.5593 rms_Q~0.5586
  step 40/685 | loss_L=2.2052 | loss_Q=8.6513 | scale_pen(L)= 1.0853e-07 | scale_pen(Q)= 2.2426e-07 | grad_norm=0.71 | sec/step~1.08 | rms_L~0.5602 rms_Q~0.5591
  step 50/685 | loss_L=1.9173 | loss_Q=8.0650 | scale_pen(L)= 2.0166e-07 | scale_pen(Q)= 1.3918e-07 | grad_norm=0.61 | sec/step~1.12 | rms_L~0.5609 rms_Q~0.5596
  step 60/685 | loss_L=1.9244 | loss_Q=7.3515 | scale_pen(L)= 2.6111e-07 | scale_pen(Q)= 1.1169e-07 | grad_norm=0.70 | sec/step~1.07 | rms_L~0.5614 rms_Q~0.5600
  step 70/685 | loss_L=2.0605 | loss_Q=7.1289 | scale_pen(L)= 2.9265e-07 | scale_pen(Q)= 1.0916e-07 | grad_norm=0.68 | sec/step~1.09 | rms_L~0.5618 rms_Q~0.5604
  step 80/685 | loss_L=2.0991 | loss_Q=6.0757 | scale_pen(L)= 4.1924e-07 | scale_pen(Q)= 1.1869e-07 | grad_norm=0.67 | sec/step~1.06 | rms_L~0.5622 rms_Q~0.5609
  step 90/685 | loss_L=1.7652 | loss_Q=4.6912 | scale_pen(L)= 4.5068e-07 | scale_pen(Q)= 1.1616e-07 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.5624 rms_Q~0.5614
  step 100/685 | loss_L=1.7587 | loss_Q=3.5936 | scale_pen(L)= 4.1431e-07 | scale_pen(Q)= 1.7573e-07 | grad_norm=0.66 | sec/step~1.08 | rms_L~0.5627 rms_Q~0.5619
  step 110/685 | loss_L=1.9689 | loss_Q=3.4230 | scale_pen(L)= 3.7115e-07 | scale_pen(Q)= 3.9034e-07 | grad_norm=0.69 | sec/step~1.10 | rms_L~0.5630 rms_Q~0.5625
  step 120/685 | loss_L=1.8216 | loss_Q=2.8741 | scale_pen(L)= 3.0385e-07 | scale_pen(Q)= 4.2505e-07 | grad_norm=0.67 | sec/step~1.14 | rms_L~0.5633 rms_Q~0.5630
  step 130/685 | loss_L=1.8356 | loss_Q=2.7918 | scale_pen(L)= 1.8091e-07 | scale_pen(Q)= 6.1696e-07 | grad_norm=0.67 | sec/step~1.06 | rms_L~0.5636 rms_Q~0.5636
  step 140/685 | loss_L=1.8188 | loss_Q=3.1542 | scale_pen(L)= 1.3662e-08 | scale_pen(Q)= 4.8260e-07 | grad_norm=0.66 | sec/step~1.14 | rms_L~0.5639 rms_Q~0.5641
  step 150/685 | loss_L=1.7772 | loss_Q=2.6558 | scale_pen(L)= 1.2002e-08 | scale_pen(Q)= 4.8583e-07 | grad_norm=0.73 | sec/step~1.08 | rms_L~0.5643 rms_Q~0.5646
  step 160/685 | loss_L=1.6927 | loss_Q=2.6825 | scale_pen(L)= 3.5712e-07 | scale_pen(Q)= 4.0220e-07 | grad_norm=0.65 | sec/step~1.28 | rms_L~0.5647 rms_Q~0.5651
  step 170/685 | loss_L=1.7222 | loss_Q=2.4949 | scale_pen(L)= 1.3228e-06 | scale_pen(Q)= 4.4121e-07 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.5651 rms_Q~0.5656
  step 180/685 | loss_L=1.6016 | loss_Q=2.3120 | scale_pen(L)= 3.6964e-06 | scale_pen(Q)= 4.7641e-07 | grad_norm=0.67 | sec/step~1.13 | rms_L~0.5657 rms_Q~0.5661
  step 190/685 | loss_L=1.6680 | loss_Q=2.6580 | scale_pen(L)= 5.7676e-06 | scale_pen(Q)= 4.6577e-07 | grad_norm=0.66 | sec/step~1.08 | rms_L~0.5663 rms_Q~0.5666
  step 200/685 | loss_L=1.6867 | loss_Q=2.1710 | scale_pen(L)= 7.5621e-06 | scale_pen(Q)= 4.5405e-07 | grad_norm=0.61 | sec/step~1.08 | rms_L~0.5670 rms_Q~0.5671
  step 210/685 | loss_L=1.7253 | loss_Q=2.1969 | scale_pen(L)= 9.8078e-06 | scale_pen(Q)= 2.5971e-07 | grad_norm=0.65 | sec/step~1.10 | rms_L~0.5676 rms_Q~0.5676
  step 220/685 | loss_L=1.5140 | loss_Q=1.7730 | scale_pen(L)= 1.1678e-05 | scale_pen(Q)= 4.0039e-07 | grad_norm=0.63 | sec/step~1.09 | rms_L~0.5683 rms_Q~0.5680
  step 230/685 | loss_L=1.7643 | loss_Q=1.9982 | scale_pen(L)= 1.5571e-05 | scale_pen(Q)= 5.6304e-07 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.5690 rms_Q~0.5685
  step 240/685 | loss_L=1.7476 | loss_Q=1.9425 | scale_pen(L)= 1.9572e-05 | scale_pen(Q)= 7.1859e-07 | grad_norm=0.64 | sec/step~1.06 | rms_L~0.5698 rms_Q~0.5690
  step 250/685 | loss_L=1.5038 | loss_Q=1.6867 | scale_pen(L)= 2.3581e-05 | scale_pen(Q)= 7.0712e-07 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.5706 rms_Q~0.5695
  step 260/685 | loss_L=1.6236 | loss_Q=1.8391 | scale_pen(L)= 2.8631e-05 | scale_pen(Q)= 9.0393e-07 | grad_norm=0.65 | sec/step~1.07 | rms_L~0.5716 rms_Q~0.5699
  step 270/685 | loss_L=1.5678 | loss_Q=1.7611 | scale_pen(L)= 3.3691e-05 | scale_pen(Q)= 9.9676e-07 | grad_norm=0.56 | sec/step~1.21 | rms_L~0.5725 rms_Q~0.5704
  step 280/685 | loss_L=1.6840 | loss_Q=1.8744 | scale_pen(L)= 3.6993e-05 | scale_pen(Q)= 1.1469e-06 | grad_norm=0.55 | sec/step~1.12 | rms_L~0.5735 rms_Q~0.5708
  step 290/685 | loss_L=1.6034 | loss_Q=1.7921 | scale_pen(L)= 4.1942e-05 | scale_pen(Q)= 9.7392e-07 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.5745 rms_Q~0.5713
  step 300/685 | loss_L=1.4612 | loss_Q=1.6428 | scale_pen(L)= 4.6738e-05 | scale_pen(Q)= 1.0610e-06 | grad_norm=0.56 | sec/step~1.06 | rms_L~0.5756 rms_Q~0.5717
  step 310/685 | loss_L=1.6910 | loss_Q=1.9405 | scale_pen(L)= 5.2225e-05 | scale_pen(Q)= 9.8052e-07 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.5767 rms_Q~0.5721
  step 320/685 | loss_L=1.5658 | loss_Q=1.7684 | scale_pen(L)= 5.7750e-05 | scale_pen(Q)= 9.9854e-07 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5777 rms_Q~0.5725
  step 330/685 | loss_L=1.4302 | loss_Q=1.6529 | scale_pen(L)= 6.4216e-05 | scale_pen(Q)= 8.7225e-07 | grad_norm=0.52 | sec/step~1.13 | rms_L~0.5788 rms_Q~0.5729
  step 340/685 | loss_L=1.3170 | loss_Q=1.4678 | scale_pen(L)= 7.0806e-05 | scale_pen(Q)= 8.4191e-07 | grad_norm=0.54 | sec/step~1.07 | rms_L~0.5799 rms_Q~0.5733
  step 350/685 | loss_L=1.3699 | loss_Q=1.5135 | scale_pen(L)= 7.6576e-05 | scale_pen(Q)= 8.2309e-07 | grad_norm=0.53 | sec/step~1.12 | rms_L~0.5811 rms_Q~0.5736
  step 360/685 | loss_L=1.3311 | loss_Q=1.4694 | scale_pen(L)= 8.2547e-05 | scale_pen(Q)= 9.2284e-07 | grad_norm=0.58 | sec/step~1.06 | rms_L~0.5822 rms_Q~0.5740
  step 370/685 | loss_L=1.3805 | loss_Q=1.5572 | scale_pen(L)= 8.7794e-05 | scale_pen(Q)= 8.9286e-07 | grad_norm=0.53 | sec/step~1.17 | rms_L~0.5833 rms_Q~0.5743
  step 380/685 | loss_L=1.5599 | loss_Q=1.8123 | scale_pen(L)= 9.4756e-05 | scale_pen(Q)= 9.2056e-07 | grad_norm=0.55 | sec/step~1.07 | rms_L~0.5844 rms_Q~0.5747
  step 390/685 | loss_L=1.4941 | loss_Q=1.6901 | scale_pen(L)= 9.9916e-05 | scale_pen(Q)= 9.4473e-07 | grad_norm=0.51 | sec/step~1.12 | rms_L~0.5855 rms_Q~0.5750
  step 400/685 | loss_L=1.5458 | loss_Q=1.7938 | scale_pen(L)= 1.0700e-04 | scale_pen(Q)= 9.3860e-07 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.5866 rms_Q~0.5754
  step 410/685 | loss_L=1.3585 | loss_Q=1.5324 | scale_pen(L)= 1.1433e-04 | scale_pen(Q)= 9.6453e-07 | grad_norm=0.54 | sec/step~1.16 | rms_L~0.5876 rms_Q~0.5757
  step 420/685 | loss_L=1.3010 | loss_Q=1.4496 | scale_pen(L)= 1.2089e-04 | scale_pen(Q)= 9.0439e-07 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.5887 rms_Q~0.5761
  step 430/685 | loss_L=1.3889 | loss_Q=1.5501 | scale_pen(L)= 1.2586e-04 | scale_pen(Q)= 8.0984e-07 | grad_norm=0.52 | sec/step~1.12 | rms_L~0.5897 rms_Q~0.5764
  step 440/685 | loss_L=1.5017 | loss_Q=1.6867 | scale_pen(L)= 1.3166e-04 | scale_pen(Q)= 9.0223e-07 | grad_norm=0.54 | sec/step~1.07 | rms_L~0.5907 rms_Q~0.5767
  step 450/685 | loss_L=1.3796 | loss_Q=1.5390 | scale_pen(L)= 1.3676e-04 | scale_pen(Q)= 8.3525e-07 | grad_norm=0.56 | sec/step~1.11 | rms_L~0.5918 rms_Q~0.5770
  step 460/685 | loss_L=1.5009 | loss_Q=1.6839 | scale_pen(L)= 1.4362e-04 | scale_pen(Q)= 9.9997e-07 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.5928 rms_Q~0.5774
  step 470/685 | loss_L=1.5217 | loss_Q=1.6642 | scale_pen(L)= 1.5005e-04 | scale_pen(Q)= 9.2915e-07 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.5937 rms_Q~0.5777
  step 480/685 | loss_L=1.3953 | loss_Q=1.5741 | scale_pen(L)= 1.5590e-04 | scale_pen(Q)= 1.0465e-06 | grad_norm=0.49 | sec/step~1.12 | rms_L~0.5947 rms_Q~0.5780
  step 490/685 | loss_L=1.4114 | loss_Q=1.5296 | scale_pen(L)= 1.6175e-04 | scale_pen(Q)= 1.0650e-06 | grad_norm=0.45 | sec/step~1.16 | rms_L~0.5956 rms_Q~0.5783
  step 500/685 | loss_L=1.4039 | loss_Q=1.5425 | scale_pen(L)= 1.6815e-04 | scale_pen(Q)= 1.0744e-06 | grad_norm=0.45 | sec/step~1.06 | rms_L~0.5966 rms_Q~0.5786
  step 510/685 | loss_L=1.2957 | loss_Q=1.3962 | scale_pen(L)= 1.7353e-04 | scale_pen(Q)= 1.1883e-06 | grad_norm=0.57 | sec/step~1.06 | rms_L~0.5975 rms_Q~0.5790
  step 520/685 | loss_L=1.4053 | loss_Q=1.5633 | scale_pen(L)= 1.8001e-04 | scale_pen(Q)= 1.3251e-06 | grad_norm=0.55 | sec/step~1.12 | rms_L~0.5984 rms_Q~0.5793
  step 530/685 | loss_L=1.2986 | loss_Q=1.4307 | scale_pen(L)= 1.8429e-04 | scale_pen(Q)= 1.4593e-06 | grad_norm=0.52 | sec/step~1.06 | rms_L~0.5993 rms_Q~0.5796
  step 540/685 | loss_L=1.5387 | loss_Q=1.7478 | scale_pen(L)= 1.9014e-04 | scale_pen(Q)= 1.5101e-06 | grad_norm=0.58 | sec/step~1.13 | rms_L~0.6002 rms_Q~0.5799
  step 550/685 | loss_L=1.4157 | loss_Q=1.5291 | scale_pen(L)= 1.9169e-04 | scale_pen(Q)= 1.4990e-06 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.6010 rms_Q~0.5802
  step 560/685 | loss_L=1.6115 | loss_Q=1.7168 | scale_pen(L)= 1.9380e-04 | scale_pen(Q)= 1.3587e-06 | grad_norm=0.53 | sec/step~1.22 | rms_L~0.6019 rms_Q~0.5805
  step 570/685 | loss_L=1.5094 | loss_Q=1.6437 | scale_pen(L)= 1.9711e-04 | scale_pen(Q)= 1.2370e-06 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.6027 rms_Q~0.5808
  step 580/685 | loss_L=1.3312 | loss_Q=1.4464 | scale_pen(L)= 2.0033e-04 | scale_pen(Q)= 1.2963e-06 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.6035 rms_Q~0.5811
  step 590/685 | loss_L=1.5322 | loss_Q=1.6256 | scale_pen(L)= 2.0474e-04 | scale_pen(Q)= 1.3316e-06 | grad_norm=0.58 | sec/step~1.07 | rms_L~0.6043 rms_Q~0.5814
  step 600/685 | loss_L=1.5370 | loss_Q=1.6212 | scale_pen(L)= 2.0954e-04 | scale_pen(Q)= 1.4229e-06 | grad_norm=0.52 | sec/step~1.17 | rms_L~0.6051 rms_Q~0.5817
  step 610/685 | loss_L=1.3485 | loss_Q=1.4600 | scale_pen(L)= 2.1510e-04 | scale_pen(Q)= 1.3828e-06 | grad_norm=0.47 | sec/step~1.06 | rms_L~0.6058 rms_Q~0.5820
  step 620/685 | loss_L=1.3004 | loss_Q=1.3745 | scale_pen(L)= 2.2107e-04 | scale_pen(Q)= 1.2430e-06 | grad_norm=0.53 | sec/step~1.11 | rms_L~0.6066 rms_Q~0.5823
  step 630/685 | loss_L=1.4473 | loss_Q=1.5662 | scale_pen(L)= 2.2723e-04 | scale_pen(Q)= 1.1501e-06 | grad_norm=0.52 | sec/step~1.12 | rms_L~0.6073 rms_Q~0.5826
  step 640/685 | loss_L=1.5913 | loss_Q=1.7095 | scale_pen(L)= 2.3254e-04 | scale_pen(Q)= 1.1296e-06 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.6081 rms_Q~0.5829
  step 650/685 | loss_L=1.4906 | loss_Q=1.5809 | scale_pen(L)= 2.3791e-04 | scale_pen(Q)= 1.0634e-06 | grad_norm=0.56 | sec/step~1.12 | rms_L~0.6088 rms_Q~0.5832
  step 660/685 | loss_L=1.3392 | loss_Q=1.3835 | scale_pen(L)= 2.4586e-04 | scale_pen(Q)= 1.0650e-06 | grad_norm=0.53 | sec/step~1.08 | rms_L~0.6095 rms_Q~0.5835
  step 670/685 | loss_L=1.3737 | loss_Q=1.4704 | scale_pen(L)= 2.5012e-04 | scale_pen(Q)= 1.0378e-06 | grad_norm=0.50 | sec/step~1.07 | rms_L~0.6102 rms_Q~0.5838
  step 680/685 | loss_L=1.3994 | loss_Q=1.4861 | scale_pen(L)= 2.5598e-04 | scale_pen(Q)= 9.4914e-07 | grad_norm=0.55 | sec/step~1.12 | rms_L~0.6109 rms_Q~0.5840
  step 685/685 | loss_L=1.5217 | loss_Q=1.5791 | scale_pen(L)= 2.5783e-04 | scale_pen(Q)= 9.9153e-07 | grad_norm=0.53 | sec/step~0.51 | rms_L~0.6113 rms_Q~0.5842
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.6112774246800555, 'count': 685}, 'qwen': {'rms_mean': 0.5841779087581773, 'count': 685}}

Evaluating epoch 1 checkpoint...
Evaluating: runs/8B_clean_answer/epoch1 -> runs/8B_clean_answer/eval_epoch1
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch1/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3318.28it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 10.362550882525454, 'neutral_chat': 10.38062169546443, 'llama_chat': 10.40938394572459} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer/eval_epoch1/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.64524 -> target=0.01058
[debug:llama] adapter.scale=1.0161 | Z.std=1.0009 Z.mean||=16.0145 | prefix.std=0.0106 prefix.mean||=0.6764 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '1. 1. 1. 1'
  1: '1.      2'
  2: ''
  3: '1       2'
  4: '1.      2'

[DEBUG] First-step top-k (Llama):
  ex0: <|eot_id|>:0.987, <|begin_of_text|>:0.001, <|eom_id|>:0.000, <|python_tag|>:0.000, the:0.000, s:0.000, ":0.000, <|start_header_id|>:0.000, of:0.000, a:0.000
  ex1: <|eot_id|>:0.994, <|begin_of_text|>:0.001, <|eom_id|>:0.000, <|python_tag|>:0.000, <|start_header_id|>:0.000, ol:0.000, the:0.000, s:0.000, n:0.000, W:0.000
Saved Llama results to runs/8B_clean_answer/eval_epoch1/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3383.19it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.08it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 8.973344100215447, 'neutral_chat': 8.962767481173158, 'qwen_chat': 8.926999508703828} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_clean_answer/eval_epoch1/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.60129 -> target=0.01365
[debug:qwen] adapter.scale=0.9990 | Z.std=1.0012 Z.mean||=16.0199 | prefix.std=0.0137 prefix.mean||=0.8174 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '㎏ ㎏ ㎏ ㎏ ㎏ ㎏'
  1: '㎏ ㎏ ㎏ ㎏ ㎏ ㎏'
  2: '0.0000000000'
  3: '1 1 1 1 1 1'
  4: '1 1 1 1 1 1'

[DEBUG] First-step top-k (Qwen):
  ex0: <|endoftext|>:0.961, ㎏:0.001, 摇了摇头:0.000, 0:0.000, 1:0.000, 2:0.000, ico:0.000, 9:0.000, 4:0.000, 6:0.000
  ex1: <|endoftext|>:0.979, ㎏:0.000, 摇了摇头:0.000, 0:0.000, 1:0.000, 2:0.000, ico:0.000, 9:0.000, .setCellValue:0.000, ute:0.000
Saved Qwen results to runs/8B_clean_answer/eval_epoch1/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3250.14it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.03it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.38it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3146.51it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.25it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.20it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.78s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.005  |  NLL/token (gold): 10.36437278215577
Qwen   EM: 0.000   F1: 0.001   |  NLL/token (gold): 8.930839572634016
Wall clock: 13.84s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 12.73s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.005

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.784362077713013,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.841001749038696,
    "llama": {
      "em": 0.0,
      "f1": 0.004733877233877234,
      "nll_token": 10.36437278215577
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0011111111111111111,
      "nll_token": 8.930839572634016
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 12.733243942260742,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004733877233877234,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0160571336746216,
      "Z_std": 1.0009082555770874,
      "Z_mean_norm": 16.0145206451416,
      "prefix_std": 0.010575059801340103,
      "prefix_mean_norm": 0.6763795614242554,
      "embed_rms": 0.010577211156487465,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 0.9990042448043823,
      "Z_std": 1.0012441873550415,
      "Z_mean_norm": 16.019895553588867,
      "prefix_std": 0.01365412026643753,
      "prefix_mean_norm": 0.8173739314079285,
      "embed_rms": 0.013626697473227978,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "no",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.004733877233877234
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch1/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch1/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.005 | Qwen 0.001


=========================================
EPOCH 2/16
=========================================

Training epoch 2...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3301.95it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:02,  1.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3067.13it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.32it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
   -> start_epoch=1, global_step=685
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.0, 'count': 0}, 'qwen': {'rms_mean': 0.0, 'count': 0}}

Evaluating epoch 2 checkpoint...
Evaluating: runs/8B_clean_answer/epoch2 -> runs/8B_clean_answer/eval_epoch2
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer/epoch2/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer/eval_epoch2/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4719.33it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.16it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.11it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 12.397659044957756, 'neutral_chat': 12.375027285681831, 'llama_chat': 12.413782427911045} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_clean_answer/eval_epoch2/Z_llama_neutral_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.55800 -> target=0.01056
[debug:llama] adapter.scale=1.0000 | Z.std=0.9991 Z.mean||=15.9858 | prefix.std=0.0106 prefix.mean||=0.6760 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '1. The first step is to create a new file called'
  1: '1. The first step is to determine the number of times'
  2: '1. The first step is to determine the number of times'
  3: '1. The first step is to determine the number of iterations'
  4: '1. The first is that the use of the term'

[DEBUG] First-step top-k (Llama):
  ex0: 1:0.293, 2:0.061, 0:0.040, 3:0.037, 4:0.027, 5:0.017, 7:0.015, 6:0.014, <NL>:0.012, 8:0.011
  ex1: 1:0.099, 0:0.077, 2:0.047, 3:0.036, 4:0.030, 5:0.022, 7:0.018, 8:0.015, 6:0.015, 9:0.011
Saved Llama results to runs/8B_clean_answer/eval_epoch2/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3792.32it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 12.141340175160655, 'neutral_chat': 12.175048920370283, 'qwen_chat': 12.203775432847795} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer/eval_epoch2/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.55978 -> target=0.01366
[debug:qwen] adapter.scale=1.0000 | Z.std=0.9991 Z.mean||=15.9858 | prefix.std=0.0137 prefix.mean||=0.8175 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '1. 120'
  1: '1. **Understanding the Problem:**'
  2: '1. **(A) 120**'
  3: '120000000000'
  4: '1. **Understanding the Problem:**'

[DEBUG] First-step top-k (Qwen):
  ex0: 1:0.377, 2:0.178, 3:0.090, 5:0.051, 4:0.051, 6:0.037, 8:0.027, 9:0.024, 7:0.023, 0:0.021
  ex1: 1:0.363, 2:0.194, 3:0.092, 5:0.052, 4:0.049, 6:0.034, 9:0.023, 0:0.020, 8:0.020, 7:0.016
Saved Qwen results to runs/8B_clean_answer/eval_epoch2/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2911.19it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.05it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5491.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.15it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.5x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.38s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.018  |  NLL/token (gold): 12.372998518738347
Qwen   EM: 0.000   F1: 0.003   |  NLL/token (gold): 12.135886958982578
Wall clock: 13.89s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 13.68s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.007
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.020

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.480625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 16.38197088241577,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.887508630752563,
    "llama": {
      "em": 0.0,
      "f1": 0.01789639892271471,
      "nll_token": 12.372998518738347
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002575757575757576,
      "nll_token": 12.135886958982578
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 13.675716400146484,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.006932201570359465,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0,
      "Z_std": 0.999110221862793,
      "Z_mean_norm": 15.985754013061523,
      "prefix_std": 0.010562936775386333,
      "prefix_mean_norm": 0.6760448813438416,
      "embed_rms": 0.010562349110841751,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0,
      "Z_std": 0.9991106390953064,
      "Z_mean_norm": 15.985759735107422,
      "prefix_std": 0.013656198047101498,
      "prefix_mean_norm": 0.8175397515296936,
      "embed_rms": 0.01364919450134039,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "no",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.019563065589381378
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer/eval_epoch2/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer/eval_epoch2/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.018 | Qwen 0.003


=========================================
EPOCH 3/16
=========================================

Training epoch 3...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2912.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.13it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.11it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.45it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4744.69it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.31it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.20it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer/ckpt/state.pt
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 474, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 270, in main
    epoch_loaded, global_loaded = load_checkpoint(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 59, in load_checkpoint
    epoch = int(state.get("epoch", 0))
TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'
