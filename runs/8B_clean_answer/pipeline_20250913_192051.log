
=========================================
Starting pipeline at Sat Sep 13 19:20:51 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 16 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_answer/ckpt


=========================================
EPOCH 1/16
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2880.21it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.18s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3417.64it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.23it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
  step 10/685 | loss_L=2.6576 | loss_Q=10.9235 | scale_pen(L)= 1.8050e-08 | scale_pen(Q)= 4.8800e-09 | grad_norm=0.69 | sec/step~1.07 | rms_L~0.5603 rms_Q~0.5572
  step 20/685 | loss_L=2.6444 | loss_Q=6.4740 | scale_pen(L)= 8.2948e-09 | scale_pen(Q)= 3.7399e-09 | grad_norm=0.64 | sec/step~1.18 | rms_L~0.5605 rms_Q~0.5577
  step 30/685 | loss_L=2.4211 | loss_Q=4.5272 | scale_pen(L)= 2.2806e-07 | scale_pen(Q)= 2.1904e-08 | grad_norm=0.66 | sec/step~1.06 | rms_L~0.5609 rms_Q~0.5583
  step 40/685 | loss_L=2.2478 | loss_Q=3.3552 | scale_pen(L)= 9.6020e-07 | scale_pen(Q)= 8.1684e-08 | grad_norm=0.63 | sec/step~1.10 | rms_L~0.5615 rms_Q~0.5589
  step 50/685 | loss_L=2.1486 | loss_Q=2.6411 | scale_pen(L)= 2.1953e-06 | scale_pen(Q)= 2.4369e-08 | grad_norm=0.70 | sec/step~1.07 | rms_L~0.5623 rms_Q~0.5596
  step 60/685 | loss_L=1.9721 | loss_Q=2.3045 | scale_pen(L)= 4.1919e-06 | scale_pen(Q)= 1.3620e-08 | grad_norm=0.68 | sec/step~1.13 | rms_L~0.5634 rms_Q~0.5604
  step 70/685 | loss_L=1.7702 | loss_Q=1.9658 | scale_pen(L)= 7.2828e-06 | scale_pen(Q)= 1.7980e-07 | grad_norm=0.64 | sec/step~1.07 | rms_L~0.5647 rms_Q~0.5613
  step 80/685 | loss_L=1.7987 | loss_Q=1.9018 | scale_pen(L)= 1.0855e-05 | scale_pen(Q)= 5.7193e-07 | grad_norm=0.69 | sec/step~1.13 | rms_L~0.5662 rms_Q~0.5622
  step 90/685 | loss_L=1.7044 | loss_Q=1.7148 | scale_pen(L)= 1.5531e-05 | scale_pen(Q)= 1.3454e-06 | grad_norm=0.66 | sec/step~1.08 | rms_L~0.5679 rms_Q~0.5632
  step 100/685 | loss_L=1.5085 | loss_Q=1.4892 | scale_pen(L)= 2.1227e-05 | scale_pen(Q)= 1.4542e-06 | grad_norm=0.58 | sec/step~1.11 | rms_L~0.5697 rms_Q~0.5641
  step 110/685 | loss_L=1.7315 | loss_Q=1.7935 | scale_pen(L)= 2.7754e-05 | scale_pen(Q)= 1.4892e-06 | grad_norm=0.63 | sec/step~1.11 | rms_L~0.5716 rms_Q~0.5650
  step 120/685 | loss_L=1.5954 | loss_Q=1.6321 | scale_pen(L)= 3.3845e-05 | scale_pen(Q)= 1.6310e-06 | grad_norm=0.62 | sec/step~1.14 | rms_L~0.5736 rms_Q~0.5658
  step 130/685 | loss_L=1.4516 | loss_Q=1.4763 | scale_pen(L)= 3.9492e-05 | scale_pen(Q)= 1.8362e-06 | grad_norm=0.49 | sec/step~1.08 | rms_L~0.5755 rms_Q~0.5665
  step 140/685 | loss_L=1.5346 | loss_Q=1.5946 | scale_pen(L)= 4.5134e-05 | scale_pen(Q)= 2.1622e-06 | grad_norm=0.59 | sec/step~1.17 | rms_L~0.5773 rms_Q~0.5672
  step 150/685 | loss_L=1.4309 | loss_Q=1.4452 | scale_pen(L)= 5.0023e-05 | scale_pen(Q)= 2.3575e-06 | grad_norm=0.61 | sec/step~1.13 | rms_L~0.5791 rms_Q~0.5679
  step 160/685 | loss_L=1.4016 | loss_Q=1.3942 | scale_pen(L)= 5.4980e-05 | scale_pen(Q)= 2.5853e-06 | grad_norm=0.61 | sec/step~1.19 | rms_L~0.5808 rms_Q~0.5686
  step 170/685 | loss_L=1.2999 | loss_Q=1.3228 | scale_pen(L)= 5.9466e-05 | scale_pen(Q)= 2.8129e-06 | grad_norm=0.60 | sec/step~1.06 | rms_L~0.5825 rms_Q~0.5692
  step 180/685 | loss_L=1.4740 | loss_Q=1.5073 | scale_pen(L)= 6.4275e-05 | scale_pen(Q)= 2.9296e-06 | grad_norm=0.65 | sec/step~1.16 | rms_L~0.5841 rms_Q~0.5698
  step 190/685 | loss_L=1.4951 | loss_Q=1.5089 | scale_pen(L)= 6.9347e-05 | scale_pen(Q)= 3.1355e-06 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.5856 rms_Q~0.5704
  step 200/685 | loss_L=1.4812 | loss_Q=1.4752 | scale_pen(L)= 7.4749e-05 | scale_pen(Q)= 3.3218e-06 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.5871 rms_Q~0.5710
  step 210/685 | loss_L=1.3968 | loss_Q=1.3879 | scale_pen(L)= 7.8271e-05 | scale_pen(Q)= 3.8591e-06 | grad_norm=0.61 | sec/step~1.07 | rms_L~0.5885 rms_Q~0.5715
  step 220/685 | loss_L=1.3293 | loss_Q=1.3540 | scale_pen(L)= 8.3155e-05 | scale_pen(Q)= 4.1949e-06 | grad_norm=0.54 | sec/step~1.19 | rms_L~0.5899 rms_Q~0.5720
  step 230/685 | loss_L=1.3551 | loss_Q=1.3991 | scale_pen(L)= 8.8614e-05 | scale_pen(Q)= 4.5625e-06 | grad_norm=0.63 | sec/step~1.07 | rms_L~0.5912 rms_Q~0.5725
  step 240/685 | loss_L=1.4632 | loss_Q=1.5089 | scale_pen(L)= 9.3941e-05 | scale_pen(Q)= 5.0994e-06 | grad_norm=0.61 | sec/step~1.10 | rms_L~0.5925 rms_Q~0.5730
  step 250/685 | loss_L=1.3844 | loss_Q=1.4042 | scale_pen(L)= 9.8847e-05 | scale_pen(Q)= 5.6186e-06 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.5937 rms_Q~0.5735
  step 260/685 | loss_L=1.5508 | loss_Q=1.6507 | scale_pen(L)= 1.0172e-04 | scale_pen(Q)= 6.1600e-06 | grad_norm=0.59 | sec/step~1.14 | rms_L~0.5949 rms_Q~0.5739
  step 270/685 | loss_L=1.6671 | loss_Q=1.7660 | scale_pen(L)= 1.0648e-04 | scale_pen(Q)= 6.8768e-06 | grad_norm=0.52 | sec/step~1.07 | rms_L~0.5961 rms_Q~0.5744
  step 280/685 | loss_L=1.5005 | loss_Q=1.5603 | scale_pen(L)= 1.1335e-04 | scale_pen(Q)= 8.0239e-06 | grad_norm=0.64 | sec/step~1.14 | rms_L~0.5972 rms_Q~0.5748
  step 290/685 | loss_L=1.3347 | loss_Q=1.3420 | scale_pen(L)= 1.1910e-04 | scale_pen(Q)= 9.0811e-06 | grad_norm=0.60 | sec/step~1.08 | rms_L~0.5983 rms_Q~0.5752
  step 300/685 | loss_L=1.3630 | loss_Q=1.4064 | scale_pen(L)= 1.2582e-04 | scale_pen(Q)= 9.6420e-06 | grad_norm=0.60 | sec/step~1.07 | rms_L~0.5994 rms_Q~0.5756
  step 310/685 | loss_L=1.4068 | loss_Q=1.4129 | scale_pen(L)= 1.3114e-04 | scale_pen(Q)= 9.8984e-06 | grad_norm=0.52 | sec/step~1.08 | rms_L~0.6005 rms_Q~0.5760
  step 320/685 | loss_L=1.3908 | loss_Q=1.4452 | scale_pen(L)= 1.3694e-04 | scale_pen(Q)= 1.1335e-05 | grad_norm=0.60 | sec/step~1.11 | rms_L~0.6015 rms_Q~0.5764
  step 330/685 | loss_L=1.2877 | loss_Q=1.3175 | scale_pen(L)= 1.4289e-04 | scale_pen(Q)= 1.1593e-05 | grad_norm=0.56 | sec/step~1.08 | rms_L~0.6026 rms_Q~0.5768
  step 340/685 | loss_L=1.2768 | loss_Q=1.2866 | scale_pen(L)= 1.4744e-04 | scale_pen(Q)= 1.2683e-05 | grad_norm=0.57 | sec/step~1.08 | rms_L~0.6035 rms_Q~0.5772
  step 350/685 | loss_L=1.3048 | loss_Q=1.3588 | scale_pen(L)= 1.5316e-04 | scale_pen(Q)= 1.3687e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.6045 rms_Q~0.5776
  step 360/685 | loss_L=1.2948 | loss_Q=1.3406 | scale_pen(L)= 1.6074e-04 | scale_pen(Q)= 1.3982e-05 | grad_norm=0.53 | sec/step~1.08 | rms_L~0.6055 rms_Q~0.5780
  step 370/685 | loss_L=1.5001 | loss_Q=1.5762 | scale_pen(L)= 1.6633e-04 | scale_pen(Q)= 1.5051e-05 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.6064 rms_Q~0.5783
  step 380/685 | loss_L=1.3159 | loss_Q=1.3709 | scale_pen(L)= 1.7087e-04 | scale_pen(Q)= 1.5694e-05 | grad_norm=0.58 | sec/step~1.15 | rms_L~0.6074 rms_Q~0.5787
  step 390/685 | loss_L=1.3615 | loss_Q=1.4089 | scale_pen(L)= 1.7605e-04 | scale_pen(Q)= 1.6126e-05 | grad_norm=0.59 | sec/step~1.07 | rms_L~0.6083 rms_Q~0.5791
  step 400/685 | loss_L=1.2412 | loss_Q=1.2903 | scale_pen(L)= 1.8138e-04 | scale_pen(Q)= 1.6889e-05 | grad_norm=0.54 | sec/step~1.12 | rms_L~0.6092 rms_Q~0.5794
  step 410/685 | loss_L=1.3728 | loss_Q=1.4406 | scale_pen(L)= 1.8591e-04 | scale_pen(Q)= 1.6451e-05 | grad_norm=0.56 | sec/step~1.07 | rms_L~0.6100 rms_Q~0.5798
  step 420/685 | loss_L=1.3553 | loss_Q=1.3969 | scale_pen(L)= 1.9230e-04 | scale_pen(Q)= 1.7026e-05 | grad_norm=0.58 | sec/step~1.08 | rms_L~0.6109 rms_Q~0.5801
  step 430/685 | loss_L=1.3286 | loss_Q=1.4031 | scale_pen(L)= 1.9778e-04 | scale_pen(Q)= 1.6845e-05 | grad_norm=0.59 | sec/step~1.08 | rms_L~0.6117 rms_Q~0.5804
  step 440/685 | loss_L=1.4106 | loss_Q=1.4577 | scale_pen(L)= 2.0475e-04 | scale_pen(Q)= 1.7142e-05 | grad_norm=0.61 | sec/step~1.09 | rms_L~0.6125 rms_Q~0.5808
