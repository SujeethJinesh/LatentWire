/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
TESTING: Anchor-Guided Cross-Model Interlingua Architecture
================================================================================

Device: cuda

[1/7] Loading frozen models...
  - SentenceTransformer...
  - Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 224.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.22s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
  ✓ All models loaded

[2/7] Instantiating new architecture components...
  ✓ AlignmentTransformer: 13,856,256 params
  ✓ InterlinguaAdapter (Llama): 10,563,585 params
  Total trainable: 24,419,841 params (24.4M)

[3/7] Loading test data (SQuAD, n=1000)...
  ✓ Loaded 1000 examples
  Example: Context: Tesla was the fourth of five children. He had an older brother named Dane and three sisters...

[4/7] Testing forward pass (single example)...
  ✓ z_sem shape: torch.Size([1, 384])
  ✓ llama_embeds shape: torch.Size([1, 137, 4096])
  ✓ z_llama shape: torch.Size([1, 512])
  ✓ prefix_embeds_llama shape: torch.Size([1, 32, 4096])
  ✓ Expected: [1, 32, 4096]

  Testing generation (before training)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  Gold answer: Dane
  Pred (untrained): 1. 'Fracusmúdúveras

[5/7] Testing loss computation...
  ✓ Generation loss (K=4): 12.1747
  ✓ Semantic anchor loss: 1.0016
  ✓ Total loss: 12.6755

[6/7] Running 500 training steps on 1000 examples...
  Step 1/500: loss=11.9648 gen=11.4657 sem=0.9980
  Step 51/500: loss=4.7529 gen=4.2530 sem=0.9998
  Step 101/500: loss=4.4330 gen=3.9387 sem=0.9885
  Step 151/500: loss=6.3160 gen=5.8308 sem=0.9705
  Step 201/500: loss=3.0778 gen=2.5984 sem=0.9587
  Step 251/500: loss=4.3956 gen=3.9235 sem=0.9442
  Step 301/500: loss=4.7520 gen=4.2881 sem=0.9278
  Step 351/500: loss=3.2308 gen=2.7745 sem=0.9125
  Step 401/500: loss=7.1250 gen=6.6763 sem=0.8975
  Step 451/500: loss=6.8735 gen=6.4294 sem=0.8882
  Step 500/500: loss=6.9734 gen=6.5348 sem=0.8773
  ✓ Training complete

[7/7] Testing generation after training...

  Results:
  ============================================================================
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  [1] Gold: Dane
      Pred: The following is a list of the 2010–

  [2] Gold: Muslims
      Pred: the 19th century, the British Empire was the

  [3] Gold: orientalism and tropicality
      Pred: the 19th century, the British Empire was the

  [4] Gold: numeracy
      Pred: the 19th century, the British Empire was the

  [5] Gold: Mental Health (Care and Treatment) (Scotland) Act 2003
      Pred: the 19th century, the British Empire was the

  ============================================================================

  Diversity check: 2/5 unique predictions
  ⚠️  WARNING: Low diversity (2/5)

================================================================================
TEST COMPLETE
================================================================================

Key findings:
  ✓ All components instantiate correctly
  ✓ Forward pass works (z_llama: torch.Size([1, 512]))
  ✓ Loss computation succeeds
  ✓ Training loop runs (500 steps)
  ✓ Post-training predictions: 2/5 unique

Total trainable parameters: 24,419,841 (24.4M)
Architecture: d_inter=512, num_slots=32

Next steps:
  1. Run longer training: --samples 1000 --steps 500
  2. Enable Qwen testing: --test_qwen
  3. Full experiment: --samples 10000 --steps 5000 --test_qwen

