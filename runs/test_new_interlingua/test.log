/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
TESTING: Anchor-Guided Cross-Model Interlingua Architecture
================================================================================

Device: cuda

[1/7] Loading frozen models...
  - SentenceTransformer...
  - Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2577.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
  ✓ All models loaded

[2/7] Instantiating new architecture components...
  ✓ AlignmentTransformer: 13,856,256 params
  ✓ InterlinguaAdapter (Llama): 10,563,585 params
  Total trainable: 24,419,841 params (24.4M)

[3/7] Loading test data (SQuAD, n=1000)...
  ✓ Loaded 1000 examples
  Example: Context: Tesla was the fourth of five children. He had an older brother named Dane and three sisters...

[4/7] Testing forward pass (single example)...
  ✓ z_sem shape: torch.Size([1, 384])
  ✓ llama_embeds shape: torch.Size([1, 137, 4096])
  ✓ z_llama shape: torch.Size([1, 512])
  ✓ prefix_embeds_llama shape: torch.Size([1, 32, 4096])
  ✓ Expected: [1, 32, 4096]

  Testing generation (before training)...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  Gold answer: Dane
  Pred (untrained): a new and a new anakhmemories of the memory

[5/7] Testing loss computation...
  ✓ Generation loss (K=4): 9.1895
  ✓ Total loss: 9.1895

[6/7] Running 500 training steps on 1000 examples...
  Step 1/500: loss=10.7594 gen=10.7594
  Step 51/500: loss=2.9357 gen=2.9357
  Step 101/500: loss=5.7505 gen=5.7505
  Step 151/500: loss=4.4807 gen=4.4807
  Step 201/500: loss=2.3193 gen=2.3193
  Step 251/500: loss=3.4183 gen=3.4183
  Step 301/500: loss=4.5284 gen=4.5284
  Step 351/500: loss=3.2289 gen=3.2289
  Step 401/500: loss=2.7308 gen=2.7308
  Step 451/500: loss=2.5622 gen=2.5622
  Step 500/500: loss=4.3274 gen=4.3274
  ✓ Training complete

[7/7] Testing generation after training...

  Results:
  ============================================================================
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  [1] Gold: Dane
      Pred: 2000s, the company has been a major player

  [2] Gold: Muslims
      Pred: 2000s, the company's sales and profits have

  [3] Gold: orientalism and tropicality
      Pred: 2000s, the company has been a major player

  [4] Gold: numeracy
      Pred: 2000s, the company has been a major player

  [5] Gold: Mental Health (Care and Treatment) (Scotland) Act 2003
      Pred: 2000s, the company's market share in the

  ============================================================================

  Diversity check: 3/5 unique predictions
  ✓ Good diversity!

================================================================================
TEST COMPLETE
================================================================================

Key findings:
  ✓ All components instantiate correctly
  ✓ Forward pass works (z_llama: torch.Size([1, 512]))
  ✓ Loss computation succeeds
  ✓ Training loop runs (500 steps)
  ✓ Post-training predictions: 3/5 unique

Total trainable parameters: 24,419,841 (24.4M)
Architecture: d_inter=512, num_slots=32

Next steps:
  1. Run longer training: --samples 1000 --steps 500
  2. Enable Qwen testing: --test_qwen
  3. Full experiment: --samples 10000 --steps 5000 --test_qwen

