
=========================================
Starting pipeline at Sat Sep 13 17:37:21 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING (optional)
=========================================

Checkpoint will be saved to: runs/8B_runs/ckpt

(training skipped)


=========================================
PHASE 2: EVALUATION
=========================================

Using checkpoint from: runs/8B_runs/ckpt

Running eval (samples=16) -> runs/8B_runs/eval_squad_answer_text_smoke
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_runs/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_runs/eval_squad_answer_text_smoke/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3383.87it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.080956685341011, 'neutral_chat': 11.029998310541702, 'llama_chat': 11.134790210400597} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_runs/eval_squad_answer_text_smoke/Z_llama_neutral_chat.pt
[calib:llama] mode=train_stats prefix_rms=0.80668 -> target=0.74782
[debug:llama] adapter.scale=1.0252 | Z.std=0.9988 Z.mean||=15.9806 | prefix.std=0.9348 prefix.mean||=59.5772 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'Question 12" or 15" in 25'
  1: 'Tousser sroches sroches in'
  2: 'Question orceirr"r" or "mell'
  3: 'Welcome or Welcome or WaliDor M2500'
  4: 'Atrochiances a new arrival of the Holy Spirit'

[DEBUG] First-step top-k (Llama):
  ex0: Question:0.021, #:0.008, The:0.006, St:0.005, ://:0.004, Abstract:0.004, What:0.003, php:0.003, “:0.003, def:0.003
  ex1: ://:0.017, Question:0.014, php:0.012, #:0.004, Abstract:0.004, The:0.004, St:0.004, Title:0.003, def:0.003, sup:0.003
Saved Llama results to runs/8B_runs/eval_squad_answer_text_smoke/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3108.04it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.08it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 11.225593299865723, 'neutral_chat': 11.184417905807495, 'qwen_chat': 11.153053131103515} | picked=qwen_chat
Saved Z[qwen_qwen_chat] to runs/8B_runs/eval_squad_answer_text_smoke/Z_qwen_qwen_chat.pt
[calib:qwen]  mode=train_stats prefix_rms=0.65406 -> target=0.62392
[debug:qwen] adapter.scale=1.0042 | Z.std=0.9987 Z.mean||=15.9798 | prefix.std=0.7799 prefix.mean||=46.6166 | embed.RMS=0.0137

[DEBUG] First generations (Qwen, latent):
  0: '1. **引言**'
  1: '5. 000000000'
  2: '1. 请提供更多的信息或问题，以便我'
  3: '1. 介绍'
  4: '1. 介绍'

[DEBUG] First-step top-k (Qwen):
  ex0: 1:0.110, <NL>:0.076, 《:0.067, 请:0.067, 2:0.067, 无法:0.038, 由于:0.032, 本:0.028, 4:0.022, 以下:0.019
  ex1: 1:0.266, 2:0.125, 3:0.063, 4:0.049, 5:0.032, �:0.032, <NL>:0.028, 请:0.022, 《:0.018, 6:0.016
Saved Qwen results to runs/8B_runs/eval_squad_answer_text_smoke/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5633.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.23s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5815.33it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.21it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.21it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 16  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 290.8 | (Qwen): 278.6 | Latent length M: 16
Compression ratio (Llama): 18.2x | (Qwen): 17.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.18x

— Baseline: Text prompting
Llama  EM: 0.562  F1: 0.736  |  NLL/token (gold): 11.496574388513089
Qwen   EM: 0.625   F1: 0.762   |  NLL/token (gold): 25.969573822021484
Wall clock: 1.94s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 11.159771305019573
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 11.271695280075074
Wall clock: 1.70s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.000
Qwen   EM: 0.000   F1: 0.031
Wall clock: 0.87s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000

==== METRICS_JSON ====
{
  "samples": 16,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 290.75,
    "qwen": 278.5625
  },
  "compression": {
    "llama": 18.171875,
    "qwen": 17.41015625
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1476,
      "qwen_avg": 1323,
      "max_avg": 1476
    },
    "text_bytes_twocopies": {
      "sum_avg": 2799
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.18017578125,
      "vs_onecopy_fp32": 0.090087890625
    }
  },
  "text": {
    "wall_clock_sec": 1.9381165504455566,
    "llama": {
      "em": 0.5625,
      "f1": 0.7356004901960784,
      "nll_token": 11.496574388513089
    },
    "qwen": {
      "em": 0.625,
      "f1": 0.761904761904762,
      "nll_token": 25.969573822021484
    }
  },
  "latent": {
    "wall_clock_sec": 1.7024500370025635,
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 11.159771305019573
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 11.271695280075074
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.0
    },
    "wall_clock_sec": 0.8706493377685547,
    "qwen": {
      "em": 0.0,
      "f1": 0.03125
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0251884460449219,
      "Z_std": 0.9987947344779968,
      "Z_mean_norm": 15.98057746887207,
      "prefix_std": 0.9347713589668274,
      "prefix_mean_norm": 59.577178955078125,
      "embed_rms": 0.010569767095148563,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "train_stats",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0042412281036377,
      "Z_std": 0.9987480044364929,
      "Z_mean_norm": 15.979828834533691,
      "prefix_std": 0.779900312423706,
      "prefix_mean_norm": 46.61662292480469,
      "embed_rms": 0.013651985675096512,
      "encoder_text_mode": "qwen_chat",
      "calibration_mode": "train_stats",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.25,
    "calibration_mode": "train_stats",
    "append_bos_after_prefix": "yes",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_runs/eval_squad_answer_text_smoke/predictions.jsonl

✓ Evaluation metrics saved at: runs/8B_runs/eval_squad_answer_text_smoke/metrics.json

Key metrics:
  Compression: Llama 18.2x | Qwen 17.4x
  Text F1:     Llama 0.736 | Qwen 0.762
  Latent F1:   Llama 0.000 | Qwen 0.000
  Joint:       EM 0.000 | F1 0.000

Running eval (samples=200) -> runs/8B_runs/eval_squad_answer_text
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_runs/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_runs/eval_squad_answer_text/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4742.01it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.555223761231991, 'neutral_chat': 11.499689198405294, 'llama_chat': 11.6033049862401} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/8B_runs/eval_squad_answer_text/Z_llama_neutral_chat.pt
[calib:llama] mode=train_stats prefix_rms=0.80773 -> target=0.74782
[debug:llama] adapter.scale=1.0252 | Z.std=0.9988 Z.mean||=15.9813 | prefix.std=0.9348 prefix.mean||=59.5762 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: '正常lyrassuces a new barman, a'
  1: 'Christinquiets the moment of its own moment of its'
  2: 'Cirr" orr"r"one of its'
  3: 'Financial District or District 100m District 100m District'
  4: 'Visionary of the Duke of York" or "Duke'

[DEBUG] First-step top-k (Llama):
  ex0: Question:0.021, #:0.008, The:0.006, St:0.005, ://:0.004, Abstract:0.004, What:0.004, php:0.003, “:0.003, def:0.003
  ex1: ://:0.017, Question:0.014, php:0.012, #:0.004, Abstract:0.004, St:0.004, The:0.004, Title:0.003, def:0.003, sup:0.003
Saved Llama results to runs/8B_runs/eval_squad_answer_text/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3095.43it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.36it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.29it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 11.459802874497004, 'neutral_chat': 11.458411546138228, 'qwen_chat': 11.467107569099102} | picked=neutral_chat
Saved Z[qwen_neutral_chat] to runs/8B_runs/eval_squad_answer_text/Z_qwen_neutral_chat.pt
[calib:qwen]  mode=train_stats prefix_rms=0.65570 -> target=0.62392
[debug:qwen] adapter.scale=1.0042 | Z.std=0.9988 Z.mean||=15.9813 | prefix.std=0.7799 prefix.mean||=46.6182 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '1. 介绍'
  1: '1. **Question**: What is the name of the book'
  2: '1. **Identify the type of logical reasoning involved'
  3: '请提供更多的信息或问题，以便我能够提供'
  4: '5'

[DEBUG] First-step top-k (Qwen):
  ex0: 1:0.134, <NL>:0.072, 《:0.067, 2:0.067, 请:0.056, 无法:0.043, 由于:0.036, 本:0.023, 4:0.021, 以下:0.019
  ex1: 1:0.293, 2:0.130, 3:0.074, 4:0.051, 5:0.037, <NL>:0.023, �:0.023, 0:0.023, 请:0.021, 6:0.019
Saved Qwen results to runs/8B_runs/eval_squad_answer_text/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3510.61it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.01it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6314.35it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.22it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.04it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.11it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.04it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.690   F1: 0.858   |  NLL/token (gold): 26.15362625052689
Wall clock: 17.14s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.014  |  NLL/token (gold): 11.622274498279943
Qwen   EM: 0.000   F1: 0.006   |  NLL/token (gold): 11.674608321732315
Wall clock: 14.41s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 12.72s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.019

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 230.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1106,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2365
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 17.136199951171875,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.69,
      "f1": 0.85772697967667,
      "nll_token": 26.15362625052689
    }
  },
  "latent": {
    "wall_clock_sec": 14.414754867553711,
    "llama": {
      "em": 0.0,
      "f1": 0.013725940725940728,
      "nll_token": 11.622274498279943
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.006280588692353398,
      "nll_token": 11.674608321732315
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 12.72462272644043,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0038333333333333336,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0251884460449219,
      "Z_std": 0.9988353848457336,
      "Z_mean_norm": 15.981337547302246,
      "prefix_std": 0.9347707629203796,
      "prefix_mean_norm": 59.57624053955078,
      "embed_rms": 0.010571768507361412,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "train_stats",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0042412281036377,
      "Z_std": 0.9988353848457336,
      "Z_mean_norm": 15.981337547302246,
      "prefix_std": 0.7798996567726135,
      "prefix_mean_norm": 46.618221282958984,
      "embed_rms": 0.013649269007146358,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "train_stats",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.25,
    "calibration_mode": "train_stats",
    "append_bos_after_prefix": "yes",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.018963748669631023
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_runs/eval_squad_answer_text/predictions.jsonl

✓ Evaluation metrics saved at: runs/8B_runs/eval_squad_answer_text/metrics.json

Key metrics:
  Compression: Llama 15.3x | Qwen 14.4x
  Text F1:     Llama 0.799 | Qwen 0.858
  Latent F1:   Llama 0.014 | Qwen 0.006
  Joint:       EM 0.000 | F1 0.004


=========================================
PIPELINE SUMMARY
=========================================

Run ID: 8B_runs
Completed: Sat Sep 13 17:40:18 PDT 2025
Outputs:
  Training checkpoint: runs/8B_runs/ckpt/
  Evaluation (full):   runs/8B_runs/eval_squad_answer_text/
  Evaluation (smoke):  runs/8B_runs/eval_squad_answer_text_smoke/
