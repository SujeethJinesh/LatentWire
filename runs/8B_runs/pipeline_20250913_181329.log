
=========================================
Starting pipeline at Sat Sep 13 18:13:29 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING (optional)
=========================================

Checkpoint will be saved to: runs/8B_runs/ckpt

(training skipped)


=========================================
PHASE 2: EVALUATION
=========================================

Using checkpoint from: runs/8B_runs/ckpt

(smoke eval skipped)
Running eval (samples=200) -> runs/8B_runs/eval_squad_answer_text
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_runs/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_runs/eval_squad_answer_text/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3358.13it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.02s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.20s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 11.227557263947398, 'neutral_chat': 11.232913823895444, 'llama_chat': 11.209458803103354} | picked=llama_chat
Saved Z[llama_llama_chat] to runs/8B_runs/eval_squad_answer_text/Z_llama_llama_chat.pt
[calib:llama] mode=embed_rms prefix_rms=0.80720 -> target=0.01057
[debug:llama] adapter.scale=1.0252 | Z.std=0.9987 Z.mean||=15.9794 | prefix.std=0.0106 prefix.mean||=0.6733 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: ''
  1: ''
  2: ''
  3: ''
  4: ''

[DEBUG] First-step top-k (Llama):
  ex0: <NL>:0.080, <NL>:0.080, <NL>:0.059, 202:0.036, 1:0.033, 0:0.026, 5:0.013, 6:0.010, :0.010, 8:0.009
  ex1: <NL>:0.086, <NL>:0.081, <NL>:0.034, :0.026, 1:0.020, 202:0.019, 0:0.016, 5:0.013, 6:0.013, <NL>:0.010
Saved Llama results to runs/8B_runs/eval_squad_answer_text/llama_results.json

Evaluating Qwen...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2917.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.10s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 7.527375071925461, 'neutral_chat': 7.457985383650613, 'qwen_chat': 7.489647951075639} | picked=neutral_chat
Saved Z[qwen_neutral_chat] to runs/8B_runs/eval_squad_answer_text/Z_qwen_neutral_chat.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.65570 -> target=0.01365
[debug:qwen] adapter.scale=1.0042 | Z.std=0.9988 Z.mean||=15.9813 | prefix.std=0.0136 prefix.mean||=0.8158 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: '100000000000'
  1: '10-100000000'
  2: '1990s the of the of the of the'
  3: '100000000000'
  4: '100000000000'

[DEBUG] First-step top-k (Qwen):
  ex0: 1:0.229, 2:0.122, 3:0.090, 0:0.070, 5:0.066, 4:0.058, 7:0.031, 9:0.031, 6:0.031, 8:0.027
  ex1: 1:0.309, 2:0.146, 3:0.100, 5:0.094, 4:0.073, 6:0.057, 7:0.054, 8:0.047, 9:0.039, 0:0.035
Saved Qwen results to runs/8B_runs/eval_squad_answer_text/qwen_results.json

Joint rescoring...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1832.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.28s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 6114.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.24it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.17it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 15.3x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.15x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.690   F1: 0.858   |  NLL/token (gold): 26.15362625052689
Wall clock: 17.45s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 11.213177377674855
Qwen   EM: 0.000   F1: 0.009   |  NLL/token (gold): 7.4523999653165305
Wall clock: 14.40s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.038
Qwen   EM: 0.010   F1: 0.043
Wall clock: 12.55s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.009
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.009

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 230.69
  },
  "compression": {
    "llama": 15.3165625,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1106,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2365
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.1536865234375,
      "vs_onecopy_fp32": 0.07684326171875
    }
  },
  "text": {
    "wall_clock_sec": 17.44855284690857,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.69,
      "f1": 0.85772697967667,
      "nll_token": 26.15362625052689
    }
  },
  "latent": {
    "wall_clock_sec": 14.3952956199646,
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 11.213177377674855
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.008796628809786704,
      "nll_token": 7.4523999653165305
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.038205738705738714
    },
    "wall_clock_sec": 12.548540353775024,
    "qwen": {
      "em": 0.01,
      "f1": 0.04269888444888446
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.008796628809786704,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0251884460449219,
      "Z_std": 0.9987140893936157,
      "Z_mean_norm": 15.97939682006836,
      "prefix_std": 0.010565312579274178,
      "prefix_mean_norm": 0.6733431816101074,
      "embed_rms": 0.010577648878097534,
      "encoder_text_mode": "llama_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 1.0042412281036377,
      "Z_std": 0.9988353848457336,
      "Z_mean_norm": 15.981337547302246,
      "prefix_std": 0.013647433370351791,
      "prefix_mean_norm": 0.8157703876495361,
      "embed_rms": 0.013640254735946655,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "no",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.008796628809786704
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_runs/eval_squad_answer_text/predictions.jsonl

✓ Evaluation metrics saved at: runs/8B_runs/eval_squad_answer_text/metrics.json

Key metrics:
  Compression: Llama 15.3x | Qwen 14.4x
  Text F1:     Llama 0.799 | Qwen 0.858
  Latent F1:   Llama 0.000 | Qwen 0.009
  Joint:       EM 0.000 | F1 0.009


=========================================
PIPELINE SUMMARY
=========================================

Run ID: 8B_runs
Completed: Sat Sep 13 18:15:48 PDT 2025
Outputs:
  Training checkpoint: runs/8B_runs/ckpt/
  Evaluation (full):   runs/8B_runs/eval_squad_answer_text/
