
=========================================
Starting pipeline at Thu Sep 18 12:05:09 PDT 2025
=========================================


=========================================
TRAIN + PER-EPOCH EVAL
=========================================


=========================================
EPOCH 1/12
=========================================

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3025.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.18s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.64s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.62s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.14s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2986.33it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.26s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/1000 | grad_norm=915.09 | sec/step~7.18 | keep=1.00 | K=8 | llama: tf=11.3386 first=9.2157 kCE=7.8493 KD=4.0935 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.3059 first=15.1465 kCE=13.0882 KD=4.8307 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/1000 | grad_norm=928.15 | sec/step~8.09 | keep=1.00 | K=8 | llama: tf=11.9822 first=9.3339 kCE=8.9935 KD=3.6999 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=11.7884 first=15.7071 kCE=13.9159 KD=4.1725 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/1000 | grad_norm=1908.15 | sec/step~7.65 | keep=1.00 | K=8 | llama: tf=13.4012 first=9.6186 kCE=7.2622 KD=3.6440 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.9160 first=16.8981 kCE=14.9865 KD=3.7393 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/1000 | grad_norm=330.37 | sec/step~7.16 | keep=1.00 | K=8 | llama: tf=12.2665 first=10.0567 kCE=8.7202 KD=4.0864 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.0779 first=16.4873 kCE=15.1403 KD=3.9382 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/1000 | grad_norm=588.53 | sec/step~8.01 | keep=1.00 | K=8 | llama: tf=12.1928 first=9.8338 kCE=9.0192 KD=4.4836 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.0435 first=16.3562 kCE=14.4309 KD=4.7226 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/1000 | grad_norm=759.31 | sec/step~8.91 | keep=1.00 | K=8 | llama: tf=13.3672 first=10.4032 kCE=7.0347 KD=3.9498 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=10.8316 first=14.8326 kCE=15.3039 KD=3.2001 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/1000 | grad_norm=240.49 | sec/step~8.88 | keep=1.00 | K=8 | llama: tf=13.2072 first=9.6065 kCE=7.5609 KD=3.5323 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=10.2209 first=13.3602 kCE=14.3298 KD=3.7413 man=0.0002 | scale_pen(qwen)=2.5899e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/1000 | grad_norm=650.64 | sec/step~7.45 | keep=1.00 | K=8 | llama: tf=12.4455 first=9.2046 kCE=8.2623 KD=4.0963 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=11.7501 first=14.6216 kCE=14.8756 KD=4.2316 man=0.0002 | scale_pen(qwen)=2.5899e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/1000 | grad_norm=1007.72 | sec/step~7.42 | keep=1.00 | K=8 | llama: tf=13.9004 first=8.8224 kCE=6.9994 KD=4.1219 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=13.2748 first=16.8120 kCE=15.5939 KD=3.5322 man=0.0002 | scale_pen(qwen)=2.5899e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/1000 | grad_norm=105.87 | sec/step~9.47 | keep=1.00 | K=8 | llama: tf=11.8885 first=8.5102 kCE=8.0293 KD=3.0928 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=11.5553 first=12.2133 kCE=14.7498 KD=4.3213 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/1000 | grad_norm=326.29 | sec/step~7.75 | keep=1.00 | K=8 | llama: tf=13.1489 first=8.3104 kCE=7.3277 KD=3.7863 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=12.0587 first=13.7149 kCE=15.1179 KD=4.0305 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/1000 | grad_norm=598.75 | sec/step~8.41 | keep=1.00 | K=8 | llama: tf=12.5252 first=9.4802 kCE=6.6270 KD=3.6274 man=0.0001 | scale_pen(llama)=1.6428e-11 | qwen: tf=11.7680 first=14.7313 kCE=15.8944 KD=3.3090 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/1000 | grad_norm=84.92 | sec/step~8.07 | keep=1.00 | K=8 | llama: tf=13.0633 first=9.3515 kCE=6.2995 KD=3.6002 man=0.0001 | scale_pen(llama)=2.8141e-11 | qwen: tf=13.1786 first=15.7885 kCE=16.0962 KD=4.1402 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/1000 | grad_norm=252.07 | sec/step~7.24 | keep=1.00 | K=8 | llama: tf=12.2443 first=9.6059 kCE=6.6524 KD=3.9366 man=0.0001 | scale_pen(llama)=2.8141e-11 | qwen: tf=10.6245 first=15.1075 kCE=14.5703 KD=3.7879 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/1000 | grad_norm=417.20 | sec/step~7.90 | keep=1.00 | K=8 | llama: tf=12.7132 first=9.9264 kCE=6.6386 KD=3.9308 man=0.0001 | scale_pen(llama)=2.8141e-11 | qwen: tf=12.8132 first=16.3627 kCE=15.7242 KD=4.8479 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/1000 | grad_norm=560.37 | sec/step~8.17 | keep=1.00 | K=8 | llama: tf=11.9344 first=10.2403 kCE=7.1280 KD=3.8714 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=11.7483 first=15.6131 kCE=14.6728 KD=4.3733 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/1000 | grad_norm=153.60 | sec/step~7.70 | keep=1.00 | K=8 | llama: tf=13.1079 first=9.1338 kCE=7.0768 KD=3.7225 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=12.5077 first=14.1634 kCE=15.1714 KD=4.5343 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/1000 | grad_norm=687.29 | sec/step~7.86 | keep=1.00 | K=8 | llama: tf=11.6518 first=10.0168 kCE=5.8677 KD=3.6728 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=10.8706 first=14.2321 kCE=15.3476 KD=4.4659 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/1000 | grad_norm=977.23 | sec/step~7.68 | keep=1.00 | K=8 | llama: tf=11.6323 first=9.7908 kCE=6.4537 KD=3.6530 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=11.6798 first=12.9289 kCE=15.0892 KD=4.3604 man=0.0002 | scale_pen(qwen)=6.9633e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/1000 | grad_norm=219.79 | sec/step~7.51 | keep=1.00 | K=8 | llama: tf=11.5870 first=9.2643 kCE=7.1391 KD=3.8832 man=0.0001 | scale_pen(llama)=2.4475e-11 | qwen: tf=12.8590 first=15.4200 kCE=15.5096 KD=5.2922 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/1000 | grad_norm=495.72 | sec/step~8.85 | keep=1.00 | K=8 | llama: tf=11.3431 first=8.8976 kCE=7.2995 KD=3.3965 man=0.0001 | scale_pen(llama)=2.4475e-11 | qwen: tf=12.3718 first=13.6288 kCE=14.7199 KD=5.4415 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/1000 | grad_norm=806.28 | sec/step~7.28 | keep=1.00 | K=8 | llama: tf=12.6464 first=8.6695 kCE=8.5299 KD=4.6085 man=0.0001 | scale_pen(llama)=2.4475e-11 | qwen: tf=13.4243 first=16.7340 kCE=14.8711 KD=5.9429 man=0.0002 | scale_pen(qwen)=5.1159e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/1000 | grad_norm=122.62 | sec/step~8.30 | keep=1.00 | K=8 | llama: tf=11.5616 first=9.4463 kCE=6.7479 KD=3.6596 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=12.2692 first=15.2731 kCE=14.5855 KD=5.3674 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/1000 | grad_norm=318.09 | sec/step~8.01 | keep=1.00 | K=8 | llama: tf=11.6304 first=8.5937 kCE=7.1164 KD=3.3388 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=11.0923 first=10.2305 kCE=13.7012 KD=5.0502 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/1000 | grad_norm=537.72 | sec/step~9.06 | keep=1.00 | K=8 | llama: tf=12.0431 first=8.9662 kCE=7.0160 KD=2.7838 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=11.1010 first=13.6058 kCE=14.5525 KD=4.4377 man=0.0002 | scale_pen(qwen)=2.7853e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/1000 | grad_norm=82.25 | sec/step~7.87 | keep=1.00 | K=8 | llama: tf=12.8659 first=9.1681 kCE=6.6861 KD=3.6611 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=12.8408 first=13.0277 kCE=15.0756 KD=5.0772 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/1000 | grad_norm=177.92 | sec/step~8.19 | keep=1.00 | K=8 | llama: tf=11.4080 first=9.1010 kCE=7.4944 KD=3.7953 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=13.4617 first=13.5369 kCE=15.0575 KD=5.4982 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/1000 | grad_norm=389.78 | sec/step~8.29 | keep=1.00 | K=8 | llama: tf=12.3219 first=9.7077 kCE=6.6233 KD=3.4199 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=12.5299 first=14.8948 kCE=15.2872 KD=4.2073 man=0.0002 | scale_pen(qwen)=6.2670e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/1000 | grad_norm=51.73 | sec/step~8.21 | keep=1.00 | K=8 | llama: tf=12.1885 first=10.1058 kCE=6.5617 KD=3.6154 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=12.7638 first=15.4327 kCE=15.2353 KD=4.6044 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/1000 | grad_norm=161.08 | sec/step~7.32 | keep=1.00 | K=8 | llama: tf=11.8256 first=8.9762 kCE=7.2292 KD=3.6101 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.8017 first=12.6219 kCE=15.4169 KD=4.2367 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/1000 | grad_norm=248.01 | sec/step~7.02 | keep=1.00 | K=8 | llama: tf=12.2656 first=9.3269 kCE=7.1786 KD=3.4800 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.9595 first=12.1172 kCE=14.7943 KD=4.0681 man=0.0002 | scale_pen(qwen)=8.1855e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/1000 | grad_norm=378.13 | sec/step~7.87 | keep=1.00 | K=8 | llama: tf=11.8301 first=9.6562 kCE=6.8612 KD=3.2507 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=10.4296 first=12.9662 kCE=14.6394 KD=5.0233 man=0.0002 | scale_pen(qwen)=7.5175e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/1000 | grad_norm=152.51 | sec/step~10.02 | keep=1.00 | K=8 | llama: tf=11.3362 first=9.5830 kCE=6.6554 KD=3.1067 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.1468 first=12.7325 kCE=13.7058 KD=4.5463 man=0.0002 | scale_pen(qwen)=7.5175e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/1000 | grad_norm=309.75 | sec/step~7.68 | keep=1.00 | K=8 | llama: tf=12.0499 first=8.4241 kCE=8.6855 KD=4.0061 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=12.9616 first=12.9024 kCE=14.6478 KD=5.8135 man=0.0002 | scale_pen(qwen)=7.5175e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/1000 | grad_norm=442.93 | sec/step~7.63 | keep=1.00 | K=8 | llama: tf=12.7417 first=9.2695 kCE=6.8202 KD=3.3282 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=12.8842 first=14.4611 kCE=16.0807 KD=4.0446 man=0.0002 | scale_pen(qwen)=7.5175e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  360/1000 | grad_norm=68.53 | sec/step~8.14 | keep=1.00 | K=8 | llama: tf=13.0481 first=9.9712 kCE=6.3378 KD=3.0111 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=9.1698 first=12.9729 kCE=15.0225 KD=3.7445 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  370/1000 | grad_norm=396.38 | sec/step~6.96 | keep=1.00 | K=8 | llama: tf=12.4965 first=8.6750 kCE=7.1155 KD=3.6825 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=12.3449 first=12.8484 kCE=15.2382 KD=4.3051 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  380/1000 | grad_norm=408.37 | sec/step~7.76 | keep=1.00 | K=8 | llama: tf=11.3123 first=8.3191 kCE=7.3788 KD=3.4844 man=0.0001 | scale_pen(llama)=3.6380e-12 | qwen: tf=12.0033 first=12.0750 kCE=14.5294 KD=4.9070 man=0.0002 | scale_pen(qwen)=5.1301e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  390/1000 | grad_norm=69.77 | sec/step~7.92 | keep=1.00 | K=8 | llama: tf=11.8708 first=7.7549 kCE=8.1787 KD=3.9165 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=12.9703 first=12.0342 kCE=14.6550 KD=4.9612 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  400/1000 | grad_norm=163.67 | sec/step~8.08 | keep=1.00 | K=8 | llama: tf=12.9390 first=8.9115 kCE=6.5039 KD=3.0987 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=11.8662 first=13.8539 kCE=15.6952 KD=4.0248 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  410/1000 | grad_norm=264.46 | sec/step~7.94 | keep=1.00 | K=8 | llama: tf=11.5849 first=9.3616 kCE=6.7933 KD=3.2579 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=10.3191 first=14.4302 kCE=14.9111 KD=4.3049 man=0.0002 | scale_pen(qwen)=4.1069e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  420/1000 | grad_norm=68.59 | sec/step~8.29 | keep=1.00 | K=8 | llama: tf=11.2693 first=9.3501 kCE=7.1058 KD=3.5444 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.7623 first=14.5997 kCE=14.7513 KD=4.5453 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  430/1000 | grad_norm=229.96 | sec/step~8.86 | keep=1.00 | K=8 | llama: tf=11.4922 first=9.1811 kCE=7.2601 KD=3.4755 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=13.3182 first=14.0738 kCE=15.2939 KD=4.4332 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  440/1000 | grad_norm=401.06 | sec/step~8.37 | keep=1.00 | K=8 | llama: tf=10.7288 first=9.1907 kCE=6.5720 KD=3.1302 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.1436 first=13.2748 kCE=15.0833 KD=4.4048 man=0.0002 | scale_pen(qwen)=2.4016e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  450/1000 | grad_norm=30.91 | sec/step~8.42 | keep=1.00 | K=8 | llama: tf=10.7571 first=8.6577 kCE=7.1001 KD=3.0285 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=9.7261 first=10.4909 kCE=13.6625 KD=3.9637 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  460/1000 | grad_norm=179.54 | sec/step~8.06 | keep=1.00 | K=8 | llama: tf=11.5100 first=9.3524 kCE=6.5681 KD=3.2851 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.8994 first=11.5504 kCE=15.7182 KD=3.8860 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  470/1000 | grad_norm=339.43 | sec/step~7.92 | keep=1.00 | K=8 | llama: tf=11.2740 first=9.2385 kCE=6.7725 KD=3.1912 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.3470 first=12.1200 kCE=15.1441 KD=3.9781 man=0.0002 | scale_pen(qwen)=1.1511e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  480/1000 | grad_norm=492.22 | sec/step~8.09 | keep=1.00 | K=8 | llama: tf=11.5032 first=9.0413 kCE=5.8881 KD=3.2837 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=11.7493 first=13.8649 kCE=15.1102 KD=4.2562 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  490/1000 | grad_norm=104.14 | sec/step~7.47 | keep=1.00 | K=8 | llama: tf=11.5284 first=8.2865 kCE=6.7820 KD=3.3113 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=12.0502 first=11.4687 kCE=15.1132 KD=4.0591 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  500/1000 | grad_norm=233.70 | sec/step~8.54 | keep=1.00 | K=8 | llama: tf=11.5587 first=8.0532 kCE=6.6912 KD=3.5657 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=11.2274 first=12.5297 kCE=15.4184 KD=4.5583 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  510/1000 | grad_norm=364.09 | sec/step~7.49 | keep=1.00 | K=8 | llama: tf=11.2259 first=9.5168 kCE=7.2018 KD=3.6884 man=0.0001 | scale_pen(llama)=5.6843e-12 | qwen: tf=11.5330 first=13.9057 kCE=14.5038 KD=4.1676 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  520/1000 | grad_norm=104.47 | sec/step~7.96 | keep=1.00 | K=8 | llama: tf=11.4262 first=9.0580 kCE=6.9761 KD=3.8016 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=12.9071 first=13.9441 kCE=15.7456 KD=4.4252 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  530/1000 | grad_norm=200.33 | sec/step~8.15 | keep=1.00 | K=8 | llama: tf=12.0741 first=9.8996 kCE=6.2441 KD=3.0397 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=12.1130 first=12.1267 kCE=15.4434 KD=3.5372 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  540/1000 | grad_norm=312.83 | sec/step~8.43 | keep=1.00 | K=8 | llama: tf=11.7373 first=8.5955 kCE=6.1146 KD=3.3725 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.9273 first=10.8300 kCE=15.1398 KD=4.3767 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  550/1000 | grad_norm=56.54 | sec/step~7.79 | keep=1.00 | K=8 | llama: tf=11.1529 first=8.4774 kCE=7.4262 KD=3.5641 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=12.2381 first=11.7683 kCE=14.4984 KD=4.8807 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  560/1000 | grad_norm=138.03 | sec/step~7.91 | keep=1.00 | K=8 | llama: tf=11.3811 first=8.6799 kCE=7.3699 KD=4.0333 man=0.0001 | scale_pen(llama)=6.9633e-13 | qwen: tf=12.4047 first=11.7983 kCE=14.4714 KD=4.8275 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1182, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 907, in main
    loss_kd = kd_first_k_prefix_vs_text(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 117, in kd_first_k_prefix_vs_text
    T_logits = _teacher_step_logits_text(teacher_llm, scaffold_ids, gold_ids, t)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 15, in _teacher_step_logits_text
    out = llm.model(input_ids=ids_t, attention_mask=attn_mask, use_cache=False, return_dict=True)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1214, in forward
    logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.93 GiB. GPU 1 has a total capacity of 79.19 GiB of which 8.73 GiB is free. Including non-PyTorch memory, this process has 70.45 GiB memory in use. Of the allocated memory 67.93 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
