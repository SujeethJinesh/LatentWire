
=========================================
Starting pipeline at Thu Sep 18 13:24:20 PDT 2025
=========================================


=========================================
TRAIN + PER-EPOCH EVAL
=========================================


=========================================
EPOCH 1/12
=========================================

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3183.53it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.57s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3046.53it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/625 | grad_norm=953.63 | sec/step~6.69 | keep=1.00 | K=8 | llama: tf=12.6102 first=10.0202 kCE=7.5428 KD=3.2179 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=9.5436 first=13.5803 kCE=14.1587 KD=2.8215 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  20/625 | grad_norm=2033.71 | sec/step~6.62 | keep=1.00 | K=8 | llama: tf=12.6432 first=8.9841 kCE=8.6632 KD=3.9059 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.3435 first=16.4987 kCE=13.7384 KD=4.7634 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  30/625 | grad_norm=3081.97 | sec/step~6.76 | keep=1.00 | K=8 | llama: tf=11.2184 first=9.6310 kCE=8.7724 KD=3.8267 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=10.9436 first=15.6828 kCE=13.3664 KD=4.7535 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  40/625 | grad_norm=313.25 | sec/step~6.70 | keep=1.00 | K=8 | llama: tf=12.4782 first=9.5048 kCE=8.3794 KD=3.9590 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.2283 first=14.5163 kCE=13.7929 KD=4.2147 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  50/625 | grad_norm=497.47 | sec/step~6.56 | keep=1.00 | K=8 | llama: tf=13.2808 first=9.3646 kCE=8.5266 KD=3.5341 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=12.2853 first=15.4939 kCE=15.0155 KD=3.5523 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  60/625 | grad_norm=674.87 | sec/step~7.64 | keep=1.00 | K=8 | llama: tf=13.9300 first=10.0848 kCE=7.4058 KD=4.3518 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=12.9198 first=16.8868 kCE=15.4023 KD=3.8593 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  70/625 | grad_norm=100.74 | sec/step~7.17 | keep=1.00 | K=8 | llama: tf=12.1239 first=9.7547 kCE=7.4085 KD=3.9801 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=10.5118 first=12.9369 kCE=14.0126 KD=4.4859 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  80/625 | grad_norm=269.88 | sec/step~6.74 | keep=1.00 | K=8 | llama: tf=12.9890 first=9.2179 kCE=7.8797 KD=3.7753 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=12.8630 first=15.6422 kCE=15.1928 KD=4.0492 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  90/625 | grad_norm=603.95 | sec/step~7.21 | keep=1.00 | K=8 | llama: tf=12.5130 first=9.1445 kCE=7.8696 KD=4.0218 man=0.0001 | scale_pen(llama)=8.5301e-12 | qwen: tf=11.0855 first=12.6559 kCE=13.9728 KD=4.7990 man=0.0002 | scale_pen(qwen)=2.9878e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  100/625 | grad_norm=48.49 | sec/step~6.95 | keep=1.00 | K=8 | llama: tf=11.9837 first=9.6827 kCE=7.7779 KD=3.9412 man=0.0001 | scale_pen(llama)=3.2742e-11 | qwen: tf=12.3418 first=16.1404 kCE=14.0656 KD=4.4772 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  110/625 | grad_norm=219.46 | sec/step~6.33 | keep=1.00 | K=8 | llama: tf=12.4891 first=10.1223 kCE=6.5200 KD=3.9257 man=0.0001 | scale_pen(llama)=3.2742e-11 | qwen: tf=12.7446 first=16.7969 kCE=15.3395 KD=3.8179 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  120/625 | grad_norm=308.52 | sec/step~6.74 | keep=1.00 | K=8 | llama: tf=12.6729 first=8.7419 kCE=7.7594 KD=3.5818 man=0.0001 | scale_pen(llama)=3.2742e-11 | qwen: tf=10.9117 first=15.7646 kCE=14.7873 KD=3.9023 man=0.0002 | scale_pen(qwen)=1.3657e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  130/625 | grad_norm=53.50 | sec/step~6.83 | keep=1.00 | K=8 | llama: tf=13.0337 first=9.5804 kCE=6.7502 KD=3.7763 man=0.0001 | scale_pen(llama)=5.2015e-11 | qwen: tf=13.0863 first=15.0336 kCE=15.4711 KD=3.5348 man=0.0002 | scale_pen(qwen)=2.9420e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  140/625 | grad_norm=469.83 | sec/step~7.72 | keep=1.00 | K=8 | llama: tf=11.2449 first=9.5966 kCE=7.3017 KD=3.6859 man=0.0001 | scale_pen(llama)=5.2015e-11 | qwen: tf=11.0752 first=15.9412 kCE=14.3393 KD=4.6700 man=0.0002 | scale_pen(qwen)=2.9420e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  150/625 | grad_norm=784.31 | sec/step~7.31 | keep=1.00 | K=8 | llama: tf=12.9462 first=9.1369 kCE=6.9766 KD=4.0926 man=0.0001 | scale_pen(llama)=5.2015e-11 | qwen: tf=12.6303 first=16.3276 kCE=15.2156 KD=4.2187 man=0.0002 | scale_pen(qwen)=2.9420e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  160/625 | grad_norm=1033.04 | sec/step~6.61 | keep=1.00 | K=8 | llama: tf=14.1094 first=9.6756 kCE=6.0454 KD=4.1397 man=0.0001 | scale_pen(llama)=5.5511e-11 | qwen: tf=11.4689 first=15.4368 kCE=15.1930 KD=3.6940 man=0.0002 | scale_pen(qwen)=4.4565e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  170/625 | grad_norm=132.47 | sec/step~7.27 | keep=1.00 | K=8 | llama: tf=11.7691 first=10.4329 kCE=7.8194 KD=3.9026 man=0.0001 | scale_pen(llama)=5.5511e-11 | qwen: tf=12.3942 first=16.6659 kCE=14.0357 KD=5.3602 man=0.0002 | scale_pen(qwen)=4.4565e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  180/625 | grad_norm=272.34 | sec/step~7.49 | keep=1.00 | K=8 | llama: tf=11.3043 first=8.7330 kCE=7.2019 KD=3.6775 man=0.0001 | scale_pen(llama)=5.5511e-11 | qwen: tf=9.7276 first=12.0985 kCE=13.6653 KD=3.6876 man=0.0002 | scale_pen(qwen)=4.4565e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  190/625 | grad_norm=414.68 | sec/step~6.67 | keep=1.00 | K=8 | llama: tf=11.7021 first=9.6650 kCE=6.8921 KD=4.4896 man=0.0001 | scale_pen(llama)=5.5511e-11 | qwen: tf=12.4967 first=14.2036 kCE=14.3307 KD=4.1410 man=0.0002 | scale_pen(qwen)=4.4565e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  200/625 | grad_norm=84.23 | sec/step~6.48 | keep=1.00 | K=8 | llama: tf=12.6839 first=8.3385 kCE=6.6761 KD=4.5429 man=0.0001 | scale_pen(llama)=3.9169e-11 | qwen: tf=13.1206 first=15.5030 kCE=15.1027 KD=4.5127 man=0.0002 | scale_pen(qwen)=4.8633e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  210/625 | grad_norm=228.81 | sec/step~6.77 | keep=1.00 | K=8 | llama: tf=13.0742 first=8.4291 kCE=5.3807 KD=3.9119 man=0.0001 | scale_pen(llama)=3.9169e-11 | qwen: tf=10.1388 first=12.4062 kCE=15.1426 KD=3.4690 man=0.0002 | scale_pen(qwen)=4.8633e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  220/625 | grad_norm=339.99 | sec/step~7.26 | keep=1.00 | K=8 | llama: tf=11.0264 first=8.8090 kCE=7.2179 KD=3.7835 man=0.0001 | scale_pen(llama)=3.9169e-11 | qwen: tf=10.8500 first=13.5672 kCE=14.4686 KD=3.5471 man=0.0002 | scale_pen(qwen)=4.8633e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  230/625 | grad_norm=52.44 | sec/step~6.48 | keep=1.00 | K=8 | llama: tf=12.2591 first=9.4822 kCE=6.3562 KD=4.1823 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=12.9073 first=14.2687 kCE=15.1261 KD=4.9042 man=0.0002 | scale_pen(qwen)=4.0675e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  240/625 | grad_norm=138.16 | sec/step~6.31 | keep=1.00 | K=8 | llama: tf=11.7728 first=9.7631 kCE=6.1182 KD=3.6911 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=11.5258 first=13.6548 kCE=14.6823 KD=3.5016 man=0.0002 | scale_pen(qwen)=4.0675e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  250/625 | grad_norm=229.20 | sec/step~7.01 | keep=1.00 | K=8 | llama: tf=12.4686 first=9.8349 kCE=6.6654 KD=3.7241 man=0.0001 | scale_pen(llama)=1.7408e-11 | qwen: tf=11.4206 first=14.1258 kCE=14.7039 KD=4.2236 man=0.0002 | scale_pen(qwen)=4.0675e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  260/625 | grad_norm=37.04 | sec/step~6.84 | keep=1.00 | K=8 | llama: tf=11.7981 first=9.1738 kCE=7.6129 KD=3.9921 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.0323 first=13.3478 kCE=14.2614 KD=4.3406 man=0.0002 | scale_pen(qwen)=2.3888e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  270/625 | grad_norm=141.52 | sec/step~7.18 | keep=1.00 | K=8 | llama: tf=12.4283 first=9.0226 kCE=7.4897 KD=3.5767 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=12.2096 first=15.6896 kCE=15.6363 KD=4.2083 man=0.0002 | scale_pen(qwen)=2.3888e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  280/625 | grad_norm=234.96 | sec/step~5.78 | keep=1.00 | K=8 | llama: tf=10.5664 first=9.6466 kCE=7.7833 KD=4.1084 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=10.3094 first=11.7481 kCE=13.3723 KD=4.0539 man=0.0002 | scale_pen(qwen)=2.3888e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  290/625 | grad_norm=29.19 | sec/step~6.66 | keep=1.00 | K=8 | llama: tf=13.6586 first=9.2794 kCE=6.8948 KD=3.9928 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.3853 first=13.7836 kCE=15.1449 KD=4.0882 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  300/625 | grad_norm=169.17 | sec/step~7.22 | keep=1.00 | K=8 | llama: tf=11.7610 first=9.2148 kCE=8.1545 KD=4.1396 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.2894 first=12.4458 kCE=14.1091 KD=4.5799 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  310/625 | grad_norm=291.13 | sec/step~7.41 | keep=1.00 | K=8 | llama: tf=11.9071 first=9.6372 kCE=7.2665 KD=3.4382 man=0.0001 | scale_pen(llama)=3.1974e-12 | qwen: tf=10.2877 first=12.5525 kCE=14.7138 KD=3.9351 man=0.0002 | scale_pen(qwen)=8.8818e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  320/625 | grad_norm=466.96 | sec/step~6.83 | keep=1.00 | K=8 | llama: tf=11.9984 first=8.5159 kCE=7.5235 KD=3.6373 man=0.0001 | scale_pen(llama)=1.5476e-11 | qwen: tf=10.9111 first=14.5039 kCE=14.1480 KD=4.1730 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  330/625 | grad_norm=135.40 | sec/step~6.93 | keep=1.00 | K=8 | llama: tf=12.3909 first=9.5820 kCE=7.3872 KD=3.5314 man=0.0001 | scale_pen(llama)=1.5476e-11 | qwen: tf=11.6614 first=15.8102 kCE=14.8048 KD=3.8496 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  340/625 | grad_norm=258.14 | sec/step~9.18 | keep=1.00 | K=8 | llama: tf=12.0680 first=8.9445 kCE=8.2499 KD=2.8064 man=0.0001 | scale_pen(llama)=1.5476e-11 | qwen: tf=12.3981 first=14.7720 kCE=14.8743 KD=4.3738 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  350/625 | grad_norm=385.13 | sec/step~6.87 | keep=1.00 | K=8 | llama: tf=12.0723 first=9.0011 kCE=6.3974 KD=3.6533 man=0.0001 | scale_pen(llama)=1.5476e-11 | qwen: tf=8.5629 first=12.0629 kCE=14.3267 KD=3.0723 man=0.0002 | scale_pen(qwen)=1.2825e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  360/625 | grad_norm=91.06 | sec/step~7.15 | keep=1.00 | K=8 | llama: tf=13.0454 first=9.5887 kCE=6.0555 KD=2.8416 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=9.6660 first=14.2058 kCE=14.5745 KD=2.9506 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  370/625 | grad_norm=233.33 | sec/step~6.55 | keep=1.00 | K=8 | llama: tf=13.0546 first=8.2372 kCE=6.4169 KD=3.8043 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=12.1916 first=13.0701 kCE=15.3809 KD=4.2154 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  380/625 | grad_norm=344.16 | sec/step~7.73 | keep=1.00 | K=8 | llama: tf=13.1752 first=9.0650 kCE=6.4324 KD=2.9348 man=0.0001 | scale_pen(llama)=2.7512e-11 | qwen: tf=11.7318 first=11.6855 kCE=14.8285 KD=3.7293 man=0.0002 | scale_pen(qwen)=2.2737e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  390/625 | grad_norm=100.69 | sec/step~7.53 | keep=1.00 | K=8 | llama: tf=13.0935 first=9.7445 kCE=6.1657 KD=2.6262 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=11.1762 first=12.9999 kCE=15.0735 KD=3.4431 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  400/625 | grad_norm=263.05 | sec/step~6.95 | keep=1.00 | K=8 | llama: tf=12.9936 first=9.0750 kCE=6.4759 KD=2.5339 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=10.5007 first=10.7432 kCE=15.0634 KD=2.7323 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  410/625 | grad_norm=664.29 | sec/step~6.24 | keep=1.00 | K=8 | llama: tf=11.8021 first=9.2210 kCE=7.1937 KD=3.6369 man=0.0001 | scale_pen(llama)=3.0070e-11 | qwen: tf=11.9995 first=13.9983 kCE=15.1978 KD=3.5761 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  420/625 | grad_norm=57.51 | sec/step~7.21 | keep=1.00 | K=8 | llama: tf=12.0866 first=9.4686 kCE=7.9693 KD=3.3360 man=0.0001 | scale_pen(llama)=2.8777e-11 | qwen: tf=11.8921 first=14.6736 kCE=14.5392 KD=4.0204 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  430/625 | grad_norm=163.53 | sec/step~6.96 | keep=1.00 | K=8 | llama: tf=10.8623 first=9.8193 kCE=7.3133 KD=3.2347 man=0.0001 | scale_pen(llama)=2.8777e-11 | qwen: tf=11.1124 first=11.9594 kCE=13.3098 KD=4.2498 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  440/625 | grad_norm=277.12 | sec/step~7.01 | keep=1.00 | K=8 | llama: tf=11.6240 first=9.1398 kCE=7.1376 KD=2.8804 man=0.0001 | scale_pen(llama)=2.8777e-11 | qwen: tf=11.1250 first=12.3262 kCE=14.7271 KD=3.6777 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  450/625 | grad_norm=28.21 | sec/step~8.10 | keep=1.00 | K=8 | llama: tf=12.3609 first=9.2230 kCE=7.5275 KD=2.7190 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=11.5015 first=13.0421 kCE=14.2132 KD=2.9683 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  460/625 | grad_norm=162.99 | sec/step~6.26 | keep=1.00 | K=8 | llama: tf=11.6139 first=8.8086 kCE=6.8437 KD=3.5677 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=11.0224 first=11.3282 kCE=14.6573 KD=3.9171 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  470/625 | grad_norm=277.42 | sec/step~7.19 | keep=1.00 | K=8 | llama: tf=11.4487 first=10.1117 kCE=7.6513 KD=2.7406 man=0.0001 | scale_pen(llama)=2.6276e-11 | qwen: tf=9.1473 first=13.8413 kCE=13.7691 KD=3.1829 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  480/625 | grad_norm=397.34 | sec/step~7.10 | keep=1.00 | K=8 | llama: tf=11.4251 first=9.6512 kCE=6.9079 KD=3.0654 man=0.0001 | scale_pen(llama)=1.9455e-11 | qwen: tf=11.5291 first=11.6851 kCE=14.2199 KD=3.7353 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  490/625 | grad_norm=106.77 | sec/step~6.32 | keep=1.00 | K=8 | llama: tf=11.0571 first=8.8629 kCE=7.0906 KD=3.1833 man=0.0001 | scale_pen(llama)=1.9455e-11 | qwen: tf=10.0615 first=8.9866 kCE=13.8334 KD=3.9623 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  500/625 | grad_norm=206.87 | sec/step~7.83 | keep=1.00 | K=8 | llama: tf=12.2107 first=8.9191 kCE=6.4190 KD=2.5376 man=0.0001 | scale_pen(llama)=1.9455e-11 | qwen: tf=10.6383 first=10.8199 kCE=14.3461 KD=3.4944 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  510/625 | grad_norm=312.74 | sec/step~6.76 | keep=1.00 | K=8 | llama: tf=10.7235 first=9.7878 kCE=8.2801 KD=3.7130 man=0.0001 | scale_pen(llama)=1.9455e-11 | qwen: tf=11.1362 first=12.0932 kCE=13.8678 KD=4.4077 man=0.0002 | scale_pen(qwen)=1.1951e-11 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  520/625 | grad_norm=120.37 | sec/step~6.18 | keep=1.00 | K=8 | llama: tf=13.0476 first=9.9900 kCE=5.8077 KD=3.5030 man=0.0001 | scale_pen(llama)=1.0360e-11 | qwen: tf=11.5422 first=12.7174 kCE=15.2409 KD=3.3793 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  530/625 | grad_norm=249.80 | sec/step~7.05 | keep=1.00 | K=8 | llama: tf=12.2584 first=9.3998 kCE=5.6684 KD=2.7925 man=0.0001 | scale_pen(llama)=1.0360e-11 | qwen: tf=10.8791 first=11.1251 kCE=15.1217 KD=3.4535 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  540/625 | grad_norm=395.91 | sec/step~7.43 | keep=1.00 | K=8 | llama: tf=11.0602 first=8.2585 kCE=7.4366 KD=3.3774 man=0.0001 | scale_pen(llama)=1.0360e-11 | qwen: tf=10.6317 first=11.6541 kCE=14.2731 KD=4.0551 man=0.0002 | scale_pen(qwen)=9.6065e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  550/625 | grad_norm=81.34 | sec/step~7.27 | keep=1.00 | K=8 | llama: tf=11.4089 first=8.7087 kCE=6.5512 KD=3.5797 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=10.9188 first=9.7781 kCE=14.1945 KD=4.1883 man=0.0002 | scale_pen(qwen)=6.8781e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  560/625 | grad_norm=199.69 | sec/step~6.76 | keep=1.00 | K=8 | llama: tf=11.9919 first=9.0539 kCE=6.4725 KD=3.4128 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=11.3950 first=11.8981 kCE=14.5121 KD=4.2320 man=0.0002 | scale_pen(qwen)=6.8781e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  570/625 | grad_norm=319.90 | sec/step~6.90 | keep=1.00 | K=8 | llama: tf=11.8024 first=9.1319 kCE=6.4618 KD=3.0475 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=10.8689 first=9.9666 kCE=14.9469 KD=3.2288 man=0.0002 | scale_pen(qwen)=6.8781e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  580/625 | grad_norm=91.09 | sec/step~6.58 | keep=1.00 | K=8 | llama: tf=12.1042 first=9.5259 kCE=6.2018 KD=3.8671 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=12.5900 first=13.0889 kCE=15.2525 KD=3.2858 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  590/625 | grad_norm=327.62 | sec/step~6.84 | keep=1.00 | K=8 | llama: tf=11.9595 first=9.7920 kCE=6.6422 KD=3.4825 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=9.9126 first=12.0524 kCE=13.6494 KD=3.6356 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  600/625 | grad_norm=613.53 | sec/step~7.58 | keep=1.00 | K=8 | llama: tf=11.8086 first=8.9969 kCE=6.1709 KD=3.4860 man=0.0001 | scale_pen(llama)=1.4211e-12 | qwen: tf=12.8203 first=12.1749 kCE=15.6279 KD=3.5693 man=0.0002 | scale_pen(qwen)=3.6380e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  610/625 | grad_norm=61.86 | sec/step~7.75 | keep=1.00 | K=8 | llama: tf=11.1924 first=9.6602 kCE=7.3370 KD=3.5110 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.3686 first=12.1826 kCE=13.6947 KD=4.2846 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  620/625 | grad_norm=330.78 | sec/step~7.21 | keep=1.00 | K=8 | llama: tf=11.2099 first=9.7035 kCE=6.2312 KD=3.3317 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=9.9031 first=11.2441 kCE=13.8389 KD=3.7015 man=0.0002 | scale_pen(qwen)=1.4211e-12 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
  step  625/625 | grad_norm=471.78 | sec/step~7.52 | keep=1.00 | K=8 | llama: tf=9.9382 first=9.6400 kCE=7.0270 KD=3.7267 man=0.0001 | scale_pen(llama)=2.2737e-13 | qwen: tf=9.2265 first=10.0088 kCE=12.8570 KD=4.7038 man=0.0002 | scale_pen(qwen)=3.5527e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0136 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hero_stq_m48_bos/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010582709915935993, 'rms_mean_cal': 0.010571271428465843, 'embed_rms': 0.01057521253824234, 'count': 625}, 'qwen': {'rms_mean_raw': 0.013636246041953563, 'rms_mean_cal': 0.013640909308195115, 'embed_rms': 0.013643525540828705, 'count': 625}}
Evaluating: runs/8B_hero_stq_m48_bos/epoch1/state.pt -> runs/8B_hero_stq_m48_bos/eval_epoch1 (bf16=0)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: eval.py [-h] --ckpt CKPT [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
               [--llama_device_map LLAMA_DEVICE_MAP]
               [--qwen_device_map QWEN_DEVICE_MAP]
               [--dataset {hotpot,squad,squad_v2}] [--samples SAMPLES]
               [--max_new_tokens MAX_NEW_TOKENS] [--load_4bit]
               [--device DEVICE] [--hotpot_config HOTPOT_CONFIG]
               [--out_dir OUT_DIR]
               [--token_budget_mode {chat_full,content_only}]
               [--token_budget_k TOKEN_BUDGET_K] [--gpu_mem_gib GPU_MEM_GIB]
               [--latent_anchor_mode {auto,chat,text,none}]
               [--latent_anchor_text LATENT_ANCHOR_TEXT]
               [--append_bos_after_prefix {auto,yes,no}] [--sequential_eval]
               [--chunk_size CHUNK_SIZE] [--hf_encoder_id HF_ENCODER_ID]
               [--max_enc_tokens MAX_ENC_TOKENS] [--fresh_eval] [--debug]
               [--debug_print_first DEBUG_PRINT_FIRST]
               [--debug_topk DEBUG_TOPK]
               [--debug_topk_examples DEBUG_TOPK_EXAMPLES]
               [--min_new_tokens MIN_NEW_TOKENS]
               [--eos_ban_steps EOS_BAN_STEPS]
               [--first_token_top_p FIRST_TOKEN_TOP_P]
               [--first_token_temperature FIRST_TOKEN_TEMPERATURE]
               [--latent_quant_bits {16,8,6,4}]
               [--latent_quant_group_size LATENT_QUANT_GROUP_SIZE]
               [--latent_quant_scale_bits LATENT_QUANT_SCALE_BITS]
               [--prefix_gain PREFIX_GAIN]
               [--calibration {none,embed_rms,fixed,train_stats}]
               [--prefix_target_rms PREFIX_TARGET_RMS]
               [--encoder_text_mode {auto,raw,neutral_chat,llama_chat,qwen_chat}]
               [--seed SEED]
eval.py: error: unrecognized arguments: 1
