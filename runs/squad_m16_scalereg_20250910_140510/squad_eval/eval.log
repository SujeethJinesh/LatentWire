Using forced device: mps
Using fp16 precision on MPS for memory efficiency
Building encoder and computing Z...
Saved Z to runs/squad_m16_scalereg_20250910_140510/squad_eval/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [llama] text: 8/200 | 2.36 ex/s | elapsed=3.4s | eta=1m21s
  [llama] text: 16/200 | 2.15 ex/s | elapsed=7.4s | eta=1m25s
  [llama] text: 24/200 | 2.37 ex/s | elapsed=10.1s | eta=1m14s
  [llama] text: 32/200 | 2.52 ex/s | elapsed=12.7s | eta=1m06s
  [llama] text: 40/200 | 2.56 ex/s | elapsed=15.6s | eta=1m02s
  [llama] text: 48/200 | 2.69 ex/s | elapsed=17.9s | eta=56.6s
  [llama] text: 56/200 | 2.77 ex/s | elapsed=20.2s | eta=52.0s
  [llama] text: 64/200 | 2.84 ex/s | elapsed=22.5s | eta=47.9s
  [llama] text: 72/200 | 2.85 ex/s | elapsed=25.3s | eta=45.0s
  [llama] text: 80/200 | 2.88 ex/s | elapsed=27.8s | eta=41.7s
  [llama] text: 88/200 | 2.89 ex/s | elapsed=30.4s | eta=38.7s
  [llama] text: 96/200 | 2.83 ex/s | elapsed=34.0s | eta=36.8s
  [llama] text: 104/200 | 2.87 ex/s | elapsed=36.3s | eta=33.5s
  [llama] text: 112/200 | 2.88 ex/s | elapsed=38.9s | eta=30.6s
  [llama] text: 120/200 | 2.87 ex/s | elapsed=41.8s | eta=27.9s
  [llama] text: 128/200 | 2.94 ex/s | elapsed=43.6s | eta=24.5s
  [llama] text: 136/200 | 2.97 ex/s | elapsed=45.8s | eta=21.6s
  [llama] text: 144/200 | 3.01 ex/s | elapsed=47.9s | eta=18.6s
  [llama] text: 152/200 | 3.01 ex/s | elapsed=50.5s | eta=16.0s
  [llama] text: 160/200 | 3.03 ex/s | elapsed=52.9s | eta=13.2s
  [llama] text: 168/200 | 3.05 ex/s | elapsed=55.1s | eta=10.5s
  [llama] text: 176/200 | 3.05 ex/s | elapsed=57.8s | eta=7.9s
  [llama] text: 184/200 | 3.04 ex/s | elapsed=1m00s | eta=5.3s
  [llama] text: 192/200 | 3.06 ex/s | elapsed=1m02s | eta=2.6s
  [llama] text: 200/200 | 3.02 ex/s | elapsed=1m06s | eta=0.0s
[debug:llama] adapter.scale=0.9990 | Z.std=0.9983 Z.mean||=15.9725 | prefix.std=0.6480 prefix.mean||=29.2011 | embed.RMS=0.0149
  [llama] latent: 8/200 | 3.45 ex/s | elapsed=2.3s | eta=55.7s
  [llama] latent: 16/200 | 4.54 ex/s | elapsed=3.5s | eta=40.5s
  [llama] latent: 24/200 | 5.06 ex/s | elapsed=4.7s | eta=34.8s
  [llama] latent: 32/200 | 5.38 ex/s | elapsed=5.9s | eta=31.2s
  [llama] latent: 40/200 | 5.58 ex/s | elapsed=7.2s | eta=28.7s
  [llama] latent: 48/200 | 5.73 ex/s | elapsed=8.4s | eta=26.5s
  [llama] latent: 56/200 | 5.85 ex/s | elapsed=9.6s | eta=24.6s
  [llama] latent: 64/200 | 5.92 ex/s | elapsed=10.8s | eta=23.0s
  [llama] latent: 72/200 | 5.96 ex/s | elapsed=12.1s | eta=21.5s
  [llama] latent: 80/200 | 6.00 ex/s | elapsed=13.3s | eta=20.0s
  [llama] latent: 88/200 | 6.05 ex/s | elapsed=14.5s | eta=18.5s
  [llama] latent: 96/200 | 6.09 ex/s | elapsed=15.8s | eta=17.1s
  [llama] latent: 104/200 | 6.13 ex/s | elapsed=17.0s | eta=15.7s
  [llama] latent: 112/200 | 6.16 ex/s | elapsed=18.2s | eta=14.3s
  [llama] latent: 120/200 | 6.19 ex/s | elapsed=19.4s | eta=12.9s
  [llama] latent: 128/200 | 6.22 ex/s | elapsed=20.6s | eta=11.6s
  [llama] latent: 136/200 | 6.23 ex/s | elapsed=21.8s | eta=10.3s
  [llama] latent: 144/200 | 6.24 ex/s | elapsed=23.1s | eta=9.0s
  [llama] latent: 152/200 | 6.26 ex/s | elapsed=24.3s | eta=7.7s
  [llama] latent: 160/200 | 6.28 ex/s | elapsed=25.5s | eta=6.4s
  [llama] latent: 168/200 | 6.29 ex/s | elapsed=26.7s | eta=5.1s
  [llama] latent: 176/200 | 6.31 ex/s | elapsed=27.9s | eta=3.8s
  [llama] latent: 184/200 | 6.32 ex/s | elapsed=29.1s | eta=2.5s
  [llama] latent: 192/200 | 6.32 ex/s | elapsed=30.4s | eta=1.3s
  [llama] latent: 200/200 | 6.33 ex/s | elapsed=31.6s | eta=0.0s
  [llama] text: 8/200 | 4.49 ex/s | elapsed=1.8s | eta=42.8s
  [llama] text: 16/200 | 5.59 ex/s | elapsed=2.9s | eta=32.9s
  [llama] text: 24/200 | 6.13 ex/s | elapsed=3.9s | eta=28.7s
  [llama] text: 32/200 | 6.45 ex/s | elapsed=5.0s | eta=26.0s
  [llama] text: 40/200 | 6.66 ex/s | elapsed=6.0s | eta=24.0s
  [llama] text: 48/200 | 6.80 ex/s | elapsed=7.1s | eta=22.4s
  [llama] text: 56/200 | 6.91 ex/s | elapsed=8.1s | eta=20.8s
  [llama] text: 64/200 | 7.01 ex/s | elapsed=9.1s | eta=19.4s
  [llama] text: 72/200 | 7.08 ex/s | elapsed=10.2s | eta=18.1s
  [llama] text: 80/200 | 7.13 ex/s | elapsed=11.2s | eta=16.8s
  [llama] text: 88/200 | 7.19 ex/s | elapsed=12.2s | eta=15.6s
  [llama] text: 96/200 | 7.23 ex/s | elapsed=13.3s | eta=14.4s
  [llama] text: 104/200 | 7.25 ex/s | elapsed=14.3s | eta=13.2s
  [llama] text: 112/200 | 7.28 ex/s | elapsed=15.4s | eta=12.1s
  [llama] text: 120/200 | 7.29 ex/s | elapsed=16.5s | eta=11.0s
  [llama] text: 128/200 | 7.31 ex/s | elapsed=17.5s | eta=9.9s
  [llama] text: 136/200 | 7.33 ex/s | elapsed=18.6s | eta=8.7s
  [llama] text: 144/200 | 7.34 ex/s | elapsed=19.6s | eta=7.6s
  [llama] text: 152/200 | 7.36 ex/s | elapsed=20.7s | eta=6.5s
  [llama] text: 160/200 | 7.37 ex/s | elapsed=21.7s | eta=5.4s
  [llama] text: 168/200 | 7.38 ex/s | elapsed=22.8s | eta=4.3s
  [llama] text: 176/200 | 7.39 ex/s | elapsed=23.8s | eta=3.2s
  [llama] text: 184/200 | 7.40 ex/s | elapsed=24.9s | eta=2.2s
  [llama] text: 192/200 | 7.41 ex/s | elapsed=25.9s | eta=1.1s
  [llama] text: 200/200 | 7.42 ex/s | elapsed=26.9s | eta=0.0s
Saved Llama results to runs/squad_m16_scalereg_20250910_140510/squad_eval/llama_results.json

Evaluating Qwen...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [qwen] text: 8/200 | 1.81 ex/s | elapsed=4.4s | eta=1m45s
  [qwen] text: 16/200 | 1.90 ex/s | elapsed=8.4s | eta=1m37s
  [qwen] text: 24/200 | 2.00 ex/s | elapsed=12.0s | eta=1m28s
  [qwen] text: 32/200 | 2.10 ex/s | elapsed=15.3s | eta=1m20s
  [qwen] text: 40/200 | 2.11 ex/s | elapsed=19.0s | eta=1m15s
  [qwen] text: 48/200 | 2.16 ex/s | elapsed=22.3s | eta=1m10s
  [qwen] text: 56/200 | 2.25 ex/s | elapsed=24.9s | eta=1m04s
  [qwen] text: 64/200 | 2.29 ex/s | elapsed=28.0s | eta=59.4s
  [qwen] text: 72/200 | 2.28 ex/s | elapsed=31.6s | eta=56.1s
  [qwen] text: 80/200 | 2.28 ex/s | elapsed=35.0s | eta=52.6s
  [qwen] text: 88/200 | 2.41 ex/s | elapsed=36.6s | eta=46.5s
  [qwen] text: 96/200 | 2.38 ex/s | elapsed=40.4s | eta=43.8s
  [qwen] text: 104/200 | 2.37 ex/s | elapsed=43.8s | eta=40.5s
  [qwen] text: 112/200 | 2.37 ex/s | elapsed=47.3s | eta=37.2s
  [qwen] text: 120/200 | 2.39 ex/s | elapsed=50.2s | eta=33.5s
  [qwen] text: 128/200 | 2.42 ex/s | elapsed=52.9s | eta=29.8s
  [qwen] text: 136/200 | 2.42 ex/s | elapsed=56.1s | eta=26.4s
  [qwen] text: 144/200 | 2.44 ex/s | elapsed=59.0s | eta=23.0s
  [qwen] text: 152/200 | 2.44 ex/s | elapsed=1m02s | eta=19.7s
  [qwen] text: 160/200 | 2.48 ex/s | elapsed=1m04s | eta=16.1s
  [qwen] text: 168/200 | 2.49 ex/s | elapsed=1m07s | eta=12.9s
  [qwen] text: 176/200 | 2.49 ex/s | elapsed=1m10s | eta=9.6s
  [qwen] text: 184/200 | 2.51 ex/s | elapsed=1m13s | eta=6.4s
  [qwen] text: 192/200 | 2.50 ex/s | elapsed=1m16s | eta=3.2s
  [qwen] text: 200/200 | 2.45 ex/s | elapsed=1m21s | eta=0.0s
[debug:qwen] adapter.scale=0.9967 | Z.std=0.9983 Z.mean||=15.9725 | prefix.std=0.6392 prefix.mean||=19.1024 | embed.RMS=0.0152
  [qwen] latent: 8/200 | 1.92 ex/s | elapsed=4.2s | eta=1m40s
  [qwen] latent: 16/200 | 2.85 ex/s | elapsed=5.6s | eta=1m04s
  [qwen] latent: 24/200 | 3.41 ex/s | elapsed=7.0s | eta=51.6s
  [qwen] latent: 32/200 | 3.78 ex/s | elapsed=8.5s | eta=44.4s
  [qwen] latent: 40/200 | 4.03 ex/s | elapsed=9.9s | eta=39.7s
  [qwen] latent: 48/200 | 4.21 ex/s | elapsed=11.4s | eta=36.1s
  [qwen] latent: 56/200 | 4.36 ex/s | elapsed=12.9s | eta=33.0s
  [qwen] latent: 64/200 | 4.47 ex/s | elapsed=14.3s | eta=30.4s
  [qwen] latent: 72/200 | 4.57 ex/s | elapsed=15.7s | eta=28.0s
  [qwen] latent: 80/200 | 4.65 ex/s | elapsed=17.2s | eta=25.8s
  [qwen] latent: 88/200 | 4.71 ex/s | elapsed=18.7s | eta=23.8s
  [qwen] latent: 96/200 | 4.77 ex/s | elapsed=20.1s | eta=21.8s
  [qwen] latent: 104/200 | 4.82 ex/s | elapsed=21.6s | eta=19.9s
  [qwen] latent: 112/200 | 4.86 ex/s | elapsed=23.0s | eta=18.1s
  [qwen] latent: 120/200 | 4.90 ex/s | elapsed=24.5s | eta=16.3s
  [qwen] latent: 128/200 | 4.93 ex/s | elapsed=26.0s | eta=14.6s
  [qwen] latent: 136/200 | 4.96 ex/s | elapsed=27.4s | eta=12.9s
  [qwen] latent: 144/200 | 4.99 ex/s | elapsed=28.8s | eta=11.2s
  [qwen] latent: 152/200 | 5.02 ex/s | elapsed=30.3s | eta=9.6s
  [qwen] latent: 160/200 | 5.04 ex/s | elapsed=31.7s | eta=7.9s
  [qwen] latent: 168/200 | 5.06 ex/s | elapsed=33.2s | eta=6.3s
  [qwen] latent: 176/200 | 5.08 ex/s | elapsed=34.7s | eta=4.7s
  [qwen] latent: 184/200 | 5.08 ex/s | elapsed=36.2s | eta=3.2s
  [qwen] latent: 192/200 | 5.09 ex/s | elapsed=37.7s | eta=1.6s
  [qwen] latent: 200/200 | 5.07 ex/s | elapsed=39.5s | eta=0.0s
  [qwen] text: 8/200 | 1.91 ex/s | elapsed=4.2s | eta=1m40s
  [qwen] text: 16/200 | 2.95 ex/s | elapsed=5.4s | eta=1m02s
  [qwen] text: 24/200 | 3.64 ex/s | elapsed=6.6s | eta=48.4s
  [qwen] text: 32/200 | 4.10 ex/s | elapsed=7.8s | eta=41.0s
  [qwen] text: 40/200 | 4.43 ex/s | elapsed=9.0s | eta=36.1s
  [qwen] text: 48/200 | 4.67 ex/s | elapsed=10.3s | eta=32.5s
  [qwen] text: 56/200 | 4.86 ex/s | elapsed=11.5s | eta=29.6s
  [qwen] text: 64/200 | 5.02 ex/s | elapsed=12.7s | eta=27.1s
  [qwen] text: 72/200 | 5.16 ex/s | elapsed=14.0s | eta=24.8s
  [qwen] text: 80/200 | 5.29 ex/s | elapsed=15.1s | eta=22.7s
  [qwen] text: 88/200 | 5.41 ex/s | elapsed=16.3s | eta=20.7s
  [qwen] text: 96/200 | 5.50 ex/s | elapsed=17.5s | eta=18.9s
  [qwen] text: 104/200 | 5.56 ex/s | elapsed=18.7s | eta=17.3s
  [qwen] text: 112/200 | 5.63 ex/s | elapsed=19.9s | eta=15.6s
  [qwen] text: 120/200 | 5.68 ex/s | elapsed=21.1s | eta=14.1s
  [qwen] text: 128/200 | 5.73 ex/s | elapsed=22.4s | eta=12.6s
  [qwen] text: 136/200 | 5.78 ex/s | elapsed=23.5s | eta=11.1s
  [qwen] text: 144/200 | 5.82 ex/s | elapsed=24.7s | eta=9.6s
  [qwen] text: 152/200 | 5.87 ex/s | elapsed=25.9s | eta=8.2s
  [qwen] text: 160/200 | 5.88 ex/s | elapsed=27.2s | eta=6.8s
  [qwen] text: 168/200 | 5.90 ex/s | elapsed=28.5s | eta=5.4s
  [qwen] text: 176/200 | 5.94 ex/s | elapsed=29.6s | eta=4.0s
  [qwen] text: 184/200 | 5.96 ex/s | elapsed=30.9s | eta=2.7s
  [qwen] text: 192/200 | 5.98 ex/s | elapsed=32.1s | eta=1.3s
  [qwen] text: 200/200 | 6.00 ex/s | elapsed=33.3s | eta=0.0s
Saved Qwen results to runs/squad_m16_scalereg_20250910_140510/squad_eval/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: mps  |  Dtype: torch.float16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.005  F1: 0.057  |  NLL/token (gold): 15.681324996504673
Qwen   EM: 0.230   F1: 0.396   |  NLL/token (gold): 13.044476828877889
Wall clock: 147.87s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.027  |  NLL/token (gold): 7.842418723882631
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 8.918943154433418
Wall clock: 71.04s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.029
Qwen   EM: 0.005   F1: 0.041
Wall clock: 60.27s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.080
Oracle upper bound:  EM 0.000  F1 0.027

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "mps",
  "dtype": "torch.float16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 147.8706238269806,
    "llama": {
      "em": 0.005,
      "f1": 0.056785177241059595,
      "nll_token": 15.681324996504673
    },
    "qwen": {
      "em": 0.23,
      "f1": 0.39575623885918004,
      "nll_token": 13.044476828877889
    }
  },
  "latent": {
    "wall_clock_sec": 71.04051303863525,
    "llama": {
      "em": 0.0,
      "f1": 0.026987234987234987,
      "nll_token": 7.842418723882631
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 8.918943154433418
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.029140570815493417
    },
    "wall_clock_sec": 60.273768186569214,
    "qwen": {
      "em": 0.005,
      "f1": 0.04103176209755158
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.08,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 0.9990198612213135,
      "Z_std": 0.9982794523239136,
      "Z_mean_norm": 15.972457885742188,
      "prefix_std": 0.6479999423027039,
      "prefix_mean_norm": 29.201139450073242,
      "embed_rms": 0.014909257180988789
    },
    "qwen": {
      "adapter_scale": 0.9966846704483032,
      "Z_std": 0.9982794523239136,
      "Z_mean_norm": 15.972457885742188,
      "prefix_std": 0.6391584277153015,
      "prefix_mean_norm": 19.102418899536133,
      "embed_rms": 0.015230061486363411
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.026987234987234987
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_m16_scalereg_20250910_140510/squad_eval/predictions.jsonl
