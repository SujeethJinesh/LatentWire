/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Device: cuda:0

======================================================================
PROMPT TUNING BASELINE (8 tokens): ARC_EASY
======================================================================
This proves whether Llama (sender) actually helps.

--- Seed 42 ---
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 3152.82it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:03,  1.99s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:04<00:02,  2.02s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.93s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.95s/it]
Training:   0%|          | 0/1500 [00:00<?, ?it/s]Training:   0%|          | 0/1500 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 1635, in <module>
    main()
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 1585, in main
    results = run_prompt_tuning_baseline(args, config, device)
  File "/scratch/m000066/sujinesh/LatentWire/telepathy/run_baselines.py", line 946, in run_prompt_tuning_baseline
    label_texts = [config["label_map"][l] for l in batch[config["label_field"]].tolist()]
AttributeError: 'list' object has no attribute 'tolist'
