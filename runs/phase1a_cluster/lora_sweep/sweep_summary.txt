Phase 1a + LoRA Sweep
Started: Tue Oct 14 21:27:32 PDT 2025
================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Dataset: squad
Samples: 10000 (PCA: 5000)
Epochs:  3
Batch:   36
Compression: 4096 -> 1024
Adapter LR: 5e-4
Loss: Cosine (1.0) + MSE (0.1)

Configuration: baseline
  LoRA: r=0, alpha=0, layers=none
  gen_weight: 0.0
  F1: 0.0022222221728395073
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/baseline/train_20251014_212732.log
