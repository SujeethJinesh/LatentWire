Phase 1a + LoRA Sweep
Started: Wed Oct 15 20:16:16 PDT 2025
================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Dataset: squad
Samples: 10000 (PCA: 5000)
Epochs:  3
Batch:   48
Compression: 4096 -> 1024
Adapter LR: 5e-4
Loss: Cosine (1.0) + MSE (0.1)

Configuration: baseline
  LoRA: r=0, alpha=0, layers=none
  gen_weight: 0.0
  F1: 0.01116666649855556
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/baseline/train_20251015_201616.log
