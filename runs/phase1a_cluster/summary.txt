Baseline results:
  Log:  runs/phase1a_cluster/baseline/train_20251015_202708.log
  Diag: runs/phase1a_cluster/baseline/diagnostics.jsonl
Best F1: 0.59%
Best EM: 0.00%


LoRA sweep summary:
Phase 1a + LoRA Sweep
Started: Wed Oct 15 20:29:56 PDT 2025
================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Dataset: squad
Samples: 10000 (PCA: 5000)
Epochs:  3
Batch:   48
Compression: 4096 -> 1024
Adapter LR: 5e-4
Loss: Cosine (1.0) + MSE (0.1)

Configuration: baseline
  LoRA: r=0, alpha=0, layers=none
  gen_weight: 0.0
  F1: 0.01116666649855556
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/baseline/train_20251015_202956.log

Configuration: r4_a8_l8
  LoRA: r=4, alpha=8, layers=8
  gen_weight: 0.02
  F1: 0.004871794801413546
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r4_a8_l8/train_20251015_203247.log

Configuration: r8_a16_l12
  LoRA: r=8, alpha=16, layers=12
  gen_weight: 0.02
  F1: 0.0
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r8_a16_l12/train_20251015_204631.log

Configuration: r16_a32_full
  LoRA: r=16, alpha=32, layers=all
  gen_weight: 0.02
  F1: 0.0
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r16_a32_full/train_20251015_205943.log
baseline 0.01116666649855556 0.0
r4_a8_l8 0.004871794801413546 0.0
r8_a16_l12 0.0 0.0
r16_a32_full 0.0 0.0
