Baseline results:
  Log:  runs/phase1a_cluster/baseline/train_20251015_225746.log
  Diag: runs/phase1a_cluster/baseline/diagnostics.jsonl
Best F1: 1.49%
Best EM: 0.00%


LoRA sweep summary:
Phase 1a + LoRA Sweep
Started: Wed Oct 15 23:03:21 PDT 2025
================================================
Model: meta-llama/Meta-Llama-3.1-8B-Instruct
Dataset: squad
Samples: 10000 (PCA: 5000)
Epochs:  3
Batch:   48
Compression: 4096 -> 1024
Adapter LR: 5e-4
Loss: Cosine (1.0) + MSE (0.1)

Configuration: baseline
  LoRA: r=0, alpha=0, layers=none
  gen_weight: 0.0
  F1: 0.009555555395932102
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/baseline/train_20251015_230321.log

Configuration: r4_a8_l8
  LoRA: r=4, alpha=8, layers=8
  gen_weight: 0.02
  F1: 0.006111111033950618
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r4_a8_l8/train_20251015_230631.log

Configuration: r8_a16_l12
  LoRA: r=8, alpha=16, layers=12
  gen_weight: 0.02
  F1: 0.0033333332888888893
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r8_a16_l12/train_20251015_232216.log

Configuration: r16_a32_full
  LoRA: r=16, alpha=32, layers=all
  gen_weight: 0.02
  F1: 0.0033333333055555565
  EM: 0.0
  Log: runs/phase1a_cluster/lora_sweep/r16_a32_full/train_20251015_233650.log
baseline 0.009555555395932102 0.0
r4_a8_l8 0.006111111033950618 0.0
r8_a16_l12 0.0033333332888888893 0.0
r16_a32_full 0.0033333333055555565 0.0
