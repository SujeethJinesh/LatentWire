/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
LOSS WEIGHT SWEEP
================================================================================

Device: cuda
Samples: 1000, Steps per config: 300

[1/3] Loading frozen models...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 55.43it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]
  ✓ Models loaded

[2/3] Instantiating architecture...
  ✓ Architecture ready

[3/3] Loading test data (SQuAD, n=1000)...
  ✓ Loaded 1000 examples

================================================================================
Configuration: No semantic loss (buggy version)
  sem_weight=0.0, K=4
================================================================================

Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  Step 1/300: loss=10.8906 gen=10.8906 sem=0.0000
  Step 51/300: loss=3.9083 gen=3.9083 sem=0.0000
  Step 101/300: loss=5.6342 gen=5.6342 sem=0.0000
  Step 151/300: loss=4.1859 gen=4.1859 sem=0.0000
  Step 201/300: loss=3.0392 gen=3.0392 sem=0.0000
  Step 251/300: loss=3.1551 gen=3.1551 sem=0.0000
  Step 300/300: loss=4.7158 gen=4.7158 sem=0.0000

  Predictions:
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/transformer.py:409: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    [1] Gold: Dane                           → Pred: the first time in 2003. The team's
    [2] Gold: Muslims                        → Pred: the first time in 2004. The team's
    [3] Gold: orientalism and tropicality    → Pred: the first time in 2004. The team's
    [4] Gold: numeracy                       → Pred: the first time in 2004. The team's
    [5] Gold: Mental Health (Care and Treatm → Pred: the first time in 2004. The team's

  Diversity: 2/10 unique (20.0%)

================================================================================
Configuration: Very weak semantic (0.01)
  sem_weight=0.01, K=4
================================================================================

  Step 1/300: loss=11.7631 gen=11.7531 sem=1.0014
  Step 51/300: loss=5.8327 gen=5.8227 sem=1.0058
  Step 101/300: loss=3.7548 gen=3.7447 sem=1.0089
  Step 151/300: loss=5.9089 gen=5.8987 sem=1.0115
  Step 201/300: loss=3.8370 gen=3.8269 sem=1.0071
  Step 251/300: loss=6.2583 gen=6.2481 sem=1.0145
  Step 300/300: loss=4.6000 gen=4.5899 sem=1.0094

  Predictions:
    [1] Gold: Dane                           → Pred: The first time I saw the movie, I was a
    [2] Gold: Muslims                        → Pred: The first time I saw the movie, I was a
    [3] Gold: orientalism and tropicality    → Pred: The first time I saw the movie, I was a
    [4] Gold: numeracy                       → Pred: The first time I saw the movie, I was a
    [5] Gold: Mental Health (Care and Treatm → Pred: The first time I saw the movie, I was a

  Diversity: 1/10 unique (10.0%)

================================================================================
Configuration: Weak semantic (0.05)
  sem_weight=0.05, K=4
================================================================================

  Step 1/300: loss=11.8420 gen=11.7921 sem=0.9986
  Step 51/300: loss=4.0522 gen=4.0022 sem=1.0001
  Step 101/300: loss=2.6058 gen=2.5554 sem=1.0073
  Step 151/300: loss=5.5561 gen=5.5060 sem=1.0038
  Step 201/300: loss=5.8242 gen=5.7744 sem=0.9972
  Step 251/300: loss=5.2063 gen=5.1562 sem=1.0016
  Step 300/300: loss=4.7240 gen=4.6739 sem=1.0009

  Predictions:
    [1] Gold: Dane                           → Pred: The first of the three volumes of the 1960
    [2] Gold: Muslims                        → Pred: The first of the three volumes of the 1967
    [3] Gold: orientalism and tropicality    → Pred: The first of the three volumes of the 1967
    [4] Gold: numeracy                       → Pred: The first of the three volumes of the 1960
    [5] Gold: Mental Health (Care and Treatm → Pred: The first of the three volumes of the 1967

  Diversity: 2/10 unique (20.0%)

================================================================================
Configuration: Medium semantic (0.1)
  sem_weight=0.1, K=4
================================================================================

  Step 1/300: loss=12.5360 gen=12.4356 sem=1.0037
  Step 51/300: loss=6.7941 gen=6.6941 sem=1.0002
  Step 101/300: loss=6.4339 gen=6.3332 sem=1.0070
  Step 151/300: loss=4.1592 gen=4.0596 sem=0.9962
  Step 201/300: loss=5.0216 gen=4.9221 sem=0.9947
  Step 251/300: loss=5.4043 gen=5.3052 sem=0.9919
  Step 300/300: loss=2.9132 gen=2.8141 sem=0.9914

  Predictions:
    [1] Gold: Dane                           → Pred: 2005, 2006, 2007,
    [2] Gold: Muslims                        → Pred: 2005, 2006, 2007,
    [3] Gold: orientalism and tropicality    → Pred: 2005, 2006, 2007,
    [4] Gold: numeracy                       → Pred: 2005, 2006, 2007,
    [5] Gold: Mental Health (Care and Treatm → Pred: 2005, 2006, 2007,

  Diversity: 1/10 unique (10.0%)

================================================================================
Configuration: Strong semantic (0.5)
  sem_weight=0.5, K=4
================================================================================

  Step 1/300: loss=10.9540 gen=10.4528 sem=1.0023
  Step 51/300: loss=7.2650 gen=6.7656 sem=0.9986
  Step 101/300: loss=2.5261 gen=2.0327 sem=0.9867
  Step 151/300: loss=3.6881 gen=3.2005 sem=0.9751
  Step 201/300: loss=3.6431 gen=3.1616 sem=0.9629
  Step 251/300: loss=3.1452 gen=2.6715 sem=0.9474
  Step 300/300: loss=4.6127 gen=4.1466 sem=0.9322

  Predictions:
    [1] Gold: Dane                           → Pred: the 1960s and 1970s saw
    [2] Gold: Muslims                        → Pred: the 1960s and 1970s saw
    [3] Gold: orientalism and tropicality    → Pred: the 1960s and 1970s saw
    [4] Gold: numeracy                       → Pred: the 1960s and 1970s saw
    [5] Gold: Mental Health (Care and Treatm → Pred: the 1960s and 1970s saw

  Diversity: 1/10 unique (10.0%)

================================================================================
Configuration: Increased K-token (K=8)
  sem_weight=0.05, K=8
================================================================================

  Step 1/300: loss=10.4626 gen=10.4122 sem=1.0087
  Step 51/300: loss=4.5001 gen=4.4499 sem=1.0050
  Step 101/300: loss=5.6765 gen=5.6263 sem=1.0045
  Step 151/300: loss=3.1926 gen=3.1426 sem=1.0004
  Step 201/300: loss=3.9160 gen=3.8658 sem=1.0034
  Step 251/300: loss=3.1169 gen=3.0668 sem=1.0010
  Step 300/300: loss=3.4216 gen=3.3718 sem=0.9953

  Predictions:
    [1] Gold: Dane                           → Pred: Question 1: What is the name of the first
    [2] Gold: Muslims                        → Pred: Question 1: What is the name of the first
    [3] Gold: orientalism and tropicality    → Pred: Question 1: What is the name of the first
    [4] Gold: numeracy                       → Pred: Question 1: What is the name of the first
    [5] Gold: Mental Health (Care and Treatm → Pred: Question 1: What is the name of the first

  Diversity: 1/10 unique (10.0%)

================================================================================
Configuration: Increased K-token (K=12)
  sem_weight=0.05, K=12
================================================================================

  Step 1/300: loss=7.8308 gen=7.7806 sem=1.0029
  Step 51/300: loss=6.5257 gen=6.4757 sem=1.0001
  Step 101/300: loss=3.0555 gen=3.0056 sem=0.9967
  Step 151/300: loss=2.5746 gen=2.5247 sem=0.9997
  Step 201/300: loss=1.8270 gen=1.7771 sem=0.9979
  Step 251/300: loss=7.9811 gen=7.9311 sem=1.0004
  Step 300/300: loss=5.4963 gen=5.4466 sem=0.9933

  Predictions:
    [1] Gold: Dane                           → Pred: 2013) and the 2014 FIFA World Cup
    [2] Gold: Muslims                        → Pred: 2013) and the 2014 FIFA World Cup
    [3] Gold: orientalism and tropicality    → Pred: 2013, the company was acquired by the private equity
    [4] Gold: numeracy                       → Pred: 2013, the company was acquired by the private equity
    [5] Gold: Mental Health (Care and Treatm → Pred: 2013) and the 2014 FIFA World Cup

  Diversity: 2/10 unique (20.0%)

================================================================================
SWEEP RESULTS SUMMARY
================================================================================

Configuration                                   K   Div%       Loss   Gen Loss
--------------------------------------------------------------------------------
No semantic loss (buggy version)                4  20.0%     4.7158     4.7158
Very weak semantic (0.01)                       4  10.0%     4.6000     4.5899
Weak semantic (0.05)                            4  20.0%     4.7240     4.6739
Medium semantic (0.1)                           4  10.0%     2.9132     2.8141
Strong semantic (0.5)                           4  10.0%     4.6127     4.1466
Increased K-token (K=8)                         8  10.0%     3.4216     3.3718
Increased K-token (K=12)                       12  20.0%     5.4963     5.4466

================================================================================
BEST: No semantic loss (buggy version) with 20.0% diversity
================================================================================

Results saved to: runs/loss_weight_sweep/
  - results.json (full data)
  - summary.txt (readable summary)
