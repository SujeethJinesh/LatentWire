=== Starting python -m latentwire.cli.eval --config configs/baseline/embedding_baselines.json --override ckpt=runs/smoke/base/ckpt --override out_dir=runs/baseline/embedding/eval --tag baseline-embedding ===
<frozen runpy>:128: RuntimeWarning: 'latentwire.cli.eval' found in sys.modules after import of package 'latentwire.cli', but prior to execution of 'latentwire.cli.eval'; this may result in unpredictable behaviour
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/cli/eval.py", line 153, in <module>
    main()
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/cli/eval.py", line 134, in main
    run_eval(argv)
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/cli/eval.py", line 109, in run_eval
    eval_module.main()
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/eval.py", line 1883, in main
    encoder_wire.load_state_dict(_safe_load(os.path.join(ckpt_dir, "encoder.pt"), map_location=device))
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2153, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for InterlinguaEncoder:
	Missing key(s) in state_dict: "backbone.byte_emb.weight", "backbone.encoder.layers.0.self_attn.in_proj_weight", "backbone.encoder.layers.0.self_attn.in_proj_bias", "backbone.encoder.layers.0.self_attn.out_proj.weight", "backbone.encoder.layers.0.self_attn.out_proj.bias", "backbone.encoder.layers.0.linear1.weight", "backbone.encoder.layers.0.linear1.bias", "backbone.encoder.layers.0.linear2.weight", "backbone.encoder.layers.0.linear2.bias", "backbone.encoder.layers.0.norm1.weight", "backbone.encoder.layers.0.norm1.bias", "backbone.encoder.layers.0.norm2.weight", "backbone.encoder.layers.0.norm2.bias", "backbone.encoder.layers.1.self_attn.in_proj_weight", "backbone.encoder.layers.1.self_attn.in_proj_bias", "backbone.encoder.layers.1.self_attn.out_proj.weight", "backbone.encoder.layers.1.self_attn.out_proj.bias", "backbone.encoder.layers.1.linear1.weight", "backbone.encoder.layers.1.linear1.bias", "backbone.encoder.layers.1.linear2.weight", "backbone.encoder.layers.1.linear2.bias", "backbone.encoder.layers.1.norm1.weight", "backbone.encoder.layers.1.norm1.bias", "backbone.encoder.layers.1.norm2.weight", "backbone.encoder.layers.1.norm2.bias", "backbone.encoder.layers.2.self_attn.in_proj_weight", "backbone.encoder.layers.2.self_attn.in_proj_bias", "backbone.encoder.layers.2.self_attn.out_proj.weight", "backbone.encoder.layers.2.self_attn.out_proj.bias", "backbone.encoder.layers.2.linear1.weight", "backbone.encoder.layers.2.linear1.bias", "backbone.encoder.layers.2.linear2.weight", "backbone.encoder.layers.2.linear2.bias", "backbone.encoder.layers.2.norm1.weight", "backbone.encoder.layers.2.norm1.bias", "backbone.encoder.layers.2.norm2.weight", "backbone.encoder.layers.2.norm2.bias", "backbone.encoder.layers.3.self_attn.in_proj_weight", "backbone.encoder.layers.3.self_attn.in_proj_bias", "backbone.encoder.layers.3.self_attn.out_proj.weight", "backbone.encoder.layers.3.self_attn.out_proj.bias", "backbone.encoder.layers.3.linear1.weight", "backbone.encoder.layers.3.linear1.bias", "backbone.encoder.layers.3.linear2.weight", "backbone.encoder.layers.3.linear2.bias", "backbone.encoder.layers.3.norm1.weight", "backbone.encoder.layers.3.norm1.bias", "backbone.encoder.layers.3.norm2.weight", "backbone.encoder.layers.3.norm2.bias", "backbone.encoder.layers.4.self_attn.in_proj_weight", "backbone.encoder.layers.4.self_attn.in_proj_bias", "backbone.encoder.layers.4.self_attn.out_proj.weight", "backbone.encoder.layers.4.self_attn.out_proj.bias", "backbone.encoder.layers.4.linear1.weight", "backbone.encoder.layers.4.linear1.bias", "backbone.encoder.layers.4.linear2.weight", "backbone.encoder.layers.4.linear2.bias", "backbone.encoder.layers.4.norm1.weight", "backbone.encoder.layers.4.norm1.bias", "backbone.encoder.layers.4.norm2.weight", "backbone.encoder.layers.4.norm2.bias", "backbone.encoder.layers.5.self_attn.in_proj_weight", "backbone.encoder.layers.5.self_attn.in_proj_bias", "backbone.encoder.layers.5.self_attn.out_proj.weight", "backbone.encoder.layers.5.self_attn.out_proj.bias", "backbone.encoder.layers.5.linear1.weight", "backbone.encoder.layers.5.linear1.bias", "backbone.encoder.layers.5.linear2.weight", "backbone.encoder.layers.5.linear2.bias", "backbone.encoder.layers.5.norm1.weight", "backbone.encoder.layers.5.norm1.bias", "backbone.encoder.layers.5.norm2.weight", "backbone.encoder.layers.5.norm2.bias", "backbone.ln.weight", "backbone.ln.bias", "shared_pooler.latent", "shared_pooler.cross_attn.in_proj_weight", "shared_pooler.cross_attn.in_proj_bias", "shared_pooler.cross_attn.out_proj.weight", "shared_pooler.cross_attn.out_proj.bias", "shared_pooler.ff.0.weight", "shared_pooler.ff.0.bias", "shared_pooler.ff.1.weight", "shared_pooler.ff.1.bias", "shared_pooler.ff.3.weight", "shared_pooler.ff.3.bias", "shared_pooler.gate.0.weight", "shared_pooler.gate.0.bias", "shared_pooler.gate.1.weight", "shared_pooler.gate.1.bias". 
	Unexpected key(s) in state_dict: "encoder.weight", "encoder.bias". 
=== LatentWire Eval CLI ===
Config: /Users/sujeethjinesh/Desktop/LatentWire/configs/baseline/embedding_baselines.json
Overrides:
  - ckpt=runs/smoke/base/ckpt
  - out_dir=runs/baseline/embedding/eval
Derived argv: --ckpt runs/smoke/base/ckpt --models llama --llama_id TinyLlama/TinyLlama-1.1B-Chat-v1.0 --dataset squad --samples 5 --max_new_tokens 24 --latent_anchor_mode auto --latent_anchor_text Answer:  --append_bos_after_prefix auto --use_chat_template yes --token_budget_mode content_only --token_budget_k 32 --chunk_size 8 --fresh_eval --embedding_replay --embedding_baseline_modes ["raw", "anchor", "adapter"] --out_dir runs/baseline/embedding/eval --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 1 --eos_ban_steps 0 --calibration embed_rms --prefix_gain 1.0 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto
Auto-detected device: mps
Using fp16 precision on MPS for memory efficiency
Encoder input alignment: mode=raw | strip_anchor=yes | samples=5
Building encoder and computing Z...
--- baseline-embedding failed with exit code 1 ---

