
=========================================
Starting pipeline at Fri Sep 19 00:31:21 PDT 2025
=========================================

Preset: FAST_2H  |  GPUs: 0,1,2,3  |  Llama devices: 0,1  |  Qwen devices: 2,3
Run dir: runs/8B_hailmary_allknobs_fast_2h


=========================================
TRAIN + PER-EPOCH EVAL (All knobs enabled)
=========================================


=========================================
EPOCH 1/1
=========================================

Running pre-train eval on existing checkpoint...
+ CUDA_VISIBLE_DEVICES=0,1,2,3
+ python -u -m latentwire.eval --ckpt runs/8B_hailmary_allknobs_fast_2h/ckpt/state.pt --llama_id meta-llama/Meta-Llama-3.1-8B-Instruct --qwen_id Qwen/Qwen2.5-7B-Instruct --samples 200 --out_dir runs/8B_hailmary_allknobs_fast_2h/eval_epoch1_pre --dataset squad --max_new_tokens 16 --latent_anchor_mode auto --latent_anchor_text '' --append_bos_after_prefix yes --calibration embed_rms --prefix_gain 1.15 --token_budget_mode content_only --token_budget_k 32 --first_token_top_p 1.0 --first_token_temperature 0.0 --min_new_tokens 3 --eos_ban_steps 6 --chunk_size 96 --hf_encoder_id sentence-transformers/all-MiniLM-L6-v2 --max_enc_tokens 1024 --llama_device_map auto --qwen_device_map auto --llama_devices 0,1 --qwen_devices 2,3 --gpu_mem_gib 78 --sequential_eval --fresh_eval
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hailmary_allknobs_fast_2h/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hailmary_allknobs_fast_2h/eval_epoch1_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3001.83it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.31s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3207.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.03s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:00,  1.05it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 28672 bytes (fp32); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.585  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.655   F1: 0.831   |  NLL/token (gold): 25.81069942126198
Wall clock: 21.00s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.005  |  NLL/token (gold): 10.92755247946499
       First-token acc: top1=0.025  top5=0.035
Qwen   EM: 0.000   F1: 0.016  |  NLL/token (gold): 10.426495039904559
       First-token acc: top1=0.000  top5=0.000
Wall clock: 2.84s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 3.34s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.004
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.020

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 28672,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.585,
      "f1": 0.7986400586346852,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.655,
      "f1": 0.8313113259894596,
      "nll_token": 25.81069942126198
    },
    "wall_clock_sec": 21.000956773757935
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.005282679572196547,
      "nll": 10.92755247946499,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.03500000014901161,
      "nll_token": 10.92755247946499
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.016187293993030486,
      "nll": 10.426495039904559,
      "first_token_top1": 0.0,
      "first_token_top5": 0.0,
      "nll_token": 10.426495039904559
    },
    "wall_clock_sec": 2.8367977142333984
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04095895236860437
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711898018827
    },
    "wall_clock_sec": 3.336111307144165
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004027777660633685,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.0203261827585373
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "auto",
      "latent_anchor_text": "",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0203261827585373
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hailmary_allknobs_fast_2h/eval_epoch1_pre/predictions.jsonl
+ set +x
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3193.23it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:03,  1.51s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3264.68it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.08it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.13it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.10it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hailmary_allknobs_fast_2h/ckpt/state.pt
   -> failed to load weights from state.pt (Error(s) in loading state_dict for STQueryEncoder:
	size mismatch for query: copying a param with shape torch.Size([32, 384]) from checkpoint, the shape in current model is torch.Size([48, 384]).); will try .pt files
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 1183, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 701, in main
    epoch_loaded, global_loaded = load_checkpoint(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 167, in load_checkpoint
    encoder.load_state_dict(_safe_load(enc_path, map_location=device), strict=strict)
  File "/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 2215, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for STQueryEncoder:
	size mismatch for query: copying a param with shape torch.Size([32, 384]) from checkpoint, the shape in current model is torch.Size([48, 384]).
