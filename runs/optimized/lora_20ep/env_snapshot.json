{
  "timestamp": "2025-10-11T05:32:23.655467Z",
  "python": "3.9.21 (main, Mar 20 2025, 12:26:11)  [GCC 13.1.0]",
  "platform": {
    "system": "Linux",
    "release": "5.15.0-1081-nvidia",
    "machine": "x86_64"
  },
  "versions": {
    "torch": "2.4.0+cu121",
    "transformers": "4.45.2",
    "datasets": "4.1.1",
    "sentence_transformers": "5.1.1",
    "bitsandbytes": "0.47.0"
  },
  "argv": [
    "latentwire/train.py",
    "--llama_id",
    "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "--models",
    "llama",
    "--dataset",
    "squad",
    "--samples",
    "20000",
    "--epochs",
    "20",
    "--batch_size",
    "96",
    "--grad_accum_steps",
    "2",
    "--lr",
    "2e-4",
    "--latent_len",
    "32",
    "--d_z",
    "256",
    "--encoder_type",
    "byte",
    "--sequential_models",
    "--first_token_ce_weight",
    "3.0",
    "--first_token_entropy_weight",
    "0.5",
    "--k_ce_weight",
    "1.5",
    "--K",
    "8",
    "--kd_first_k_weight",
    "0.5",
    "--kd_tau",
    "2.0",
    "--use_lora",
    "--lora_r",
    "64",
    "--lora_alpha",
    "128",
    "--lora_dropout",
    "0.05",
    "--warm_anchor_text",
    "Answer: ",
    "--save_dir",
    "runs/optimized/lora_20ep",
    "--save_every",
    "500",
    "--diagnostic_log",
    "runs/optimized/lora_20ep/logs/diagnostics.jsonl",
    "--grad_diag_interval",
    "20",
    "--llama_device_map",
    "auto",
    "--grad_ckpt",
    "--max_grad_norm",
    "0.5",
    "--seed",
    "42"
  ],
  "phase": "train"
}