/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
usage: train.py [-h] [--llama_id LLAMA_ID] [--qwen_id QWEN_ID]
                [--llama_device_map LLAMA_DEVICE_MAP]
                [--qwen_device_map QWEN_DEVICE_MAP] [--require_cuda {yes,no}]
                [--dataset {hotpot,squad,squad_v2}] [--models MODELS]
                [--hotpot_config HOTPOT_CONFIG] [--samples SAMPLES]
                [--epochs EPOCHS] [--batch_size BATCH_SIZE]
                [--grad_accum_steps GRAD_ACCUM_STEPS] [--seed SEED]
                [--data_seed DATA_SEED] [--latent_len LATENT_LEN]
                [--latent_shared_len LATENT_SHARED_LEN]
                [--latent_private_len LATENT_PRIVATE_LEN] [--d_z D_Z]
                [--max_bytes MAX_BYTES] [--encoder_type {byte,simple-st,stq}]
                [--encoder_use_chat_template]
                [--encoder_backbone ENCODER_BACKBONE]
                [--hf_encoder_id HF_ENCODER_ID]
                [--max_enc_tokens MAX_ENC_TOKENS] [--freeze_encoder]
                [--use_chat_template] [--max_answer_tokens MAX_ANSWER_TOKENS]
                [--lr LR] [--scale_l2 SCALE_L2]
                [--adapter_rms_l2 ADAPTER_RMS_L2]
                [--max_grad_norm MAX_GRAD_NORM]
                [--grad_diag_interval GRAD_DIAG_INTERVAL]
                [--grad_diag_components GRAD_DIAG_COMPONENTS]
                [--diagnostic_log DIAGNOSTIC_LOG] [--adapter_freeze_scale]
                [--first_token_ce_weight FIRST_TOKEN_CE_WEIGHT]
                [--first_token_ce_schedule {none,cosine,warmup}]
                [--first_token_entropy_weight FIRST_TOKEN_ENTROPY_WEIGHT]
                [--first_token_ce_peak FIRST_TOKEN_CE_PEAK]
                [--first_token_ce_warmup_frac FIRST_TOKEN_CE_WARMUP_FRAC]
                [--first_token_autoscale {yes,no}]
                [--train_append_bos_after_prefix {auto,yes,no}]
                [--adapter_hidden_mult ADAPTER_HIDDEN_MULT]
                [--adapter_dropout ADAPTER_DROPOUT] [--adapter_colorize]
                [--no_adapter_metadata]
                [--manifold_stat_weight MANIFOLD_STAT_WEIGHT]
                [--state_kd_weight STATE_KD_WEIGHT]
                [--state_kd_layers STATE_KD_LAYERS] [--use_gist_head]
                [--gist_target_len GIST_TARGET_LEN]
                [--gist_hidden GIST_HIDDEN] [--gist_layers GIST_LAYERS]
                [--gist_dropout GIST_DROPOUT] [--gist_weight GIST_WEIGHT]
                [--gist_mask_prob GIST_MASK_PROB] [--use_coprocessor]
                [--coprocessor_len COPROCESSOR_LEN]
                [--coprocessor_width COPROCESSOR_WIDTH]
                [--coprocessor_dropout COPROCESSOR_DROPOUT]
                [--coprocessor_kv_scale COPROCESSOR_KV_SCALE]
                [--coprocessor_pool COPROCESSOR_POOL]
                [--coprocessor_heads COPROCESSOR_HEADS] [--use_lora]
                [--lora_r LORA_R] [--lora_alpha LORA_ALPHA]
                [--lora_dropout LORA_DROPOUT] [--lora_firstN LORA_FIRSTN]
                [--lora_target_modules LORA_TARGET_MODULES] [--use_prefix]
                [--prefix_tokens PREFIX_TOKENS] [--prefix_projection]
                [--peft_prefix_all_layers PEFT_PREFIX_ALL_LAYERS]
                [--use_latent_adapters]
                [--latent_adapter_layers LATENT_ADAPTER_LAYERS]
                [--latent_adapter_heads LATENT_ADAPTER_HEADS]
                [--latent_adapter_dropout LATENT_ADAPTER_DROPOUT]
                [--use_deep_prefix] [--deep_prefix_len DEEP_PREFIX_LEN]
                [--deep_prefix_dropout DEEP_PREFIX_DROPOUT] [--K K]
                [--adaptive_k_start ADAPTIVE_K_START]
                [--adaptive_k_end ADAPTIVE_K_END]
                [--latent_keep_start LATENT_KEEP_START]
                [--latent_keep_end LATENT_KEEP_END]
                [--latent_keep_power LATENT_KEEP_POWER]
                [--warmup_text_latent_steps WARMUP_TEXT_LATENT_STEPS]
                [--warmup_text_latent_epochs WARMUP_TEXT_LATENT_EPOCHS]
                [--warmup_align_tokens WARMUP_ALIGN_TOKENS]
                [--warmup_align_weight WARMUP_ALIGN_WEIGHT]
                [--warmup_text_teacher_weight WARMUP_TEXT_TEACHER_WEIGHT]
                [--warmup_text_latent_weight WARMUP_TEXT_LATENT_WEIGHT]
                [--warmup_text_latent_weight_end WARMUP_TEXT_LATENT_WEIGHT_END]
                [--warmup_tail_prob WARMUP_TAIL_PROB]
                [--latent_align_weight LATENT_ALIGN_WEIGHT]
                [--latent_prefix_align_weight LATENT_PREFIX_ALIGN_WEIGHT]
                [--latent_align_metric {mse,cosine,both}]
                [--k_ce_weight K_CE_WEIGHT]
                [--kd_first_k_weight KD_FIRST_K_WEIGHT] [--kd_tau KD_TAU]
                [--teacher_llama_id TEACHER_LLAMA_ID]
                [--teacher_qwen_id TEACHER_QWEN_ID] [--kd_skip_text]
                [--latent_refiner_layers LATENT_REFINER_LAYERS]
                [--latent_refiner_heads LATENT_REFINER_HEADS]
                [--use_latent_refiner] [--load_4bit] [--sequential_models]
                [--llama_devices LLAMA_DEVICES] [--qwen_devices QWEN_DEVICES]
                [--gpu_mem_gib GPU_MEM_GIB] [--grad_ckpt] [--fp16_mps]
                [--warm_anchor_text WARM_ANCHOR_TEXT]
                [--warm_anchor_mode {auto,text,chat,none}]
                [--max_anchor_tokens MAX_ANCHOR_TOKENS] [--debug]
                [--save_dir SAVE_DIR] [--save_every SAVE_EVERY]
                [--resume_from RESUME_FROM] [--auto_resume]
                [--no_load_optimizer] [--no_load_lr_scheduler] [--reset_epoch]
                [--save_training_stats] [--baseline_verification]
train.py: error: unrecognized arguments: --lr_scheduler cosine --warmup_steps 100 --weight_decay 0.01 --k_token_ce_weight 1.5 --k_token_ce_k 8 --kd_weight 0.5 --entropy_weight 0.5 1 --max_checkpoints_to_keep 5 --save_best --eval_every 200 --eval_samples 200 --early_stopping_patience 5 --early_stopping_metric first_token_top1 --diagnostic_interval 20 --gradient_checkpointing --mixed_precision bf16
