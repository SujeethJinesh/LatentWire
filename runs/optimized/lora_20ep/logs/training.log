/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3938.31it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.21s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.04it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}

ðŸ”§ Applying LoRA (r=64, alpha=128)...
   Llama BEFORE LoRA: 0 trainable / 8,030,261,248 total
trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465
   Llama AFTER LoRA:  167,772,160 trainable / 8,198,033,408 total
   âœ“ Added 167,772,160 LoRA parameters to Llama
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 2, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'model.rotary_emb': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:3.3GB(4%), GPU1:4.6GB(5%), GPU2:4.6GB(5%), GPU3:4.3GB(5%) | Total: 16.8GB allocated, 16.8GB reserved, 323.3GB free, Peak: 16.8GB
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[INFO] llama anchor tokens: 3
[Optimizer] Gathering latent adapter parameters (direct wrappers fallback)...
[Optimizer]   Meta-Llama-3.1-8B-Instruct: skipped (use_latent_adapters=False)
[Optimizer] Latent adapter summary: 0 params in 0 tensors
[Optimizer] No latent adapters enabled (expected)
[Optimization] Using fused AdamW optimizer
[Optimizer] Created 3 parameter groups:
  [1] encoder(90 tensors)
  [2] llama_adapter(20 tensors)
  [3] llama_extra(448 tensors)
[INFO] LR scheduler: CosineAnnealingLR (T_max=6240, eta_min=4.00e-06)
âš ï¸  No valid checkpoint found to resume; starting fresh.
Epoch 1/20
[Epoch 1 Start] [GPU Memory] GPU0:3.4GB(9%), GPU1:4.6GB(5%), GPU2:4.6GB(5%), GPU3:4.3GB(5%) | Total: 16.8GB allocated, 21.4GB reserved, 318.8GB free, Peak: 19.9GB
    [Memory after encoder] 23.9GB allocated
[INFO] KD teacher: adapters disabled successfully (clean text baseline)
    [Memory after backward] 19.1GB allocated, peak 94.3GB
  [Step 1] [GPU Memory] GPU0:4.9GB(69%), GPU1:4.9GB(12%), GPU2:4.9GB(12%), GPU3:4.5GB(33%) | Total: 19.1GB allocated, 108.1GB reserved, 232.1GB free, Peak: 94.3GB
    [Memory after encoder] 26.2GB allocated
    [Memory after backward] 20.4GB allocated, peak 94.7GB
  [Step 2] [GPU Memory] GPU0:4.9GB(69%), GPU1:4.9GB(13%), GPU2:4.9GB(13%), GPU3:4.5GB(33%) | Total: 19.1GB allocated, 108.4GB reserved, 231.7GB free, Peak: 94.7GB
    [Memory after encoder] 26.2GB allocated
    [Memory after backward] 20.4GB allocated, peak 95.5GB
    [Memory after optimizer] 21.1GB allocated
  [Step 3] [GPU Memory] GPU0:5.0GB(69%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(35%) | Total: 19.8GB allocated, 110.2GB reserved, 229.9GB free, Peak: 95.5GB
  [Step 4] [GPU Memory] GPU0:5.2GB(69%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 111.8GB reserved, 228.4GB free, Peak: 98.3GB
  [Step 5] [GPU Memory] GPU0:5.2GB(69%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 112.1GB reserved, 228.1GB free, Peak: 98.7GB
  [Batch Size Suggestion after 5 steps] Low peak memory (29.0%), can cautiously increase batch size
    Current: 64, Suggested: 76
    To apply: set BATCH_SIZE_STAGEA/B=76 in run script
  [Step 6] [GPU Memory] GPU0:5.0GB(69%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 112.1GB reserved, 228.1GB free, Peak: 98.7GB
  [Step 7] [GPU Memory] GPU0:5.2GB(69%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 112.1GB reserved, 228.1GB free, Peak: 98.7GB
  [Step 8] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 9] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  step  10/313 | grad_norm=162.66 | sec/step~9.42 | lr=2.00e-04 | keep=1.00 | K=8 | llama(L): tf=11.7969 first=9.5866 kCE=10.1062 KD=6.0363 acc=0.000 ent=8.282 align=0.0000 | scale_pen(llama)=8.8818e-12 | feature_grads[encoder=8.694e+01, adapter_llama=1.375e+02, extra_llama=8.838e+01] | K=8 tau=2.00
  [Step 10] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 11] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 12] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 13] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 14] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 15] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 16] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 17] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 18] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 19] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  step  20/313 | grad_norm=22.54 | sec/step~9.24 | lr=2.00e-04 | keep=1.00 | K=8 | llama(L): tf=9.4977 first=8.9105 kCE=5.8158 KD=6.1799 acc=0.000 ent=8.152 align=0.0000 | scale_pen(llama)=4.9468e-11 | feature_grads[encoder=1.058e+01, adapter_llama=1.991e+01, extra_llama=2.661e+01] | K=8 tau=2.00
  [Step 20] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 21] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 22] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 23] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 24] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 25] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 26] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 27] [GPU Memory] GPU0:5.0GB(89%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 28] [GPU Memory] GPU0:5.2GB(89%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 129.3GB reserved, 210.8GB free, Peak: 102.8GB
  [Step 29] [GPU Memory] GPU0:5.2GB(79%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  step  30/313 | grad_norm=143.12 | sec/step~9.97 | lr=2.00e-04 | keep=1.00 | K=8 | llama(L): tf=9.1073 first=8.5905 kCE=6.3896 KD=4.2189 acc=0.047 [âœ“'the'] ent=8.563 align=0.0000 | scale_pen(llama)=2.9809e-09 | feature_grads[encoder=5.820e+01, adapter_llama=1.308e+02, extra_llama=2.616e+02] | K=8 tau=2.00
  [Step 30] [GPU Memory] GPU0:5.0GB(79%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=1.1% (raw_batch=4.7%) at step 30 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Sac'
        âœ— pred='the' | gold='December'
        âœ— pred='the' | gold='M'
        âœ— pred='the' | gold='middle'
        âœ— pred='the' | gold='Ba'
      Prediction diversity: 4/64 unique tokens
      Top-3 predictions: 'the'(55) 'a'(4) ' ('(3) 
  [Step 31] [GPU Memory] GPU0:5.2GB(79%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=1.3% (raw_batch=3.1%) at step 31 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='C'
        âœ— pred='the' | gold='S'
        âœ— pred='the' | gold='less'
        âœ— pred='the' | gold='30'
        âœ— pred='the' | gold='CT'
      Prediction diversity: 2/64 unique tokens
      Top-3 predictions: 'the'(59) 'Question'(5) 
  [Step 32] [GPU Memory] GPU0:5.2GB(79%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=1.5% (raw_batch=3.1%) at step 32 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='The'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='193'
        âœ— pred='the' | gold='EA'
        âœ— pred='the' | gold='French'
      Prediction diversity: 2/64 unique tokens
      Top-3 predictions: 'the'(60) 'Question'(4) 
  [Step 33] [GPU Memory] GPU0:5.0GB(79%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=2.1% (raw_batch=7.8%) at step 33 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='21'
        âœ— pred='the' | gold='met'
        âœ— pred='the' | gold='55'
        âœ— pred='the' | gold='printing'
        âœ— pred='the' | gold='field'
      Prediction diversity: 2/64 unique tokens
      Top-3 predictions: 'the'(60) 'Question'(4) 
  [Step 34] [GPU Memory] GPU0:5.2GB(79%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=2.5% (raw_batch=6.2%) at step 34 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='100'
        âœ— pred='the' | gold='open'
        âœ— pred='the' | gold='a'
        âœ— pred='the' | gold='f'
        âœ— pred='Question' | gold='July'
      Prediction diversity: 3/64 unique tokens
      Top-3 predictions: 'the'(56) 'Question'(7) 'The'(1) 
  [Step 35] [GPU Memory] GPU0:5.2GB(79%), GPU1:5.3GB(13%), GPU2:5.3GB(13%), GPU3:4.8GB(36%) | Total: 20.5GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  [Step 36] [GPU Memory] GPU0:5.0GB(79%), GPU1:5.1GB(13%), GPU2:5.1GB(13%), GPU3:4.6GB(36%) | Total: 19.8GB allocated, 120.5GB reserved, 219.6GB free, Peak: 104.3GB
  ðŸŒŸ NEW PEAK: first_acc_ema=2.7% (raw_batch=4.7%) at step 36 â†’ saved to runs/optimized/lora_20ep_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Sy'
        âœ— pred='the' | gold='H'
        âœ— pred='the' | gold='a'
        âœ— pred='the' | gold='United'
        âœ— pred='the' | gold='enz'
      Prediction diversity: 2/64 unique tokens
      Top-3 predictions: 'the'(60) 'Question'(4) 
[WARN] KD teacher forward failed; retrying per-example: CUDA out of memory. Tried to allocate 23.27 GiB. GPU 0 has a total capacity of 79.19 GiB of which 16.91 GiB is free. Including non-PyTorch memory, this process has 62.27 GiB memory in use. Of the allocated memory 37.44 GiB is allocated by PyTorch, and 24.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 162, in kd_first_k_prefix_vs_text
    teacher_logits_full = torch.cat(logits_chunks, dim=0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.27 GiB. GPU 0 has a total capacity of 79.19 GiB of which 16.91 GiB is free. Including non-PyTorch memory, this process has 62.27 GiB memory in use. Of the allocated memory 37.44 GiB is allocated by PyTorch, and 24.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 3234, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/train.py", line 2134, in main
    loss_kd_raw = kd_first_k_prefix_vs_text(
  File "/projects/m000066/sujinesh/LatentWire/latentwire/losses.py", line 184, in kd_first_k_prefix_vs_text
    teacher_logits_full = torch.cat(logits_chunks, dim=0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 23.27 GiB. GPU 0 has a total capacity of 79.19 GiB of which 20.77 GiB is free. Including non-PyTorch memory, this process has 58.41 GiB memory in use. Of the allocated memory 38.17 GiB is allocated by PyTorch, and 19.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
