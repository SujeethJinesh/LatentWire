Auto-detected device: mps
Using fp16 precision on MPS for memory efficiency
Loading Z from runs/squad_qwen_only_m16_20250910_032844/squad_eval/Z.pt

[Sequential Evaluation Mode - one model at a time]
Loading cached Llama results: runs/squad_qwen_only_m16_20250910_032844/squad_eval/llama_results.json

Evaluating Qwen...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [qwen] text: 8/200 | 2.72 ex/s | elapsed=2.9s | eta=1m10s
  [qwen] text: 16/200 | 3.11 ex/s | elapsed=5.1s | eta=59.2s
  [qwen] text: 24/200 | 3.46 ex/s | elapsed=6.9s | eta=50.9s
  [qwen] text: 32/200 | 3.68 ex/s | elapsed=8.7s | eta=45.7s
  [qwen] text: 40/200 | 3.79 ex/s | elapsed=10.5s | eta=42.2s
  [qwen] text: 48/200 | 3.90 ex/s | elapsed=12.3s | eta=38.9s
  [qwen] text: 56/200 | 4.02 ex/s | elapsed=13.9s | eta=35.8s
  [qwen] text: 64/200 | 4.10 ex/s | elapsed=15.6s | eta=33.2s
  [qwen] text: 72/200 | 4.11 ex/s | elapsed=17.5s | eta=31.2s
  [qwen] text: 80/200 | 4.14 ex/s | elapsed=19.3s | eta=29.0s
  [qwen] text: 88/200 | 4.24 ex/s | elapsed=20.7s | eta=26.4s
  [qwen] text: 96/200 | 4.20 ex/s | elapsed=22.9s | eta=24.8s
  [qwen] text: 104/200 | 4.22 ex/s | elapsed=24.6s | eta=22.7s
  [qwen] text: 112/200 | 4.23 ex/s | elapsed=26.5s | eta=20.8s
  [qwen] text: 120/200 | 4.25 ex/s | elapsed=28.3s | eta=18.8s
  [qwen] text: 128/200 | 4.28 ex/s | elapsed=29.9s | eta=16.8s
  [qwen] text: 136/200 | 4.30 ex/s | elapsed=31.6s | eta=14.9s
  [qwen] text: 144/200 | 4.31 ex/s | elapsed=33.4s | eta=13.0s
  [qwen] text: 152/200 | 4.33 ex/s | elapsed=35.1s | eta=11.1s
  [qwen] text: 160/200 | 4.37 ex/s | elapsed=36.6s | eta=9.1s
  [qwen] text: 168/200 | 4.39 ex/s | elapsed=38.3s | eta=7.3s
  [qwen] text: 176/200 | 4.39 ex/s | elapsed=40.1s | eta=5.5s
  [qwen] text: 184/200 | 4.40 ex/s | elapsed=41.8s | eta=3.6s
  [qwen] text: 192/200 | 4.39 ex/s | elapsed=43.7s | eta=1.8s
  [qwen] text: 200/200 | 4.36 ex/s | elapsed=45.9s | eta=0.0s
[debug:qwen] adapter.scale=1.0010 | Z.std=0.9983 Z.mean||=15.9733 | prefix.std=0.6527 prefix.mean||=19.5059 | embed.RMS=0.0152
  [qwen] latent: 8/200 | 3.67 ex/s | elapsed=2.2s | eta=52.3s
  [qwen] latent: 16/200 | 4.64 ex/s | elapsed=3.4s | eta=39.7s
  [qwen] latent: 24/200 | 5.07 ex/s | elapsed=4.7s | eta=34.7s
  [qwen] latent: 32/200 | 5.32 ex/s | elapsed=6.0s | eta=31.6s
  [qwen] latent: 40/200 | 5.47 ex/s | elapsed=7.3s | eta=29.2s
  [qwen] latent: 48/200 | 5.59 ex/s | elapsed=8.6s | eta=27.2s
  [qwen] latent: 56/200 | 5.67 ex/s | elapsed=9.9s | eta=25.4s
  [qwen] latent: 64/200 | 5.73 ex/s | elapsed=11.2s | eta=23.7s
  [qwen] latent: 72/200 | 5.78 ex/s | elapsed=12.5s | eta=22.1s
  [qwen] latent: 80/200 | 5.83 ex/s | elapsed=13.7s | eta=20.6s
  [qwen] latent: 88/200 | 5.87 ex/s | elapsed=15.0s | eta=19.1s
  [qwen] latent: 96/200 | 5.90 ex/s | elapsed=16.3s | eta=17.6s
  [qwen] latent: 104/200 | 5.93 ex/s | elapsed=17.5s | eta=16.2s
  [qwen] latent: 112/200 | 5.96 ex/s | elapsed=18.8s | eta=14.8s
  [qwen] latent: 120/200 | 5.98 ex/s | elapsed=20.1s | eta=13.4s
  [qwen] latent: 128/200 | 5.99 ex/s | elapsed=21.4s | eta=12.0s
  [qwen] latent: 136/200 | 6.01 ex/s | elapsed=22.6s | eta=10.7s
  [qwen] latent: 144/200 | 6.02 ex/s | elapsed=23.9s | eta=9.3s
  [qwen] latent: 152/200 | 6.03 ex/s | elapsed=25.2s | eta=8.0s
  [qwen] latent: 160/200 | 6.04 ex/s | elapsed=26.5s | eta=6.6s
  [qwen] latent: 168/200 | 6.06 ex/s | elapsed=27.7s | eta=5.3s
  [qwen] latent: 176/200 | 6.07 ex/s | elapsed=29.0s | eta=4.0s
  [qwen] latent: 184/200 | 6.08 ex/s | elapsed=30.3s | eta=2.6s
  [qwen] latent: 192/200 | 6.09 ex/s | elapsed=31.6s | eta=1.3s
  [qwen] latent: 200/200 | 6.09 ex/s | elapsed=32.8s | eta=0.0s
  [qwen] text: 8/200 | 5.20 ex/s | elapsed=1.5s | eta=36.9s
  [qwen] text: 16/200 | 6.24 ex/s | elapsed=2.6s | eta=29.5s
  [qwen] text: 24/200 | 6.76 ex/s | elapsed=3.6s | eta=26.0s
  [qwen] text: 32/200 | 7.04 ex/s | elapsed=4.5s | eta=23.9s
  [qwen] text: 40/200 | 7.24 ex/s | elapsed=5.5s | eta=22.1s
  [qwen] text: 48/200 | 7.39 ex/s | elapsed=6.5s | eta=20.6s
  [qwen] text: 56/200 | 7.50 ex/s | elapsed=7.5s | eta=19.2s
  [qwen] text: 64/200 | 7.58 ex/s | elapsed=8.4s | eta=17.9s
  [qwen] text: 72/200 | 7.63 ex/s | elapsed=9.4s | eta=16.8s
  [qwen] text: 80/200 | 7.68 ex/s | elapsed=10.4s | eta=15.6s
  [qwen] text: 88/200 | 7.73 ex/s | elapsed=11.4s | eta=14.5s
  [qwen] text: 96/200 | 7.77 ex/s | elapsed=12.4s | eta=13.4s
  [qwen] text: 104/200 | 7.80 ex/s | elapsed=13.3s | eta=12.3s
  [qwen] text: 112/200 | 7.82 ex/s | elapsed=14.3s | eta=11.3s
  [qwen] text: 120/200 | 7.84 ex/s | elapsed=15.3s | eta=10.2s
  [qwen] text: 128/200 | 7.86 ex/s | elapsed=16.3s | eta=9.2s
  [qwen] text: 136/200 | 7.88 ex/s | elapsed=17.3s | eta=8.1s
  [qwen] text: 144/200 | 7.90 ex/s | elapsed=18.2s | eta=7.1s
  [qwen] text: 152/200 | 7.90 ex/s | elapsed=19.2s | eta=6.1s
  [qwen] text: 160/200 | 7.91 ex/s | elapsed=20.2s | eta=5.1s
  [qwen] text: 168/200 | 7.92 ex/s | elapsed=21.2s | eta=4.0s
  [qwen] text: 176/200 | 7.94 ex/s | elapsed=22.2s | eta=3.0s
  [qwen] text: 184/200 | 7.95 ex/s | elapsed=23.2s | eta=2.0s
  [qwen] text: 192/200 | 7.95 ex/s | elapsed=24.1s | eta=1.0s
  [qwen] text: 200/200 | 7.96 ex/s | elapsed=25.1s | eta=0.0s
Saved Qwen results to runs/squad_qwen_only_m16_20250910_032844/squad_eval/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: mps  |  Dtype: torch.float16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.005  F1: 0.057  |  NLL/token (gold): 15.681324996504673
Qwen   EM: 0.230   F1: 0.396   |  NLL/token (gold): 13.044476828877889
Wall clock: 108.91s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.028  |  NLL/token (gold): 7.579885916820793
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 9.65742606176901
Wall clock: 65.48s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.033
Qwen   EM: 0.000   F1: 0.052
Wall clock: 51.27s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.040
Oracle upper bound:  EM 0.000  F1 0.028

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "mps",
  "dtype": "torch.float16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 108.91459918022156,
    "llama": {
      "em": 0.005,
      "f1": 0.056785177241059595,
      "nll_token": 15.681324996504673
    },
    "qwen": {
      "em": 0.23,
      "f1": 0.39575623885918004,
      "nll_token": 13.044476828877889
    }
  },
  "latent": {
    "wall_clock_sec": 65.47808766365051,
    "llama": {
      "em": 0.0,
      "f1": 0.028276066039223943,
      "nll_token": 7.579885916820793
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 9.65742606176901
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0,
      "f1": 0.03346892873208663
    },
    "wall_clock_sec": 51.26872706413269,
    "qwen": {
      "em": 0.0,
      "f1": 0.051748848373848376
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.04,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.008729100227356,
      "Z_std": 0.9983295202255249,
      "Z_mean_norm": 15.973258972167969,
      "prefix_std": 0.6587362289428711,
      "prefix_mean_norm": 29.698486328125,
      "embed_rms": 0.014909257180988789
    },
    "qwen": {
      "adapter_scale": 1.0009733438491821,
      "Z_std": 0.9983295202255249,
      "Z_mean_norm": 15.973258972167969,
      "prefix_std": 0.6527382135391235,
      "prefix_mean_norm": 19.505859375,
      "embed_rms": 0.015230061486363411
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.028276066039223943
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_qwen_only_m16_20250910_032844/squad_eval/predictions.jsonl
