Auto-detected device: mps
Using fp16 precision on MPS for memory efficiency
Building encoder and computing Z...
Saved Z to runs/squad_qwen_only_m16_20250910_032844/squad_eval_gain_1.25/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [llama] text: 8/200 | 2.54 ex/s | elapsed=3.2s | eta=1m15s
  [llama] text: 16/200 | 2.26 ex/s | elapsed=7.1s | eta=1m21s
  [llama] text: 24/200 | 2.51 ex/s | elapsed=9.6s | eta=1m10s
  [llama] text: 32/200 | 2.68 ex/s | elapsed=11.9s | eta=1m02s
  [llama] text: 40/200 | 2.72 ex/s | elapsed=14.7s | eta=58.8s
  [llama] text: 48/200 | 2.85 ex/s | elapsed=16.8s | eta=53.3s
  [llama] text: 56/200 | 2.93 ex/s | elapsed=19.1s | eta=49.1s
  [llama] text: 64/200 | 3.01 ex/s | elapsed=21.2s | eta=45.1s
  [llama] text: 72/200 | 3.02 ex/s | elapsed=23.9s | eta=42.4s
  [llama] text: 80/200 | 3.06 ex/s | elapsed=26.2s | eta=39.3s
  [llama] text: 88/200 | 3.07 ex/s | elapsed=28.7s | eta=36.5s
  [llama] text: 96/200 | 3.00 ex/s | elapsed=32.0s | eta=34.7s
  [llama] text: 104/200 | 3.04 ex/s | elapsed=34.2s | eta=31.5s
  [llama] text: 112/200 | 3.06 ex/s | elapsed=36.6s | eta=28.8s
  [llama] text: 120/200 | 3.05 ex/s | elapsed=39.3s | eta=26.2s
  [llama] text: 128/200 | 3.12 ex/s | elapsed=41.0s | eta=23.1s
  [llama] text: 136/200 | 3.16 ex/s | elapsed=43.1s | eta=20.3s
  [llama] text: 144/200 | 3.20 ex/s | elapsed=45.0s | eta=17.5s
  [llama] text: 152/200 | 3.20 ex/s | elapsed=47.5s | eta=15.0s
  [llama] text: 160/200 | 3.22 ex/s | elapsed=49.7s | eta=12.4s
  [llama] text: 168/200 | 3.24 ex/s | elapsed=51.8s | eta=9.9s
  [llama] text: 176/200 | 3.24 ex/s | elapsed=54.3s | eta=7.4s
  [llama] text: 184/200 | 3.24 ex/s | elapsed=56.8s | eta=4.9s
  [llama] text: 192/200 | 3.26 ex/s | elapsed=59.0s | eta=2.5s
  [llama] text: 200/200 | 3.21 ex/s | elapsed=1m02s | eta=0.0s
[debug:llama] adapter.scale=1.0087 | Z.std=0.9983 Z.mean||=15.9733 | prefix.std=0.8234 prefix.mean||=37.1231 | embed.RMS=0.0149
  [llama] latent: 8/200 | 3.73 ex/s | elapsed=2.1s | eta=51.5s
  [llama] latent: 16/200 | 4.94 ex/s | elapsed=3.2s | eta=37.3s
  [llama] latent: 24/200 | 5.54 ex/s | elapsed=4.3s | eta=31.8s
  [llama] latent: 32/200 | 5.89 ex/s | elapsed=5.4s | eta=28.5s
  [llama] latent: 40/200 | 6.12 ex/s | elapsed=6.5s | eta=26.1s
  [llama] latent: 48/200 | 6.28 ex/s | elapsed=7.6s | eta=24.2s
  [llama] latent: 56/200 | 6.37 ex/s | elapsed=8.8s | eta=22.6s
  [llama] latent: 64/200 | 6.44 ex/s | elapsed=9.9s | eta=21.1s
  [llama] latent: 72/200 | 6.51 ex/s | elapsed=11.1s | eta=19.7s
  [llama] latent: 80/200 | 6.57 ex/s | elapsed=12.2s | eta=18.3s
  [llama] latent: 88/200 | 6.62 ex/s | elapsed=13.3s | eta=16.9s
  [llama] latent: 96/200 | 6.66 ex/s | elapsed=14.4s | eta=15.6s
  [llama] latent: 104/200 | 6.70 ex/s | elapsed=15.5s | eta=14.3s
  [llama] latent: 112/200 | 6.74 ex/s | elapsed=16.6s | eta=13.1s
  [llama] latent: 120/200 | 6.77 ex/s | elapsed=17.7s | eta=11.8s
  [llama] latent: 128/200 | 6.80 ex/s | elapsed=18.8s | eta=10.6s
  [llama] latent: 136/200 | 6.83 ex/s | elapsed=19.9s | eta=9.4s
  [llama] latent: 144/200 | 6.85 ex/s | elapsed=21.0s | eta=8.2s
  [llama] latent: 152/200 | 6.87 ex/s | elapsed=22.1s | eta=7.0s
  [llama] latent: 160/200 | 6.88 ex/s | elapsed=23.2s | eta=5.8s
  [llama] latent: 168/200 | 6.90 ex/s | elapsed=24.3s | eta=4.6s
  [llama] latent: 176/200 | 6.92 ex/s | elapsed=25.5s | eta=3.5s
  [llama] latent: 184/200 | 6.92 ex/s | elapsed=26.6s | eta=2.3s
  [llama] latent: 192/200 | 6.93 ex/s | elapsed=27.7s | eta=1.2s
  [llama] latent: 200/200 | 6.94 ex/s | elapsed=28.8s | eta=0.0s
  [llama] text: 8/200 | 4.78 ex/s | elapsed=1.7s | eta=40.2s
  [llama] text: 16/200 | 5.87 ex/s | elapsed=2.7s | eta=31.4s
  [llama] text: 24/200 | 6.42 ex/s | elapsed=3.7s | eta=27.4s
  [llama] text: 32/200 | 6.74 ex/s | elapsed=4.7s | eta=24.9s
  [llama] text: 40/200 | 6.94 ex/s | elapsed=5.8s | eta=23.1s
  [llama] text: 48/200 | 7.09 ex/s | elapsed=6.8s | eta=21.4s
  [llama] text: 56/200 | 7.20 ex/s | elapsed=7.8s | eta=20.0s
  [llama] text: 64/200 | 7.28 ex/s | elapsed=8.8s | eta=18.7s
  [llama] text: 72/200 | 7.34 ex/s | elapsed=9.8s | eta=17.4s
  [llama] text: 80/200 | 7.38 ex/s | elapsed=10.8s | eta=16.3s
  [llama] text: 88/200 | 7.42 ex/s | elapsed=11.9s | eta=15.1s
  [llama] text: 96/200 | 7.46 ex/s | elapsed=12.9s | eta=13.9s
  [llama] text: 104/200 | 7.50 ex/s | elapsed=13.9s | eta=12.8s
  [llama] text: 112/200 | 7.53 ex/s | elapsed=14.9s | eta=11.7s
  [llama] text: 120/200 | 7.56 ex/s | elapsed=15.9s | eta=10.6s
  [llama] text: 128/200 | 7.58 ex/s | elapsed=16.9s | eta=9.5s
  [llama] text: 136/200 | 7.61 ex/s | elapsed=17.9s | eta=8.4s
  [llama] text: 144/200 | 7.63 ex/s | elapsed=18.9s | eta=7.3s
  [llama] text: 152/200 | 7.65 ex/s | elapsed=19.9s | eta=6.3s
  [llama] text: 160/200 | 7.67 ex/s | elapsed=20.9s | eta=5.2s
  [llama] text: 168/200 | 7.69 ex/s | elapsed=21.8s | eta=4.2s
  [llama] text: 176/200 | 7.71 ex/s | elapsed=22.8s | eta=3.1s
  [llama] text: 184/200 | 7.72 ex/s | elapsed=23.8s | eta=2.1s
  [llama] text: 192/200 | 7.74 ex/s | elapsed=24.8s | eta=1.0s
  [llama] text: 200/200 | 7.75 ex/s | elapsed=25.8s | eta=0.0s
Saved Llama results to runs/squad_qwen_only_m16_20250910_032844/squad_eval_gain_1.25/llama_results.json

Evaluating Qwen...
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
  [qwen] text: 8/200 | 1.99 ex/s | elapsed=4.0s | eta=1m36s
  [qwen] text: 16/200 | 2.09 ex/s | elapsed=7.6s | eta=1m27s
  [qwen] text: 24/200 | 2.23 ex/s | elapsed=10.8s | eta=1m19s
  [qwen] text: 32/200 | 2.33 ex/s | elapsed=13.7s | eta=1m12s
  [qwen] text: 40/200 | 2.36 ex/s | elapsed=17.0s | eta=1m07s
  [qwen] text: 48/200 | 2.41 ex/s | elapsed=19.9s | eta=1m02s
  [qwen] text: 56/200 | 2.52 ex/s | elapsed=22.2s | eta=57.2s
  [qwen] text: 64/200 | 2.55 ex/s | elapsed=25.1s | eta=53.3s
  [qwen] text: 72/200 | 2.54 ex/s | elapsed=28.3s | eta=50.3s
  [qwen] text: 80/200 | 2.55 ex/s | elapsed=31.4s | eta=47.1s
  [qwen] text: 88/200 | 2.68 ex/s | elapsed=32.8s | eta=41.8s
  [qwen] text: 96/200 | 2.64 ex/s | elapsed=36.3s | eta=39.3s
  [qwen] text: 104/200 | 2.64 ex/s | elapsed=39.4s | eta=36.4s
  [qwen] text: 112/200 | 2.63 ex/s | elapsed=42.5s | eta=33.4s
  [qwen] text: 120/200 | 2.66 ex/s | elapsed=45.1s | eta=30.1s
  [qwen] text: 128/200 | 2.69 ex/s | elapsed=47.6s | eta=26.8s
  [qwen] text: 136/200 | 2.69 ex/s | elapsed=50.5s | eta=23.8s
  [qwen] text: 144/200 | 2.71 ex/s | elapsed=53.2s | eta=20.7s
  [qwen] text: 152/200 | 2.73 ex/s | elapsed=55.7s | eta=17.6s
  [qwen] text: 160/200 | 2.78 ex/s | elapsed=57.6s | eta=14.4s
  [qwen] text: 168/200 | 2.78 ex/s | elapsed=1m00s | eta=11.5s
  [qwen] text: 176/200 | 2.79 ex/s | elapsed=1m03s | eta=8.6s
  [qwen] text: 184/200 | 2.80 ex/s | elapsed=1m05s | eta=5.7s
  [qwen] text: 192/200 | 2.78 ex/s | elapsed=1m09s | eta=2.9s
  [qwen] text: 200/200 | 2.75 ex/s | elapsed=1m12s | eta=0.0s
[debug:qwen] adapter.scale=1.0010 | Z.std=0.9983 Z.mean||=15.9733 | prefix.std=0.8159 prefix.mean||=24.3823 | embed.RMS=0.0152
  [qwen] latent: 8/200 | 2.24 ex/s | elapsed=3.6s | eta=1m25s
  [qwen] latent: 16/200 | 3.28 ex/s | elapsed=4.9s | eta=56.1s
  [qwen] latent: 24/200 | 3.84 ex/s | elapsed=6.3s | eta=45.8s
  [qwen] latent: 32/200 | 4.25 ex/s | elapsed=7.5s | eta=39.6s
  [qwen] latent: 40/200 | 4.53 ex/s | elapsed=8.8s | eta=35.3s
  [qwen] latent: 48/200 | 4.75 ex/s | elapsed=10.1s | eta=32.0s
  [qwen] latent: 56/200 | 4.93 ex/s | elapsed=11.4s | eta=29.2s
  [qwen] latent: 64/200 | 5.07 ex/s | elapsed=12.6s | eta=26.8s
  [qwen] latent: 72/200 | 5.18 ex/s | elapsed=13.9s | eta=24.7s
  [qwen] latent: 80/200 | 5.27 ex/s | elapsed=15.2s | eta=22.8s
  [qwen] latent: 88/200 | 5.34 ex/s | elapsed=16.5s | eta=21.0s
  [qwen] latent: 96/200 | 5.41 ex/s | elapsed=17.8s | eta=19.2s
  [qwen] latent: 104/200 | 5.47 ex/s | elapsed=19.0s | eta=17.6s
  [qwen] latent: 112/200 | 5.52 ex/s | elapsed=20.3s | eta=15.9s
  [qwen] latent: 120/200 | 5.56 ex/s | elapsed=21.6s | eta=14.4s
  [qwen] latent: 128/200 | 5.60 ex/s | elapsed=22.8s | eta=12.9s
  [qwen] latent: 136/200 | 5.64 ex/s | elapsed=24.1s | eta=11.3s
  [qwen] latent: 144/200 | 5.66 ex/s | elapsed=25.4s | eta=9.9s
  [qwen] latent: 152/200 | 5.69 ex/s | elapsed=26.7s | eta=8.4s
  [qwen] latent: 160/200 | 5.72 ex/s | elapsed=28.0s | eta=7.0s
  [qwen] latent: 168/200 | 5.74 ex/s | elapsed=29.3s | eta=5.6s
  [qwen] latent: 176/200 | 5.77 ex/s | elapsed=30.5s | eta=4.2s
  [qwen] latent: 184/200 | 5.79 ex/s | elapsed=31.8s | eta=2.8s
  [qwen] latent: 192/200 | 5.81 ex/s | elapsed=33.1s | eta=1.4s
  [qwen] latent: 200/200 | 5.83 ex/s | elapsed=34.3s | eta=0.0s
  [qwen] text: 8/200 | 2.69 ex/s | elapsed=3.0s | eta=1m11s
  [qwen] text: 16/200 | 3.96 ex/s | elapsed=4.0s | eta=46.4s
  [qwen] text: 24/200 | 4.78 ex/s | elapsed=5.0s | eta=36.8s
  [qwen] text: 32/200 | 5.32 ex/s | elapsed=6.0s | eta=31.6s
  [qwen] text: 40/200 | 5.72 ex/s | elapsed=7.0s | eta=28.0s
  [qwen] text: 48/200 | 6.03 ex/s | elapsed=8.0s | eta=25.2s
  [qwen] text: 56/200 | 6.26 ex/s | elapsed=8.9s | eta=23.0s
  [qwen] text: 64/200 | 6.45 ex/s | elapsed=9.9s | eta=21.1s
  [qwen] text: 72/200 | 6.60 ex/s | elapsed=10.9s | eta=19.4s
  [qwen] text: 80/200 | 6.73 ex/s | elapsed=11.9s | eta=17.8s
  [qwen] text: 88/200 | 6.84 ex/s | elapsed=12.9s | eta=16.4s
  [qwen] text: 96/200 | 6.93 ex/s | elapsed=13.8s | eta=15.0s
  [qwen] text: 104/200 | 7.01 ex/s | elapsed=14.8s | eta=13.7s
  [qwen] text: 112/200 | 7.07 ex/s | elapsed=15.8s | eta=12.4s
  [qwen] text: 120/200 | 7.14 ex/s | elapsed=16.8s | eta=11.2s
  [qwen] text: 128/200 | 7.19 ex/s | elapsed=17.8s | eta=10.0s
  [qwen] text: 136/200 | 7.24 ex/s | elapsed=18.8s | eta=8.8s
  [qwen] text: 144/200 | 7.29 ex/s | elapsed=19.7s | eta=7.7s
  [qwen] text: 152/200 | 7.33 ex/s | elapsed=20.7s | eta=6.5s
  [qwen] text: 160/200 | 7.36 ex/s | elapsed=21.7s | eta=5.4s
  [qwen] text: 168/200 | 7.39 ex/s | elapsed=22.7s | eta=4.3s
  [qwen] text: 176/200 | 7.43 ex/s | elapsed=23.7s | eta=3.2s
  [qwen] text: 184/200 | 7.45 ex/s | elapsed=24.7s | eta=2.1s
  [qwen] text: 192/200 | 7.48 ex/s | elapsed=25.7s | eta=1.1s
  [qwen] text: 200/200 | 7.50 ex/s | elapsed=26.7s | eta=0.0s
Saved Qwen results to runs/squad_qwen_only_m16_20250910_032844/squad_eval_gain_1.25/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: mps  |  Dtype: torch.float16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.005  F1: 0.057  |  NLL/token (gold): 15.681324996504673
Qwen   EM: 0.230   F1: 0.396   |  NLL/token (gold): 13.044476828877889
Wall clock: 135.07s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.024  |  NLL/token (gold): 7.552110149250474
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 10.940421521663666
Wall clock: 63.12s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.033
Qwen   EM: 0.000   F1: 0.052
Wall clock: 52.46s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.035
Oracle upper bound:  EM 0.000  F1 0.024

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "mps",
  "dtype": "torch.float16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 135.0717442035675,
    "llama": {
      "em": 0.005,
      "f1": 0.056785177241059595,
      "nll_token": 15.681324996504673
    },
    "qwen": {
      "em": 0.23,
      "f1": 0.39575623885918004,
      "nll_token": 13.044476828877889
    }
  },
  "latent": {
    "wall_clock_sec": 63.11927890777588,
    "llama": {
      "em": 0.0,
      "f1": 0.023984801747959646,
      "nll_token": 7.552110149250474
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 10.940421521663666
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.0,
      "f1": 0.03346892873208663
    },
    "wall_clock_sec": 52.45989012718201,
    "qwen": {
      "em": 0.0,
      "f1": 0.051748848373848376
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0,
    "agreement": 0.035,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.008729100227356,
      "Z_std": 0.9983295202255249,
      "Z_mean_norm": 15.973258972167969,
      "prefix_std": 0.8234203457832336,
      "prefix_mean_norm": 37.12310791015625,
      "embed_rms": 0.014909257180988789
    },
    "qwen": {
      "adapter_scale": 1.0009733438491821,
      "Z_std": 0.9983295202255249,
      "Z_mean_norm": 15.973258972167969,
      "prefix_std": 0.8159228563308716,
      "prefix_mean_norm": 24.38232421875,
      "embed_rms": 0.015230061486363411
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.25,
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.023984801747959646
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/squad_qwen_only_m16_20250910_032844/squad_eval_gain_1.25/predictions.jsonl
