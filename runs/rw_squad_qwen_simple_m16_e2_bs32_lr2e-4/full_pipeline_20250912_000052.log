=========================================
Starting pipeline at Fri Sep 12 00:00:52 PDT 2025
=========================================

=========================================
PHASE 1: TRAINING
=========================================
Starting training at Fri Sep 12 00:00:52 PDT 2025
Checkpoint will be saved to: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/ckpt

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Llama hidden size: 2048, Qwen hidden size: 896
Epoch 1/8
  step 10/128 | loss_L=2.8729 | loss_Q=6.6668 | scale_pen(L)=3.3974e-07 | scale_pen(Q)=6.0207e-07 | grad_norm=16.54 | sec/step~0.48 | rms_L~0.5645 rms_Q~0.5565
  step 20/128 | loss_L=2.6739 | loss_Q=3.6114 | scale_pen(L)=3.8515e-07 | scale_pen(Q)=1.4468e-06 | grad_norm=7.82 | sec/step~0.36 | rms_L~0.5656 rms_Q~0.5584
  step 30/128 | loss_L=2.2151 | loss_Q=2.4054 | scale_pen(L)=5.6923e-07 | scale_pen(Q)=1.9817e-06 | grad_norm=2.96 | sec/step~0.32 | rms_L~0.5666 rms_Q~0.5605
  step 40/128 | loss_L=2.0663 | loss_Q=1.9792 | scale_pen(L)=7.7871e-07 | scale_pen(Q)=2.4009e-06 | grad_norm=2.39 | sec/step~0.32 | rms_L~0.5675 rms_Q~0.5627
  step 50/128 | loss_L=2.5151 | loss_Q=2.2466 | scale_pen(L)=9.1131e-07 | scale_pen(Q)=2.4198e-06 | grad_norm=1.68 | sec/step~0.31 | rms_L~0.5682 rms_Q~0.5645
  step 60/128 | loss_L=2.4093 | loss_Q=2.0775 | scale_pen(L)=9.3675e-07 | scale_pen(Q)=2.2357e-06 | grad_norm=0.98 | sec/step~0.29 | rms_L~0.5689 rms_Q~0.5659
  step 70/128 | loss_L=1.9363 | loss_Q=1.6598 | scale_pen(L)=9.5670e-07 | scale_pen(Q)=2.0491e-06 | grad_norm=1.09 | sec/step~0.29 | rms_L~0.5695 rms_Q~0.5671
  step 80/128 | loss_L=1.9932 | loss_Q=1.7394 | scale_pen(L)=8.4125e-07 | scale_pen(Q)=1.7617e-06 | grad_norm=0.89 | sec/step~0.29 | rms_L~0.5701 rms_Q~0.5681
  step 90/128 | loss_L=2.0097 | loss_Q=1.7110 | scale_pen(L)=6.5489e-07 | scale_pen(Q)=1.5775e-06 | grad_norm=0.74 | sec/step~0.29 | rms_L~0.5707 rms_Q~0.5689
  step 100/128 | loss_L=1.8634 | loss_Q=1.6168 | scale_pen(L)=5.1338e-07 | scale_pen(Q)=1.3595e-06 | grad_norm=0.74 | sec/step~0.30 | rms_L~0.5712 rms_Q~0.5697
  step 110/128 | loss_L=1.8801 | loss_Q=1.6547 | scale_pen(L)=3.8080e-07 | scale_pen(Q)=1.1618e-06 | grad_norm=0.72 | sec/step~0.30 | rms_L~0.5717 rms_Q~0.5703
  step 120/128 | loss_L=1.8597 | loss_Q=1.6223 | scale_pen(L)=2.7813e-07 | scale_pen(Q)=1.0221e-06 | grad_norm=0.69 | sec/step~0.30 | rms_L~0.5723 rms_Q~0.5708
  step 128/128 | loss_L=1.8564 | loss_Q=1.6384 | scale_pen(L)=1.8695e-07 | scale_pen(Q)=9.1656e-07 | grad_norm=0.60 | sec/step~0.30 | rms_L~0.5727 rms_Q~0.5712
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 128
Epoch 2/8
  step 10/128 | loss_L=2.1100 | loss_Q=1.8663 | scale_pen(L)=8.7685e-08 | scale_pen(Q)=7.6270e-07 | grad_norm=0.66 | sec/step~0.30 | rms_L~0.5732 rms_Q~0.5717
  step 20/128 | loss_L=1.8672 | loss_Q=1.6126 | scale_pen(L)=4.1505e-08 | scale_pen(Q)=6.3564e-07 | grad_norm=0.56 | sec/step~0.30 | rms_L~0.5738 rms_Q~0.5721
  step 30/128 | loss_L=1.8238 | loss_Q=1.6899 | scale_pen(L)=4.9300e-09 | scale_pen(Q)=5.1227e-07 | grad_norm=0.56 | sec/step~0.30 | rms_L~0.5743 rms_Q~0.5725
  step 40/128 | loss_L=1.7972 | loss_Q=1.6607 | scale_pen(L)=2.5789e-09 | scale_pen(Q)=4.7148e-07 | grad_norm=0.60 | sec/step~0.30 | rms_L~0.5749 rms_Q~0.5729
  step 50/128 | loss_L=1.9365 | loss_Q=1.7956 | scale_pen(L)=1.8598e-08 | scale_pen(Q)=3.7515e-07 | grad_norm=0.58 | sec/step~0.30 | rms_L~0.5754 rms_Q~0.5732
  step 60/128 | loss_L=1.6917 | loss_Q=1.5641 | scale_pen(L)=5.3264e-08 | scale_pen(Q)=3.2728e-07 | grad_norm=0.57 | sec/step~0.29 | rms_L~0.5760 rms_Q~0.5735
  step 70/128 | loss_L=1.6551 | loss_Q=1.5018 | scale_pen(L)=1.0747e-07 | scale_pen(Q)=2.6644e-07 | grad_norm=0.56 | sec/step~0.29 | rms_L~0.5766 rms_Q~0.5739
  step 80/128 | loss_L=1.7741 | loss_Q=1.5973 | scale_pen(L)=2.3505e-07 | scale_pen(Q)=2.2229e-07 | grad_norm=0.54 | sec/step~0.29 | rms_L~0.5771 rms_Q~0.5742
  step 90/128 | loss_L=1.5873 | loss_Q=1.4225 | scale_pen(L)=3.6327e-07 | scale_pen(Q)=1.8571e-07 | grad_norm=0.41 | sec/step~0.29 | rms_L~0.5777 rms_Q~0.5745
  step 100/128 | loss_L=1.6745 | loss_Q=1.5388 | scale_pen(L)=4.0205e-07 | scale_pen(Q)=1.6651e-07 | grad_norm=0.54 | sec/step~0.29 | rms_L~0.5782 rms_Q~0.5748
  step 110/128 | loss_L=1.4923 | loss_Q=1.4085 | scale_pen(L)=5.4345e-07 | scale_pen(Q)=1.3569e-07 | grad_norm=0.52 | sec/step~0.29 | rms_L~0.5788 rms_Q~0.5751
  step 120/128 | loss_L=1.5217 | loss_Q=1.4525 | scale_pen(L)=6.8031e-07 | scale_pen(Q)=1.0321e-07 | grad_norm=0.55 | sec/step~0.30 | rms_L~0.5794 rms_Q~0.5754
  step 128/128 | loss_L=1.6521 | loss_Q=1.5548 | scale_pen(L)=7.5419e-07 | scale_pen(Q)=1.1373e-07 | grad_norm=0.43 | sec/step~0.30 | rms_L~0.5798 rms_Q~0.5757
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 256
Epoch 3/8
  step 10/128 | loss_L=1.7483 | loss_Q=1.6379 | scale_pen(L)=8.5596e-07 | scale_pen(Q)=1.1038e-07 | grad_norm=0.40 | sec/step~0.29 | rms_L~0.5804 rms_Q~0.5760
  step 20/128 | loss_L=1.6472 | loss_Q=1.5611 | scale_pen(L)=9.0790e-07 | scale_pen(Q)=1.1277e-07 | grad_norm=0.49 | sec/step~0.29 | rms_L~0.5809 rms_Q~0.5762
  step 30/128 | loss_L=1.6787 | loss_Q=1.5660 | scale_pen(L)=1.0515e-06 | scale_pen(Q)=1.1527e-07 | grad_norm=0.39 | sec/step~0.30 | rms_L~0.5814 rms_Q~0.5765
  step 40/128 | loss_L=1.6968 | loss_Q=1.6325 | scale_pen(L)=1.2025e-06 | scale_pen(Q)=8.1855e-08 | grad_norm=0.42 | sec/step~0.30 | rms_L~0.5820 rms_Q~0.5768
  step 50/128 | loss_L=1.4471 | loss_Q=1.3487 | scale_pen(L)=1.3404e-06 | scale_pen(Q)=8.5719e-08 | grad_norm=0.34 | sec/step~0.29 | rms_L~0.5825 rms_Q~0.5771
  step 60/128 | loss_L=1.3747 | loss_Q=1.2882 | scale_pen(L)=1.4262e-06 | scale_pen(Q)=6.1778e-08 | grad_norm=0.71 | sec/step~0.30 | rms_L~0.5830 rms_Q~0.5773
  step 70/128 | loss_L=1.5330 | loss_Q=1.4728 | scale_pen(L)=1.5480e-06 | scale_pen(Q)=4.9588e-08 | grad_norm=0.41 | sec/step~0.30 | rms_L~0.5835 rms_Q~0.5776
  step 80/128 | loss_L=1.6334 | loss_Q=1.5872 | scale_pen(L)=1.7073e-06 | scale_pen(Q)=4.7695e-08 | grad_norm=0.39 | sec/step~0.30 | rms_L~0.5840 rms_Q~0.5778
  step 90/128 | loss_L=1.4559 | loss_Q=1.3926 | scale_pen(L)=1.8439e-06 | scale_pen(Q)=4.8112e-08 | grad_norm=0.43 | sec/step~0.30 | rms_L~0.5845 rms_Q~0.5781
  step 100/128 | loss_L=1.5111 | loss_Q=1.4116 | scale_pen(L)=1.9915e-06 | scale_pen(Q)=4.4170e-08 | grad_norm=0.33 | sec/step~0.29 | rms_L~0.5850 rms_Q~0.5783
  step 110/128 | loss_L=1.4781 | loss_Q=1.4055 | scale_pen(L)=2.1280e-06 | scale_pen(Q)=5.8043e-08 | grad_norm=0.32 | sec/step~0.29 | rms_L~0.5855 rms_Q~0.5786
  step 120/128 | loss_L=1.6422 | loss_Q=1.5593 | scale_pen(L)=2.1844e-06 | scale_pen(Q)=5.3871e-08 | grad_norm=0.35 | sec/step~0.30 | rms_L~0.5859 rms_Q~0.5788
  step 128/128 | loss_L=1.6767 | loss_Q=1.6025 | scale_pen(L)=2.3178e-06 | scale_pen(Q)=3.9918e-08 | grad_norm=0.38 | sec/step~0.31 | rms_L~0.5863 rms_Q~0.5790
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 384
Epoch 4/8
  step 10/128 | loss_L=1.6117 | loss_Q=1.5299 | scale_pen(L)=2.4727e-06 | scale_pen(Q)=3.1380e-08 | grad_norm=0.37 | sec/step~0.30 | rms_L~0.5868 rms_Q~0.5793
  step 20/128 | loss_L=1.5574 | loss_Q=1.4943 | scale_pen(L)=2.5647e-06 | scale_pen(Q)=3.1804e-08 | grad_norm=0.36 | sec/step~0.30 | rms_L~0.5872 rms_Q~0.5795
  step 30/128 | loss_L=1.4251 | loss_Q=1.3512 | scale_pen(L)=2.8417e-06 | scale_pen(Q)=3.4717e-08 | grad_norm=0.42 | sec/step~0.29 | rms_L~0.5877 rms_Q~0.5797
  step 40/128 | loss_L=1.5489 | loss_Q=1.4677 | scale_pen(L)=3.0002e-06 | scale_pen(Q)=3.9018e-08 | grad_norm=0.35 | sec/step~0.29 | rms_L~0.5881 rms_Q~0.5800
  step 50/128 | loss_L=1.4753 | loss_Q=1.4397 | scale_pen(L)=3.1524e-06 | scale_pen(Q)=3.3834e-08 | grad_norm=0.35 | sec/step~0.29 | rms_L~0.5886 rms_Q~0.5802
  step 60/128 | loss_L=1.6539 | loss_Q=1.5897 | scale_pen(L)=3.1292e-06 | scale_pen(Q)=3.4142e-08 | grad_norm=0.34 | sec/step~0.29 | rms_L~0.5890 rms_Q~0.5804
  step 70/128 | loss_L=1.6418 | loss_Q=1.5776 | scale_pen(L)=3.5400e-06 | scale_pen(Q)=2.4536e-08 | grad_norm=0.68 | sec/step~0.30 | rms_L~0.5894 rms_Q~0.5807
  step 80/128 | loss_L=1.3871 | loss_Q=1.3370 | scale_pen(L)=3.7185e-06 | scale_pen(Q)=1.9888e-08 | grad_norm=0.32 | sec/step~0.29 | rms_L~0.5898 rms_Q~0.5809
  step 90/128 | loss_L=1.3544 | loss_Q=1.2966 | scale_pen(L)=3.9671e-06 | scale_pen(Q)=1.0535e-08 | grad_norm=0.29 | sec/step~0.29 | rms_L~0.5902 rms_Q~0.5811
  step 100/128 | loss_L=1.3697 | loss_Q=1.3158 | scale_pen(L)=3.9685e-06 | scale_pen(Q)=8.7571e-09 | grad_norm=0.32 | sec/step~0.30 | rms_L~0.5906 rms_Q~0.5813
  step 110/128 | loss_L=1.6299 | loss_Q=1.4850 | scale_pen(L)=4.0458e-06 | scale_pen(Q)=6.8051e-09 | grad_norm=0.42 | sec/step~0.30 | rms_L~0.5910 rms_Q~0.5816
  step 120/128 | loss_L=1.3717 | loss_Q=1.3486 | scale_pen(L)=4.3322e-06 | scale_pen(Q)=7.4490e-09 | grad_norm=0.39 | sec/step~0.29 | rms_L~0.5914 rms_Q~0.5818
  step 128/128 | loss_L=1.3946 | loss_Q=1.3660 | scale_pen(L)=4.5757e-06 | scale_pen(Q)=2.2885e-08 | grad_norm=0.42 | sec/step~0.28 | rms_L~0.5917 rms_Q~0.5819
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 512
Epoch 5/8
  step 10/128 | loss_L=1.6353 | loss_Q=1.5447 | scale_pen(L)=4.5315e-06 | scale_pen(Q)=2.7063e-08 | grad_norm=0.40 | sec/step~0.30 | rms_L~0.5921 rms_Q~0.5822
  step 20/128 | loss_L=1.3872 | loss_Q=1.3320 | scale_pen(L)=4.4980e-06 | scale_pen(Q)=1.2934e-08 | grad_norm=0.37 | sec/step~0.29 | rms_L~0.5925 rms_Q~0.5824
  step 30/128 | loss_L=1.4023 | loss_Q=1.3646 | scale_pen(L)=4.8816e-06 | scale_pen(Q)=6.9038e-09 | grad_norm=0.42 | sec/step~0.29 | rms_L~0.5929 rms_Q~0.5826
  step 40/128 | loss_L=1.5719 | loss_Q=1.4850 | scale_pen(L)=5.0827e-06 | scale_pen(Q)=2.7387e-09 | grad_norm=0.41 | sec/step~0.29 | rms_L~0.5933 rms_Q~0.5828
  step 50/128 | loss_L=1.5607 | loss_Q=1.5041 | scale_pen(L)=5.2852e-06 | scale_pen(Q)=4.8966e-09 | grad_norm=0.34 | sec/step~0.30 | rms_L~0.5936 rms_Q~0.5830
  step 60/128 | loss_L=1.3681 | loss_Q=1.3221 | scale_pen(L)=5.5426e-06 | scale_pen(Q)=3.2470e-09 | grad_norm=0.33 | sec/step~0.30 | rms_L~0.5940 rms_Q~0.5833
  step 70/128 | loss_L=1.5700 | loss_Q=1.5048 | scale_pen(L)=5.3429e-06 | scale_pen(Q)=1.0147e-08 | grad_norm=0.33 | sec/step~0.29 | rms_L~0.5943 rms_Q~0.5835
  step 80/128 | loss_L=1.2959 | loss_Q=1.2386 | scale_pen(L)=5.3049e-06 | scale_pen(Q)=1.1221e-09 | grad_norm=0.42 | sec/step~0.29 | rms_L~0.5947 rms_Q~0.5837
  step 90/128 | loss_L=1.4714 | loss_Q=1.3953 | scale_pen(L)=5.7442e-06 | scale_pen(Q)=1.2200e-09 | grad_norm=0.34 | sec/step~0.29 | rms_L~0.5950 rms_Q~0.5839
  step 100/128 | loss_L=1.4068 | loss_Q=1.3277 | scale_pen(L)=5.7533e-06 | scale_pen(Q)=1.1005e-10 | grad_norm=0.33 | sec/step~0.30 | rms_L~0.5954 rms_Q~0.5841
  step 110/128 | loss_L=1.4258 | loss_Q=1.3521 | scale_pen(L)=5.6548e-06 | scale_pen(Q)=3.5527e-11 | grad_norm=0.26 | sec/step~0.30 | rms_L~0.5957 rms_Q~0.5843
  step 120/128 | loss_L=1.4358 | loss_Q=1.3604 | scale_pen(L)=5.7179e-06 | scale_pen(Q)=1.5948e-09 | grad_norm=0.30 | sec/step~0.30 | rms_L~0.5960 rms_Q~0.5845
  step 128/128 | loss_L=1.3872 | loss_Q=1.3496 | scale_pen(L)=5.8112e-06 | scale_pen(Q)=3.2810e-09 | grad_norm=0.35 | sec/step~0.29 | rms_L~0.5963 rms_Q~0.5847
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 640
Epoch 6/8
  step 10/128 | loss_L=1.6595 | loss_Q=1.5856 | scale_pen(L)=6.0698e-06 | scale_pen(Q)=1.1511e-10 | grad_norm=0.34 | sec/step~0.31 | rms_L~0.5966 rms_Q~0.5849
  step 20/128 | loss_L=1.3740 | loss_Q=1.3268 | scale_pen(L)=5.9360e-06 | scale_pen(Q)=2.5899e-10 | grad_norm=0.26 | sec/step~0.31 | rms_L~0.5969 rms_Q~0.5851
  step 30/128 | loss_L=1.3039 | loss_Q=1.2805 | scale_pen(L)=6.0622e-06 | scale_pen(Q)=1.0131e-09 | grad_norm=0.37 | sec/step~0.31 | rms_L~0.5972 rms_Q~0.5853
  step 40/128 | loss_L=1.4719 | loss_Q=1.4053 | scale_pen(L)=6.1772e-06 | scale_pen(Q)=5.5778e-09 | grad_norm=0.27 | sec/step~0.31 | rms_L~0.5976 rms_Q~0.5855
  step 50/128 | loss_L=1.4866 | loss_Q=1.4327 | scale_pen(L)=6.4922e-06 | scale_pen(Q)=1.1434e-08 | grad_norm=0.31 | sec/step~0.31 | rms_L~0.5979 rms_Q~0.5857
  step 60/128 | loss_L=1.4295 | loss_Q=1.4065 | scale_pen(L)=6.4843e-06 | scale_pen(Q)=1.9888e-08 | grad_norm=0.36 | sec/step~0.31 | rms_L~0.5982 rms_Q~0.5859
  step 70/128 | loss_L=1.5314 | loss_Q=1.4858 | scale_pen(L)=6.4056e-06 | scale_pen(Q)=3.0396e-08 | grad_norm=0.28 | sec/step~0.30 | rms_L~0.5985 rms_Q~0.5861
  step 80/128 | loss_L=1.4916 | loss_Q=1.4219 | scale_pen(L)=6.5684e-06 | scale_pen(Q)=2.7516e-08 | grad_norm=0.36 | sec/step~0.30 | rms_L~0.5988 rms_Q~0.5863
  step 90/128 | loss_L=1.4372 | loss_Q=1.3860 | scale_pen(L)=6.7523e-06 | scale_pen(Q)=3.9490e-08 | grad_norm=0.45 | sec/step~0.29 | rms_L~0.5991 rms_Q~0.5865
  step 100/128 | loss_L=1.2393 | loss_Q=1.1831 | scale_pen(L)=6.6800e-06 | scale_pen(Q)=4.5508e-08 | grad_norm=0.32 | sec/step~0.29 | rms_L~0.5994 rms_Q~0.5867
  step 110/128 | loss_L=1.5358 | loss_Q=1.4909 | scale_pen(L)=6.6198e-06 | scale_pen(Q)=5.5656e-08 | grad_norm=0.43 | sec/step~0.30 | rms_L~0.5996 rms_Q~0.5869
  step 120/128 | loss_L=1.2901 | loss_Q=1.2626 | scale_pen(L)=6.6038e-06 | scale_pen(Q)=8.9887e-08 | grad_norm=0.57 | sec/step~0.30 | rms_L~0.5999 rms_Q~0.5871
  step 128/128 | loss_L=1.3808 | loss_Q=1.3271 | scale_pen(L)=6.8643e-06 | scale_pen(Q)=7.6324e-08 | grad_norm=0.42 | sec/step~0.29 | rms_L~0.6001 rms_Q~0.5873
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 768
Epoch 7/8
  step 10/128 | loss_L=1.4817 | loss_Q=1.4433 | scale_pen(L)=7.0898e-06 | scale_pen(Q)=1.1241e-07 | grad_norm=0.42 | sec/step~0.29 | rms_L~0.6004 rms_Q~0.5874
  step 20/128 | loss_L=1.3691 | loss_Q=1.3246 | scale_pen(L)=7.2834e-06 | scale_pen(Q)=1.1763e-07 | grad_norm=0.42 | sec/step~0.30 | rms_L~0.6007 rms_Q~0.5876
  step 30/128 | loss_L=1.6138 | loss_Q=1.5548 | scale_pen(L)=7.0955e-06 | scale_pen(Q)=1.4725e-07 | grad_norm=0.34 | sec/step~0.30 | rms_L~0.6010 rms_Q~0.5878
  step 40/128 | loss_L=1.3746 | loss_Q=1.3084 | scale_pen(L)=6.9432e-06 | scale_pen(Q)=1.5317e-07 | grad_norm=0.38 | sec/step~0.30 | rms_L~0.6013 rms_Q~0.5880
  step 50/128 | loss_L=1.4832 | loss_Q=1.4646 | scale_pen(L)=7.2173e-06 | scale_pen(Q)=2.4433e-07 | grad_norm=0.45 | sec/step~0.30 | rms_L~0.6015 rms_Q~0.5882
  step 60/128 | loss_L=1.4321 | loss_Q=1.3588 | scale_pen(L)=7.2924e-06 | scale_pen(Q)=2.2868e-07 | grad_norm=0.28 | sec/step~0.30 | rms_L~0.6018 rms_Q~0.5884
  step 70/128 | loss_L=1.3619 | loss_Q=1.3290 | scale_pen(L)=7.4049e-06 | scale_pen(Q)=2.4907e-07 | grad_norm=0.29 | sec/step~0.30 | rms_L~0.6021 rms_Q~0.5886
  step 80/128 | loss_L=1.3038 | loss_Q=1.2670 | scale_pen(L)=7.6153e-06 | scale_pen(Q)=2.2144e-07 | grad_norm=0.49 | sec/step~0.29 | rms_L~0.6023 rms_Q~0.5887
  step 90/128 | loss_L=1.2374 | loss_Q=1.2229 | scale_pen(L)=7.5398e-06 | scale_pen(Q)=2.1289e-07 | grad_norm=0.52 | sec/step~0.29 | rms_L~0.6026 rms_Q~0.5889
  step 100/128 | loss_L=1.2463 | loss_Q=1.2173 | scale_pen(L)=7.9787e-06 | scale_pen(Q)=2.0064e-07 | grad_norm=0.57 | sec/step~0.29 | rms_L~0.6028 rms_Q~0.5891
  step 110/128 | loss_L=1.4753 | loss_Q=1.4405 | scale_pen(L)=8.0652e-06 | scale_pen(Q)=2.0607e-07 | grad_norm=0.40 | sec/step~0.29 | rms_L~0.6031 rms_Q~0.5893
  step 120/128 | loss_L=1.5892 | loss_Q=1.5294 | scale_pen(L)=7.7753e-06 | scale_pen(Q)=2.7225e-07 | grad_norm=0.43 | sec/step~0.29 | rms_L~0.6034 rms_Q~0.5894
  step 128/128 | loss_L=1.2831 | loss_Q=1.2289 | scale_pen(L)=7.9552e-06 | scale_pen(Q)=3.0747e-07 | grad_norm=0.49 | sec/step~0.30 | rms_L~0.6036 rms_Q~0.5896
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 896
Epoch 8/8
  step 10/128 | loss_L=1.2084 | loss_Q=1.1884 | scale_pen(L)=7.9129e-06 | scale_pen(Q)=3.2653e-07 | grad_norm=0.45 | sec/step~0.30 | rms_L~0.6038 rms_Q~0.5898
  step 20/128 | loss_L=1.4905 | loss_Q=1.4289 | scale_pen(L)=7.9834e-06 | scale_pen(Q)=3.5286e-07 | grad_norm=0.36 | sec/step~0.30 | rms_L~0.6041 rms_Q~0.5899
  step 30/128 | loss_L=1.3991 | loss_Q=1.3972 | scale_pen(L)=8.3569e-06 | scale_pen(Q)=3.9963e-07 | grad_norm=0.36 | sec/step~0.30 | rms_L~0.6043 rms_Q~0.5901
  step 40/128 | loss_L=1.3283 | loss_Q=1.3083 | scale_pen(L)=8.5273e-06 | scale_pen(Q)=4.0334e-07 | grad_norm=0.36 | sec/step~0.28 | rms_L~0.6046 rms_Q~0.5903
  step 50/128 | loss_L=1.4783 | loss_Q=1.4431 | scale_pen(L)=8.7670e-06 | scale_pen(Q)=5.2671e-07 | grad_norm=0.59 | sec/step~0.28 | rms_L~0.6048 rms_Q~0.5905
  step 60/128 | loss_L=1.3607 | loss_Q=1.3746 | scale_pen(L)=8.6503e-06 | scale_pen(Q)=5.8217e-07 | grad_norm=0.39 | sec/step~0.27 | rms_L~0.6051 rms_Q~0.5906
  step 70/128 | loss_L=1.3350 | loss_Q=1.3023 | scale_pen(L)=8.7586e-06 | scale_pen(Q)=6.6671e-07 | grad_norm=0.37 | sec/step~0.27 | rms_L~0.6053 rms_Q~0.5908
  step 80/128 | loss_L=1.5164 | loss_Q=1.4503 | scale_pen(L)=8.8357e-06 | scale_pen(Q)=6.2006e-07 | grad_norm=0.40 | sec/step~0.27 | rms_L~0.6055 rms_Q~0.5910
  step 90/128 | loss_L=1.3846 | loss_Q=1.3350 | scale_pen(L)=9.0589e-06 | scale_pen(Q)=6.2015e-07 | grad_norm=0.55 | sec/step~0.27 | rms_L~0.6058 rms_Q~0.5911
  step 100/128 | loss_L=1.4062 | loss_Q=1.3549 | scale_pen(L)=8.6980e-06 | scale_pen(Q)=6.9166e-07 | grad_norm=0.33 | sec/step~0.27 | rms_L~0.6060 rms_Q~0.5913
  step 110/128 | loss_L=1.3608 | loss_Q=1.3030 | scale_pen(L)=9.0187e-06 | scale_pen(Q)=7.0121e-07 | grad_norm=0.28 | sec/step~0.28 | rms_L~0.6063 rms_Q~0.5914
  step 120/128 | loss_L=1.3197 | loss_Q=1.2733 | scale_pen(L)=9.3285e-06 | scale_pen(Q)=6.8909e-07 | grad_norm=0.28 | sec/step~0.28 | rms_L~0.6065 rms_Q~0.5916
  step 128/128 | loss_L=1.3203 | loss_Q=1.2952 | scale_pen(L)=9.5254e-06 | scale_pen(Q)=6.8454e-07 | grad_norm=0.32 | sec/step~0.27 | rms_L~0.6067 rms_Q~0.5917
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1024
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean': 0.6066729327430949, 'count': 1024}, 'qwen': {'rms_mean': 0.5917430291883647, 'count': 1024}}

Training completed at Fri Sep 12 00:06:10 PDT 2025 with exit code: 0

=========================================
PHASE 2: EVALUATION
=========================================
Starting evaluation at Fri Sep 12 00:06:10 PDT 2025
Using checkpoint from: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/ckpt
Results will be saved to: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/ckpt/training_stats.json
Building encoder and computing Z...
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:540: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder_wire.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))
Saved Z to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/Z.pt

[Sequential Evaluation Mode - one model at a time]
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:846: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  encoder.load_state_dict(torch.load(os.path.join(args.ckpt, "encoder.pt"), map_location=device))

Evaluating Llama...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:863: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_llama.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_llama.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw', 'neutral_chat', 'llama_chat'] | nlls={'raw': 8.802150650911553, 'neutral_chat': 8.772471772570944, 'llama_chat': 8.795138837903044} | picked=neutral_chat
Saved Z[llama_neutral_chat] to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/Z_llama_neutral_chat.pt
[calib:llama] mode=train_stats prefix_rms=0.62970 -> target=0.60667
[debug:llama] adapter.scale=1.0031 | Z.std=0.9998 Z.mean||=15.9969 | prefix.std=0.6067 prefix.mean||=27.4271 | embed.RMS=0.0149
Saved Llama results to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/llama_results.json

Evaluating Qwen...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/projects/m000066/sujinesh/LatentWire/latentwire/eval.py:948: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  adp_qwen.load_state_dict(torch.load(os.path.join(args.ckpt, "adapter_qwen.pt"), map_location=device), strict=True)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw', 'neutral_chat', 'qwen_chat'] | nlls={'raw': 9.186257161791362, 'neutral_chat': 9.437735375588533, 'qwen_chat': 9.404758159130338} | picked=raw
Saved Z[qwen_raw] to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/Z_qwen_raw.pt
[calib:qwen]  mode=train_stats prefix_rms=0.59767 -> target=0.59174
[debug:qwen] adapter.scale=0.9992 | Z.std=0.9994 Z.mean||=15.9903 | prefix.std=0.5916 prefix.mean||=17.6990 | embed.RMS=0.0152
Saved Qwen results to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/qwen_results.json

Joint rescoring...

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 8
Device: cuda  |  Dtype: torch.bfloat16
Avg prompt tokens (Llama): 269.2 | (Qwen): 230.7 | Latent length M: 16
Compression ratio (Llama): 16.8x | (Qwen): 14.4x
Approx interlingua payload per example: 16384 bytes (fp32), and 8192 bytes (fp16); wire compression vs one-copy text (fp16): 0.14x

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.077  |  NLL/token (gold): 15.679386170409446
Qwen   EM: 0.365   F1: 0.569   |  NLL/token (gold): 13.036324268295651
Wall clock: 7.01s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.002  |  NLL/token (gold): 8.779297657234725
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 9.197777521673334
Wall clock: 7.92s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.000  F1: 0.030
Qwen   EM: 0.005   F1: 0.042
Wall clock: 6.03s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.002
Inter-model agreement (normalized): 0.770
Oracle upper bound:  EM 0.000  F1 0.002

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 8,
  "latent_len": 16,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 269.205,
    "qwen": 230.69
  },
  "compression": {
    "llama": 16.8253125,
    "qwen": 14.418125
  },
  "payload_bytes": 16384,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1070,
      "qwen_avg": 1106,
      "max_avg": 1106
    },
    "text_bytes_twocopies": {
      "sum_avg": 2176
    },
    "latent_bytes": {
      "fp32": 16384,
      "fp16": 8192
    },
    "wire_compression": {
      "vs_onecopy_fp16": 0.135009765625,
      "vs_onecopy_fp32": 0.0675048828125
    }
  },
  "text": {
    "wall_clock_sec": 7.0087504386901855,
    "llama": {
      "em": 0.0,
      "f1": 0.07699812687312685,
      "nll_token": 15.679386170409446
    },
    "qwen": {
      "em": 0.365,
      "f1": 0.5692489063224359,
      "nll_token": 13.036324268295651
    }
  },
  "latent": {
    "wall_clock_sec": 7.92188835144043,
    "llama": {
      "em": 0.0,
      "f1": 0.0023333333333333335,
      "nll_token": 8.779297657234725
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.0,
      "nll_token": 9.197777521673334
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 16,
    "llama": {
      "em": 0.0,
      "f1": 0.02959489998305788
    },
    "wall_clock_sec": 6.0267815589904785,
    "qwen": {
      "em": 0.005,
      "f1": 0.042400183150183145
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0023333333333333335,
    "agreement": 0.77,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 1.0030863285064697,
      "Z_std": 0.9998056292533875,
      "Z_mean_norm": 15.996875762939453,
      "prefix_std": 0.6066684722900391,
      "prefix_mean_norm": 27.427108764648438,
      "embed_rms": 0.014909257180988789,
      "encoder_text_mode": "neutral_chat",
      "calibration_mode": "train_stats"
    },
    "qwen": {
      "adapter_scale": 0.9991726279258728,
      "Z_std": 0.9993961453437805,
      "Z_mean_norm": 15.990324020385742,
      "prefix_std": 0.591614305973053,
      "prefix_mean_norm": 17.698978424072266,
      "embed_rms": 0.015224291011691093,
      "encoder_text_mode": "raw",
      "calibration_mode": "train_stats"
    },
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "train_stats",
    "decode": {
      "min_new_tokens": 2,
      "eos_ban_steps": 6,
      "first_token_top_p": 0.9,
      "first_token_temperature": 0.7
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0023333333333333335
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/predictions.jsonl

Evaluation completed at Fri Sep 12 00:07:10 PDT 2025 with exit code: 0

=========================================
PIPELINE SUMMARY
=========================================
Run ID: rw_squad_qwen_simple_m16_e2_bs32_lr2e-4
Started: Fri Sep 12 00:00:52 PDT 2025
Completed: Fri Sep 12 00:07:10 PDT 2025

Outputs:
  Training checkpoint: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/ckpt/
  Training log: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/train.log
  Evaluation results: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval_squad/
  Evaluation log: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/eval.log
  Full pipeline log: runs/rw_squad_qwen_simple_m16_e2_bs32_lr2e-4/full_pipeline_20250912_000052.log

✓ Encoder checkpoint saved
✓ Llama adapter checkpoint saved
✓ Qwen adapter checkpoint saved
✓ Evaluation metrics saved

Key metrics:
  Compression: Llama 16.8x, Qwen 14.4x
  Text F1: Llama 0.077, Qwen 0.569
  Latent F1: Llama 0.002, Qwen 0.000

=========================================
Pipeline completed at Fri Sep 12 00:07:10 PDT 2025
=========================================
