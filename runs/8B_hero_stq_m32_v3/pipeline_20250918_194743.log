
=========================================
Starting pipeline at Thu Sep 18 19:47:43 PDT 2025
=========================================


=========================================
TRAIN + PER-EPOCH EVAL
=========================================


=========================================
EPOCH 1/12
=========================================

Running pre-train eval on existing checkpoint...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch1_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7910.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.47s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5714.31it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.72s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.39s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 35.00s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.004  |  NLL/token (gold): 11.42535460562933
       First-token acc: top1=0.005  top5=0.005
Qwen   EM: 0.000   F1: 0.018  |  NLL/token (gold): 10.422491482010594
       First-token acc: top1=0.050  top5=0.070
Wall clock: 40.63s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 29.47s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.013
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.020

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 34.99869179725647
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.004437691284205056,
      "nll": 11.42535460562933,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 11.42535460562933
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.017582643166260535,
      "nll": 10.422491482010594,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.07000000029802322,
      "nll_token": 10.422491482010594
    },
    "wall_clock_sec": 40.62505578994751
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 29.466665267944336
  },
  "joint": {
    "em": 0.0,
    "f1": 0.01267959319115324,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.019970069968132192
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.019970069968132192
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch1_pre/predictions.jsonl
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3217.72it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:03,  1.96s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.25s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.53s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3150.65it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.28s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hero_stq_m32_v3/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=6, global_step=360
Epoch 7/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/100 | grad_norm=62.23 | sec/step~7.39 | keep=1.00 | K=8 | llama: tf=11.7036 first=12.5031 kCE=5.6537 KD=3.8311 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=10.9292 first=12.7571 kCE=10.6011 KD=4.6438 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  20/100 | grad_norm=118.66 | sec/step~6.69 | keep=1.00 | K=8 | llama: tf=10.7792 first=11.8413 kCE=5.7005 KD=3.7808 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=10.7912 first=11.7913 kCE=10.0102 KD=4.6518 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  30/100 | grad_norm=171.94 | sec/step~7.30 | keep=1.00 | K=8 | llama: tf=11.3797 first=11.4333 kCE=5.5305 KD=3.5399 man=0.0001 | scale_pen(llama)=1.1951e-11 | qwen: tf=10.4055 first=11.1350 kCE=8.9180 KD=4.5757 man=0.0002 | scale_pen(qwen)=3.5527e-15 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  40/100 | grad_norm=64.55 | sec/step~8.30 | keep=1.00 | K=8 | llama: tf=11.4308 first=12.0785 kCE=5.3250 KD=3.2672 man=0.0001 | scale_pen(llama)=8.8818e-12 | qwen: tf=10.0427 first=12.9393 kCE=11.0376 KD=3.5273 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  50/100 | grad_norm=132.31 | sec/step~7.14 | keep=1.00 | K=8 | llama: tf=11.8285 first=12.9121 kCE=5.5679 KD=3.4933 man=0.0001 | scale_pen(llama)=8.8818e-12 | qwen: tf=10.9942 first=13.8247 kCE=11.6033 KD=4.5636 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  60/100 | grad_norm=199.07 | sec/step~8.68 | keep=1.00 | K=8 | llama: tf=11.5332 first=12.2899 kCE=5.5547 KD=3.0090 man=0.0001 | scale_pen(llama)=8.8818e-12 | qwen: tf=10.0666 first=11.8868 kCE=9.9740 KD=4.4234 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  70/100 | grad_norm=66.94 | sec/step~6.58 | keep=1.00 | K=8 | llama: tf=11.6619 first=11.6480 kCE=5.2833 KD=3.5869 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=10.5057 first=11.7388 kCE=10.1176 KD=4.1508 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  80/100 | grad_norm=166.04 | sec/step~6.80 | keep=1.00 | K=8 | llama: tf=11.3018 first=11.9197 kCE=5.4588 KD=3.3191 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=9.7948 first=11.1143 kCE=9.7613 KD=4.5382 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  90/100 | grad_norm=280.65 | sec/step~6.98 | keep=1.00 | K=8 | llama: tf=11.9093 first=11.8962 kCE=5.4495 KD=3.6271 man=0.0001 | scale_pen(llama)=6.8781e-12 | qwen: tf=9.8577 first=12.2553 kCE=11.6406 KD=4.5660 man=0.0002 | scale_pen(qwen)=3.1974e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  100/100 | grad_norm=37.51 | sec/step~6.37 | keep=1.00 | K=8 | llama: tf=11.5883 first=11.5119 kCE=5.3764 KD=3.5079 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=9.0490 first=12.1441 kCE=11.3238 KD=3.9683 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hero_stq_m32_v3/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010556842451915145, 'rms_mean_cal': 0.010571260238066316, 'embed_rms': 0.010566912591457367, 'count': 100}, 'qwen': {'rms_mean_raw': 0.013669955478981137, 'rms_mean_cal': 0.013640755228698254, 'embed_rms': 0.013625300489366055, 'count': 100}}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch1/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2974.16it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.78s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.89s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3302.60it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 34.62s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.001  |  NLL/token (gold): 11.060807631399626
       First-token acc: top1=0.000  top5=0.005
Qwen   EM: 0.000   F1: 0.015  |  NLL/token (gold): 10.36449353152482
       First-token acc: top1=0.050  top5=0.070
Wall clock: 39.04s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 28.55s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.008
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.017

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 34.623138427734375
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0012499999812500002,
      "nll": 11.060807631399626,
      "first_token_top1": 0.0,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 11.060807631399626
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.015401879565847534,
      "nll": 10.36449353152482,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.07000000029802322,
      "nll_token": 10.36449353152482
    },
    "wall_clock_sec": 39.04214906692505
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 28.55026650428772
  },
  "joint": {
    "em": 0.0,
    "f1": 0.008291666467446186,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.016651879547097532
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.016651879547097532
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch1/predictions.jsonl
✓ Metrics from: runs/8B_hero_stq_m32_v3/eval_epoch1/metrics.json
  Text F1:     Llama 0.809 | Qwen 0.852
  Latent F1:   Llama 0.001 | Qwen 0.015
  FirstTok@1:  Llama 0.000 | Qwen 0.050
  Compression: Llama×7.658 | Qwen×7.240
Top 5 latent predictions from runs/8B_hero_stq_m32_v3/eval_epoch1/predictions.jsonl
  1. Llama: 1 | Qwen: 1. **Identify the type of logical reasoning involved:** | Gold: linear
  2. Llama:  | | | | | | | | | Qwen: 1. **Identify the type of logical reasoning involved:** | Gold: Lampea
  3. Llama: no | Qwen: 1. **What is the main purpose of the passage?** | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1. 2. 3. 4. 5. 6 | Qwen: 1. **Identify the type of logical reasoning involved:** | Gold: San Jose
  5. Llama:  | | | | | | | | | Qwen: 1. **Identify the type of logical reasoning involved:** | Gold: oxides

=========================================
EPOCH 2/12
=========================================

Running pre-train eval on existing checkpoint...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch2_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2823.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.73s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3060.42it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 33.81s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.001  |  NLL/token (gold): 11.060807631399626
       First-token acc: top1=0.000  top5=0.005
Qwen   EM: 0.000   F1: 0.015  |  NLL/token (gold): 10.36449353152482
       First-token acc: top1=0.050  top5=0.070
Wall clock: 39.30s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 28.20s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.008
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.017

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 33.80649185180664
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0012499999812500002,
      "nll": 11.060807631399626,
      "first_token_top1": 0.0,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 11.060807631399626
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.015401879565847534,
      "nll": 10.36449353152482,
      "first_token_top1": 0.04999999701976776,
      "first_token_top5": 0.07000000029802322,
      "nll_token": 10.36449353152482
    },
    "wall_clock_sec": 39.2973210811615
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 28.199232578277588
  },
  "joint": {
    "em": 0.0,
    "f1": 0.008291666467446186,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.016651879547097532
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.016651879547097532
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch2_pre/predictions.jsonl
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3271.05it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.82s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3282.57it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.49s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.35s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.31s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hero_stq_m32_v3/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=7, global_step=460
Epoch 8/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/100 | grad_norm=170.27 | sec/step~6.90 | keep=1.00 | K=8 | llama: tf=10.9557 first=11.4875 kCE=5.6592 KD=3.0597 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=11.9003 first=11.9801 kCE=10.0777 KD=4.7285 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  20/100 | grad_norm=341.52 | sec/step~6.67 | keep=1.00 | K=8 | llama: tf=11.1425 first=10.1273 kCE=5.4574 KD=3.5454 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=10.2287 first=9.9245 kCE=9.1451 KD=4.5798 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  30/100 | grad_norm=507.53 | sec/step~6.77 | keep=1.00 | K=8 | llama: tf=10.3336 first=10.9895 kCE=6.0820 KD=4.0704 man=0.0001 | scale_pen(llama)=6.2670e-12 | qwen: tf=10.2567 first=11.7683 kCE=9.0840 KD=5.1974 man=0.0002 | scale_pen(qwen)=8.8818e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  40/100 | grad_norm=150.45 | sec/step~6.48 | keep=1.00 | K=8 | llama: tf=10.9068 first=9.7237 kCE=5.7316 KD=3.6799 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=10.4754 first=10.2813 kCE=8.8735 KD=5.0692 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  50/100 | grad_norm=325.21 | sec/step~6.70 | keep=1.00 | K=8 | llama: tf=10.5617 first=9.0534 kCE=6.2699 KD=3.7264 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.3689 first=9.5044 kCE=8.7545 KD=5.2696 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  60/100 | grad_norm=499.01 | sec/step~6.88 | keep=1.00 | K=8 | llama: tf=10.8100 first=10.6880 kCE=6.4630 KD=4.1164 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=11.5919 first=12.2067 kCE=9.1129 KD=5.6152 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  70/100 | grad_norm=58.46 | sec/step~7.69 | keep=1.00 | K=8 | llama: tf=11.3251 first=10.0256 kCE=5.6995 KD=3.2635 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.3841 first=10.9686 kCE=9.9547 KD=4.5716 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  80/100 | grad_norm=158.81 | sec/step~7.34 | keep=1.00 | K=8 | llama: tf=10.1477 first=8.7531 kCE=5.6328 KD=3.4156 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.0564 first=9.3126 kCE=9.7439 KD=4.4784 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  90/100 | grad_norm=254.79 | sec/step~7.10 | keep=1.00 | K=8 | llama: tf=11.4220 first=9.7143 kCE=5.6958 KD=3.7559 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.9346 first=12.3518 kCE=10.2538 KD=4.6202 man=0.0002 | scale_pen(qwen)=1.7408e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  100/100 | grad_norm=50.97 | sec/step~7.99 | keep=1.00 | K=8 | llama: tf=10.5592 first=9.1026 kCE=5.8584 KD=3.0954 man=0.0001 | scale_pen(llama)=1.2367e-11 | qwen: tf=12.4671 first=12.4923 kCE=11.0921 KD=4.8522 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hero_stq_m32_v3/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.01056134139187634, 'rms_mean_cal': 0.010571797918528318, 'embed_rms': 0.010571404360234737, 'count': 100}, 'qwen': {'rms_mean_raw': 0.01367536368779838, 'rms_mean_cal': 0.01364170366898179, 'embed_rms': 0.01364052202552557, 'count': 100}}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/epoch2/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch2/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3189.58it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.78s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2944.92it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.40s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 34.13s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.002  |  NLL/token (gold): 10.789257227969008
       First-token acc: top1=0.005  top5=0.005
Qwen   EM: 0.000   F1: 0.012  |  NLL/token (gold): 10.266884765927754
       First-token acc: top1=0.000  top5=0.070
Wall clock: 42.68s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 29.59s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.011
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.015

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 34.125367879867554
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0023611110701388894,
      "nll": 10.789257227969008,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 10.789257227969008
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.012249193821759841,
      "nll": 10.266884765927754,
      "first_token_top1": 0.0,
      "first_token_top5": 0.07000000029802322,
      "nll_token": 10.266884765927754
    },
    "wall_clock_sec": 42.68409729003906
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 29.588571310043335
  },
  "joint": {
    "em": 0.0,
    "f1": 0.010680097357568641,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.014610304891898731
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.014610304891898731
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch2/predictions.jsonl
✓ Metrics from: runs/8B_hero_stq_m32_v3/eval_epoch2/metrics.json
  Text F1:     Llama 0.809 | Qwen 0.852
  Latent F1:   Llama 0.002 | Qwen 0.012
  FirstTok@1:  Llama 0.005 | Qwen 0.000
  Compression: Llama×7.658 | Qwen×7.240
Top 5 latent predictions from runs/8B_hero_stq_m32_v3/eval_epoch2/predictions.jsonl
  1. Llama: 1 | Qwen: 1. **What is the significance of the phrase "The best laid plans of | Gold: linear
  2. Llama: 1 | Qwen:  (1993)  -   A    B | Gold: Lampea
  3. Llama: 1. 2. 3. 4. 5. 6 | Qwen:  (1998) 1998  |  | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1. 2. 3. 4. 5. 6 | Qwen: 1. **What is the main idea of the passage?** | Gold: San Jose
  5. Llama: 1 | Qwen:  (1994)  -   A       | Gold: oxides

=========================================
EPOCH 3/12
=========================================

Running pre-train eval on existing checkpoint...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch3_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3094.29it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.01s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.61s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2609.62it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.33s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 33.64s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.002  |  NLL/token (gold): 10.789257227969008
       First-token acc: top1=0.005  top5=0.005
Qwen   EM: 0.000   F1: 0.012  |  NLL/token (gold): 10.266884765927754
       First-token acc: top1=0.000  top5=0.070
Wall clock: 39.50s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 28.57s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.011
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.015

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 33.635950326919556
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0023611110701388894,
      "nll": 10.789257227969008,
      "first_token_top1": 0.004999999888241291,
      "first_token_top5": 0.004999999888241291,
      "nll_token": 10.789257227969008
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.012249193821759841,
      "nll": 10.266884765927754,
      "first_token_top1": 0.0,
      "first_token_top5": 0.07000000029802322,
      "nll_token": 10.266884765927754
    },
    "wall_clock_sec": 39.4967896938324
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 28.567622661590576
  },
  "joint": {
    "em": 0.0,
    "f1": 0.010680097357568641,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.014610304891898731
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.014610304891898731
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch3_pre/predictions.jsonl
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2761.68it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.83s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3342.07it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.79s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hero_stq_m32_v3/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=8, global_step=560
Epoch 9/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/100 | grad_norm=75.46 | sec/step~7.37 | keep=1.00 | K=8 | llama: tf=10.1167 first=8.0910 kCE=5.7103 KD=3.7990 man=0.0001 | scale_pen(llama)=1.2367e-11 | qwen: tf=9.4017 first=8.8564 kCE=10.9866 KD=4.9841 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  20/100 | grad_norm=181.34 | sec/step~7.93 | keep=1.00 | K=8 | llama: tf=11.2727 first=9.6892 kCE=5.7103 KD=3.0916 man=0.0001 | scale_pen(llama)=1.2367e-11 | qwen: tf=9.5016 first=10.2386 kCE=11.0835 KD=4.0608 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  30/100 | grad_norm=277.07 | sec/step~7.07 | keep=1.00 | K=8 | llama: tf=10.3283 first=9.8840 kCE=5.9313 KD=3.7074 man=0.0001 | scale_pen(llama)=1.2367e-11 | qwen: tf=9.9828 first=12.3892 kCE=10.1621 KD=4.9388 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  40/100 | grad_norm=58.96 | sec/step~6.93 | keep=1.00 | K=8 | llama: tf=10.1879 first=8.7953 kCE=5.8134 KD=3.5501 man=0.0001 | scale_pen(llama)=1.3657e-11 | qwen: tf=9.3962 first=10.3267 kCE=9.1420 KD=4.5517 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  50/100 | grad_norm=118.69 | sec/step~6.67 | keep=1.00 | K=8 | llama: tf=9.3358 first=8.2008 kCE=5.8892 KD=3.9485 man=0.0001 | scale_pen(llama)=1.3657e-11 | qwen: tf=7.8657 first=7.9011 kCE=8.5224 KD=4.5540 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  60/100 | grad_norm=180.53 | sec/step~6.95 | keep=1.00 | K=8 | llama: tf=10.7222 first=8.8733 kCE=5.9921 KD=3.2856 man=0.0001 | scale_pen(llama)=1.3657e-11 | qwen: tf=9.5314 first=10.5309 kCE=9.6874 KD=4.6670 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  70/100 | grad_norm=46.77 | sec/step~6.38 | keep=1.00 | K=8 | llama: tf=10.1569 first=8.3086 kCE=5.7843 KD=3.6657 man=0.0001 | scale_pen(llama)=1.0747e-11 | qwen: tf=9.3452 first=8.8858 kCE=9.8031 KD=4.4023 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  80/100 | grad_norm=110.35 | sec/step~8.17 | keep=1.00 | K=8 | llama: tf=10.7533 first=8.8438 kCE=6.0043 KD=3.4695 man=0.0001 | scale_pen(llama)=1.0747e-11 | qwen: tf=11.4279 first=11.6762 kCE=9.2947 KD=5.0361 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  90/100 | grad_norm=174.23 | sec/step~6.89 | keep=1.00 | K=8 | llama: tf=9.6411 first=9.0883 kCE=5.8482 KD=3.4379 man=0.0001 | scale_pen(llama)=1.0747e-11 | qwen: tf=10.0917 first=10.3470 kCE=9.3570 KD=4.9283 man=0.0002 | scale_pen(qwen)=2.8777e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
  step  100/100 | grad_norm=28.70 | sec/step~6.75 | keep=1.00 | K=8 | llama: tf=11.0875 first=8.8669 kCE=5.3588 KD=3.1762 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=9.8195 first=9.1273 kCE=8.9984 KD=4.3866 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01365]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hero_stq_m32_v3/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010559869976714254, 'rms_mean_cal': 0.010571582689881325, 'embed_rms': 0.01057388074696064, 'count': 100}, 'qwen': {'rms_mean_raw': 0.013677011765539647, 'rms_mean_cal': 0.01364018046297133, 'embed_rms': 0.013650601729750633, 'count': 100}}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/epoch3/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch3/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3181.12it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.54s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.58s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.44s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2932.57it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.80s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.45s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.55s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 34.60s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.003  |  NLL/token (gold): 10.404735472196895
       First-token acc: top1=0.010  top5=0.030
Qwen   EM: 0.000   F1: 0.007  |  NLL/token (gold): 10.193476339812001
       First-token acc: top1=0.000  top5=0.055
Wall clock: 38.73s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 28.19s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.010

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 34.597195863723755
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.003166666623916668,
      "nll": 10.404735472196895,
      "first_token_top1": 0.009999999776482582,
      "first_token_top5": 0.029999999329447746,
      "nll_token": 10.404735472196895
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.00743346202308816,
      "nll": 10.193476339812001,
      "first_token_top1": 0.0,
      "first_token_top5": 0.054999999701976776,
      "nll_token": 10.193476339812001
    },
    "wall_clock_sec": 38.72979664802551
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 28.19061851501465
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004840625889699314,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.01007381288246189
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.01007381288246189
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch3/predictions.jsonl
✓ Metrics from: runs/8B_hero_stq_m32_v3/eval_epoch3/metrics.json
  Text F1:     Llama 0.809 | Qwen 0.852
  Latent F1:   Llama 0.003 | Qwen 0.007
  FirstTok@1:  Llama 0.010 | Qwen 0.000
  Compression: Llama×7.658 | Qwen×7.240
Top 5 latent predictions from runs/8B_hero_stq_m32_v3/eval_epoch3/predictions.jsonl
  1. Llama: 1 | Qwen: A. 1922年 | Gold: linear
  2. Llama: 1 | Qwen: A. 1927年 | Gold: Lampea
  3. Llama: The & The & The & The &  | Qwen: A. 1927年 | Gold: residents willing to pay higher market rate for housing
  4. Llama: 1 | Qwen: A. 1927年 | Gold: San Jose
  5. Llama: 1 | Qwen: A. 1922年 | Gold: oxides

=========================================
EPOCH 4/12
=========================================

Running pre-train eval on existing checkpoint...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch4_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3279.36it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.12s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.68s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.24s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3387.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 32.06s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.003  |  NLL/token (gold): 10.404735472196895
       First-token acc: top1=0.010  top5=0.030
Qwen   EM: 0.000   F1: 0.007  |  NLL/token (gold): 10.193476339812001
       First-token acc: top1=0.000  top5=0.055
Wall clock: 37.26s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 26.38s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.005
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.010

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 32.06131672859192
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.003166666623916668,
      "nll": 10.404735472196895,
      "first_token_top1": 0.009999999776482582,
      "first_token_top5": 0.029999999329447746,
      "nll_token": 10.404735472196895
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.00743346202308816,
      "nll": 10.193476339812001,
      "first_token_top1": 0.0,
      "first_token_top5": 0.054999999701976776,
      "nll_token": 10.193476339812001
    },
    "wall_clock_sec": 37.25777721405029
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 26.38001585006714
  },
  "joint": {
    "em": 0.0,
    "f1": 0.004840625889699314,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.01007381288246189
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.01007381288246189
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch4_pre/predictions.jsonl
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3871.07it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.78s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.67s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.80s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3303.90it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.29s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.44s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⏪ Resuming from: runs/8B_hero_stq_m32_v3/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
[DEBUG] Optimizer state devices:
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
  param_dev=cuda:0 state_devs={'step': 'cuda:0', 'exp_avg': 'cuda:0', 'exp_avg_sq': 'cuda:0'}
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=9, global_step=660
Epoch 10/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/100 | grad_norm=84.46 | sec/step~6.05 | keep=1.00 | K=8 | llama: tf=10.3349 first=7.7697 kCE=5.6961 KD=3.7939 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=10.5327 first=8.7412 kCE=8.2632 KD=4.8451 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  20/100 | grad_norm=195.89 | sec/step~6.59 | keep=1.00 | K=8 | llama: tf=10.7230 first=8.4488 kCE=5.7778 KD=3.6544 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=11.0718 first=9.1843 kCE=9.6602 KD=4.9722 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  30/100 | grad_norm=293.86 | sec/step~7.93 | keep=1.00 | K=8 | llama: tf=10.7292 first=8.9813 kCE=5.9391 KD=3.0034 man=0.0001 | scale_pen(llama)=4.6043e-12 | qwen: tf=11.0838 first=9.9174 kCE=9.3651 KD=4.7710 man=0.0002 | scale_pen(qwen)=1.2790e-13 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  40/100 | grad_norm=39.18 | sec/step~6.55 | keep=1.00 | K=8 | llama: tf=10.3355 first=7.8970 kCE=5.6126 KD=3.4694 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=9.8931 first=9.0549 kCE=8.1800 KD=4.8911 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  50/100 | grad_norm=84.00 | sec/step~6.49 | keep=1.00 | K=8 | llama: tf=9.7071 first=8.4927 kCE=6.1490 KD=3.8467 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=9.9571 first=10.0292 kCE=9.5086 KD=5.0631 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  60/100 | grad_norm=131.37 | sec/step~6.17 | keep=1.00 | K=8 | llama: tf=10.1723 first=8.2231 kCE=5.4522 KD=3.3044 man=0.0001 | scale_pen(llama)=1.2825e-12 | qwen: tf=7.7204 first=9.2606 kCE=8.9193 KD=4.1468 man=0.0002 | scale_pen(qwen)=5.6843e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  70/100 | grad_norm=26.47 | sec/step~6.22 | keep=1.00 | K=8 | llama: tf=10.1232 first=7.3023 kCE=6.1884 KD=4.1812 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=9.2089 first=8.8864 kCE=8.0315 KD=5.2114 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  80/100 | grad_norm=80.84 | sec/step~6.87 | keep=1.00 | K=8 | llama: tf=10.1093 first=7.9387 kCE=5.9242 KD=3.4270 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.9682 first=9.8353 kCE=8.7450 KD=5.0692 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  90/100 | grad_norm=118.23 | sec/step~7.27 | keep=1.00 | K=8 | llama: tf=10.6575 first=7.9370 kCE=5.8723 KD=3.4860 man=0.0001 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.4651 first=9.4541 kCE=9.8515 KD=5.0533 man=0.0002 | scale_pen(qwen)=1.4211e-14 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
  step  100/100 | grad_norm=18.94 | sec/step~6.97 | keep=1.00 | K=8 | llama: tf=10.5532 first=8.1723 kCE=5.8290 KD=3.7312 man=0.0001 | scale_pen(llama)=1.1511e-12 | qwen: tf=11.3677 first=9.6859 kCE=9.9709 KD=4.7174 man=0.0002 | scale_pen(qwen)=0.0000e+00 | K=8 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01057; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01363]
[checkpoint] Freed 1.9KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_hero_stq_m32_v3/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010563585115596652, 'rms_mean_cal': 0.010571655249223114, 'embed_rms': 0.010573300532996655, 'count': 100}, 'qwen': {'rms_mean_raw': 0.013676938703283667, 'rms_mean_cal': 0.013641893761232496, 'embed_rms': 0.013628016225993633, 'count': 100}}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/epoch4/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch4/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3144.75it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3332.78it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:02,  1.42s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.23s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.35s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 1164800 bytes (6-bit selected); fp16 reference: 2867200 bytes; fp32 reference: 5734400 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.852   |  NLL/token (gold): 24.624676366308734
Wall clock: 31.76s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.14329021912313
       First-token acc: top1=0.025  top5=0.055
Qwen   EM: 0.000   F1: 0.003  |  NLL/token (gold): 10.114726150477374
       First-token acc: top1=0.025  top5=0.030
Wall clock: 37.91s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.040
Qwen   EM: 0.040   F1: 0.088
Wall clock: 27.10s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.003

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 1164800,
  "payload_bytes_detail": {
    "fp32": 5734400,
    "fp16": 2867200,
    "selected": 1164800
  },
  "wire": {
    "prompt_chars": {
      "llama": 251564,
      "qwen": 221164
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      28,
      256
    ],
    "latent_bytes": {
      "fp32": 5734400,
      "fp16": 2867200,
      "quantized": 1075200,
      "quantized_with_scales": 1164800
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_bits": 6,
    "selected_latent_bytes": 1164800,
    "base_latent_bytes": 28672,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.615,
      "f1": 0.8094276466075133,
      "nll_token": 12.403911339973977
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8516192862752388,
      "nll_token": 24.624676366308734
    },
    "wall_clock_sec": 31.761128425598145
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.0,
      "nll": 10.14329021912313,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.054999999701976776,
      "nll_token": 10.14329021912313
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.002873376516675134,
      "nll": 10.114726150477374,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.029999999329447746,
      "nll_token": 10.114726150477374
    },
    "wall_clock_sec": 37.910897970199585
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04021474843730812
    },
    "qwen": {
      "em": 0.04,
      "f1": 0.08823398758088613
    },
    "wall_clock_sec": 27.096097230911255
  },
  "joint": {
    "em": 0.0,
    "f1": 0.00041666664322916806,
    "agreement": 0.0,
    "oracle": {
      "em": 0.0,
      "f1": 0.002873376516675134
    }
  },
  "debug": {
    "llama": {},
    "qwen": {},
    "settings": {
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.15,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "no",
      "decode": {
        "min_new_tokens": 3,
        "eos_ban_steps": 6,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.002873376516675134
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_hero_stq_m32_v3/eval_epoch4/predictions.jsonl
✓ Metrics from: runs/8B_hero_stq_m32_v3/eval_epoch4/metrics.json
  Text F1:     Llama 0.809 | Qwen 0.852
  Latent F1:   Llama 0.000 | Qwen 0.003
  FirstTok@1:  Llama 0.025 | Qwen 0.025
  Compression: Llama×7.658 | Qwen×7.240
Top 5 latent predictions from runs/8B_hero_stq_m32_v3/eval_epoch4/predictions.jsonl
  1. Llama: The correct answer is: 1. The correct answer is: 1 | Qwen: The term "Cultural Capital" was first proposed by the French sociologist Pierre | Gold: linear
  2. Llama: The & #x00A7; #x00A7 | Qwen: The term "Cultural Capital" was first proposed by the French sociologist Pierre | Gold: Lampea
  3. Llama: The & #x00A7; #x00A7 | Qwen: The term "Cultural Capital" was first proposed by the French sociologist Pierre | Gold: residents willing to pay higher market rate for housing
  4. Llama: The correct answer is: 3 | Qwen: The term "Cultural Capital" was first proposed by the French sociologist Pierre | Gold: San Jose
  5. Llama: The correct answer is: 3 | Qwen: The term "Cultural Capital" was first proposed by the French sociologist Pierre | Gold: oxides

=========================================
EPOCH 5/12
=========================================

Running pre-train eval on existing checkpoint...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_hero_stq_m32_v3/ckpt/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_hero_stq_m32_v3/eval_epoch5_pre/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2922.86it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.96s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.37s/it]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 763.43it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.26s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.25s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
