Starting GPU monitoring → runs/smoke/gpu_monitor.log

>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=smoke
    EPOCHS_STAGEA=4 | EPOCHS_STAGEB=8
    WARMUP_TEXT_LATENT_EPOCHS_STAGEA=2.0 | WARMUP_TEXT_LATENT_EPOCHS_STAGEB=1.5

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3651.19it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.34s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.36s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[meta-llama/Meta-Llama-3.1-8B-Instruct] Initializing 3 latent adapters at layers (5, 10, 15)
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 5 on device cuda:0
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 10 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 15 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Latent adapters: 459,399,171 trainable parameters

🔧 Applying LoRA (r=8, alpha=8)...
   Llama BEFORE LoRA: 0 trainable / 8,030,261,248 total
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
   Llama AFTER LoRA:  6,815,744 trainable / 8,037,076,992 total
   ✓ Added 6,815,744 LoRA parameters to Llama
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:4.8GB(6%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 17.0GB allocated, 17.1GB reserved, 323.1GB free, Peak: 17.0GB
[INFO] llama anchor tokens: 3
[Optimization] Using fused AdamW optimizer
[INFO] LR scheduler: CosineAnnealingLR (T_max=120, eta_min=1.00e-06)
⚠️  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 60 steps
Epoch 1/4
[Epoch 1 Start] [GPU Memory] GPU0:6.1GB(10%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 18.3GB allocated, 21.0GB reserved, 319.1GB free, Peak: 21.0GB
[warmup] step=0 mode=text (warm-up)
    [Memory after encoder] 18.4GB allocated
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/30 | (warm-up text) | align=0.0002 | text_tf=16.2679 | latent_scale=0.00
    [Memory after backward] 20.7GB allocated, peak 175.4GB
[warmup] step=1 mode=latent (warm-up)
    [Memory after encoder] 20.8GB allocated
[INFO] KD teacher: adapters disabled successfully (clean text baseline)
    [Memory after backward] 21.7GB allocated, peak 175.4GB
[warmup] step=2 mode=text (warm-up)
    [Memory after encoder] 21.0GB allocated
  step  3/30 | (warm-up text) | align=0.0002 | text_tf=15.9419 | latent_scale=0.02
    [Memory after backward] 21.7GB allocated, peak 229.9GB
[warmup] step=3 mode=latent (warm-up)
[warmup] step=4 mode=text (warm-up)
  step  5/30 | (warm-up text) | align=0.0002 | text_tf=14.8455 | latent_scale=0.03
[warmup] step=5 mode=latent (warm-up)
[warmup] step=6 mode=text (warm-up)
  step  7/30 | (warm-up text) | align=0.0002 | text_tf=12.8386 | latent_scale=0.05
[warmup] step=7 mode=latent (warm-up)
[warmup] step=8 mode=text (warm-up)
  step  9/30 | (warm-up text) | align=0.0002 | text_tf=14.8582 | latent_scale=0.07
[warmup] step=9 mode=latent (warm-up)
  step  10/30 | grad_norm=66.53 | sec/step~3.50 | lr=5.00e-05 | keep=0.70 | K=8 | first_w=0.27 | llama(L): tf=14.1875 first=13.5523 kCE=14.1016 KD=5.5316 acc=0.000 state=13.7692 ent=9.200 align=0.0000 latA=0.4992 latP=0.2507 | scale_pen(llama)=9.0949e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  11/30 | (warm-up text) | align=0.0002 | text_tf=15.4055 | latent_scale=0.08
  step  13/30 | (warm-up text) | align=0.0002 | text_tf=13.7656 | latent_scale=0.10
  step  15/30 | (warm-up text) | align=0.0002 | text_tf=15.4345 | latent_scale=0.12
  step  17/30 | (warm-up text) | align=0.0002 | text_tf=14.1317 | latent_scale=0.13
  step  19/30 | (warm-up text) | align=0.0002 | text_tf=13.6525 | latent_scale=0.15
  step  20/30 | grad_norm=861.37 | sec/step~3.56 | lr=5.00e-05 | keep=0.71 | K=8 | first_w=0.57 | llama(L): tf=44.2500 first=48.9379 kCE=49.0312 KD=76.3623 acc=0.000 state=14.7639 ent=0.007 align=0.0000 latA=0.5004 latP=0.2506 | scale_pen(llama)=7.9936e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  21/30 | (warm-up text) | align=0.0002 | text_tf=12.6843 | latent_scale=0.17
  step  23/30 | (warm-up text) | align=0.0002 | text_tf=14.9133 | latent_scale=0.18
  step  25/30 | (warm-up text) | align=0.0002 | text_tf=14.4624 | latent_scale=0.20
  step  27/30 | (warm-up text) | align=0.0002 | text_tf=12.1673 | latent_scale=0.22
  step  29/30 | (warm-up text) | align=0.0002 | text_tf=13.5628 | latent_scale=0.23
  step  30/30 | grad_norm=186.15 | sec/step~3.62 | lr=4.99e-05 | keep=0.72 | K=8 | first_w=0.87 | llama(L): tf=18.2500 first=21.9461 kCE=19.1797 KD=11.9174 acc=0.000 state=14.9742 ent=1.133 align=0.0000 latA=0.4993 latP=0.2502 | scale_pen(llama)=0.0000e+00 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
Epoch 2/4
[Epoch 2 Start] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  1/30 | (warm-up text) | align=0.0002 | text_tf=11.7353 | latent_scale=0.25
  step  3/30 | (warm-up text) | align=0.0002 | text_tf=12.3720 | latent_scale=0.27
  step  5/30 | (warm-up text) | align=0.0002 | text_tf=12.1128 | latent_scale=0.28
  step  7/30 | (warm-up text) | align=0.0002 | text_tf=11.8601 | latent_scale=0.30
  step  9/30 | (warm-up text) | align=0.0002 | text_tf=12.5792 | latent_scale=0.32
  step  10/30 | grad_norm=90.58 | sec/step~3.52 | lr=4.99e-05 | keep=0.73 | K=8 | first_w=1.17 | llama(L): tf=12.6875 first=13.1985 kCE=13.2422 KD=6.2789 acc=0.031 [✓'a'] state=15.0323 ent=3.678 align=0.0000 latA=0.4995 latP=0.2498 | scale_pen(llama)=3.5527e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 179.9GB
  step  11/30 | (warm-up text) | align=0.0002 | text_tf=11.6796 | latent_scale=0.33
  step  13/30 | (warm-up text) | align=0.0002 | text_tf=12.5404 | latent_scale=0.35
  step  15/30 | (warm-up text) | align=0.0002 | text_tf=11.7055 | latent_scale=0.37
  step  17/30 | (warm-up text) | align=0.0002 | text_tf=10.4650 | latent_scale=0.38
  step  19/30 | (warm-up text) | align=0.0002 | text_tf=11.6055 | latent_scale=0.40
  step  20/30 | grad_norm=47.21 | sec/step~3.43 | lr=4.98e-05 | keep=0.75 | K=8 | first_w=1.47 | llama(L): tf=13.6250 first=11.8216 kCE=13.4219 KD=5.2389 acc=0.000 state=14.9088 ent=9.742 align=0.0000 latA=0.4994 latP=0.2492 | scale_pen(llama)=8.8818e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 189.0GB
  step  21/30 | (warm-up text) | align=0.0002 | text_tf=10.6039 | latent_scale=0.42
  step  23/30 | (warm-up text) | align=0.0002 | text_tf=10.7215 | latent_scale=0.43
  step  25/30 | (warm-up text) | align=0.0002 | text_tf=10.2405 | latent_scale=0.45
  step  27/30 | (warm-up text) | align=0.0002 | text_tf=11.5066 | latent_scale=0.47
  step  29/30 | (warm-up text) | align=0.0002 | text_tf=10.3488 | latent_scale=0.48
  step  30/30 | grad_norm=64.77 | sec/step~3.45 | lr=4.97e-05 | keep=0.77 | K=8 | first_w=1.77 | llama(L): tf=14.3125 first=12.6020 kCE=14.3047 KD=5.4513 acc=0.000 state=14.2384 ent=9.731 align=0.0000 latA=0.5019 latP=0.2491 | scale_pen(llama)=3.5527e-15 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 232.4GB
Epoch 3/4
[Epoch 3 Start] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 232.4GB
  step  10/30 | grad_norm=80.01 | sec/step~3.48 | lr=4.96e-05 | keep=0.80 | K=8 | first_w=2.07 | llama(L): tf=14.2500 first=11.9741 kCE=14.2109 KD=5.1935 acc=0.000 state=15.6638 ent=9.851 align=0.0000 latA=0.5021 latP=0.2490 | scale_pen(llama)=2.2737e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  step  20/30 | grad_norm=39.35 | sec/step~3.34 | lr=4.95e-05 | keep=0.83 | K=8 | first_w=2.37 | llama(L): tf=12.6875 first=10.7752 kCE=13.1172 KD=5.1877 acc=0.000 state=13.7523 ent=9.062 align=0.0000 latA=0.4992 latP=0.2491 | scale_pen(llama)=1.4211e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  🌟 NEW PEAK: first_acc_ema=1.2% (raw_batch=3.1%) at step 85 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='The' | gold='men'
        ✗ pred='The' | gold='is'
        ✗ pred='The' | gold='142'
        ✗ pred='The' | gold='external'
        ✗ pred='The' | gold='during'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'The'(32) 
  step  30/30 | grad_norm=66.51 | sec/step~3.51 | lr=4.93e-05 | keep=0.87 | K=8 | first_w=2.67 | llama(L): tf=12.3750 first=11.6583 kCE=12.4531 KD=5.0599 acc=0.000 state=14.6436 ent=7.013 align=0.0000 latA=0.5012 latP=0.2491 | scale_pen(llama)=1.2790e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
Epoch 4/4
[Epoch 4 Start] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  step  10/30 | grad_norm=46.16 | sec/step~3.56 | lr=4.92e-05 | keep=0.91 | K=8 | first_w=2.97 | llama(L): tf=10.6875 first=8.9578 kCE=10.7227 KD=5.1703 acc=0.000 state=13.7771 ent=7.598 align=0.0000 latA=0.5016 latP=0.2493 | scale_pen(llama)=8.8818e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  🌟 NEW PEAK: first_acc_ema=1.4% (raw_batch=6.2%) at step 103 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='13'
        ✗ pred='the' | gold='20'
        ✗ pred='the' | gold='King'
        ✗ pred='the' | gold='seven'
        ✗ pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=1.9% (raw_batch=6.2%) at step 104 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='visual'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='G'
        ✗ pred='the' | gold='a'
        ✗ pred='the' | gold='God'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=2.1% (raw_batch=6.2%) at step 108 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='240'
        ✗ pred='the' | gold='Tro'
        ✗ pred='the' | gold='M'
        ✗ pred='the' | gold='Joseph'
        ✗ pred='the' | gold='cross'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=2.2% (raw_batch=3.1%) at step 109 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='0'
        ✗ pred='the' | gold='187'
        ✗ pred='the' | gold='collapse'
        ✗ pred='the' | gold='Charles'
        ✗ pred='the' | gold='US'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=31.38 | sec/step~3.55 | lr=4.90e-05 | keep=0.95 | K=8 | first_w=3.27 | llama(L): tf=11.3750 first=9.2615 kCE=11.7266 KD=5.0302 acc=0.031 [✓'the'] state=13.8218 ent=8.599 align=0.0000 latA=0.5022 latP=0.2493 | scale_pen(llama)=1.4211e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  🌟 NEW PEAK: first_acc_ema=2.3% (raw_batch=3.1%) at step 110 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='M'
        ✗ pred='the' | gold='System'
        ✗ pred='the' | gold='Sir'
        ✗ pred='the' | gold='an'
        ✗ pred='the' | gold='January'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=2.7% (raw_batch=6.2%) at step 111 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='William'
        ✗ pred='the' | gold='Nintendo'
        ✗ pred='the' | gold='incre'
        ✗ pred='the' | gold='Lib'
        ✗ pred='the' | gold='K'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=3.1% (raw_batch=6.2%) at step 112 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='73'
        ✗ pred='the' | gold='it'
        ✗ pred='the' | gold='computer'
        ✗ pred='the' | gold='civil'
        ✗ pred='the' | gold='working'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=3.1% (raw_batch=6.2%) at step 115 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='V'
        ✗ pred='the' | gold='limitations'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='as'
        ✗ pred='the' | gold='neo'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=3.5% (raw_batch=6.2%) at step 116 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Spanish'
        ✗ pred='the' | gold='East'
        ✗ pred='the' | gold='Weather'
        ✗ pred='the' | gold='bee'
        ✗ pred='the' | gold='The'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.0% (raw_batch=9.4%) at step 117 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='20'
        ✗ pred='the' | gold='Earth'
        ✗ pred='the' | gold='Er'
        ✗ pred='the' | gold='his'
        ✗ pred='the' | gold='h'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.3% (raw_batch=6.2%) at step 118 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='United'
        ✗ pred='the' | gold='order'
        ✗ pred='the' | gold='50'
        ✗ pred='the' | gold='burn'
        ✗ pred='the' | gold='amm'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.8% (raw_batch=9.4%) at step 119 → saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='July'
        ✗ pred='the' | gold='to'
        ✗ pred='the' | gold='six'
        ✗ pred='the' | gold='201'
        ✗ pred='the' | gold='Pers'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=31.03 | sec/step~3.39 | lr=4.88e-05 | keep=1.00 | K=8 | first_w=3.57 | llama(L): tf=10.8750 first=10.3493 kCE=10.8594 KD=4.7057 acc=0.031 [✓'the'] state=12.8108 ent=9.136 align=0.0000 latA=0.5011 latP=0.2496 | scale_pen(llama)=3.5527e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
[checkpoint] Freed 3.7KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/smoke/ckpt/stageA
📝 Saved LoRA adapters for Llama
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0000211109717687, 'rms_mean_cal': 0.010571840847842396, 'embed_rms': 0.01057521253824234, 'count': 120}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3438.66it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.25s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[meta-llama/Meta-Llama-3.1-8B-Instruct] Initializing 3 latent adapters at layers (5, 10, 15)
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 5 on device cuda:0
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 10 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 15 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Latent adapters: 459,399,171 trainable parameters

🔧 Applying LoRA (r=8, alpha=8)...
   Llama BEFORE LoRA: 0 trainable / 8,030,261,248 total
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
   Llama AFTER LoRA:  6,815,744 trainable / 8,037,076,992 total
   ✓ Added 6,815,744 LoRA parameters to Llama
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:4.8GB(6%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 17.0GB allocated, 17.1GB reserved, 323.1GB free, Peak: 17.0GB
[INFO] llama anchor tokens: 3
[Optimization] Using fused AdamW optimizer
[INFO] LR scheduler: CosineAnnealingLR (T_max=240, eta_min=1.00e-06)
⏪ Resuming from: runs/smoke/ckpt/stageA/state.pt
   -> latent adapters 'llama' missing in state.pt; will retry from disk
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 45 steps
Epoch 1/8
[Epoch 1 Start] [GPU Memory] GPU0:6.1GB(10%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 18.3GB allocated, 21.0GB reserved, 319.1GB free, Peak: 21.0GB
[warmup] step=0 mode=text (warm-up)
    [Memory after encoder] 18.4GB allocated
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/30 | (warm-up text) | align=0.0003 | text_tf=16.2679 | latent_scale=0.20
    [Memory after backward] 20.7GB allocated, peak 176.2GB
[warmup] step=1 mode=latent (warm-up)
    [Memory after encoder] 20.8GB allocated
[INFO] KD teacher: adapters disabled successfully (clean text baseline)
    [Memory after backward] 21.7GB allocated, peak 176.2GB
[warmup] step=2 mode=text (warm-up)
    [Memory after encoder] 21.0GB allocated
  step  3/30 | (warm-up text) | align=0.0003 | text_tf=15.9419 | latent_scale=0.24
    [Memory after backward] 21.7GB allocated, peak 230.8GB
[warmup] step=3 mode=latent (warm-up)
[warmup] step=4 mode=text (warm-up)
  step  5/30 | (warm-up text) | align=0.0003 | text_tf=14.8455 | latent_scale=0.27
[warmup] step=5 mode=latent (warm-up)
  🌟 NEW PEAK: first_acc_ema=1.4% (raw_batch=6.2%) at step 6 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Executive'
        ✗ pred='the' | gold='micro'
        ✗ pred='the' | gold='DVD'
        ✗ pred='the' | gold='job'
        ✗ pred='assistant' | gold='Greek'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(31) 'assistant'(1) 
[warmup] step=6 mode=text (warm-up)
  step  7/30 | (warm-up text) | align=0.0003 | text_tf=12.8386 | latent_scale=0.31
[warmup] step=7 mode=latent (warm-up)
  🌟 NEW PEAK: first_acc_ema=1.9% (raw_batch=6.2%) at step 8 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Zen'
        ✗ pred='the' | gold='mer'
        ✗ pred='the' | gold='child'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='Mount'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
[warmup] step=8 mode=text (warm-up)
  step  9/30 | (warm-up text) | align=0.0003 | text_tf=14.8582 | latent_scale=0.34
[warmup] step=9 mode=latent (warm-up)
  step  10/30 | grad_norm=8.83 | sec/step~3.66 | lr=5.00e-05 | keep=0.50 | K=8 | first_w=0.27 | llama(L): tf=10.6250 first=9.8062 kCE=10.7734 KD=4.4561 acc=0.000 state=18.6786 ent=9.198 align=0.0000 latA=1.0001 latP=0.4988 | scale_pen(llama)=3.8455e-10 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  11/30 | (warm-up text) | align=0.0003 | text_tf=15.4029 | latent_scale=0.38
  step  13/30 | (warm-up text) | align=0.0003 | text_tf=13.7473 | latent_scale=0.41
  step  15/30 | (warm-up text) | align=0.0003 | text_tf=15.4790 | latent_scale=0.45
  step  17/30 | (warm-up text) | align=0.0003 | text_tf=14.1667 | latent_scale=0.48
  step  19/30 | (warm-up text) | align=0.0003 | text_tf=13.6798 | latent_scale=0.52
  step  20/30 | grad_norm=98.27 | sec/step~3.50 | lr=5.00e-05 | keep=0.50 | K=8 | first_w=0.57 | llama(L): tf=10.6875 first=10.6247 kCE=11.1641 KD=4.5105 acc=0.000 state=11.6787 ent=8.451 align=0.0000 latA=0.9989 latP=0.4985 | scale_pen(llama)=2.5668e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  21/30 | (warm-up text) | align=0.0003 | text_tf=12.6729 | latent_scale=0.56
  step  23/30 | (warm-up text) | align=0.0003 | text_tf=14.8552 | latent_scale=0.59
  step  25/30 | (warm-up text) | align=0.0003 | text_tf=14.4409 | latent_scale=0.63
  step  27/30 | (warm-up text) | align=0.0003 | text_tf=12.1482 | latent_scale=0.66
  step  29/30 | (warm-up text) | align=0.0003 | text_tf=13.5426 | latent_scale=0.70
  step  30/30 | grad_norm=126.68 | sec/step~3.61 | lr=5.00e-05 | keep=0.51 | K=8 | first_w=0.87 | llama(L): tf=11.6875 first=11.8168 kCE=11.1797 KD=4.0538 acc=0.000 state=18.7716 ent=9.430 align=0.0000 latA=0.9944 latP=0.4973 | scale_pen(llama)=6.9633e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
Epoch 2/8
[Epoch 2 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  1/30 | (warm-up text) | align=0.0003 | text_tf=11.6227 | latent_scale=0.73
  step  3/30 | (warm-up text) | align=0.0003 | text_tf=12.3348 | latent_scale=0.77
  step  5/30 | (warm-up text) | align=0.0003 | text_tf=12.0411 | latent_scale=0.80
  step  7/30 | (warm-up text) | align=0.0003 | text_tf=11.7945 | latent_scale=0.84
  step  9/30 | (warm-up text) | align=0.0003 | text_tf=12.5203 | latent_scale=0.88
  step  10/30 | grad_norm=53.82 | sec/step~3.47 | lr=5.00e-05 | keep=0.51 | K=8 | first_w=1.17 | llama(L): tf=11.3125 first=10.2423 kCE=11.2500 KD=4.3579 acc=0.000 state=10.9847 ent=10.723 align=0.0000 latA=0.9964 latP=0.4969 | scale_pen(llama)=6.0041e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
  step  11/30 | (warm-up text) | align=0.0003 | text_tf=11.5840 | latent_scale=0.91
  step  13/30 | (warm-up text) | align=0.0003 | text_tf=12.4219 | latent_scale=0.95
  step  15/30 | (warm-up text) | align=0.0003 | text_tf=11.6398 | latent_scale=0.98
[warmup] step=46 mode=text (tail)
  step  17/30 | (tail text) | align=0.0003 | text_tf=10.3769 | latent_scale=1.00
  step  20/30 | grad_norm=56.24 | sec/step~3.39 | lr=4.99e-05 | keep=0.51 | K=8 | first_w=1.47 | llama(L): tf=10.8750 first=9.8713 kCE=10.8516 KD=4.1340 acc=0.000 state=10.4545 ent=9.234 align=0.0000 latA=0.9964 latP=0.4964 | scale_pen(llama)=0.0000e+00 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 189.9GB
[warmup] step=52 mode=text (tail)
  step  23/30 | (tail text) | align=0.0003 | text_tf=10.6480 | latent_scale=1.00
[warmup] step=54 mode=text (tail)
  step  25/30 | (tail text) | align=0.0003 | text_tf=10.2140 | latent_scale=1.00
[warmup] step=57 mode=text (tail)
  step  28/30 | (tail text) | align=0.0003 | text_tf=10.4567 | latent_scale=1.00
  step  30/30 | grad_norm=55.19 | sec/step~3.54 | lr=4.99e-05 | keep=0.52 | K=8 | first_w=1.77 | llama(L): tf=10.4375 first=10.0334 kCE=10.8516 KD=4.3437 acc=0.000 state=14.4259 ent=7.737 align=0.0000 latA=1.0029 latP=0.4959 | scale_pen(llama)=4.1439e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 192.1GB
Epoch 3/8
[Epoch 3 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 192.1GB
[warmup] step=64 mode=text (tail)
  step  5/30 | (tail text) | align=0.0003 | text_tf=9.3968 | latent_scale=1.00
  step  10/30 | grad_norm=14.85 | sec/step~3.51 | lr=4.99e-05 | keep=0.53 | K=8 | first_w=2.07 | llama(L): tf=9.8125 first=8.9726 kCE=9.8047 KD=4.1127 acc=0.000 state=6.7389 ent=9.092 align=0.0000 latA=1.0039 latP=0.4960 | scale_pen(llama)=1.1141e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
[warmup] step=71 mode=text (tail)
  step  12/30 | (tail text) | align=0.0003 | text_tf=9.3192 | latent_scale=1.00
[warmup] step=72 mode=text (tail)
  step  13/30 | (tail text) | align=0.0003 | text_tf=8.9698 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=1.9% (raw_batch=9.4%) at step 74 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='T'
        ✗ pred='the' | gold='medical'
        ✗ pred='in' | gold='189'
        ✗ pred='the' | gold='super'
        ✗ pred='the' | gold='mes'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(27) 'in'(5) 
  🌟 NEW PEAK: first_acc_ema=2.1% (raw_batch=6.2%) at step 78 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='British'
        ✗ pred='the' | gold='a'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='Pos'
        ✗ pred='the' | gold='they'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(27) 'in'(5) 
  step  20/30 | grad_norm=13.87 | sec/step~3.45 | lr=4.99e-05 | keep=0.54 | K=8 | first_w=2.37 | llama(L): tf=10.0000 first=9.2411 kCE=10.3516 KD=4.2070 acc=0.000 state=4.7097 ent=9.732 align=0.0000 latA=0.9975 latP=0.4959 | scale_pen(llama)=1.8417e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
  🌟 NEW PEAK: first_acc_ema=2.2% (raw_batch=6.2%) at step 81 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='dis'
        ✗ pred='the' | gold='threat'
        ✗ pred='the' | gold='Nor'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='185'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=2.6% (raw_batch=6.2%) at step 82 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='174'
        ✗ pred='the' | gold='V'
        ✗ pred='the' | gold='ann'
        ✗ pred='the' | gold='B'
        ✗ pred='the' | gold='ch'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=2.9% (raw_batch=6.2%) at step 83 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='187'
        ✗ pred='the' | gold='198'
        ✗ pred='the' | gold='first'
        ✗ pred='the' | gold='Small'
        ✓ pred='the' | gold='the'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=3.3% (raw_batch=6.2%) at step 84 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='action'
        ✗ pred='the' | gold='third'
        ✗ pred='the' | gold='Mozilla'
        ✗ pred='the' | gold='War'
        ✓ pred='the' | gold='the'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
[warmup] step=86 mode=text (tail)
  step  27/30 | (tail text) | align=0.0003 | text_tf=9.7433 | latent_scale=1.00
[warmup] step=89 mode=text (tail)
  step  30/30 | (tail text) | align=0.0003 | text_tf=9.4591 | latent_scale=1.00
  step  30/30 | grad_norm=44.76 | sec/step~4.70 | lr=4.98e-05 | keep=0.55 | K=8 | first_w=2.67 | llama(T): tf=10.0000 first=10.0416 kCE=9.9375 KD=0.0000 acc=0.000 state=5.6027 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=6.2670e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
Epoch 4/8
[Epoch 4 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
  🌟 NEW PEAK: first_acc_ema=3.3% (raw_batch=9.4%) at step 98 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Comm'
        ✗ pred='the' | gold='final'
        ✗ pred='the' | gold='rec'
        ✗ pred='the' | gold='4'
        ✗ pred='the' | gold='Govern'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=14.57 | sec/step~3.49 | lr=4.98e-05 | keep=0.56 | K=8 | first_w=2.97 | llama(L): tf=9.1250 first=8.3742 kCE=8.7500 KD=4.0040 acc=0.062 [✓'the'] state=3.6079 ent=8.879 align=0.0000 latA=1.0026 latP=0.4958 | scale_pen(llama)=1.2367e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 111.8GB
  🌟 NEW PEAK: first_acc_ema=3.5% (raw_batch=6.2%) at step 100 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='mon'
        ✗ pred='the' | gold='18'
        ✗ pred='the' | gold='s'
        ✗ pred='the' | gold='7'
        ✗ pred='the' | gold='N'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=3.7% (raw_batch=6.2%) at step 103 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='13'
        ✗ pred='the' | gold='20'
        ✗ pred='the' | gold='King'
        ✗ pred='the' | gold='seven'
        ✗ pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.0% (raw_batch=6.2%) at step 104 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='visual'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='G'
        ✗ pred='the' | gold='a'
        ✗ pred='the' | gold='God'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=12.37 | sec/step~3.71 | lr=4.97e-05 | keep=0.57 | K=8 | first_w=3.27 | llama(L): tf=9.1250 first=8.1251 kCE=9.1719 KD=4.0130 acc=0.031 [✓'the'] state=3.9793 ent=9.305 align=0.0000 latA=1.0027 latP=0.4961 | scale_pen(llama)=2.5899e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 111.8GB
  step  23/30 | (tail text) | align=0.0003 | text_tf=8.8827 | latent_scale=1.00
  step  30/30 | grad_norm=13.21 | sec/step~3.44 | lr=4.97e-05 | keep=0.59 | K=8 | first_w=3.57 | llama(L): tf=9.6250 first=8.8847 kCE=9.6250 KD=3.8898 acc=0.031 [✓'the'] state=3.5371 ent=8.869 align=0.0000 latA=0.9994 latP=0.4961 | scale_pen(llama)=8.8818e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 165.1GB
Epoch 5/8
[Epoch 5 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 165.1GB
  step  10/30 | grad_norm=12.51 | sec/step~3.52 | lr=4.96e-05 | keep=0.60 | K=8 | first_w=3.87 | llama(L): tf=9.0625 first=8.0573 kCE=9.0938 KD=3.8577 acc=0.000 state=3.3912 ent=8.846 align=0.0000 latA=0.9973 latP=0.4965 | scale_pen(llama)=2.4016e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 109.1GB
  step  14/30 | (tail text) | align=0.0003 | text_tf=8.9082 | latent_scale=1.00
  step  15/30 | (tail text) | align=0.0003 | text_tf=10.6292 | latent_scale=1.00
  step  20/30 | (tail text) | align=0.0003 | text_tf=8.5945 | latent_scale=1.00
  step  20/30 | grad_norm=11.32 | sec/step~4.76 | lr=4.96e-05 | keep=0.62 | K=8 | first_w=4.17 | llama(T): tf=9.0625 first=7.4603 kCE=9.1328 KD=0.0000 acc=0.000 state=3.1291 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=6.8781e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
  step  26/30 | (tail text) | align=0.0003 | text_tf=8.5575 | latent_scale=1.00
  step  30/30 | grad_norm=19.31 | sec/step~4.09 | lr=4.95e-05 | keep=0.64 | K=8 | first_w=4.47 | llama(L): tf=8.8125 first=7.5383 kCE=9.1406 KD=3.7253 acc=0.000 state=3.3390 ent=7.806 align=0.0000 latA=0.9957 latP=0.4965 | scale_pen(llama)=1.1951e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
Epoch 6/8
[Epoch 6 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
  🌟 NEW PEAK: first_acc_ema=4.0% (raw_batch=12.5%) at step 155 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='as'
        ✗ pred='the' | gold='300'
        ✗ pred='the' | gold='Hotel'
        ✗ pred='the' | gold='in'
        ✗ pred='the' | gold='23'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.0% (raw_batch=9.4%) at step 156 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='civil'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='£'
        ✗ pred='the' | gold='151'
        ✗ pred='the' | gold='East'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.3% (raw_batch=15.6%) at step 158 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='degrees'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='675'
        ✗ pred='the' | gold='17'
        ✗ pred='the' | gold='a'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=15.72 | sec/step~3.44 | lr=4.95e-05 | keep=0.65 | K=8 | first_w=4.77 | llama(L): tf=9.4375 first=7.6523 kCE=9.2930 KD=3.5596 acc=0.000 state=3.0124 ent=8.520 align=0.0000 latA=0.9983 latP=0.4965 | scale_pen(llama)=2.2737e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 109.1GB
  🌟 NEW PEAK: first_acc_ema=4.3% (raw_batch=12.5%) at step 169 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='bee'
        ✗ pred='the' | gold='children'
        ✗ pred='the' | gold='single'
        ✗ pred='the' | gold='sc'
        ✗ pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | (tail text) | align=0.0003 | text_tf=9.1251 | latent_scale=1.00
  step  20/30 | grad_norm=14.51 | sec/step~4.67 | lr=4.94e-05 | keep=0.68 | K=8 | first_w=5.07 | llama(T): tf=8.8125 first=7.9470 kCE=8.6406 KD=0.0000 acc=0.000 state=2.7660 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.1141e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
  step  22/30 | (tail text) | align=0.0003 | text_tf=9.0604 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=4.3% (raw_batch=9.4%) at step 176 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Pa'
        ✗ pred='the' | gold='British'
        ✗ pred='the' | gold='p'
        ✗ pred='the' | gold='The'
        ✗ pred='the' | gold='medical'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  27/30 | (tail text) | align=0.0003 | text_tf=8.3781 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=4.4% (raw_batch=9.4%) at step 178 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='six'
        ✗ pred='the' | gold='op'
        ✗ pred='the' | gold='St'
        ✗ pred='the' | gold='Central'
        ✗ pred='the' | gold='There'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=4.9% (raw_batch=9.4%) at step 179 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Theo'
        ✗ pred='the' | gold='Dr'
        ✗ pred='the' | gold='197'
        ✗ pred='the' | gold='Ch'
        ✗ pred='the' | gold='s'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | (tail text) | align=0.0003 | text_tf=8.1266 | latent_scale=1.00
  step  30/30 | grad_norm=25.04 | sec/step~4.72 | lr=4.93e-05 | keep=0.70 | K=8 | first_w=5.37 | llama(T): tf=8.6875 first=7.2199 kCE=8.4531 KD=0.0000 acc=0.031 state=2.6338 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=9.6065e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
Epoch 7/8
[Epoch 7 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
  step  2/30 | (tail text) | align=0.0003 | text_tf=9.4372 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=4.9% (raw_batch=9.4%) at step 184 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='August'
        ✗ pred='the' | gold='190'
        ✗ pred='the' | gold='111'
        ✗ pred='the' | gold='Inter'
        ✗ pred='the' | gold='Russia'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=5.3% (raw_batch=12.5%) at step 187 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='Cr'
        ✗ pred='the' | gold='Greek'
        ✗ pred='the' | gold='l'
        ✗ pred='the' | gold='World'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=6.2%) at step 188 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='contact'
        ✗ pred='the' | gold='My'
        ✗ pred='the' | gold='Mess'
        ✗ pred='the' | gold='quant'
        ✗ pred='the' | gold='Lib'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=13.83 | sec/step~3.53 | lr=4.92e-05 | keep=0.72 | K=8 | first_w=5.67 | llama(L): tf=8.8750 first=7.3206 kCE=8.5938 KD=3.8472 acc=0.031 [✓'the'] state=2.4457 ent=8.125 align=0.0000 latA=0.9931 latP=0.4960 | scale_pen(llama)=2.2204e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 172.9GB
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 196 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='hack'
        ✗ pred='the' | gold='British'
        ✗ pred='the' | gold='po'
        ✗ pred='the' | gold='cell'
        ✗ pred='the' | gold='C'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(31) '$'(1) 
  step  20/30 | grad_norm=17.54 | sec/step~3.54 | lr=4.92e-05 | keep=0.74 | K=8 | first_w=5.97 | llama(L): tf=8.7500 first=7.1836 kCE=8.6602 KD=3.6708 acc=0.062 [✓'the'] state=2.3657 ent=8.090 align=0.0000 latA=0.9962 latP=0.4956 | scale_pen(llama)=1.5948e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 172.9GB
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 203 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='Portal'
        ✗ pred='the' | gold='3'
        ✗ pred='the' | gold='significant'
        ✗ pred='the' | gold='to'
        ✗ pred='the' | gold='7'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  26/30 | (tail text) | align=0.0003 | text_tf=8.2422 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 209 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='multiple'
        ✗ pred='the' | gold='six'
        ✗ pred='the' | gold='import'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='very'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=48.67 | sec/step~3.65 | lr=4.91e-05 | keep=0.77 | K=8 | first_w=6.00 | llama(L): tf=8.8125 first=7.1450 kCE=8.8164 KD=3.9121 acc=0.031 [✓'the'] state=2.9467 ent=7.612 align=0.0000 latA=0.9948 latP=0.4947 | scale_pen(llama)=2.1064e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
Epoch 8/8
[Epoch 8 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
  step  1/30 | (tail text) | align=0.0003 | text_tf=8.3819 | latent_scale=1.00
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 216 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='on'
        ✗ pred='the' | gold='V'
        ✗ pred='the' | gold='J'
        ✗ pred='the' | gold='142'
        ✗ pred='the' | gold='R'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=26.85 | sec/step~3.56 | lr=4.90e-05 | keep=0.79 | K=8 | first_w=6.00 | llama(L): tf=8.4375 first=6.7358 kCE=8.5781 KD=3.7053 acc=0.000 state=2.1188 ent=7.030 align=0.0000 latA=0.9958 latP=0.4945 | scale_pen(llama)=3.6380e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 176.6GB
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 221 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='neo'
        ✗ pred='the' | gold='Greek'
        ✗ pred='the' | gold='122'
        ✗ pred='the' | gold='Small'
        ✗ pred='the' | gold='many'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  12/30 | (tail text) | align=0.0003 | text_tf=8.7053 | latent_scale=1.00
  step  19/30 | (tail text) | align=0.0003 | text_tf=9.9113 | latent_scale=1.00
  step  20/30 | grad_norm=17.52 | sec/step~3.64 | lr=4.89e-05 | keep=0.82 | K=8 | first_w=6.00 | llama(L): tf=8.7500 first=7.2296 kCE=8.6172 KD=3.7091 acc=0.031 [✓'the'] state=2.1219 ent=7.905 align=0.0000 latA=0.9984 latP=0.4942 | scale_pen(llama)=6.2670e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(75%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 234.2GB reserved, 105.9GB free, Peak: 233.2GB
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 233 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='China'
        ✗ pred='the' | gold='Federal'
        ✗ pred='the' | gold='working'
        ✗ pred='the' | gold='An'
        ✗ pred='the' | gold='250'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  🌟 NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 239 → saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        ✗ pred='the' | gold='a'
        ✓ pred='the' | gold='the'
        ✗ pred='the' | gold='10'
        ✗ pred='the' | gold='£'
        ✗ pred='the' | gold='rich'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=20.21 | sec/step~3.62 | lr=4.88e-05 | keep=0.85 | K=8 | first_w=6.00 | llama(L): tf=9.0000 first=7.1369 kCE=8.8594 KD=3.4327 acc=0.062 [✓'the'] state=2.1607 ent=8.181 align=0.0000 latA=0.9965 latP=0.4932 | scale_pen(llama)=1.6428e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(75%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 234.2GB reserved, 105.9GB free, Peak: 233.2GB
[checkpoint] Freed 3.8KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/smoke/ckpt/stageB
📝 Saved LoRA adapters for Llama
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 1.00015849173069, 'rms_mean_cal': 0.010571445862296969, 'embed_rms': 0.01057625375688076, 'count': 240}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/smoke/ckpt/stageB/training_stats.json
Encoder input alignment: mode=neutral_chat | strip_anchor=yes | samples=200
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3761.71it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.19s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.16s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
✓ Loaded deep prefix generator for llama
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

— Text baseline summary:
llama: EM=0.590 F1=0.794
✓ Loaded LoRA adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 246.0 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 13107200 bytes (fp32); fp16 reference: 6553600 bytes; fp32 reference: 13107200 bytes
latent/text bytes (one-copy, fp16): n/a

— Baseline: Text prompting
Llama  EM: 0.590  F1: 0.794  |  NLL/token (gold): 13.675748455854526
Wall clock: 8.95s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.011  |  NLL/token (gold): 9.1625273660197
       First-token acc: top1=0.020  top5=0.075
Wall clock: 1.66s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.010  F1: 0.063
Wall clock: 2.12s

— 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 246.03
  },
  "compression": {
    "llama": 3.84421875
  },
  "payload_bytes": 13107200,
  "payload_bytes_detail": {
    "fp32": 13107200,
    "fp16": 6553600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251558
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 13107200,
      "fp16": 6553600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.7936993517504416,
      "nll_token": 13.675748455854526
    },
    "wall_clock_sec": 8.948624849319458
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.011145275163765487,
      "nll": 9.1625273660197,
      "first_token_top1": 0.02,
      "first_token_top5": 0.075,
      "nll_token": 9.1625273660197
    },
    "wall_clock_sec": 1.6573684215545654
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.01,
      "f1": 0.06337241856366438
    },
    "wall_clock_sec": 2.120539426803589
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Stopping GPU monitoring (PID 597486)
