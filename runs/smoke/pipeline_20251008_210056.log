Starting GPU monitoring â†’ runs/smoke/gpu_monitor.log

>>> Combination 1: m64_dz256_rl2_rh4
    RUN_TAG=smoke
    EPOCHS_STAGEA=4 | EPOCHS_STAGEB=8
    WARMUP_TEXT_LATENT_EPOCHS_STAGEA=2.0 | WARMUP_TEXT_LATENT_EPOCHS_STAGEB=1.5

=== CUDA preflight ===
torch: 2.4.0+cu121 cuda: 12.1 available: True
CUDA_VISIBLE_DEVICES: 0,1,2,3
PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True

=== Stage A: Llama latent fit ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3534.28it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.32s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.21s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.17s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.04it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[meta-llama/Meta-Llama-3.1-8B-Instruct] Initializing 3 latent adapters at layers (5, 10, 15)
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 5 on device cuda:0
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 10 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 15 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Latent adapters: 459,399,171 trainable parameters

ðŸ”§ Applying LoRA (r=8, alpha=8)...
   Llama BEFORE LoRA: 0 trainable / 8,030,261,248 total
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
   Llama AFTER LoRA:  6,815,744 trainable / 8,037,076,992 total
   âœ“ Added 6,815,744 LoRA parameters to Llama
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:4.8GB(6%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 17.0GB allocated, 17.1GB reserved, 323.1GB free, Peak: 17.0GB
[INFO] llama anchor tokens: 3
[Optimization] Using fused AdamW optimizer
[INFO] LR scheduler: CosineAnnealingLR (T_max=120, eta_min=1.00e-06)
âš ï¸  No valid checkpoint found to resume; starting fresh.
[warmup] alternating text/latent for first 60 steps
Epoch 1/4
[Epoch 1 Start] [GPU Memory] GPU0:6.1GB(10%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 18.3GB allocated, 21.0GB reserved, 319.1GB free, Peak: 21.0GB
[warmup] step=0 mode=text (warm-up)
    [Memory after encoder] 18.4GB allocated
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/30 | (warm-up text) | align=0.0002 | text_tf=16.2679 | latent_scale=0.00
    [Memory after backward] 20.7GB allocated, peak 175.4GB
[warmup] step=1 mode=latent (warm-up)
    [Memory after encoder] 20.8GB allocated
    [Memory after backward] 21.7GB allocated, peak 175.4GB
[warmup] step=2 mode=text (warm-up)
    [Memory after encoder] 21.0GB allocated
  step  3/30 | (warm-up text) | align=0.0002 | text_tf=15.9419 | latent_scale=0.02
    [Memory after backward] 21.7GB allocated, peak 229.9GB
[warmup] step=3 mode=latent (warm-up)
[warmup] step=4 mode=text (warm-up)
  step  5/30 | (warm-up text) | align=0.0002 | text_tf=14.8455 | latent_scale=0.03
[warmup] step=5 mode=latent (warm-up)
[warmup] step=6 mode=text (warm-up)
  step  7/30 | (warm-up text) | align=0.0002 | text_tf=12.8386 | latent_scale=0.05
[warmup] step=7 mode=latent (warm-up)
[warmup] step=8 mode=text (warm-up)
  step  9/30 | (warm-up text) | align=0.0002 | text_tf=14.8582 | latent_scale=0.07
[warmup] step=9 mode=latent (warm-up)
  step  10/30 | grad_norm=66.53 | sec/step~3.44 | lr=5.00e-05 | keep=0.70 | K=8 | first_w=0.27 | llama(L): tf=14.1875 first=13.5523 kCE=14.1016 KD=5.5316 acc=0.000 state=13.7692 ent=9.200 align=0.0000 latA=0.4992 latP=0.2507 | scale_pen(llama)=5.1159e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  11/30 | (warm-up text) | align=0.0002 | text_tf=15.4055 | latent_scale=0.08
  step  13/30 | (warm-up text) | align=0.0002 | text_tf=13.7656 | latent_scale=0.10
  step  15/30 | (warm-up text) | align=0.0002 | text_tf=15.4345 | latent_scale=0.12
  step  17/30 | (warm-up text) | align=0.0002 | text_tf=14.1317 | latent_scale=0.13
  step  19/30 | (warm-up text) | align=0.0002 | text_tf=13.6525 | latent_scale=0.15
  step  20/30 | grad_norm=861.25 | sec/step~3.57 | lr=5.00e-05 | keep=0.71 | K=8 | first_w=0.57 | llama(L): tf=44.2500 first=48.9199 kCE=49.0312 KD=76.3742 acc=0.000 state=14.7638 ent=0.007 align=0.0000 latA=0.5004 latP=0.2506 | scale_pen(llama)=6.0041e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  21/30 | (warm-up text) | align=0.0002 | text_tf=12.6625 | latent_scale=0.17
  step  23/30 | (warm-up text) | align=0.0002 | text_tf=14.8949 | latent_scale=0.18
  step  25/30 | (warm-up text) | align=0.0002 | text_tf=14.4473 | latent_scale=0.20
  step  27/30 | (warm-up text) | align=0.0002 | text_tf=12.1620 | latent_scale=0.22
  step  29/30 | (warm-up text) | align=0.0002 | text_tf=13.5219 | latent_scale=0.23
  step  30/30 | grad_norm=186.11 | sec/step~3.58 | lr=4.99e-05 | keep=0.72 | K=8 | first_w=0.87 | llama(L): tf=18.2500 first=21.9355 kCE=19.1797 KD=11.9162 acc=0.000 state=14.9742 ent=1.136 align=0.0000 latA=0.4993 latP=0.2502 | scale_pen(llama)=3.5527e-15 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
Epoch 2/4
[Epoch 2 Start] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 229.9GB
  step  1/30 | (warm-up text) | align=0.0002 | text_tf=11.7314 | latent_scale=0.25
  step  3/30 | (warm-up text) | align=0.0002 | text_tf=12.4161 | latent_scale=0.27
  step  5/30 | (warm-up text) | align=0.0002 | text_tf=12.1222 | latent_scale=0.28
  step  7/30 | (warm-up text) | align=0.0002 | text_tf=11.8578 | latent_scale=0.30
  step  9/30 | (warm-up text) | align=0.0002 | text_tf=12.5548 | latent_scale=0.32
  step  10/30 | grad_norm=90.61 | sec/step~3.56 | lr=4.99e-05 | keep=0.73 | K=8 | first_w=1.17 | llama(L): tf=12.6875 first=13.1970 kCE=13.2422 KD=6.2788 acc=0.031 [âœ“'a'] state=15.0322 ent=3.682 align=0.0000 latA=0.4995 latP=0.2498 | scale_pen(llama)=1.2790e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 179.9GB
  step  11/30 | (warm-up text) | align=0.0002 | text_tf=11.7013 | latent_scale=0.33
  step  13/30 | (warm-up text) | align=0.0002 | text_tf=12.5344 | latent_scale=0.35
  step  15/30 | (warm-up text) | align=0.0002 | text_tf=11.6924 | latent_scale=0.37
  step  17/30 | (warm-up text) | align=0.0002 | text_tf=10.4719 | latent_scale=0.38
  step  19/30 | (warm-up text) | align=0.0002 | text_tf=11.6099 | latent_scale=0.40
  step  20/30 | grad_norm=47.23 | sec/step~3.43 | lr=4.98e-05 | keep=0.75 | K=8 | first_w=1.47 | llama(L): tf=13.6250 first=11.8243 kCE=13.4219 KD=5.2392 acc=0.000 state=14.9088 ent=9.742 align=0.0000 latA=0.4994 latP=0.2492 | scale_pen(llama)=1.4211e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(71%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 230.9GB reserved, 109.2GB free, Peak: 189.0GB
  step  21/30 | (warm-up text) | align=0.0002 | text_tf=10.6084 | latent_scale=0.42
  step  23/30 | (warm-up text) | align=0.0002 | text_tf=10.7326 | latent_scale=0.43
  step  25/30 | (warm-up text) | align=0.0002 | text_tf=10.2474 | latent_scale=0.45
  step  27/30 | (warm-up text) | align=0.0002 | text_tf=11.5197 | latent_scale=0.47
  step  29/30 | (warm-up text) | align=0.0002 | text_tf=10.3415 | latent_scale=0.48
  step  30/30 | grad_norm=64.78 | sec/step~3.39 | lr=4.97e-05 | keep=0.77 | K=8 | first_w=1.77 | llama(L): tf=14.3125 first=12.5985 kCE=14.3047 KD=5.4514 acc=0.000 state=14.2384 ent=9.731 align=0.0000 latA=0.5019 latP=0.2491 | scale_pen(llama)=0.0000e+00 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 232.4GB
Epoch 3/4
[Epoch 3 Start] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 232.4GB
  step  10/30 | grad_norm=79.99 | sec/step~3.66 | lr=4.96e-05 | keep=0.80 | K=8 | first_w=2.07 | llama(L): tf=14.2500 first=11.9751 kCE=14.2031 KD=5.1930 acc=0.000 state=15.6637 ent=9.851 align=0.0000 latA=0.5021 latP=0.2490 | scale_pen(llama)=2.2737e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  step  20/30 | grad_norm=39.34 | sec/step~3.37 | lr=4.95e-05 | keep=0.83 | K=8 | first_w=2.37 | llama(L): tf=12.6875 first=10.7727 kCE=13.1172 KD=5.1882 acc=0.000 state=13.7520 ent=9.057 align=0.0000 latA=0.4992 latP=0.2491 | scale_pen(llama)=1.4211e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  ðŸŒŸ NEW PEAK: first_acc_ema=1.2% (raw_batch=3.1%) at step 85 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='The' | gold='men'
        âœ— pred='The' | gold='is'
        âœ— pred='The' | gold='142'
        âœ— pred='The' | gold='external'
        âœ— pred='The' | gold='during'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'The'(32) 
  step  30/30 | grad_norm=66.44 | sec/step~3.53 | lr=4.93e-05 | keep=0.87 | K=8 | first_w=2.67 | llama(L): tf=12.3750 first=11.6585 kCE=12.4531 KD=5.0602 acc=0.000 state=14.6424 ent=7.013 align=0.0000 latA=0.5012 latP=0.2491 | scale_pen(llama)=8.8818e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
Epoch 4/4
[Epoch 4 Start] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  step  10/30 | grad_norm=46.18 | sec/step~3.56 | lr=4.92e-05 | keep=0.91 | K=8 | first_w=2.97 | llama(L): tf=10.6875 first=8.9582 kCE=10.7109 KD=5.1714 acc=0.000 state=13.7769 ent=7.595 align=0.0000 latA=0.5016 latP=0.2493 | scale_pen(llama)=0.0000e+00 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  ðŸŒŸ NEW PEAK: first_acc_ema=1.4% (raw_batch=6.2%) at step 103 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='13'
        âœ— pred='the' | gold='20'
        âœ— pred='the' | gold='King'
        âœ— pred='the' | gold='seven'
        âœ— pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=1.9% (raw_batch=6.2%) at step 104 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='visual'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='G'
        âœ— pred='the' | gold='a'
        âœ— pred='the' | gold='God'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.1% (raw_batch=6.2%) at step 108 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='240'
        âœ— pred='the' | gold='Tro'
        âœ— pred='the' | gold='M'
        âœ— pred='the' | gold='Joseph'
        âœ— pred='the' | gold='cross'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.2% (raw_batch=3.1%) at step 109 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='0'
        âœ— pred='the' | gold='187'
        âœ— pred='the' | gold='collapse'
        âœ— pred='the' | gold='Charles'
        âœ— pred='the' | gold='US'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=31.39 | sec/step~3.50 | lr=4.90e-05 | keep=0.95 | K=8 | first_w=3.27 | llama(L): tf=11.3750 first=9.2590 kCE=11.7188 KD=5.0310 acc=0.031 [âœ“'the'] state=13.8217 ent=8.597 align=0.0000 latA=0.5022 latP=0.2493 | scale_pen(llama)=5.6843e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
  ðŸŒŸ NEW PEAK: first_acc_ema=2.3% (raw_batch=3.1%) at step 110 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='M'
        âœ— pred='the' | gold='System'
        âœ— pred='the' | gold='Sir'
        âœ— pred='the' | gold='an'
        âœ— pred='the' | gold='January'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.7% (raw_batch=6.2%) at step 111 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='William'
        âœ— pred='the' | gold='Nintendo'
        âœ— pred='the' | gold='incre'
        âœ— pred='the' | gold='Lib'
        âœ— pred='the' | gold='K'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.1% (raw_batch=6.2%) at step 112 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='73'
        âœ— pred='the' | gold='it'
        âœ— pred='the' | gold='computer'
        âœ— pred='the' | gold='civil'
        âœ— pred='the' | gold='working'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.1% (raw_batch=6.2%) at step 115 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='V'
        âœ— pred='the' | gold='limitations'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='as'
        âœ— pred='the' | gold='neo'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.5% (raw_batch=6.2%) at step 116 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Spanish'
        âœ— pred='the' | gold='East'
        âœ— pred='the' | gold='Weather'
        âœ— pred='the' | gold='bee'
        âœ— pred='the' | gold='The'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.0% (raw_batch=9.4%) at step 117 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='20'
        âœ— pred='the' | gold='Earth'
        âœ— pred='the' | gold='Er'
        âœ— pred='the' | gold='his'
        âœ— pred='the' | gold='h'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.3% (raw_batch=6.2%) at step 118 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='United'
        âœ— pred='the' | gold='order'
        âœ— pred='the' | gold='50'
        âœ— pred='the' | gold='burn'
        âœ— pred='the' | gold='amm'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.8% (raw_batch=9.4%) at step 119 â†’ saved to runs/smoke/ckpt/stageA_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='July'
        âœ— pred='the' | gold='to'
        âœ— pred='the' | gold='six'
        âœ— pred='the' | gold='201'
        âœ— pred='the' | gold='Pers'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=31.05 | sec/step~3.43 | lr=4.88e-05 | keep=1.00 | K=8 | first_w=3.57 | llama(L): tf=10.8750 first=10.3539 kCE=10.8516 KD=4.7048 acc=0.031 [âœ“'the'] state=12.8102 ent=9.135 align=0.0000 latA=0.5011 latP=0.2496 | scale_pen(llama)=5.6843e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0000 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(74%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 233.3GB reserved, 106.8GB free, Peak: 111.8GB
[checkpoint] Freed 3.7KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
âœ… Saved latest checkpoint to runs/smoke/ckpt/stageA
ðŸ“ Saved LoRA adapters for Llama
ðŸ“ Saved training_stats.json: {'llama': {'rms_mean_raw': 1.00002115269502, 'rms_mean_cal': 0.010571840855603417, 'embed_rms': 0.01057521253824234, 'count': 120}}

=== Stage B: Llama prefix training + warm-up ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
[Optimization] Enabled FlashAttention-2 and memory-efficient kernels
[Optimization] Enabled TF32 for matmul and cuDNN
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3546.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:04,  1.36s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.22s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.18it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.00it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[meta-llama/Meta-Llama-3.1-8B-Instruct] Initializing 3 latent adapters at layers (5, 10, 15)
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 5 on device cuda:0
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 10 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Placed latent adapter for layer 15 on device cuda:1
[meta-llama/Meta-Llama-3.1-8B-Instruct] Latent adapters: 459,399,171 trainable parameters

ðŸ”§ Applying LoRA (r=8, alpha=8)...
   Llama BEFORE LoRA: 0 trainable / 8,030,261,248 total
trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848
   Llama AFTER LoRA:  6,815,744 trainable / 8,037,076,992 total
   âœ“ Added 6,815,744 LoRA parameters to Llama
Llama hidden size: 4096
[DeviceMap] Llama: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
[After Model Loading] [GPU Memory] GPU0:4.8GB(6%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 17.0GB allocated, 17.1GB reserved, 323.1GB free, Peak: 17.0GB
[INFO] llama anchor tokens: 3
[Optimization] Using fused AdamW optimizer
[INFO] LR scheduler: CosineAnnealingLR (T_max=240, eta_min=1.00e-06)
âª Resuming from: runs/smoke/ckpt/stageA/state.pt
   -> latent adapters 'llama' missing in state.pt; will retry from disk
   -> restored RNG state
   -> reset epoch/global_step to zero as requested
   -> start_epoch=0, global_step=0
[warmup] alternating text/latent for first 45 steps
Epoch 1/8
[Epoch 1 Start] [GPU Memory] GPU0:6.1GB(10%), GPU1:4.1GB(5%), GPU2:3.5GB(4%), GPU3:4.5GB(5%) | Total: 18.3GB allocated, 21.0GB reserved, 319.1GB free, Peak: 21.0GB
[warmup] step=0 mode=text (warm-up)
    [Memory after encoder] 18.4GB allocated
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1/30 | (warm-up text) | align=0.0003 | text_tf=16.2679 | latent_scale=0.20
    [Memory after backward] 20.7GB allocated, peak 176.2GB
[warmup] step=1 mode=latent (warm-up)
    [Memory after encoder] 20.8GB allocated
    [Memory after backward] 21.7GB allocated, peak 176.2GB
[warmup] step=2 mode=text (warm-up)
    [Memory after encoder] 21.0GB allocated
  step  3/30 | (warm-up text) | align=0.0003 | text_tf=15.9419 | latent_scale=0.24
    [Memory after backward] 21.7GB allocated, peak 230.8GB
[warmup] step=3 mode=latent (warm-up)
[warmup] step=4 mode=text (warm-up)
  step  5/30 | (warm-up text) | align=0.0003 | text_tf=14.8455 | latent_scale=0.27
[warmup] step=5 mode=latent (warm-up)
  ðŸŒŸ NEW PEAK: first_acc_ema=1.4% (raw_batch=6.2%) at step 6 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Executive'
        âœ— pred='the' | gold='micro'
        âœ— pred='the' | gold='DVD'
        âœ— pred='the' | gold='job'
        âœ— pred='assistant' | gold='Greek'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(31) 'assistant'(1) 
[warmup] step=6 mode=text (warm-up)
  step  7/30 | (warm-up text) | align=0.0003 | text_tf=12.8386 | latent_scale=0.31
[warmup] step=7 mode=latent (warm-up)
  ðŸŒŸ NEW PEAK: first_acc_ema=1.9% (raw_batch=6.2%) at step 8 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Zen'
        âœ— pred='the' | gold='mer'
        âœ— pred='the' | gold='child'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='Mount'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
[warmup] step=8 mode=text (warm-up)
  step  9/30 | (warm-up text) | align=0.0003 | text_tf=14.8582 | latent_scale=0.34
[warmup] step=9 mode=latent (warm-up)
  step  10/30 | grad_norm=8.83 | sec/step~3.51 | lr=5.00e-05 | keep=0.50 | K=8 | first_w=0.27 | llama(L): tf=10.6250 first=9.8066 kCE=10.7734 KD=4.4562 acc=0.000 state=18.6785 ent=9.196 align=0.0000 latA=1.0001 latP=0.4988 | scale_pen(llama)=1.1005e-10 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  11/30 | (warm-up text) | align=0.0003 | text_tf=15.4116 | latent_scale=0.38
  step  13/30 | (warm-up text) | align=0.0003 | text_tf=13.7752 | latent_scale=0.41
  step  15/30 | (warm-up text) | align=0.0003 | text_tf=15.4431 | latent_scale=0.45
  step  17/30 | (warm-up text) | align=0.0003 | text_tf=14.1466 | latent_scale=0.48
  step  19/30 | (warm-up text) | align=0.0003 | text_tf=13.6513 | latent_scale=0.52
  step  20/30 | grad_norm=98.42 | sec/step~3.60 | lr=5.00e-05 | keep=0.50 | K=8 | first_w=0.57 | llama(L): tf=10.6875 first=10.6270 kCE=11.1641 KD=4.5141 acc=0.000 state=11.6654 ent=8.441 align=0.0000 latA=0.9989 latP=0.4985 | scale_pen(llama)=3.1974e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.0GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  21/30 | (warm-up text) | align=0.0003 | text_tf=12.6413 | latent_scale=0.56
  step  23/30 | (warm-up text) | align=0.0003 | text_tf=14.8317 | latent_scale=0.59
  step  25/30 | (warm-up text) | align=0.0003 | text_tf=14.4564 | latent_scale=0.63
  step  27/30 | (warm-up text) | align=0.0003 | text_tf=12.1410 | latent_scale=0.66
  step  29/30 | (warm-up text) | align=0.0003 | text_tf=13.5399 | latent_scale=0.70
  step  30/30 | grad_norm=130.08 | sec/step~3.68 | lr=5.00e-05 | keep=0.51 | K=8 | first_w=0.87 | llama(L): tf=11.6875 first=11.8137 kCE=11.1953 KD=4.0515 acc=0.000 state=19.0508 ent=9.439 align=0.0000 latA=0.9944 latP=0.4973 | scale_pen(llama)=7.6771e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
Epoch 2/8
[Epoch 2 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 230.8GB
  step  1/30 | (warm-up text) | align=0.0003 | text_tf=11.6591 | latent_scale=0.73
  step  3/30 | (warm-up text) | align=0.0003 | text_tf=12.3244 | latent_scale=0.77
  step  5/30 | (warm-up text) | align=0.0003 | text_tf=12.0442 | latent_scale=0.80
  step  7/30 | (warm-up text) | align=0.0003 | text_tf=11.7727 | latent_scale=0.84
  step  9/30 | (warm-up text) | align=0.0003 | text_tf=12.5207 | latent_scale=0.88
  step  10/30 | grad_norm=51.06 | sec/step~3.45 | lr=5.00e-05 | keep=0.51 | K=8 | first_w=1.17 | llama(L): tf=11.3125 first=10.2474 kCE=11.2500 KD=4.3603 acc=0.000 state=10.7590 ent=10.724 align=0.0000 latA=0.9964 latP=0.4969 | scale_pen(llama)=2.8777e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
  step  11/30 | (warm-up text) | align=0.0003 | text_tf=11.5615 | latent_scale=0.91
  step  13/30 | (warm-up text) | align=0.0003 | text_tf=12.3966 | latent_scale=0.95
  step  15/30 | (warm-up text) | align=0.0003 | text_tf=11.6353 | latent_scale=0.98
[warmup] step=46 mode=text (tail)
  step  17/30 | (tail text) | align=0.0003 | text_tf=10.3719 | latent_scale=1.00
  step  20/30 | grad_norm=57.70 | sec/step~3.52 | lr=4.99e-05 | keep=0.51 | K=8 | first_w=1.47 | llama(L): tf=10.8750 first=9.8698 kCE=10.8672 KD=4.1357 acc=0.000 state=10.5089 ent=9.219 align=0.0000 latA=0.9964 latP=0.4963 | scale_pen(llama)=4.2988e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 189.9GB
[warmup] step=52 mode=text (tail)
  step  23/30 | (tail text) | align=0.0003 | text_tf=10.6591 | latent_scale=1.00
[warmup] step=54 mode=text (tail)
  step  25/30 | (tail text) | align=0.0003 | text_tf=10.2056 | latent_scale=1.00
[warmup] step=57 mode=text (tail)
  step  28/30 | (tail text) | align=0.0003 | text_tf=10.4543 | latent_scale=1.00
  step  30/30 | grad_norm=53.26 | sec/step~3.41 | lr=4.99e-05 | keep=0.52 | K=8 | first_w=1.77 | llama(L): tf=10.4375 first=10.0362 kCE=10.8516 KD=4.3441 acc=0.000 state=14.1317 ent=7.679 align=0.0000 latA=1.0029 latP=0.4959 | scale_pen(llama)=2.7512e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 192.1GB
Epoch 3/8
[Epoch 3 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 192.1GB
[warmup] step=64 mode=text (tail)
  step  5/30 | (tail text) | align=0.0003 | text_tf=9.3966 | latent_scale=1.00
  step  10/30 | grad_norm=14.73 | sec/step~3.66 | lr=4.99e-05 | keep=0.53 | K=8 | first_w=2.07 | llama(L): tf=9.8125 first=8.9683 kCE=9.8125 KD=4.1100 acc=0.031 [âœ“'in'] state=6.6992 ent=9.088 align=0.0000 latA=1.0040 latP=0.4960 | scale_pen(llama)=7.5730e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
[warmup] step=71 mode=text (tail)
  step  12/30 | (tail text) | align=0.0003 | text_tf=9.3269 | latent_scale=1.00
[warmup] step=72 mode=text (tail)
  step  13/30 | (tail text) | align=0.0003 | text_tf=8.9803 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=1.9% (raw_batch=9.4%) at step 74 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='T'
        âœ— pred='the' | gold='medical'
        âœ— pred='in' | gold='189'
        âœ— pred='the' | gold='super'
        âœ— pred='the' | gold='mes'
      Prediction diversity: 2/32 unique tokens
      Top-3 predictions: 'the'(30) 'in'(2) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.0% (raw_batch=6.2%) at step 76 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='inter'
        âœ— pred='the' | gold='sc'
        âœ— pred='the' | gold='New'
        âœ— pred='the' | gold='minor'
        âœ— pred='the' | gold='Joseph'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.3% (raw_batch=6.2%) at step 78 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='British'
        âœ— pred='the' | gold='a'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='Pos'
        âœ— pred='the' | gold='they'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=14.50 | sec/step~3.46 | lr=4.99e-05 | keep=0.54 | K=8 | first_w=2.37 | llama(L): tf=10.0000 first=9.2369 kCE=10.3516 KD=4.2023 acc=0.031 [âœ“'the'] state=4.7802 ent=9.750 align=0.0000 latA=0.9975 latP=0.4958 | scale_pen(llama)=7.1942e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
  ðŸŒŸ NEW PEAK: first_acc_ema=2.6% (raw_batch=6.2%) at step 81 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='dis'
        âœ— pred='the' | gold='threat'
        âœ— pred='the' | gold='Nor'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='185'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=2.9% (raw_batch=6.2%) at step 82 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='174'
        âœ— pred='the' | gold='V'
        âœ— pred='the' | gold='ann'
        âœ— pred='the' | gold='B'
        âœ— pred='the' | gold='ch'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.3% (raw_batch=6.2%) at step 83 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='187'
        âœ— pred='the' | gold='198'
        âœ— pred='the' | gold='first'
        âœ— pred='the' | gold='Small'
        âœ“ pred='the' | gold='the'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.6% (raw_batch=6.2%) at step 84 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='action'
        âœ— pred='the' | gold='third'
        âœ— pred='the' | gold='Mozilla'
        âœ— pred='the' | gold='War'
        âœ“ pred='the' | gold='the'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
[warmup] step=86 mode=text (tail)
  step  27/30 | (tail text) | align=0.0003 | text_tf=9.7461 | latent_scale=1.00
[warmup] step=89 mode=text (tail)
  step  30/30 | (tail text) | align=0.0003 | text_tf=9.4495 | latent_scale=1.00
  step  30/30 | grad_norm=35.37 | sec/step~4.67 | lr=4.98e-05 | keep=0.55 | K=8 | first_w=2.67 | llama(T): tf=10.0000 first=10.0377 kCE=9.9453 KD=0.0000 acc=0.000 state=5.0016 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.8417e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
Epoch 4/8
[Epoch 4 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 187.7GB
  ðŸŒŸ NEW PEAK: first_acc_ema=3.6% (raw_batch=9.4%) at step 98 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Comm'
        âœ— pred='the' | gold='final'
        âœ— pred='the' | gold='rec'
        âœ— pred='the' | gold='4'
        âœ— pred='the' | gold='Govern'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=14.90 | sec/step~3.45 | lr=4.98e-05 | keep=0.56 | K=8 | first_w=2.97 | llama(L): tf=9.1250 first=8.3697 kCE=8.7461 KD=4.0088 acc=0.062 [âœ“'the'] state=3.4366 ent=8.825 align=0.0000 latA=1.0026 latP=0.4958 | scale_pen(llama)=1.3657e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 111.8GB
  ðŸŒŸ NEW PEAK: first_acc_ema=3.6% (raw_batch=6.2%) at step 100 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='mon'
        âœ— pred='the' | gold='18'
        âœ— pred='the' | gold='s'
        âœ— pred='the' | gold='7'
        âœ— pred='the' | gold='N'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=3.8% (raw_batch=6.2%) at step 103 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='13'
        âœ— pred='the' | gold='20'
        âœ— pred='the' | gold='King'
        âœ— pred='the' | gold='seven'
        âœ— pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.0% (raw_batch=6.2%) at step 104 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='visual'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='G'
        âœ— pred='the' | gold='a'
        âœ— pred='the' | gold='God'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=12.12 | sec/step~3.68 | lr=4.97e-05 | keep=0.57 | K=8 | first_w=3.27 | llama(L): tf=9.1250 first=8.1226 kCE=9.1602 KD=4.0150 acc=0.031 [âœ“'the'] state=3.8004 ent=9.331 align=0.0000 latA=1.0027 latP=0.4961 | scale_pen(llama)=8.8818e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 111.8GB
  step  23/30 | (tail text) | align=0.0003 | text_tf=8.8707 | latent_scale=1.00
  step  30/30 | grad_norm=13.52 | sec/step~3.40 | lr=4.97e-05 | keep=0.59 | K=8 | first_w=3.57 | llama(L): tf=9.6250 first=8.8866 kCE=9.6172 KD=3.8921 acc=0.031 [âœ“'the'] state=3.3330 ent=8.840 align=0.0000 latA=0.9994 latP=0.4961 | scale_pen(llama)=2.1064e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 165.1GB
Epoch 5/8
[Epoch 5 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 165.1GB
  step  10/30 | grad_norm=12.55 | sec/step~3.48 | lr=4.96e-05 | keep=0.60 | K=8 | first_w=3.87 | llama(L): tf=9.0625 first=8.0669 kCE=9.0977 KD=3.8529 acc=0.000 state=3.1542 ent=8.897 align=0.0000 latA=0.9972 latP=0.4965 | scale_pen(llama)=6.9633e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 109.1GB
  step  14/30 | (tail text) | align=0.0003 | text_tf=8.9059 | latent_scale=1.00
  step  15/30 | (tail text) | align=0.0003 | text_tf=10.6322 | latent_scale=1.00
  step  20/30 | (tail text) | align=0.0003 | text_tf=8.5972 | latent_scale=1.00
  step  20/30 | grad_norm=11.26 | sec/step~4.87 | lr=4.96e-05 | keep=0.62 | K=8 | first_w=4.17 | llama(T): tf=9.0625 first=7.4698 kCE=9.1328 KD=0.0000 acc=0.000 state=2.9877 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.9455e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
  step  26/30 | (tail text) | align=0.0003 | text_tf=8.5630 | latent_scale=1.00
  step  30/30 | grad_norm=20.96 | sec/step~4.10 | lr=4.95e-05 | keep=0.64 | K=8 | first_w=4.47 | llama(L): tf=8.8125 first=7.5504 kCE=9.1602 KD=3.7319 acc=0.000 state=3.3523 ent=7.722 align=0.0000 latA=0.9956 latP=0.4965 | scale_pen(llama)=1.4552e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
Epoch 6/8
[Epoch 6 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 198.6GB
  ðŸŒŸ NEW PEAK: first_acc_ema=4.0% (raw_batch=12.5%) at step 155 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='as'
        âœ— pred='the' | gold='300'
        âœ— pred='the' | gold='Hotel'
        âœ— pred='the' | gold='in'
        âœ— pred='the' | gold='23'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.0% (raw_batch=9.4%) at step 156 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='civil'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='Â£'
        âœ— pred='the' | gold='151'
        âœ— pred='the' | gold='East'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.3% (raw_batch=15.6%) at step 158 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='degrees'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='675'
        âœ— pred='the' | gold='17'
        âœ— pred='the' | gold='a'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=16.13 | sec/step~3.52 | lr=4.95e-05 | keep=0.65 | K=8 | first_w=4.77 | llama(L): tf=9.4375 first=7.6465 kCE=9.3125 KD=3.5552 acc=0.000 state=2.8740 ent=8.532 align=0.0000 latA=0.9981 latP=0.4965 | scale_pen(llama)=6.9633e-13 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 109.1GB
  ðŸŒŸ NEW PEAK: first_acc_ema=4.3% (raw_batch=12.5%) at step 169 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='bee'
        âœ— pred='the' | gold='children'
        âœ— pred='the' | gold='single'
        âœ— pred='the' | gold='sc'
        âœ— pred='the' | gold='H'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | (tail text) | align=0.0003 | text_tf=9.1187 | latent_scale=1.00
  step  20/30 | grad_norm=15.39 | sec/step~4.75 | lr=4.94e-05 | keep=0.68 | K=8 | first_w=5.07 | llama(T): tf=8.8750 first=7.9609 kCE=8.6250 KD=0.0000 acc=0.000 state=2.6038 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=1.5948e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
  step  22/30 | (tail text) | align=0.0003 | text_tf=9.0671 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=4.3% (raw_batch=9.4%) at step 176 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Pa'
        âœ— pred='the' | gold='British'
        âœ— pred='the' | gold='p'
        âœ— pred='the' | gold='The'
        âœ— pred='the' | gold='medical'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  27/30 | (tail text) | align=0.0003 | text_tf=8.3877 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=4.4% (raw_batch=9.4%) at step 178 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='six'
        âœ— pred='the' | gold='op'
        âœ— pred='the' | gold='St'
        âœ— pred='the' | gold='Central'
        âœ— pred='the' | gold='There'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=4.9% (raw_batch=9.4%) at step 179 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Theo'
        âœ— pred='the' | gold='Dr'
        âœ— pred='the' | gold='197'
        âœ— pred='the' | gold='Ch'
        âœ— pred='the' | gold='s'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | (tail text) | align=0.0003 | text_tf=8.1302 | latent_scale=1.00
  step  30/30 | grad_norm=65.64 | sec/step~4.67 | lr=4.93e-05 | keep=0.70 | K=8 | first_w=5.37 | llama(T): tf=8.7500 first=7.2597 kCE=8.4492 KD=0.0000 acc=0.031 state=3.0777 ent=0.000 align=0.0003 latA=0.0000 latP=0.0000 | scale_pen(llama)=3.3427e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
Epoch 7/8
[Epoch 7 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 183.3GB
  step  2/30 | (tail text) | align=0.0003 | text_tf=9.4346 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=4.9% (raw_batch=9.4%) at step 184 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='August'
        âœ— pred='the' | gold='190'
        âœ— pred='the' | gold='111'
        âœ— pred='the' | gold='Inter'
        âœ— pred='the' | gold='Russia'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=5.3% (raw_batch=12.5%) at step 187 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='Cr'
        âœ— pred='the' | gold='Greek'
        âœ— pred='the' | gold='l'
        âœ— pred='the' | gold='World'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=6.2%) at step 188 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='contact'
        âœ— pred='the' | gold='My'
        âœ— pred='the' | gold='Mess'
        âœ— pred='the' | gold='quant'
        âœ— pred='the' | gold='Lib'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=13.34 | sec/step~3.49 | lr=4.92e-05 | keep=0.72 | K=8 | first_w=5.67 | llama(L): tf=8.8750 first=7.3184 kCE=8.6250 KD=3.8471 acc=0.031 [âœ“'the'] state=2.4820 ent=7.986 align=0.0000 latA=0.9929 latP=0.4960 | scale_pen(llama)=8.8818e-14 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 172.9GB
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 196 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='hack'
        âœ— pred='the' | gold='British'
        âœ— pred='the' | gold='po'
        âœ— pred='the' | gold='cell'
        âœ— pred='the' | gold='C'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  20/30 | grad_norm=18.64 | sec/step~3.64 | lr=4.92e-05 | keep=0.74 | K=8 | first_w=5.97 | llama(L): tf=8.8125 first=7.1478 kCE=8.7539 KD=3.6725 acc=0.062 [âœ“'the'] state=2.8691 ent=8.042 align=0.0000 latA=0.9962 latP=0.4953 | scale_pen(llama)=2.2737e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0001 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 172.9GB
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 203 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='Portal'
        âœ— pred='the' | gold='3'
        âœ— pred='the' | gold='significant'
        âœ— pred='the' | gold='to'
        âœ— pred='the' | gold='7'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  26/30 | (tail text) | align=0.0003 | text_tf=8.2530 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 209 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='multiple'
        âœ— pred='the' | gold='six'
        âœ— pred='the' | gold='import'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='very'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=13.94 | sec/step~3.68 | lr=4.91e-05 | keep=0.77 | K=8 | first_w=6.00 | llama(L): tf=8.8125 first=7.1469 kCE=8.8711 KD=3.9086 acc=0.031 [âœ“'the'] state=2.7405 ent=7.573 align=0.0000 latA=0.9950 latP=0.4942 | scale_pen(llama)=1.4552e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
Epoch 8/8
[Epoch 8 Start] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 180.7GB
  step  1/30 | (tail text) | align=0.0003 | text_tf=8.3802 | latent_scale=1.00
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 216 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='on'
        âœ— pred='the' | gold='V'
        âœ— pred='the' | gold='J'
        âœ— pred='the' | gold='142'
        âœ— pred='the' | gold='R'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  10/30 | grad_norm=33.25 | sec/step~3.63 | lr=4.90e-05 | keep=0.79 | K=8 | first_w=6.00 | llama(L): tf=8.5000 first=6.7467 kCE=8.6094 KD=3.7046 acc=0.000 state=2.8028 ent=6.918 align=0.0000 latA=0.9961 latP=0.4942 | scale_pen(llama)=1.1511e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 10] [GPU Memory] GPU0:9.6GB(72%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 231.7GB reserved, 108.4GB free, Peak: 176.6GB
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 221 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='neo'
        âœ— pred='the' | gold='Greek'
        âœ— pred='the' | gold='122'
        âœ— pred='the' | gold='Small'
        âœ— pred='the' | gold='many'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  12/30 | (tail text) | align=0.0003 | text_tf=8.7084 | latent_scale=1.00
  step  19/30 | (tail text) | align=0.0003 | text_tf=9.9167 | latent_scale=1.00
  step  20/30 | grad_norm=27.62 | sec/step~3.71 | lr=4.89e-05 | keep=0.82 | K=8 | first_w=6.00 | llama(L): tf=8.7500 first=7.2901 kCE=8.5391 KD=3.7034 acc=0.031 [âœ“'the'] state=2.9607 ent=8.135 align=0.0000 latA=0.9982 latP=0.4940 | scale_pen(llama)=3.8689e-12 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 20] [GPU Memory] GPU0:9.6GB(75%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 234.2GB reserved, 105.9GB free, Peak: 233.2GB
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=9.4%) at step 233 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='China'
        âœ— pred='the' | gold='Federal'
        âœ— pred='the' | gold='working'
        âœ— pred='the' | gold='An'
        âœ— pred='the' | gold='250'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  ðŸŒŸ NEW PEAK: first_acc_ema=5.4% (raw_batch=12.5%) at step 239 â†’ saved to runs/smoke/ckpt/stageB_best
      Sample predictions (first 5):
        âœ— pred='the' | gold='a'
        âœ“ pred='the' | gold='the'
        âœ— pred='the' | gold='10'
        âœ— pred='the' | gold='Â£'
        âœ— pred='the' | gold='rich'
      Prediction diversity: 1/32 unique tokens
      Top-3 predictions: 'the'(32) 
  step  30/30 | grad_norm=23.05 | sec/step~3.55 | lr=4.88e-05 | keep=0.85 | K=8 | first_w=6.00 | llama(L): tf=8.9375 first=7.2219 kCE=8.8672 KD=3.4119 acc=0.062 [âœ“'the'] state=2.6164 ent=8.397 align=0.0000 latA=0.9961 latP=0.4929 | scale_pen(llama)=1.1141e-11 | K=8 tau=4.00 | stats=[llama: rms_raw~1.0002 rms_cal~0.0106 embed_rms~0.01058]
  [Step 30] [GPU Memory] GPU0:9.6GB(75%), GPU1:4.2GB(62%), GPU2:3.6GB(61%), GPU3:4.7GB(77%) | Total: 22.1GB allocated, 234.2GB reserved, 105.9GB free, Peak: 233.2GB
[checkpoint] Freed 3.8KB before save.
[checkpoint] Saved latest: encoder.pt, state.pt, config.json, adapter_llama.pt, deep_prefix_llama.pt, refiner.pt
[checkpoint] Freed 0.0B after save (non-canonical).
âœ… Saved latest checkpoint to runs/smoke/ckpt/stageB
ðŸ“ Saved LoRA adapters for Llama
ðŸ“ Saved training_stats.json: {'llama': {'rms_mean_raw': 1.0001609742641449, 'rms_mean_cal': 0.010571445846774925, 'embed_rms': 0.01057625375688076, 'count': 240}}

=== Stage C: Evaluation (Llama only) ===

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/smoke/ckpt/stageB/training_stats.json
Encoder input alignment: mode=neutral_chat | strip_anchor=yes | samples=200
Building encoder and computing Z...

[Standard Evaluation Mode]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3449.97it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:01<00:03,  1.16s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:02<00:02,  1.28s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:03<00:01,  1.21s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.02it/s]
[meta-llama/Meta-Llama-3.1-8B-Instruct] hf_device_map: {'model.embed_tokens': 0, 'model.rotary_emb': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 1, 'model.layers.11': 1, 'model.layers.12': 1, 'model.layers.13': 1, 'model.layers.14': 1, 'model.layers.15': 1, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 2, 'model.layers.23': 2, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3, 'model.norm': 3, 'lm_head': 3}
âœ“ Loaded deep prefix generator for llama
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

â€” Text baseline summary:
llama: EM=0.590 F1=0.794
âœ“ Loaded LoRA adapters for llama

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 246.0 | (Qwen): - | Latent length M: 64
Compression ratio (Llama): 3.8x | (Qwen): -x
Approx interlingua payload per example: 13107200 bytes (fp32); fp16 reference: 6553600 bytes; fp32 reference: 13107200 bytes
latent/text bytes (one-copy, fp16): n/a

â€” Baseline: Text prompting
Llama  EM: 0.590  F1: 0.794  |  NLL/token (gold): 13.675748455854526
Wall clock: 9.08s

â€” Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.010  |  NLL/token (gold): 9.099075477950427
       First-token acc: top1=0.030  top5=0.080
Wall clock: 1.77s

â€” Token-budget baseline (mode: content_only)
Llama  EM: 0.010  F1: 0.063
Wall clock: 2.13s

â€” 2-LLM joint (rescored pick on latent runs)
Joint metrics unavailable (single-model evaluation).

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 16,
  "latent_len": 64,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 246.03
  },
  "compression": {
    "llama": 3.84421875
  },
  "payload_bytes": 13107200,
  "payload_bytes_detail": {
    "fp32": 13107200,
    "fp16": 6553600,
    "selected": null
  },
  "wire": {
    "prompt_chars": {
      "llama": 251558
    },
    "prompt_count": 200,
    "latent_shape": [
      200,
      64,
      256
    ],
    "latent_bytes": {
      "fp32": 13107200,
      "fp16": 6553600
    },
    "group_size": 32,
    "scale_bits": 16,
    "selected_latent_bytes": null,
    "wire_ratio": {}
  },
  "text": {
    "llama": {
      "em": 0.59,
      "f1": 0.7936993517504416,
      "nll_token": 13.675748455854526
    },
    "wall_clock_sec": 9.077691793441772
  },
  "latent": {
    "llama": {
      "em": 0.0,
      "f1": 0.009932353893517107,
      "nll": 9.099075477950427,
      "first_token_top1": 0.03,
      "first_token_top5": 0.08,
      "nll_token": 9.099075477950427
    },
    "wall_clock_sec": 1.7683758735656738
  },
  "token_budget": {
    "mode": "content_only",
    "k": 64,
    "llama": {
      "em": 0.01,
      "f1": 0.06337241856366438
    },
    "wall_clock_sec": 2.1274001598358154
  },
  "joint": {
    "em": null,
    "f1": null,
    "agreement": null,
    "oracle": {
      "em": 0.0,
      "f1": 0.0
    }
  },
  "debug": {
    "llama": {
      "latent_anchor_text": "Answer: "
    },
    "settings": {
      "latent_anchor_mode": "chat",
      "latent_anchor_text": "Answer: ",
      "prefix_gain": 1.1,
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "decode": {
        "min_new_tokens": 1,
        "eos_ban_steps": 0,
        "first_token_top_p": 1.0,
        "first_token_temperature": 0.0
      }
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.0
  },
  "dataset": "squad"
}
Stopping GPU monitoring (PID 378360)
