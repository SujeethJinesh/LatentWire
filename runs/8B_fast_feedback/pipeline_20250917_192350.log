
=========================================
Starting pipeline at Wed Sep 17 19:23:50 PDT 2025
=========================================


=========================================
TRAIN + PER-EPOCH EVAL
=========================================


=========================================
EPOCH 1/20
=========================================

/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2745.41it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.40s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.15it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2780.45it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.30it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.14it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.21it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
[WARN] t=0 alignment failed: t=0 mismatch: got 12366, expected 60704
[WARN] t=0 alignment failed: t=0 mismatch: got 12095, expected 59604
Initialized adapter colorizers from LM embedding stats.
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/715 | grad_norm=166.29 | sec/step~7.06 | keep=1.00 | K=6 | llama: tf=12.6861 first=16.8604 kCE=9.6815 KD=3.4636 man=0.0002 | scale_pen(llama)=0.0000e+00 | qwen: tf=13.6073 first=15.4215 kCE=14.1200 KD=4.2464 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  20/715 | grad_norm=310.46 | sec/step~6.78 | keep=1.00 | K=6 | llama: tf=12.1705 first=17.9069 kCE=10.1247 KD=3.5619 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=13.2634 first=17.2776 kCE=14.9148 KD=4.0892 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  30/715 | grad_norm=477.17 | sec/step~7.90 | keep=1.00 | K=6 | llama: tf=11.6041 first=19.3755 kCE=9.4703 KD=3.6562 man=0.0001 | scale_pen(llama)=0.0000e+00 | qwen: tf=12.3526 first=17.2713 kCE=13.7359 KD=5.1783 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  40/715 | grad_norm=108.60 | sec/step~8.32 | keep=1.00 | K=6 | llama: tf=12.6254 first=15.8037 kCE=8.9418 KD=3.6792 man=0.0002 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.9151 first=13.9737 kCE=13.6811 KD=4.8086 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  50/715 | grad_norm=235.05 | sec/step~8.02 | keep=1.00 | K=6 | llama: tf=13.4970 first=16.7154 kCE=9.5022 KD=3.6578 man=0.0002 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.4240 first=14.9219 kCE=13.3112 KD=4.7633 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  60/715 | grad_norm=373.56 | sec/step~6.76 | keep=1.00 | K=6 | llama: tf=12.4711 first=15.5498 kCE=9.1342 KD=3.3596 man=0.0002 | scale_pen(llama)=3.1974e-12 | qwen: tf=11.8478 first=13.7488 kCE=13.8158 KD=4.3620 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  70/715 | grad_norm=50.00 | sec/step~6.25 | keep=1.00 | K=6 | llama: tf=12.1396 first=17.7489 kCE=9.1235 KD=3.9698 man=0.0001 | scale_pen(llama)=8.8818e-14 | qwen: tf=11.7682 first=17.1626 kCE=13.8698 KD=4.3233 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  80/715 | grad_norm=93.05 | sec/step~7.34 | keep=1.00 | K=6 | llama: tf=13.1157 first=16.1308 kCE=8.8241 KD=3.5962 man=0.0002 | scale_pen(llama)=8.8818e-14 | qwen: tf=11.3326 first=15.1725 kCE=13.6876 KD=4.2836 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  90/715 | grad_norm=163.95 | sec/step~8.67 | keep=1.00 | K=6 | llama: tf=12.8451 first=16.2442 kCE=8.7326 KD=3.1835 man=0.0002 | scale_pen(llama)=8.8818e-14 | qwen: tf=12.1320 first=15.7938 kCE=13.9208 KD=4.5654 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  100/715 | grad_norm=18.67 | sec/step~6.26 | keep=1.00 | K=6 | llama: tf=12.7454 first=15.4303 kCE=8.8213 KD=3.9152 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=10.4357 first=14.6106 kCE=13.2051 KD=4.0480 man=0.0003 | scale_pen(qwen)=3.5527e-15 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  110/715 | grad_norm=62.35 | sec/step~7.73 | keep=1.00 | K=6 | llama: tf=11.9283 first=15.6904 kCE=8.8690 KD=3.4485 man=0.0001 | scale_pen(llama)=1.4211e-14 | qwen: tf=12.0098 first=15.2161 kCE=13.6329 KD=4.4266 man=0.0003 | scale_pen(qwen)=3.5527e-15 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  120/715 | grad_norm=106.51 | sec/step~6.94 | keep=1.00 | K=6 | llama: tf=12.1052 first=14.0593 kCE=9.7201 KD=4.0025 man=0.0002 | scale_pen(llama)=1.4211e-14 | qwen: tf=11.3179 first=13.7952 kCE=13.0117 KD=4.7152 man=0.0003 | scale_pen(qwen)=3.5527e-15 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  130/715 | grad_norm=6.88 | sec/step~7.55 | keep=1.00 | K=6 | llama: tf=12.6452 first=15.8415 kCE=8.2983 KD=3.3550 man=0.0002 | scale_pen(llama)=1.7195e-12 | qwen: tf=11.9157 first=15.2526 kCE=14.8786 KD=4.3136 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  140/715 | grad_norm=42.08 | sec/step~7.96 | keep=1.00 | K=6 | llama: tf=13.0498 first=15.9886 kCE=9.1192 KD=3.5195 man=0.0002 | scale_pen(llama)=1.7195e-12 | qwen: tf=12.7480 first=16.5501 kCE=14.0327 KD=4.6169 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  150/715 | grad_norm=78.37 | sec/step~8.10 | keep=1.00 | K=6 | llama: tf=11.7542 first=14.8140 kCE=8.9205 KD=3.5153 man=0.0001 | scale_pen(llama)=1.7195e-12 | qwen: tf=12.7851 first=14.0158 kCE=13.4267 KD=5.3248 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  160/715 | grad_norm=114.22 | sec/step~7.34 | keep=1.00 | K=6 | llama: tf=13.1135 first=15.5360 kCE=8.5985 KD=3.6085 man=0.0002 | scale_pen(llama)=2.4016e-12 | qwen: tf=12.7337 first=15.4988 kCE=13.9955 KD=4.0202 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  170/715 | grad_norm=38.69 | sec/step~6.67 | keep=1.00 | K=6 | llama: tf=11.9893 first=16.0457 kCE=8.8648 KD=3.8598 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=12.2033 first=15.3740 kCE=13.7381 KD=4.5784 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  180/715 | grad_norm=77.12 | sec/step~7.56 | keep=1.00 | K=6 | llama: tf=12.9095 first=13.9067 kCE=7.7774 KD=3.1599 man=0.0002 | scale_pen(llama)=2.4016e-12 | qwen: tf=9.4430 first=12.0534 kCE=13.4877 KD=4.0899 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  190/715 | grad_norm=116.41 | sec/step~6.59 | keep=1.00 | K=6 | llama: tf=12.8326 first=16.8448 kCE=7.9525 KD=3.9675 man=0.0002 | scale_pen(llama)=2.4016e-12 | qwen: tf=13.2987 first=16.6184 kCE=14.2504 KD=4.7470 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  200/715 | grad_norm=74.21 | sec/step~7.18 | keep=1.00 | K=6 | llama: tf=12.4624 first=15.4367 kCE=7.8613 KD=3.3748 man=0.0002 | scale_pen(llama)=1.2790e-13 | qwen: tf=12.0800 first=14.5767 kCE=13.9176 KD=4.0289 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  210/715 | grad_norm=103.32 | sec/step~6.97 | keep=1.00 | K=6 | llama: tf=12.1390 first=13.9012 kCE=7.4448 KD=3.0721 man=0.0002 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.6857 first=13.2752 kCE=13.8175 KD=3.8555 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  220/715 | grad_norm=170.74 | sec/step~7.16 | keep=1.00 | K=6 | llama: tf=12.3595 first=15.3410 kCE=8.1999 KD=3.4821 man=0.0001 | scale_pen(llama)=1.2790e-13 | qwen: tf=12.8330 first=14.5662 kCE=13.9978 KD=4.3901 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  230/715 | grad_norm=18.14 | sec/step~7.75 | keep=1.00 | K=6 | llama: tf=12.0721 first=14.6177 kCE=7.8512 KD=3.0193 man=0.0001 | scale_pen(llama)=1.1543e-11 | qwen: tf=11.7393 first=13.0721 kCE=13.4205 KD=4.2767 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  240/715 | grad_norm=51.68 | sec/step~7.16 | keep=1.00 | K=6 | llama: tf=11.8214 first=15.9586 kCE=7.8329 KD=3.6997 man=0.0001 | scale_pen(llama)=1.1543e-11 | qwen: tf=12.5929 first=14.8140 kCE=14.1165 KD=4.7253 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  250/715 | grad_norm=85.17 | sec/step~7.17 | keep=1.00 | K=6 | llama: tf=12.8061 first=14.4784 kCE=8.2608 KD=3.1844 man=0.0002 | scale_pen(llama)=1.1543e-11 | qwen: tf=12.8313 first=14.0555 kCE=14.7065 KD=3.9346 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  260/715 | grad_norm=17.93 | sec/step~8.29 | keep=1.00 | K=6 | llama: tf=11.6604 first=14.7637 kCE=7.4627 KD=3.0186 man=0.0001 | scale_pen(llama)=2.8141e-11 | qwen: tf=12.1505 first=13.3983 kCE=14.3430 KD=3.8737 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  270/715 | grad_norm=55.92 | sec/step~8.83 | keep=1.00 | K=6 | llama: tf=11.3139 first=14.3352 kCE=9.6829 KD=3.1892 man=0.0002 | scale_pen(llama)=2.8141e-11 | qwen: tf=11.2304 first=13.4975 kCE=12.3987 KD=5.2620 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  280/715 | grad_norm=99.74 | sec/step~7.45 | keep=1.00 | K=6 | llama: tf=12.1001 first=13.8385 kCE=9.0293 KD=3.4253 man=0.0001 | scale_pen(llama)=2.8141e-11 | qwen: tf=10.8331 first=12.7956 kCE=11.9364 KD=5.1441 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  290/715 | grad_norm=10.35 | sec/step~6.88 | keep=1.00 | K=6 | llama: tf=12.9693 first=15.0409 kCE=8.2616 KD=3.3563 man=0.0002 | scale_pen(llama)=2.6890e-11 | qwen: tf=12.8248 first=13.9210 kCE=14.6382 KD=3.8479 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  300/715 | grad_norm=56.69 | sec/step~7.06 | keep=1.00 | K=6 | llama: tf=11.6353 first=15.1235 kCE=8.9851 KD=3.9672 man=0.0002 | scale_pen(llama)=2.6890e-11 | qwen: tf=11.9205 first=13.7696 kCE=13.4684 KD=5.1963 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  310/715 | grad_norm=107.93 | sec/step~6.64 | keep=1.00 | K=6 | llama: tf=12.5036 first=14.9864 kCE=8.2201 KD=3.2052 man=0.0002 | scale_pen(llama)=2.6890e-11 | qwen: tf=12.9461 first=13.4015 kCE=14.2071 KD=4.5090 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  320/715 | grad_norm=160.12 | sec/step~5.94 | keep=1.00 | K=6 | llama: tf=11.7729 first=14.3756 kCE=8.0055 KD=3.8652 man=0.0002 | scale_pen(llama)=1.0747e-11 | qwen: tf=10.4091 first=13.6706 kCE=13.6382 KD=3.7725 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  330/715 | grad_norm=46.88 | sec/step~7.06 | keep=1.00 | K=6 | llama: tf=11.7075 first=13.4700 kCE=8.9295 KD=3.4653 man=0.0002 | scale_pen(llama)=1.0747e-11 | qwen: tf=11.6756 first=12.7984 kCE=13.0233 KD=4.8534 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  340/715 | grad_norm=95.36 | sec/step~7.56 | keep=1.00 | K=6 | llama: tf=11.3563 first=14.2122 kCE=8.2788 KD=3.2063 man=0.0001 | scale_pen(llama)=1.0747e-11 | qwen: tf=11.4032 first=14.1446 kCE=13.7453 KD=4.7297 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  350/715 | grad_norm=145.07 | sec/step~6.52 | keep=1.00 | K=6 | llama: tf=11.8590 first=13.1488 kCE=8.3418 KD=3.6771 man=0.0002 | scale_pen(llama)=1.0747e-11 | qwen: tf=11.9637 first=11.5226 kCE=13.5659 KD=4.2043 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  360/715 | grad_norm=34.10 | sec/step~7.48 | keep=1.00 | K=6 | llama: tf=11.3709 first=13.6157 kCE=8.2056 KD=3.4176 man=0.0002 | scale_pen(llama)=3.1974e-14 | qwen: tf=11.2143 first=12.2357 kCE=13.4656 KD=4.5793 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  370/715 | grad_norm=73.24 | sec/step~6.99 | keep=1.00 | K=6 | llama: tf=11.5008 first=13.0157 kCE=8.0025 KD=3.6743 man=0.0002 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.4597 first=10.8149 kCE=12.3984 KD=4.1383 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  380/715 | grad_norm=108.83 | sec/step~7.03 | keep=1.00 | K=6 | llama: tf=11.0552 first=14.1657 kCE=8.2458 KD=3.3216 man=0.0001 | scale_pen(llama)=3.1974e-14 | qwen: tf=10.0788 first=12.6891 kCE=12.5727 KD=4.5787 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  390/715 | grad_norm=20.95 | sec/step~6.45 | keep=1.00 | K=6 | llama: tf=11.4174 first=12.6523 kCE=7.7801 KD=3.2555 man=0.0001 | scale_pen(llama)=7.5175e-12 | qwen: tf=10.7931 first=11.1242 kCE=13.2849 KD=3.7366 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  400/715 | grad_norm=57.44 | sec/step~7.59 | keep=1.00 | K=6 | llama: tf=12.0271 first=13.2236 kCE=8.9755 KD=3.6390 man=0.0002 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.5441 first=11.2722 kCE=12.5407 KD=5.2633 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  410/715 | grad_norm=94.00 | sec/step~6.17 | keep=1.00 | K=6 | llama: tf=12.0139 first=14.5063 kCE=7.9267 KD=3.9146 man=0.0002 | scale_pen(llama)=7.5175e-12 | qwen: tf=11.6107 first=13.1435 kCE=13.8047 KD=4.6647 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  420/715 | grad_norm=18.77 | sec/step~8.51 | keep=1.00 | K=6 | llama: tf=10.9045 first=14.1301 kCE=7.8364 KD=2.9246 man=0.0001 | scale_pen(llama)=1.4552e-11 | qwen: tf=10.7723 first=12.5074 kCE=13.4368 KD=4.1818 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  430/715 | grad_norm=64.23 | sec/step~6.33 | keep=1.00 | K=6 | llama: tf=11.8348 first=13.4359 kCE=7.7537 KD=3.8646 man=0.0001 | scale_pen(llama)=1.4552e-11 | qwen: tf=11.9699 first=11.9380 kCE=13.3105 KD=4.8025 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  440/715 | grad_norm=112.66 | sec/step~6.64 | keep=1.00 | K=6 | llama: tf=11.3897 first=13.2859 kCE=7.8289 KD=3.4544 man=0.0002 | scale_pen(llama)=1.4552e-11 | qwen: tf=10.4149 first=11.9518 kCE=13.0401 KD=4.2865 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  450/715 | grad_norm=7.96 | sec/step~6.67 | keep=1.00 | K=6 | llama: tf=11.7788 first=13.7975 kCE=7.9662 KD=3.4008 man=0.0002 | scale_pen(llama)=1.1141e-11 | qwen: tf=11.2793 first=12.4527 kCE=13.5883 KD=4.4433 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  460/715 | grad_norm=48.36 | sec/step~8.00 | keep=1.00 | K=6 | llama: tf=12.8652 first=13.9502 kCE=7.0592 KD=2.9144 man=0.0002 | scale_pen(llama)=1.1141e-11 | qwen: tf=11.5405 first=12.2618 kCE=13.7645 KD=3.7915 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  470/715 | grad_norm=85.28 | sec/step~10.32 | keep=1.00 | K=6 | llama: tf=10.4470 first=12.9339 kCE=8.9357 KD=2.9372 man=0.0001 | scale_pen(llama)=1.1141e-11 | qwen: tf=10.5443 first=10.4820 kCE=12.2152 KD=4.7056 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  480/715 | grad_norm=122.36 | sec/step~8.75 | keep=1.00 | K=6 | llama: tf=11.5410 first=13.7778 kCE=8.0472 KD=2.9801 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=12.0617 first=12.5855 kCE=13.7928 KD=5.1023 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  490/715 | grad_norm=45.59 | sec/step~7.38 | keep=1.00 | K=6 | llama: tf=12.3200 first=12.7070 kCE=8.6100 KD=3.7679 man=0.0002 | scale_pen(llama)=2.4016e-12 | qwen: tf=11.0328 first=10.5635 kCE=12.9528 KD=5.0521 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  500/715 | grad_norm=88.25 | sec/step~5.51 | keep=1.00 | K=6 | llama: tf=11.4804 first=13.7414 kCE=8.3566 KD=4.4769 man=0.0001 | scale_pen(llama)=2.4016e-12 | qwen: tf=10.2187 first=11.8198 kCE=13.6645 KD=4.9658 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  510/715 | grad_norm=134.30 | sec/step~8.12 | keep=1.00 | K=6 | llama: tf=12.0783 first=13.1233 kCE=7.7036 KD=3.1265 man=0.0002 | scale_pen(llama)=2.4016e-12 | qwen: tf=12.9262 first=11.3665 kCE=13.8157 KD=4.6881 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  520/715 | grad_norm=36.98 | sec/step~7.32 | keep=1.00 | K=6 | llama: tf=11.6057 first=12.9380 kCE=8.0371 KD=3.4351 man=0.0002 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.1920 first=10.7085 kCE=13.8708 KD=4.6335 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  530/715 | grad_norm=83.61 | sec/step~8.02 | keep=1.00 | K=6 | llama: tf=12.3594 first=12.4742 kCE=7.9749 KD=2.7905 man=0.0002 | scale_pen(llama)=5.6843e-14 | qwen: tf=10.3979 first=10.8116 kCE=13.9645 KD=3.5162 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  540/715 | grad_norm=134.20 | sec/step~7.74 | keep=1.00 | K=6 | llama: tf=12.0625 first=12.8325 kCE=8.3128 KD=3.7138 man=0.0002 | scale_pen(llama)=5.6843e-14 | qwen: tf=11.2849 first=11.1799 kCE=13.4815 KD=4.9390 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  550/715 | grad_norm=25.04 | sec/step~9.80 | keep=1.00 | K=6 | llama: tf=12.2568 first=13.5112 kCE=7.7704 KD=2.6454 man=0.0002 | scale_pen(llama)=1.2790e-13 | qwen: tf=12.1690 first=11.5423 kCE=13.7475 KD=4.7086 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  560/715 | grad_norm=63.84 | sec/step~8.59 | keep=1.00 | K=6 | llama: tf=10.6239 first=13.0111 kCE=7.7585 KD=2.9090 man=0.0002 | scale_pen(llama)=1.2790e-13 | qwen: tf=11.1049 first=11.2180 kCE=13.4214 KD=4.6138 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  570/715 | grad_norm=98.62 | sec/step~8.78 | keep=1.00 | K=6 | llama: tf=12.0250 first=12.5422 kCE=8.3047 KD=2.8248 man=0.0002 | scale_pen(llama)=1.2790e-13 | qwen: tf=10.4618 first=10.9120 kCE=13.0442 KD=4.3805 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  580/715 | grad_norm=15.53 | sec/step~7.32 | keep=1.00 | K=6 | llama: tf=11.2272 first=12.4313 kCE=7.0169 KD=3.3264 man=0.0002 | scale_pen(llama)=2.8777e-13 | qwen: tf=11.2028 first=9.7349 kCE=13.7043 KD=4.7007 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  590/715 | grad_norm=53.27 | sec/step~8.16 | keep=1.00 | K=6 | llama: tf=11.5441 first=11.4992 kCE=8.4419 KD=3.6714 man=0.0002 | scale_pen(llama)=2.8777e-13 | qwen: tf=10.7934 first=9.2085 kCE=12.5600 KD=5.3613 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  600/715 | grad_norm=90.66 | sec/step~6.44 | keep=1.00 | K=6 | llama: tf=11.2343 first=11.2596 kCE=8.3782 KD=3.3377 man=0.0002 | scale_pen(llama)=2.8777e-13 | qwen: tf=11.9166 first=8.9887 kCE=12.9979 KD=4.8285 man=0.0003 | scale_pen(qwen)=8.8818e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  610/715 | grad_norm=9.31 | sec/step~8.23 | keep=1.00 | K=6 | llama: tf=11.8541 first=11.0328 kCE=7.6765 KD=2.7595 man=0.0002 | scale_pen(llama)=1.0267e-12 | qwen: tf=10.3127 first=8.8801 kCE=13.2464 KD=4.0847 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  620/715 | grad_norm=55.04 | sec/step~8.11 | keep=1.00 | K=6 | llama: tf=11.6424 first=12.7232 kCE=7.0971 KD=2.8557 man=0.0002 | scale_pen(llama)=1.0267e-12 | qwen: tf=11.6173 first=10.5612 kCE=14.0292 KD=4.0392 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  630/715 | grad_norm=100.93 | sec/step~6.94 | keep=1.00 | K=6 | llama: tf=11.6683 first=11.2307 kCE=8.1515 KD=3.6268 man=0.0002 | scale_pen(llama)=1.0267e-12 | qwen: tf=11.6068 first=9.1065 kCE=13.7891 KD=4.3276 man=0.0003 | scale_pen(qwen)=5.6843e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  640/715 | grad_norm=144.90 | sec/step~7.12 | keep=1.00 | K=6 | llama: tf=11.7199 first=11.4480 kCE=7.5291 KD=3.2217 man=0.0002 | scale_pen(llama)=3.8689e-12 | qwen: tf=10.9277 first=9.6392 kCE=13.7963 KD=4.4778 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  650/715 | grad_norm=33.95 | sec/step~8.48 | keep=1.00 | K=6 | llama: tf=11.9567 first=11.6428 kCE=8.1746 KD=3.1897 man=0.0002 | scale_pen(llama)=3.8689e-12 | qwen: tf=10.4533 first=9.4475 kCE=13.0145 KD=4.2955 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  660/715 | grad_norm=67.14 | sec/step~6.82 | keep=1.00 | K=6 | llama: tf=11.5708 first=10.7404 kCE=7.4298 KD=3.7396 man=0.0002 | scale_pen(llama)=3.8689e-12 | qwen: tf=11.9385 first=9.1689 kCE=14.4529 KD=4.4853 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  670/715 | grad_norm=101.10 | sec/step~7.84 | keep=1.00 | K=6 | llama: tf=11.2520 first=11.9604 kCE=7.8758 KD=3.3797 man=0.0002 | scale_pen(llama)=3.8689e-12 | qwen: tf=11.3592 first=9.6511 kCE=13.6247 KD=4.7531 man=0.0003 | scale_pen(qwen)=3.1974e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  680/715 | grad_norm=51.53 | sec/step~6.40 | keep=1.00 | K=6 | llama: tf=11.7329 first=10.5093 kCE=8.0438 KD=3.6217 man=0.0002 | scale_pen(llama)=7.8479e-12 | qwen: tf=11.5216 first=8.2542 kCE=13.9759 KD=4.5536 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  690/715 | grad_norm=114.30 | sec/step~6.68 | keep=1.00 | K=6 | llama: tf=10.7021 first=10.8455 kCE=8.0684 KD=3.3925 man=0.0002 | scale_pen(llama)=7.8479e-12 | qwen: tf=9.7177 first=9.2262 kCE=12.7330 KD=4.4246 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  700/715 | grad_norm=183.52 | sec/step~6.29 | keep=1.00 | K=6 | llama: tf=11.7740 first=11.1777 kCE=7.8033 KD=3.7261 man=0.0002 | scale_pen(llama)=7.8479e-12 | qwen: tf=10.4865 first=9.7659 kCE=14.0239 KD=4.1035 man=0.0003 | scale_pen(qwen)=1.4211e-14 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  710/715 | grad_norm=27.30 | sec/step~7.03 | keep=1.00 | K=6 | llama: tf=12.0845 first=10.8152 kCE=8.3800 KD=3.3305 man=0.0002 | scale_pen(llama)=6.2670e-12 | qwen: tf=11.5873 first=8.9581 kCE=13.2781 KD=4.4454 man=0.0003 | scale_pen(qwen)=3.5527e-15 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
  step  715/715 | grad_norm=55.15 | sec/step~3.69 | keep=1.00 | K=6 | llama: tf=10.8459 first=11.4107 kCE=10.2143 KD=4.9230 man=0.0001 | scale_pen(llama)=2.0464e-12 | qwen: tf=11.5068 first=11.5611 kCE=13.8996 KD=6.0182 man=0.0003 | scale_pen(qwen)=0.0000e+00 | K=6 tau=1.25 | stats=[llama: rms_raw~0.0106 rms_cal~0.0106 embed_rms~0.01058; qwen: rms_raw~0.0137 rms_cal~0.0136 embed_rms~0.01364]
[checkpoint] Freed 1.8KB before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_fast_feedback/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.010608420950862077, 'rms_mean_cal': 0.010571067758112937, 'embed_rms': 0.01057521253824234, 'count': 715}, 'qwen': {'rms_mean_raw': 0.013713795876690558, 'rms_mean_cal': 0.013640664612965567, 'embed_rms': 0.013643525540828705, 'count': 715}}
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_fast_feedback/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_fast_feedback/eval_epoch1/Z.pt

[Standard Evaluation Mode - both models loaded]
(Use --sequential_eval to enable per-model encoder text auto-alignment.)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3565.07it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.00it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.08it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.53it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3060.98it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.06it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.22it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 16
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 5840 bytes (6-bit selected); fp16 reference: 14352 bytes; fp32 reference: 28688 bytes
latent/text bytes (one-copy, fp16): 11.40x

— Baseline: Text prompting
Llama  EM: 0.615  F1: 0.809  |  NLL/token (gold): 12.403911339973977
Qwen   EM: 0.680   F1: 0.854   |  NLL/token (gold): 24.624676366308734
Wall clock: 28.36s

— Latent prompting (shared interlingua)
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/latentwire/eval.py", line 1174, in <module>
    main()
  File "/projects/m000066/sujinesh/LatentWire/latentwire/eval.py", line 1115, in main
    print(f"Llama  EM: {summary['latent']['llama']['em']:.3f}  F1: {summary['latent']['llama']['f1']:.3f}  |  NLL/token (gold): {summary['latent']['llama']['nll_token']}")
KeyError: 'nll_token'
