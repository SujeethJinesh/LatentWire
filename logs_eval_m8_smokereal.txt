Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)

==== LatentWire Evaluation ====
Samples: 100  |  Max new tokens: 16
Avg prompt tokens (Llama): 33.7 | (Qwen): 28.5 | Latent length M: 8
Compression ratio (Llama): 4.2x | (Qwen): 3.6x
Approx interlingua payload per example: 8192 bytes (dtype float32, shape M=8, d_z=256)

— Baseline: Text prompting
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 11.542
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 10.948
Wall clock: 16.40s for 100 examples

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.000  |  NLL/token (gold): 10.292
Qwen   EM: 0.000   F1: 0.000   |  NLL/token (gold): 12.073
Wall clock: 9.83s for 100 examples

— Token-budget baseline (same #prefix tokens as latent)
Llama  EM: 0.000  F1: 0.022
Qwen   EM: 0.000   F1: 0.027
Wall clock: 5.71s for 100 examples

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.000
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.000
