# Codex Report – 2025-11-14

## 1. Run Snapshot & Empirical Status
- The latest sweep (`paper_writing/runs/ablations_20251113_213648/`) finished all eight DiT-bridge configs; each entry in `summary.log` shows stable source-alone (0.54) and target-alone (0.77) accuracies, while bridged accuracy ranges from 0.0 (soft_only) to ~0.59 (soft_plus_text) (paper_writing/runs/ablations_20251113_213648/summary.log:15-88).
- JSONL dumps for every eval step now include the full question, gold answer, and all three model outputs, so we can audit behavior post-hoc (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_plus_text/eval_samples_step_final.jsonl:1).
- Soft-only prompt mode yields empty bridged generations and `[invalid]` extracted answers (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_only/eval_samples_step_final.jsonl:1), confirming that the current translator cannot drive Llama without explicit text.
- Soft-plus-text mode keeps the full 8-shot GSM8K prompt attached and reaches 56–59 % bridged accuracy, with example rows showing correct “#### 18” completions (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_plus_text/eval_samples_step_final.jsonl:1).
- Training loss decays smoothly (e.g., Step 20 → 60: 13.09 → 10.80) and DiT flow metrics remain finite, indicating the recent stability fixes (smaller per-device batch, RMS matching, label padding) are effective (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_plus_text/train.log:520-560).

## 2. Prompt & Evaluation Pipeline (Code Walkthrough)
1. **Dataset to prompts.** `build_samples` clones each GSM8K entry and formats identical prompts for Mistral and Llama via `format_prompts`, toggling between simple CoT for training and 8-shot prefixes for eval (paper_writing/cross_attention.py:822-835, 858-882, 912-933).
2. **Source encoding & DiT teacher.** `build_batch_inputs` tokenizes the source prompt up to 8k tokens, collects the final hidden layer from Mistral, and (during training) pads teacher embeddings from the Llama tokenizer for either the answer or the prompt before feeding them into the DiT bridge (paper_writing/cross_attention.py:878-910).
3. **Soft token concatenation.** Soft tokens are RMS-normalized and prepended to the Llama embeddings, while attention masks and labels are extended so the translator observes no loss on its own outputs (paper_writing/cross_attention.py:915-994).
4. **Prompt modes.** During eval, `build_batch_inputs` switches on `args.eval_prompt_mode`:  
   - `soft_plus_text` concatenates the 8-shot text plus an instruction string after the soft tokens (`samples[i].tgt_prompt + "\nAnswer..."`), giving Llama literal grounding (paper_writing/cross_attention.py:946-950).  
   - `soft_only` feeds only BOS + the instruction string, which explains the empty generations when Llama relies solely on translated embeddings (paper_writing/cross_attention.py:946-952).
5. **Generation.** Evaluation calls `tgt_model.generate` with `inputs_embeds` but no `input_ids`, so HuggingFace directly decodes from the concatenated `[soft || text]` embeddings; baseline runs use standard token IDs (paper_writing/cross_attention.py:1103-1149). The absence of `input_ids` is legal but fragile—HuggingFace falls back to BOS if the embeddings collapse, which matches the `[invalid]` outputs in soft-only mode.
6. **Logging & metrics.** Every output is cleaned (strip added Q/A, capture first `####`) before computing accuracies and writing JSONL artifacts (paper_writing/cross_attention.py:1180-1266). Distributed sharding is logged (`samples 0…50` per rank) so we can match GPU usage to dataset splits (paper_writing/cross_attention.py:1075-1084).

## 3. Architecture Deep Dive
### Mistral 7B Instruct (Source, 8k context)
- Loaded via HuggingFace with sliding-window attention, grouped-query heads, and 32k SentencePiece vocab (paper_writing/run_ablations.sh:31-33; paper_writing/cross_attention.py:1442-1449). The released checkpoints report a 32,768-token vocabulary with SentencePiece training on curated web/math corpora [1].  
- **Research context:** Mistral’s sliding window mixes local/global context through ALiBi-like shifts and GQA (Mistral AI, 2023). Its hidden states encode RoPE phases tuned for 8k contexts and share key/value projections across head groups.  
- **Implication:** When we harvest only the last hidden layer (paper_writing/cross_attention.py:882-885) and drop positional metadata, Llama receives embeddings whose rotary phase and token segmentation no longer match its expectations.

### Llama‑3.1 8B Instruct (Target, 128k-ready)
- 32-layer decoder, 4096-d model dim, 128k-aware RoPE scaling, and a 128,256-token SentencePiece vocabulary (Meta specifies 128k entries plus reserved specials) [2], loaded with padding-side=left (paper_writing/cross_attention.py:1450-1479).  
- **Research context:** Meta’s report highlights new tokenizer statistics (open-source base vocabulary, 128k context) and balanced SwiGLU blocks (Meta AI, 2024).  
- **Implication:** Prepending 2,048 synthetic embeddings (paper_writing/cross_attention.py:1457-1460, 968-976) hijacks the rotary phase for ~2k positions, so the textual prompt appended afterward competes for residual attention and KV cache slots.

### DiT Bridge Translator (Rectified-Flow Variant)
- Architecture mirrors DiT/Flow Matching: timestep embeddings + AdaLN-Zero modulated Transformer blocks with conditional pooling over source states (paper_writing/cross_attention.py:569-643).  
- Training path `_forward_train_rf` injects teacher embeddings for denoising, while inference samples `steps_eval` iterations conditioned on pooled source context (paper_writing/cross_attention.py:646-719).  
- Loss stack: LM NLL + KL alignment + InfoNCE + prompt-alignment + format penalty + DiT flow loss (paper_writing/cross_attention.py:1692-1759).  
- **Research context:** DiT was developed for image diffusion (Peebles & Xie, 2023) with Gaussian noise schedules; adapting it to long text embeddings without positional supervision risks over-smoothing, especially when `soft_tokens` defaults to 2048 (paper_writing/cross_attention.py:1457-1460).

## 4. Failure Analysis & Potential Root Causes
1. **Tokenizer and RoPE mismatch.** Mistral’s SentencePiece splits and RoPE frequencies don’t align with Llama’s; we never reproject phases before concatenation (paper_writing/cross_attention.py:878-994). This can rotate embeddings away from Llama’s expected subspace, yielding nonsensical context once the textual tail starts.
2. **Attention budget saturation.** We always prepend `K` ones to the attention mask (paper_writing/cross_attention.py:972-977), so Llama must attend to thousands of soft tokens before it ever sees literal text. In soft-only mode, there is no text at all—`tgt_texts` reduces to `starter + instruction` (paper_writing/cross_attention.py:946-952)—so generation terminates after BOS, consistent with the empty strings in the logs (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_only/eval_samples_step_final.jsonl:1).
3. **Objective gap.** Training observes only gold answers (teacher forcing) and auxiliary embedding-level penalties; it never penalizes the free-form translator+Llama stack for producing incorrect answers (paper_writing/cross_attention.py:1692-1759). This explains why bridged accuracy lags target-alone by ~20 points even when reasoning text looks similar (paper_writing/runs/ablations_20251113_213648/summary.log:26,48,70).
4. **Diffusion noise vs. autoregression.** DiT predicts a full sequence before decoding begins; if later positions remain noisy, Llama starts from partially formed chains and often continues with memorized exemplars (paper_writing/runs/ablations_20251113_213648/1a_dit_2step_64tok_soft_plus_text/train.log:500-516). The absence of per-position guidance (only mean-pooled conditioning) makes it easy for DiT to collapse to a single “museum” template.
5. **Eval pipeline fragility.** Because `tgt_model.generate` receives only `inputs_embeds`, any NaN/zeroed embeddings cause HuggingFace to emit just BOS, masking translator bugs as `[invalid]` outputs (paper_writing/cross_attention.py:1103-1116). Claude’s earlier observation about dummy `input_ids` remains valid—without them, caching/streaming optimizations can’t reuse KV states, slowing eval.

## 5. Architecture-Aware Next Steps
1. **Hybrid conditioning (soft tokens + adapters).** Instead of replacing the full prompt, feed Llama its own tokenized prompt (`tgt_prompt`) and inject translator outputs via cross-attention adapters or additive residuals before layer 0. Concretely, modify `build_batch_inputs` to keep textual embeddings in-place and add a learned projection that mixes DiT outputs into Llama’s hidden states (`paper_writing/cross_attention.py:963-971`). This preserves tokenizer alignment and RoPE phases.
2. **RoPE alignment layer.** Introduce a small module that maps Mistral’s rotary phases to Llama’s before diffusion. The hook belongs right after `src_h` is captured (paper_writing/cross_attention.py:882-885). Estimating or learning the phase shift should prevent the downstream LM from treating translated embeddings as out-of-phase noise.
3. **Shorter, deterministic translators.** For the no-compression baseline, replace DiT with a transformer auto-encoder or flow-matching model that emits the same number of tokens as the textual prompt (`soft_tokens = len(tgt_prompt)`), then gradually compress to 75% length. This change touches the `args.soft_tokens` auto-cap logic (paper_writing/cross_attention.py:1457-1460) and the translator instantiation (paper_writing/cross_attention.py:1519-1541).
4. **Decode-aware supervision.** Periodically run the translator+Llama stack forward, decode answers, and backprop through sampled tokens using REINFORCE or straight-through Gumbel so the translator is directly penalized for wrong answers. The loss hook should be added after `tgt_model.generate` within the training loop (paper_writing/cross_attention.py:1690-1795). This addresses the current mismatch between teacher-forced training and free decoding.
5. **Contrastive tokenizer alignment.** Augment the InfoNCE block by comparing pooled translator embeddings against embeddings of the same question tokenized with Llama’s vocabulary (`tgt_tok`) as well as Mistral’s. This encourages the translator to maintain question-specific variance and mitigates template collapse. Modify the existing InfoNCE block (paper_writing/cross_attention.py:1716-1732) to include cross-tokenizer positives.
6. **Eval robustness.** Provide dummy `input_ids` alongside `inputs_embeds` when calling `generate` (paper_writing/cross_attention.py:1103-1116) so the HF cache stays valid; simultaneously log cosine norms of `inputs_embeds` to detect collapsed sequences. Also, keep `soft_plus_text` as the primary eval mode in `run_ablations.sh` to avoid burning time on known-failing configs (paper_writing/run_ablations.sh:38-111).
7. **Bidirectional transfer check.** Run the mirror experiment with Llama‑3.1 as the *source* encoder and Mistral as the *target* decoder to see whether the translator still collapses or if the directionality is driving the failure. This only requires swapping `--source_model`/`--target_model` in `run_ablations.sh` (paper_writing/run_ablations.sh:31-33) and verifying tokenizer-specific code paths (paper_writing/cross_attention.py:878-985) handle the reversed vocab sizes.

## Rebuttal to CLAUDE
1. **Evaluation fixes and prompt modes.** Claude correctly highlights the gold-answer bug, truncated outputs, and the dual prompt modes (paper_writing/CLAUDE_REPORT.md:33-63). Our code references (paper_writing/cross_attention.py:1180-1266, 946-952) corroborate those changes, so we agree that `soft_plus_text` is the only meaningful configuration today.
2. **KL misalignment severity.** While Claude notes the positional mismatch behind the loss spike (paper_writing/CLAUDE_REPORT.md:159-197), the report frames it as a recoverable nuisance. Because the KL term keeps comparing prompt tokens to answer tokens (paper_writing/cross_attention.py:1696-1714), it continuously injects noisy gradients even after the spike, so we treat the fix as critical rather than optional.
3. **Breakthrough framing vs. baseline gap.** The claim that 63.5% is “comparable to target-alone” (paper_writing/CLAUDE_REPORT.md:22-26) downplays the 13–20 point deficit visible in `summary.log`. We keep that gap explicit to avoid misleading stakeholders about current performance.
4. **Soft-only impossibility conclusion.** Claude attributes the 0% soft-only accuracy to inherent architectural limits (paper_writing/CLAUDE_REPORT.md:134-158), but our pipeline never aligns tokenizer vocabularies or RoPE phases (paper_writing/cross_attention.py:878-985) and never supervises free decoding. Until those issues are addressed we consider the experiment inconclusive, not disproven.
5. **Missing architectural context.** The CLAUDE report discusses compression and pooling choices but omits the mismatched vocab/positional geometry between Mistral and Llama. Our deep-dive (Section 3) fills that gap and motivates RoPE/token alignment before expanding the sweep.
6. **Next-step prioritization and runtime pressure.** Claude’s “Immediate/Short-term” plan prioritizes compression sweeps and DiT+cross-attention hybrids (paper_writing/CLAUDE_REPORT.md:499-540), but each sweep consumes ~2 hours/GPU—time we don’t have while the cluster queue is saturated. We therefore push back: fix the KL bug, reduce prompt-alignment weight, and add decode-aware supervision *before* launching any new grid, ensuring every H100-hour advances the baseline rather than revalidating known issues.

## Action Plan After CLAUDE Review
1. **Fix KL and prompt-alignment pathologies.** Rework the KL slice to compare answer tokens to answer tokens and re-tokenize the baseline with answer text (paper_writing/cross_attention.py:1696-1714); simultaneously drop the prompt-alignment weight from 0.05 to ≈0.001 so soft tokens can deviate from literal prompt embeddings, matching Claude’s own concern about over-constraint (paper_writing/CLAUDE_REPORT.md:134-158).
2. **Align tokenizers and RoPE phases.** Insert a projection layer immediately after `src_h` (paper_writing/cross_attention.py:882-885) that maps Mistral’s 32,768-token embedding space and rotary angles to Llama’s 128,256-token geometry. This directly tackles the architectural gap Claude’s report omits and should determine whether the 63.5% ceiling is due to mismatched representations.
3. **Add decode-aware supervision.** Extend the training loop to periodically decode the translator+Llama stack, compare answers against the GSM8K key, and propagate that loss/reward back through the translator—closing the training/inference gap that Claude attributes to “generation failure” in soft_only mode (paper_writing/CLAUDE_REPORT.md:134-158).
4. **Run the bidirectional and adapter baselines.** After the above fixes, perform (a) the Llama→Mistral mirror experiment and (b) the hybrid conditioning run where textual prompts stay intact and DiT outputs enter via adapters (paper_writing/cross_attention.py:963-971). These tests will show whether directionality or wholesale prompt replacement drives the collapse before we revisit Claude’s proposed compression sweeps.
5. **Conditional ablation gate (Phase 1.5).** To honor Claude’s LGTM condition, add a guard in the runbook: if the all-in Phase 1 run lands below 70 % bridged accuracy, immediately schedule the targeted ablations (B & C for 65–69 %, A+B+C for <65 %) before moving to Phase 2. This keeps the initial turnaround fast but preserves Claude’s insistence on attribution when results fall short.

These steps directly target the architectural mismatches documented above and should move us toward the stated goal: an interlingua translator that improves Llama’s task accuracy beyond the native baseline rather than suppressing it.

## References
1. Mistral AI. “Mistral 7B Technical Report.” 2023. https://mistral.ai/news/mistral-7b/
2. Meta AI. “Llama 3 Model Card & Technical Report.” 2024. https://ai.meta.com/research/publications/the-llama-3-herd-of-models/
3. Peebles, W. & Xie, S. “DiT: Diffusion Transformers for Image Generation.” ICCV 2023. https://arxiv.org/abs/2212.09748
