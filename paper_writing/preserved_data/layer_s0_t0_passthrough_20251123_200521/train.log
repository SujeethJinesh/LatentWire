W1123 20:05:22.424000 1373175 torch/distributed/run.py:793] 
W1123 20:05:22.424000 1373175 torch/distributed/run.py:793] *****************************************
W1123 20:05:22.424000 1373175 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1123 20:05:22.424000 1373175 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================================================================================================================================================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)

============================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)torch.cuda.is_available() check...

============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
  Result: True  Result: True  Result: True
torch.cuda.device_count() check...

torch.cuda.device_count() check...

torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 0/4] Starting DDP initialization...
[Rank 0] Checking torch.cuda.is_available()...
[Rank 0]   Result: True
[Rank 0] Checking torch.cuda.device_count()...
[Rank 0]   Result: 4 devices
[Rank 0] Getting device name for GPU 0...
[Rank 0] GPU 0 available: NVIDIA H100 80GB HBM3
[Rank 0] Calling init_process_group (timeout=60s)...
[W1123 20:05:31.055820278 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 0] DDP initialized successfully on GPU 0
[Rank 0] Setting up reproducibility (base_seed=1234)...
[Rank 0] Reproducibility setup complete (effective_seed=1234)
Reproducibility enabled: base_seed=1234, effective_seed=1234
==== Config ====
source_model: meta-llama/Meta-Llama-3.1-8B-Instruct
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: 128
eval_prompt_mode: soft_only
source_layer: 0
target_layer: 0
passthrough_soft: True
soft_injection: prepend
adapter_scale: 1.0
skip_source_baseline: True
depth: 2
heads: 8
lr: 0.0001
weight_decay: 0.01
train_steps: 0
warmup_steps: 200
per_device_batch: 2
eval_every: 250
eval_samples: 100
max_new_tokens: 256
log_dir: paper_writing/runs/layer_s0_t0_passthrough_20251123_200521
seed: 1234
bf16: True
save_path: paper_writing/runs/layer_s0_t0_passthrough_20251123_200521/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 0
dataset: gsm8k
info_nce_weight: 0.05
eval_batch_size: 36
decode_loss_weight: 0.0
decode_interval: 50
decode_samples: 1
decode_max_length: 4096
kl_max_length: 512
kl_tokens: 20
prompt_alignment_weight: 0.001
soft_only_curriculum_steps: 0
prompt_contrast_weight: 0.0
aux_probe_weight: 0.0
format_loss_weight: 0.1
token_alignment_weight: 0.0
no_compile: True
bridge: dit
dit_dim: 512
dit_depth: 6
dit_heads: 8
dit_steps_train: 4
dit_steps_eval: 8
dit_dropout: 0.1
dit_cfg: 0.0
dit_drop_cond: 0.1
dit_cfg_dropout: None
dit_pool: mean
dit_cond_dim: 512
dit_loss_weight: 0.1
dit_loss_warmup: 0
dit_teacher: answer

==== Reproducibility Settings ====
Base seed: 1234
Effective seed (with rank offset): 1234
CUDA deterministic: True
CUDA benchmark: False
Deterministic algorithms: True
CUBLAS workspace: :16:8
World size: 4 (each rank samples different data)
Loading source model/tokenizer...
  Result: 4 GPUs
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 2/4] Starting DDP initialization...
[Rank 2] Checking torch.cuda.is_available()...
[Rank 2]   Result: True
[Rank 2] Checking torch.cuda.device_count()...
[Rank 2]   Result: 4 devices
[Rank 2] Getting device name for GPU 2...
[Rank 2] GPU 2 available: NVIDIA H100 80GB HBM3
[Rank 2] Calling init_process_group (timeout=60s)...
[W1123 20:05:31.093313611 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 3/4] Starting DDP initialization...
[Rank 3] Checking torch.cuda.is_available()...
[Rank 3]   Result: True
[Rank 3] Checking torch.cuda.device_count()...
[Rank 3]   Result: 4 devices
[Rank 3] Getting device name for GPU 3...
[Rank 3] GPU 3 available: NVIDIA H100 80GB HBM3
[Rank 3] Calling init_process_group (timeout=60s)...
[W1123 20:05:31.122064090 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 1/4] Starting DDP initialization...
[Rank 1] Checking torch.cuda.is_available()...
[Rank 1]   Result: True
[Rank 1] Checking torch.cuda.device_count()...
[Rank 1]   Result: 4 devices
[Rank 1] Getting device name for GPU 1...
[Rank 1] GPU 1 available: NVIDIA H100 80GB HBM3
[Rank 1] Calling init_process_group (timeout=60s)...
[W1123 20:05:31.137472556 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 3] DDP initialized successfully on GPU 3
[Rank 3] Setting up reproducibility (base_seed=1234)...
[Rank 3] Reproducibility setup complete (effective_seed=1237)
[Rank 1] DDP initialized successfully on GPU 1
[Rank 1] Setting up reproducibility (base_seed=1234)...
[Rank 1] Reproducibility setup complete (effective_seed=1235)
[Rank 2] DDP initialized successfully on GPU 2
[Rank 2] Setting up reproducibility (base_seed=1234)...
[Rank 2] Reproducibility setup complete (effective_seed=1236)
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2098.20it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2011.42it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1347.68it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1022.69it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.30s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.20s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.26s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 11.00s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.04s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.01s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:30<00:10, 10.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  7.16s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:32<00:00,  8.23s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  7.17s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.26s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  7.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.41s/it]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7377.84it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 10727.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 10672.53it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 10979.85it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.75s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:32, 10.72s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:31, 10.54s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:31, 10.60s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.59s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:20<00:20, 10.46s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:21, 10.59s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.48s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.44s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:31<00:10, 10.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.60s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.41s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.46s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.60s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.54s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  7.38s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:34<00:00,  8.56s/it]
torch.compile() disabled via --no_compile flag (prevents CUDA Graph OOM)
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
NOTE: RMS scale matching is applied during batch collation (before concat)
Bridge: dit (DiTBridgeTranslator)
  Using DiTBridge: dim=512, depth=6, heads=8, steps(train/eval)=4/8, cfg=0.0, pool=mean, cond_dim=512
Translator parameters: 60.9M
Loading GSM8K...
Data sampler initialized with seed 1234 (base=1234, rank=0)
Optimizer groups: 38 decay, 36 no_decay, 0 gates (LR ×3)

============================================================
INITIAL EVALUATION (Step 0 - Before Training)
============================================================
[Distributed Eval] Sharding 100 samples across 4 ranks:
  Rank 0: samples 0 to 25 (25 samples)
  Rank 1: samples 25 to 50 (25 samples)
  Rank 2: samples 50 to 75 (25 samples)
  Rank 3: samples 75 to 100 (25 samples)
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================
============================================================

[Eval] Step 0 | Source-alone acc: 0.000 | Target-alone acc: 0.000 | Bridged acc: 0.000
============================================================

Saved translator to paper_writing/runs/layer_s0_t0_passthrough_20251123_200521/checkpoint.pt
[Distributed Eval] Sharding 100 samples across 4 ranks:
  Rank 0: samples 0 to 25 (25 samples)
  Rank 1: samples 25 to 50 (25 samples)
  Rank 2: samples 50 to 75 (25 samples)
  Rank 3: samples 75 to 100 (25 samples)

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================
============================================================

[Final Eval] Source-alone acc: 0.000 | Target-alone acc: 0.000 | Bridged acc: 0.000
