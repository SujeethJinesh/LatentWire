W1108 23:12:38.193000 2955072 torch/distributed/run.py:793] 
W1108 23:12:38.193000 2955072 torch/distributed/run.py:793] *****************************************
W1108 23:12:38.193000 2955072 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1108 23:12:38.193000 2955072 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: 64
depth: 8
heads: 16
lr: 0.0001
weight_decay: 0.01
train_steps: 3000
warmup_steps: 750
per_device_batch: 10
eval_every: 250
eval_samples: 500
max_new_tokens: 256
seed: 1234
bf16: True
save_path: paper_writing/runs/ablations_20251108_231236/1a_stable_64tok/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 5
dataset: gsm8k
info_nce_weight: 0.05
Loading source model/tokenizer...
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1147.76it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1908.24it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 4130.96it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 6689.48it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.50it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.11it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.10it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.92it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]
Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.27it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.36it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:00<00:00,  2.35it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.57it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.40it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.70it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.65it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.51it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.50it/s]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4306.27it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00, 12.71it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8919.31it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1652.11it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9063.87it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.87it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.17it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.35it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.36it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.11it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.55it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.63it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.54it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.80it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.61it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.71it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.62it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.52it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.52it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.36it/s]
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
Translator parameters: 189.0M
Loading GSM8K...
Optimizer groups: 66 decay, 99 no_decay, 16 gates (LR ×3)

============================================================
LABEL ALIGNMENT DIAGNOSTIC (Step 0)
============================================================
  Input embeddings shape: torch.Size([10, 380, 4096])
  Labels shape: torch.Size([10, 380])
  Attention mask shape: torch.Size([10, 380])
  Soft token count (K): 64
  Total tokens: 3800
  Supervised tokens: 1014
  Masked tokens: 2786
  Sample 0 first 10 labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
  Sample 0 last 10 labels: [717, 2511, 717, 2033, 12556, 3153, 627, 827, 220, 717]
  All soft token labels == -100? True
  Supervised tokens per sample: [84, 131, 60, 123, 94, 53, 140, 211, 50, 68]
============================================================

Step 20/3000 | Loss (avg over last 20): 3.2820
Step 40/3000 | Loss (avg over last 20): 2.9877
Step 60/3000 | Loss (avg over last 20): 2.1273
Step 80/3000 | Loss (avg over last 20): 0.8575
Step 100/3000 | Loss (avg over last 20): 0.5495
Step 120/3000 | Loss (avg over last 20): 0.4994
Step 140/3000 | Loss (avg over last 20): 0.4947
Step 160/3000 | Loss (avg over last 20): 0.4833
Step 180/3000 | Loss (avg over last 20): 0.4595
Step 200/3000 | Loss (avg over last 20): 0.4772
Step 220/3000 | Loss (avg over last 20): 0.4492
Step 240/3000 | Loss (avg over last 20): 0.4721
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
