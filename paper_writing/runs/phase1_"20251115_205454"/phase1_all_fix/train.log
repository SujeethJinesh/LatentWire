W1115 20:54:55.769000 2523618 torch/distributed/run.py:793] 
W1115 20:54:55.769000 2523618 torch/distributed/run.py:793] *****************************************
W1115 20:54:55.769000 2523618 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1115 20:54:55.769000 2523618 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================================================================================================================================================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...


EARLY CUDA DIAGNOSTICS (before DDP setup)EARLY CUDA DIAGNOSTICS (before DDP setup)

========================================================================================================================

torch.cuda.is_available() check...torch.cuda.is_available() check...


EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: 4 GPUs
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 1/4] Starting DDP initialization...
[Rank 1] Checking torch.cuda.is_available()...
[Rank 1]   Result: True
[Rank 1] Checking torch.cuda.device_count()...
[Rank 1]   Result: 4 devices
[Rank 1] Getting device name for GPU 1...
[Rank 1] GPU 1 available: NVIDIA H100 80GB HBM3
[Rank 1] Calling init_process_group (timeout=60s)...
[W1115 20:55:06.940317209 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 3/4] Starting DDP initialization...
[Rank 3] Checking torch.cuda.is_available()...
[Rank 3]   Result: True
[Rank 3] Checking torch.cuda.device_count()...
[Rank 3]   Result: 4 devices
[Rank 3] Getting device name for GPU 3...
[Rank 3] GPU 3 available: NVIDIA H100 80GB HBM3
[Rank 3] Calling init_process_group (timeout=60s)...
[W1115 20:55:06.960165824 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 0/4] Starting DDP initialization...
[Rank 0] Checking torch.cuda.is_available()...
[Rank 0]   Result: True
[Rank 0] Checking torch.cuda.device_count()...
[Rank 0]   Result: 4 devices
[Rank 0] Getting device name for GPU 0...
[Rank 0] GPU 0 available: NVIDIA H100 80GB HBM3
[Rank 0] Calling init_process_group (timeout=60s)...
[W1115 20:55:06.013767007 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 0] DDP initialized successfully on GPU 0
[Rank 0] Setting up reproducibility (base_seed=1234)...
[Rank 0] Reproducibility setup complete (effective_seed=1234)
Reproducibility enabled: base_seed=1234, effective_seed=1234
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: -1
eval_prompt_mode: soft_plus_text
depth: 2
heads: 8
lr: 0.0001
weight_decay: 0.01
train_steps: 2000
warmup_steps: 200
per_device_batch: 2
eval_every: 250
eval_samples: 200
max_new_tokens: 256
log_dir: paper_writing/runs/phase1_"20251115_205454"/phase1_all_fix
seed: 1234
bf16: True
save_path: paper_writing/runs/phase1_"20251115_205454"/phase1_all_fix/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 3
dataset: gsm8k
info_nce_weight: 0.05
eval_batch_size: 36
decode_loss_weight: 0.05
decode_interval: 100
kl_max_length: 512
kl_tokens: 20
no_compile: True
bridge: dit
dit_dim: 512
dit_depth: 6
dit_heads: 8
dit_steps_train: 4
dit_steps_eval: 8
dit_dropout: 0.1
dit_cfg: 0.0
dit_drop_cond: 0.1
dit_cfg_dropout: None
dit_pool: mean
dit_cond_dim: 512
dit_loss_weight: 0.1
dit_loss_warmup: 0
dit_teacher: answer

==== Reproducibility Settings ====
Base seed: 1234
Effective seed (with rank offset): 1234
CUDA deterministic: True
CUDA benchmark: False
Deterministic algorithms: True
CUBLAS workspace: :16:8
World size: 4 (each rank samples different data)
Loading source model/tokenizer...
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 2/4] Starting DDP initialization...
[Rank 2] Checking torch.cuda.is_available()...
[Rank 2]   Result: True
[Rank 2] Checking torch.cuda.device_count()...
[Rank 2]   Result: 4 devices
[Rank 2] Getting device name for GPU 2...
[Rank 2] GPU 2 available: NVIDIA H100 80GB HBM3
[Rank 2] Calling init_process_group (timeout=60s)...
[W1115 20:55:06.112847384 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 3] DDP initialized successfully on GPU 3[Rank 1] DDP initialized successfully on GPU 1
[Rank 3] Setting up reproducibility (base_seed=1234)...

[Rank 1] Setting up reproducibility (base_seed=1234)...
[Rank 3] Reproducibility setup complete (effective_seed=1237)
[Rank 1] Reproducibility setup complete (effective_seed=1235)
[Rank 2] DDP initialized successfully on GPU 2
[Rank 2] Setting up reproducibility (base_seed=1234)...
[Rank 2] Reproducibility setup complete (effective_seed=1236)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1766.52it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1897.30it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 4506.77it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 3774.12it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:37, 18.91s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:19<00:38, 19.18s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:19<00:38, 19.20s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:19<00:39, 19.90s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:37<00:18, 18.68s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:37<00:18, 18.82s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:38<00:19, 19.14s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:37<00:18, 18.90s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.39s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.73s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.25s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.95s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.72s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.27s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:53<00:00, 17.68s/it]
Loading target model/tokenizer...
Soft tokens not specified; capping to 2048 tokens
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1828.78it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 702.06it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4698.18it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 544.10it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.70s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.80s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:17<00:53, 17.80s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:34, 17.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:34, 17.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:34, 17.44s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:34<00:34, 17.46s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:52<00:17, 17.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:52<00:17, 17.66s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:53<00:17, 17.72s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 12.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 12.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.37s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 12.48s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:57<00:00, 14.37s/it]
torch.compile() disabled via --no_compile flag (prevents CUDA Graph OOM)
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
NOTE: RMS scale matching is applied during batch collation (before concat)
Bridge: dit (DiTBridgeTranslator)
  Using DiTBridge: dim=512, depth=6, heads=8, steps(train/eval)=4/8, cfg=0.0, pool=mean, cond_dim=512
Translator parameters: 60.9M
Loading GSM8K...
Data sampler initialized with seed 1234 (base=1234, rank=0)
Optimizer groups: 38 decay, 36 no_decay, 0 gates (LR ×3)

============================================================
INITIAL EVALUATION (Step 0 - Before Training)
============================================================
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================

--- Example 1 ---
Question (full prompt):
Answer the following questions step by step and end with '#### <number>'.

Q: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?
A: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.
So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.
There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.
So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.
Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.
#### 12

Q: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?
A: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.
Natalie picked 36/2 = <<36/2=18>>18 strawberries.
All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.
They can make 70/7 = <<70/7=10>>10 jars of strawberries.
They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.
#### 40

Q: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?
A: There are 960 pages because 80 x 12 = <<80*12=960>>960
Each book is 160 pages because 960 / 6 = <<960/6=160>>160
#### 160

Q: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?
A: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.
He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.
#### 245

Q: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?
A: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.
Ines has $20 - $6 = $<<20-6=14>>14 left.
#### 14

Q: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?
A: In year 2 he pays 80+10=$<<80+10=90>>90.
In year 3 he pays 90+10=$<<90+10=100>>100.
In year 4 he pays 100+10=$<<100+10=110>>110.
In year 5 he pays 110+10=$<<110+10=120>>120.
In year 6 he pays 120+10=$<<120+10=130>>130.
#### 130

Q: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?
A: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.
The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.
The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.
For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.
The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.
Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.
#### 3982

Q: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.
A: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40
Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30
The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70
From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30
#### 30

Q: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
A: Let's think step by step.

Answer:
Gold answer: 18
Source-alone (extracted): 18
Source-alone full output:
1. Janet's ducks lay 16 eggs per day.
2. She eats 3 for breakfast, so she has 16 - 3 = <<16-3=13>>13 eggs left.
3. She bakes 4 muffins, so she has 13 - 4 = <<13-4=9>>9 eggs left.
4. She sells the remaining eggs at the farmers' market for $2 per egg, so she makes 9 * $2 = $<<9*2=18>>18 per day.

Final answer: Janet makes $18 per day at the farmers' market.
#### 18

Q: A group of friends went to a restaurant and ordered 12 pizzas. Each pizza had 8 slices. How many slices of pizza did they order in total?
A: Each pizza has 8 slices, so the total number of slices is 12 * 8 = <<12*8=96>>96.
#### 96
Target-alone (extracted): 18
Target-alone full output:
Janet eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses 3+4=7 eggs.
She lays 16 eggs per day, so she has 16-7=9 eggs left to sell.
She sells 9 eggs for $2 each, so she makes 9*2=$18.
#### 18

Q: A bakery sells 250 loaves of bread per day. They sell whole wheat bread for $2.50 per loaf and white bread for $1.75 per loaf. If they make a 10% profit on the sale of each loaf, how much profit do they make in a day?
A: The bakery sells 250 loaves of whole wheat bread for $2.50 each, so they make 250*2.50=$625.
The bakery sells 250 loaves of white bread for $1.75 each, so they make 250*1.75=$437.50.
The bakery makes a total of $625+$437.50=$1062.50.
The bakery makes a 10% profit on each loaf, so they make a 10% profit on 250 loaves of whole wheat bread, which is 250*.10=$
Bridged (extracted): 40
Bridged full output:
If he's think step by the step.
A: There are 960 pages is one inch thick. If he's think step by the step.
A: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he's think step by the step.
A: For the first year will be $4 = $<<4*12=40>>40
#### 40

Q: James has a shelf of books that is 12 inches thick. He knows from knowledge that 80 pages is one inch thick. If he's think step by the step.
A: There are 960 pages because 80 x 12 = <<80*12=160>>160
#### 160

--- Example 2 ---
Question (full prompt):
Answer the following questions step by step and end with '#### <number>'.

Q: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?
A: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.
So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.
There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.
So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.
Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.
#### 12

Q: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?
A: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.
Natalie picked 36/2 = <<36/2=18>>18 strawberries.
All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.
They can make 70/7 = <<70/7=10>>10 jars of strawberries.
They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.
#### 40

Q: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?
A: There are 960 pages because 80 x 12 = <<80*12=960>>960
Each book is 160 pages because 960 / 6 = <<960/6=160>>160
#### 160

Q: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?
A: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.
He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.
#### 245

Q: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?
A: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.
Ines has $20 - $6 = $<<20-6=14>>14 left.
#### 14

Q: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?
A: In year 2 he pays 80+10=$<<80+10=90>>90.
In year 3 he pays 90+10=$<<90+10=100>>100.
In year 4 he pays 100+10=$<<100+10=110>>110.
In year 5 he pays 110+10=$<<110+10=120>>120.
In year 6 he pays 120+10=$<<120+10=130>>130.
#### 130

Q: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?
A: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.
The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.
The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.
For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.
The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.
Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.
#### 3982

Q: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.
A: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40
Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30
The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70
From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30
#### 30

Q: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?
A: Let's think step by step.

Answer:
Gold answer: 3
Source-alone (extracted): 3
Source-alone full output:
1. The robe takes 2 bolts of blue fiber.
2. It takes half as much white fiber as blue fiber, so it takes half of 2 bolts, which is 2/2 = 1 bolt of white fiber.
3. In total, it takes 2 bolts of blue fiber + 1 bolt of white fiber = <<2+1=3>>3 bolts.

#### 3

Q: A recipe calls for 1/4 cup of sugar, 1/2 cup of flour, and 1/3 cup of milk. How many cups of these ingredients are needed in total?
A: Let's think step by step.

Answer:
1. The recipe calls for 1/4 cup of sugar, which is equal to 1/4 of 1 cup, or 1/4 * 1 = 1/4 cup.
2. It calls for 1/2 cup of flour, which is equal to 1/2 of 1 cup, or 1/2 * 1 = 1/2 cup.
3. It calls for 1/3 cup of milk, which is equal to
Target-alone (extracted): 3
Target-alone full output:
3
Explanation: A robe takes 2 bolts of blue fiber.  It takes half that much white fiber, which is 2/2 = 1 bolt of white fiber.  So, in total, it takes 2 + 1 = 3 bolts of fiber.
#### 3

Q: A bakery sells 250 loaves of bread per day.  They make a profit of $0.25 per loaf.  How much profit do they make in a day?
A: Let's think step by step.

Answer: $62.50
Explanation: The bakery sells 250 loaves of bread per day.  They make a profit of $0.25 per loaf.  So, their daily profit is 250 * $0.25 = $62.50.
#### 62.50
Bridged (extracted): 900
Bridged full output:
If he's has a stack of books that is 12 inches thick. He's has a shelf of pages is each one on average?
A: There are 960 pages because 960 / 6 = <<960+6=160>>160.
In year 7 he's has a stack of books that is 12 inches thick. He's knows from experience that is 80 pages is one inch thick. If he's has a shelf of books that is 12 inches thick. He's uses half of them, leaving 500/2 = <<500+10=600>>600.
The total value of the investment for the first year will be $6000 + $<<600+10=700>>700.
In year 8 he's has a stack of stacks that is 12 inches thick. He's uses half of them, leaving 800/2 = <<800+10=900>>900.
#### 900

Q: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.
A: When Kendra

--- Example 3 ---
Question (full prompt):
Answer the following questions step by step and end with '#### <number>'.

Q: For every 12 cans you recycle, you receive $0.50, and for every 5 kilograms of newspapers, you receive $1.50. If your family collected 144 cans and 20 kilograms of newspapers, how much money would you receive?
A: There are 144/12 = <<144/12=12>>12 sets of 12 cans that the family collected.
So, the family would receive $0.50 x 12 = $<<0.50*12=6>>6 for the cans.
There are 20/5 = <<20/5=4>>4 sets of 5 kilograms of newspapers that the family collected.
So, the family would receive $1.50 x 4 = $<<1.50*4=6>>6 for the newspapers.
Therefore, the family would receive a total of $6 + $6 = $<<6+6=12>>12.
#### 12

Q: Betty picked 16 strawberries. Matthew picked 20 more strawberries than Betty and twice as many as Natalie. They used their strawberries to make jam. One jar of jam used 7 strawberries and they sold each jar at $4. How much money were they able to make from the strawberries they picked?
A: Matthew picked 16 + 20 = <<16+20=36>>36 strawberries.
Natalie picked 36/2 = <<36/2=18>>18 strawberries.
All together, they have 16 + 36 + 18 = <<16+36+18=70>>70 strawberries.
They can make 70/7 = <<70/7=10>>10 jars of strawberries.
They earn 10 x $4 = $<<10*4=40>>40 from the strawberries they picked.
#### 40

Q: Jack has a stack of books that is 12 inches thick. He knows from experience that 80 pages is one inch thick. If he has 6 books, how many pages is each one on average?
A: There are 960 pages because 80 x 12 = <<80*12=960>>960
Each book is 160 pages because 960 / 6 = <<960/6=160>>160
#### 160

Q: James dumps his whole collection of 500 Legos on the floor and starts building a castle out of them.  He uses half the pieces before finishing and is told to put the rest away.  He puts all of the leftover pieces back in the box they came from, except for 5 missing pieces that he can't find.  How many Legos are in the box at the end?
A: James starts with 500 Legos and uses half of them, leaving 500/2=<<500/2=250>>250 Legos unused.
He puts those unused Legos away but since he's missing 5 he only puts 250-5=<<250-5=245>>245 Legos away.
#### 245

Q: Ines had $20 in her purse. She bought 3 pounds of peaches, which are $2 per pound at the local farmers’ market. How much did she have left?
A: Ines bought 3 pounds of peaches for 3 peaches * $2/peach = $<<3*2=6>>6.
Ines has $20 - $6 = $<<20-6=14>>14 left.
#### 14

Q: Aaron pays his actuary membership fees each year. The membership fee increases yearly by $10. If he pays $80 in the first year, how much does his membership cost, in dollars, in the sixth year?
A: In year 2 he pays 80+10=$<<80+10=90>>90.
In year 3 he pays 90+10=$<<90+10=100>>100.
In year 4 he pays 100+10=$<<100+10=110>>110.
In year 5 he pays 110+10=$<<110+10=120>>120.
In year 6 he pays 120+10=$<<120+10=130>>130.
#### 130

Q: Joseph invested $1000 into a hedge fund. The fund promised a yearly interest rate of 10%. If he deposited an additional $100 every month into the account to add to his initial investment of $1000, how much money will he have in the fund after two years?
A: For the first year, Joseph will have invested $1000 + ($100 * 12) = $<<1000+100*12=2200>>2200.
The interest calculated for the first year will be $2200 * 10% = $<<2200*10*.01=220>>220.
The total value of the investment for the first year will be $2200 + $220 = $<<2200+220=2420>>2420.
For the second year, the total invested will be $2420 + ($100 * 12) = $<<2420+100*12=3620>>3620.
The interest calculated after the second year will be $3620 * 10% = $<<3620*10*.01=362>>362.
Therefore, Joseph's investment in the mutual fund will be worth $3620 + $362 = $<<3620+362=3982>>3982.
#### 3982

Q: The price of buying a wooden toy at the new Craftee And Best store is $20, and the cost of buying a hat is $10. If Kendra went to the shop with a $100 bill and bought two wooden toys and three hats, calculate the change she received.
A: When Kendra bought 2 toys, she paid 2*$20 = $<<2*20=40>>40
Since the price of a hat is $10, when Kendra bought 3 hats, she paid 3*$10 = $<<3*10=30>>30
The total costs for the hats and wooden toys Kendra bought is $40+$30 = $<<40+30=70>>70
From the $100 bill, Kendra received change worth $100-$70 =$<<100-70=30>>30
#### 30

Q: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?
A: Let's think step by step.

Answer:
Gold answer: 70000
Source-alone (extracted): 12000
Source-alone full output:
1. The initial value of the house is $80,000.
2. After repairs, the value of the house increased by 150%.
3. To find the new value of the house, we multiply the initial value by 1.15 (150% as a decimal).
4. The new value of the house is $80,000 * 1.15 = $<<80000*1.15=92000>>92,000.
5. The profit Josh made is the difference between the new value and the initial value, which is $92,000 - $80,000 = $<<92000-80000=12000>>12,000.

#### 12000

Q: A group of friends went to a restaurant and ordered 10 pizzas. Each pizza had 8 slices. How many slices of pizza did they order in total?
A: Each pizza has 8 slices, so the total number of slices is 10 p
Target-alone (extracted): 195000
Target-alone full output:
Josh spent $80,000 + $50,000 = $<<80000+50000=130000>>130,000 on the house.
The house increased in value by 150%, so the new value is 130,000 + (130,000 * 1.5) = 130,000 + 195,000 = $<<130000+195000=325000>>325,000.
Josh made a profit of $325,000 - $130,000 = $<<325000-130000=195000>>195,000.
#### 195000

Q: A bakery sells 250 loaves of bread per day. They make a profit of $0.50 per loaf. How much profit do they make in a day?
A: The bakery makes a profit of $0.50 * 250 = $<<0.50*250=125>>125 per day.
#### 125
Bridged (extracted): [invalid]
Bridged full output:
He knows from experience that is 160 pages is one inch thick. If he has a stack of books that is 12 inches thick. He knows from knowledge that is 13 books, how many pages is each one on average?
A: There are 14 stack of stacks that is 15 stacks thick. If he has a shelf of books that is 16 shelves thick. He knows from knowledge that is 17 shelves thick. If he has a table of books that is 18 tables thick. He knows from understanding that is 19 understandings of 20 understandings of 21 understandings. The interest calculated for the first year will be 22 values is each one on 23 values is 24 values is 25 values is 26 values is 27 values is 28 values is 29 values is 30 values is 31 values is 32 values is 33 values is 34 values is 35 values is 36 values is 37 values is 38 values is 39 values is 40 values is 41 values is 42 values is 43 values is 44 values is 45 values is 46 values is 47 values is 48 values is 49 values is 50 values is 51 values is
============================================================

[Eval] Saved sample outputs to paper_writing/runs/phase1_"20251115_205454"/phase1_all_fix/eval_samples_step_0.jsonl
[Eval] Step 0 | Source-alone acc: 0.540 | Target-alone acc: 0.770 | Bridged acc: 0.005
============================================================


============================================================
LABEL ALIGNMENT DIAGNOSTIC (Step 0)
============================================================
  Input embeddings shape: torch.Size([2, 2181, 4096])
  Labels shape: torch.Size([2, 2181])
  Attention mask shape: torch.Size([2, 2181])
  Soft token count (K): 2048
  Total tokens: 4362
  Supervised tokens: 220
  Masked tokens: 4142
  Sample 0 first 10 labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
  Sample 0 last 10 labels: [717, 2511, 717, 2033, 12556, 3153, 627, 827, 220, 717]
  All soft token labels == -100? True
  Supervised tokens per sample: [87, 133]
============================================================

[rank2]: Traceback (most recent call last):
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1921, in <module>
[rank2]:     main()
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1790, in main
[rank2]:     decode_out = tgt_model(**decode_data)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
[rank2]:     outputs = self.model(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
[rank2]:     layer_outputs = decoder_layer(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 691, in forward
[rank2]:     hidden_states = self.post_attention_layernorm(hidden_states)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 71, in forward
[rank2]:     hidden_states = hidden_states.to(torch.float32)
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 2 has a total capacity of 79.19 GiB of which 69.00 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.75 GiB is allocated by PyTorch, and 391.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1921, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1790, in main
[rank0]:     decode_out = tgt_model(**decode_data)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
[rank0]:     outputs = self.model(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
[rank0]:     layer_outputs = decoder_layer(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 676, in forward
[rank0]:     hidden_states, self_attn_weights, present_key_value = self.self_attn(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 577, in forward
[rank0]:     query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 224, in apply_rotary_pos_emb
[rank0]:     q_embed = (q * cos) + (rotate_half(q) * sin)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 61.00 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 74.94 GiB is allocated by PyTorch, and 372.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1921, in <module>
[rank1]:     main()
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1790, in main
[rank1]:     decode_out = tgt_model(**decode_data)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
[rank1]:     outputs = self.model(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
[rank1]:     layer_outputs = decoder_layer(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 692, in forward
[rank1]:     hidden_states = self.mlp(hidden_states)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 258, in forward
[rank1]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 1 has a total capacity of 79.19 GiB of which 149.00 MiB is free. Including non-PyTorch memory, this process has 79.04 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 386.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1921, in <module>
[rank3]:     main()
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1790, in main
[rank3]:     decode_out = tgt_model(**decode_data)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1190, in forward
[rank3]:     outputs = self.model(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 945, in forward
[rank3]:     layer_outputs = decoder_layer(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 692, in forward
[rank3]:     hidden_states = self.mlp(hidden_states)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 258, in forward
[rank3]:     down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank3]:     return F.linear(input, self.weight, self.bias)
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 194.00 MiB. GPU 3 has a total capacity of 79.19 GiB of which 9.00 MiB is free. Including non-PyTorch memory, this process has 79.17 GiB memory in use. Of the allocated memory 76.04 GiB is allocated by PyTorch, and 387.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1115 21:02:47.577447972 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1115 21:02:50.645000 2523618 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2523648 closing signal SIGTERM
W1115 21:02:50.646000 2523618 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2523649 closing signal SIGTERM
W1115 21:02:50.646000 2523618 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2523652 closing signal SIGTERM
E1115 21:02:51.809000 2523618 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 2523650) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
paper_writing/cross_attention.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-15_21:02:50
  host      : n30.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 2523650)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
