W1113 21:07:59.173000 3283290 torch/distributed/run.py:793] 
W1113 21:07:59.173000 3283290 torch/distributed/run.py:793] *****************************************
W1113 21:07:59.173000 3283290 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1113 21:07:59.173000 3283290 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
====================================================================================================================================================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================

torch.cuda.is_available() check...
EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
============================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
  Result: True  Result: True
torch.cuda.device_count() check...

torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: 4 GPUs
  Result: 4 GPUs
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 0/4] Starting DDP initialization...
[Rank 0] Checking torch.cuda.is_available()...
[Rank 0]   Result: True
[Rank 0] Checking torch.cuda.device_count()...
[Rank 0]   Result: 4 devices
[Rank 0] Getting device name for GPU 0...
[Rank 0] GPU 0 available: NVIDIA H100 80GB HBM3
[Rank 0] Calling init_process_group (timeout=60s)...
  GPU 0: NVIDIA H100 80GB HBM3[W1113 21:08:09.597277923 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 0] DDP initialized successfully on GPU 0
[Rank 0] Setting up reproducibility (base_seed=1234)...

  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 1/4] Starting DDP initialization...
[Rank 1] Checking torch.cuda.is_available()...
[Rank 1]   Result: True
[Rank 1] Checking torch.cuda.device_count()...
[Rank 1]   Result: 4 devices
[Rank 1] Getting device name for GPU 1...
[Rank 1] GPU 1 available: NVIDIA H100 80GB HBM3
[Rank 1] Calling init_process_group (timeout=60s)...
  GPU 0: NVIDIA H100 80GB HBM3[Rank 0] Reproducibility setup complete (effective_seed=1234)
Reproducibility enabled: base_seed=1234, effective_seed=1234
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: -1
depth: 2
heads: 8
lr: 0.0001
weight_decay: 0.01
train_steps: 2000
warmup_steps: 200
per_device_batch: 4
eval_every: 250
eval_samples: 200
max_new_tokens: 256
log_dir: paper_writing/runs/ablations_20251113_210757/1a_dit_2step_64tok
seed: 1234
bf16: True
save_path: paper_writing/runs/ablations_20251113_210757/1a_dit_2step_64tok/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 3
dataset: gsm8k
info_nce_weight: 0.05
eval_batch_size: 36
no_compile: True
bridge: dit
dit_dim: 512
dit_depth: 6
dit_heads: 8
dit_steps_train: 2
dit_steps_eval: 4
dit_dropout: 0.1
dit_cfg: 0.0
dit_drop_cond: 0.1
dit_cfg_dropout: None
dit_pool: mean
dit_cond_dim: 512
dit_loss_weight: 0.1
dit_loss_warmup: 0
dit_teacher: answer

==== Reproducibility Settings ====
Base seed: 1234
Effective seed (with rank offset): 1234
CUDA deterministic: True
CUDA benchmark: False
Deterministic algorithms: True
CUBLAS workspace: :16:8
World size: 4 (each rank samples different data)
Loading source model/tokenizer...
[W1113 21:08:09.603846138 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 2/4] Starting DDP initialization...
[Rank 2] Checking torch.cuda.is_available()...
[Rank 2]   Result: True
[Rank 2] Checking torch.cuda.device_count()...
[Rank 2]   Result: 4 devices
[Rank 2] Getting device name for GPU 2...
[Rank 2] GPU 2 available: NVIDIA H100 80GB HBM3
[Rank 2] Calling init_process_group (timeout=60s)...
[W1113 21:08:09.617587020 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 3/4] Starting DDP initialization...
[Rank 3] Checking torch.cuda.is_available()...
[Rank 3]   Result: True
[Rank 3] Checking torch.cuda.device_count()...
[Rank 3]   Result: 4 devices
[Rank 3] Getting device name for GPU 3...
[Rank 3] GPU 3 available: NVIDIA H100 80GB HBM3
[Rank 3] Calling init_process_group (timeout=60s)...
[W1113 21:08:09.649381932 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 1] DDP initialized successfully on GPU 1
[Rank 1] Setting up reproducibility (base_seed=1234)...
[Rank 1] Reproducibility setup complete (effective_seed=1235)
[Rank 3] DDP initialized successfully on GPU 3
[Rank 3] Setting up reproducibility (base_seed=1234)...
[Rank 3] Reproducibility setup complete (effective_seed=1237)
[Rank 2] DDP initialized successfully on GPU 2
[Rank 2] Setting up reproducibility (base_seed=1234)...
[Rank 2] Reproducibility setup complete (effective_seed=1236)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 838.08it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1420.67it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 2274.57it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 5475.59it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.44s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.47s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:12<00:24, 12.29s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:11<00:22, 11.49s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:23<00:11, 11.60s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:23<00:11, 11.61s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:24<00:11, 11.95s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:23<00:11, 11.66s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:34<00:00, 11.26s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:34<00:00, 11.48s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.09s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.22s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.21s/it]
Loading target model/tokenizer...
Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:33<00:00, 11.27s/it]
Soft tokens not specified; using full prompt length: 8192
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1338.11it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3858.61it/s]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9193.00it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7843.49it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.43s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:11<00:34, 11.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.30s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.31s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:22<00:22, 11.30s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.19s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:33<00:11, 11.22s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  8.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  8.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  8.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.20s/it]

Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  8.02s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:36<00:00,  9.22s/it]
torch.compile() disabled via --no_compile flag (prevents CUDA Graph OOM)
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
NOTE: RMS scale matching is applied during batch collation (before concat)
Bridge: dit (DiTBridgeTranslator)
  Using DiTBridge: dim=512, depth=6, heads=8, steps(train/eval)=2/4, cfg=0.0, pool=mean, cond_dim=512
Translator parameters: 44.1M
Loading GSM8K...
Data sampler initialized with seed 1234 (base=1234, rank=0)
Optimizer groups: 37 decay, 36 no_decay, 0 gates (LR ×3)

============================================================
INITIAL EVALUATION (Step 0 - Before Training)
============================================================
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
[rank3]: Traceback (most recent call last):
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1848, in <module>
[rank3]:     main()
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1616, in main
[rank3]:     acc_source, acc_base, acc_bridged = evaluate_numeric_accuracy(
[rank3]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1093, in evaluate_numeric_accuracy
[rank3]:     gen = tgt_model.generate(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2084, in generate
[rank3]:     self._prepare_cache_for_generation(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1731, in _prepare_cache_for_generation
[rank3]:     model_kwargs[cache_name] = self._get_cache(
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1637, in _get_cache
[rank3]:     self._cache = cache_cls(**cache_kwargs)
[rank3]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/cache_utils.py", line 1173, in __init__
[rank3]:     self.register_buffer(f"value_cache_{idx}", torch.zeros(cache_shape, dtype=dtype, device=layer_device))
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 3 has a total capacity of 79.19 GiB of which 359.00 MiB is free. Including non-PyTorch memory, this process has 78.83 GiB memory in use. Of the allocated memory 76.14 GiB is allocated by PyTorch, and 55.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1848, in <module>
[rank1]:     main()
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1616, in main
[rank1]:     acc_source, acc_base, acc_bridged = evaluate_numeric_accuracy(
[rank1]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1093, in evaluate_numeric_accuracy
[rank1]:     gen = tgt_model.generate(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2084, in generate
[rank1]:     self._prepare_cache_for_generation(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1731, in _prepare_cache_for_generation
[rank1]:     model_kwargs[cache_name] = self._get_cache(
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1637, in _get_cache
[rank1]:     self._cache = cache_cls(**cache_kwargs)
[rank1]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/cache_utils.py", line 1173, in __init__
[rank1]:     self.register_buffer(f"value_cache_{idx}", torch.zeros(cache_shape, dtype=dtype, device=layer_device))
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 1 has a total capacity of 79.19 GiB of which 119.00 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.13 GiB is allocated by PyTorch, and 69.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1848, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1616, in main
[rank0]:     acc_source, acc_base, acc_bridged = evaluate_numeric_accuracy(
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1093, in evaluate_numeric_accuracy
[rank0]:     gen = tgt_model.generate(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2084, in generate
[rank0]:     self._prepare_cache_for_generation(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1731, in _prepare_cache_for_generation
[rank0]:     model_kwargs[cache_name] = self._get_cache(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1637, in _get_cache
[rank0]:     self._cache = cache_cls(**cache_kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/cache_utils.py", line 1173, in __init__
[rank0]:     self.register_buffer(f"value_cache_{idx}", torch.zeros(cache_shape, dtype=dtype, device=layer_device))
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 267.00 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 76.07 GiB is allocated by PyTorch, and 31.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1848, in <module>
[rank2]:     main()
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1616, in main
[rank2]:     acc_source, acc_base, acc_bridged = evaluate_numeric_accuracy(
[rank2]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1093, in evaluate_numeric_accuracy
[rank2]:     gen = tgt_model.generate(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 2084, in generate
[rank2]:     self._prepare_cache_for_generation(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1731, in _prepare_cache_for_generation
[rank2]:     model_kwargs[cache_name] = self._get_cache(
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/utils.py", line 1637, in _get_cache
[rank2]:     self._cache = cache_cls(**cache_kwargs)
[rank2]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/cache_utils.py", line 1173, in __init__
[rank2]:     self.register_buffer(f"value_cache_{idx}", torch.zeros(cache_shape, dtype=dtype, device=layer_device))
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 698.00 MiB. GPU 2 has a total capacity of 79.19 GiB of which 159.00 MiB is free. Including non-PyTorch memory, this process has 79.03 GiB memory in use. Of the allocated memory 76.10 GiB is allocated by PyTorch, and 58.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W1113 21:10:30.414925149 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1113 21:10:32.586000 3283290 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3283298 closing signal SIGTERM
W1113 21:10:32.587000 3283290 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3283299 closing signal SIGTERM
W1113 21:10:32.587000 3283290 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3283300 closing signal SIGTERM
E1113 21:10:34.375000 3283290 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 3 (pid: 3283301) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
paper_writing/cross_attention.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-13_21:10:32
  host      : n20.cm.cluster
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3283301)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
