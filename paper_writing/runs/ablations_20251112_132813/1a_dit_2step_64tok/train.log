W1112 13:28:15.065000 2425835 torch/distributed/run.py:793] 
W1112 13:28:15.065000 2425835 torch/distributed/run.py:793] *****************************************
W1112 13:28:15.065000 2425835 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1112 13:28:15.065000 2425835 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================================================================================================================================================================================

EARLY CUDA DIAGNOSTICS (before DDP setup)
EARLY CUDA DIAGNOSTICS (before DDP setup)

============================================================EARLY CUDA DIAGNOSTICS (before DDP setup)============================================================


torch.cuda.is_available() check...torch.cuda.is_available() check...============================================================


torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
  Result: True  Result: True
torch.cuda.device_count() check...

torch.cuda.device_count() check...
  Result: True  Result: True
torch.cuda.device_count() check...

torch.cuda.device_count() check...
  Result: 4 GPUs
  Result: 4 GPUs
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 3/4] Starting DDP initialization...
[Rank 3] Checking torch.cuda.is_available()...
[Rank 3]   Result: True
[Rank 3] Checking torch.cuda.device_count()...
[Rank 3]   Result: 4 devices
[Rank 3] Getting device name for GPU 3...
[Rank 3] GPU 3 available: NVIDIA H100 80GB HBM3
[Rank 3] Calling init_process_group (timeout=60s)...
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 0/4] Starting DDP initialization...
[Rank 0] Checking torch.cuda.is_available()...
[Rank 0]   Result: True
[Rank 0] Checking torch.cuda.device_count()...
[Rank 0]   Result: 4 devices
[Rank 0] Getting device name for GPU 0...
[Rank 0] GPU 0 available: NVIDIA H100 80GB HBM3
[Rank 0] Calling init_process_group (timeout=60s)...
  GPU 0: NVIDIA H100 80GB HBM3[W1112 13:28:23.792249424 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W1112 13:28:23.803108365 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 0] DDP initialized successfully on GPU 0
[Rank 0] Setting up reproducibility (base_seed=1234)...

  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 1/4] Starting DDP initialization...
[Rank 1] Checking torch.cuda.is_available()...
[Rank 1]   Result: True
[Rank 1] Checking torch.cuda.device_count()...
[Rank 1]   Result: 4 devices
[Rank 1] Getting device name for GPU 1...
[Rank 1] GPU 1 available: NVIDIA H100 80GB HBM3
[Rank 1] Calling init_process_group (timeout=60s)...
[W1112 13:28:23.806049362 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
[Rank 0] Reproducibility setup complete (effective_seed=1234)
Reproducibility enabled: base_seed=1234, effective_seed=1234
==== Config ====
source_model: mistralai/Mistral-7B-Instruct-v0.3
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: 64
depth: 2
heads: 8
lr: 0.0001
weight_decay: 0.01
train_steps: 2000
warmup_steps: 200
per_device_batch: 10
eval_every: 250
eval_samples: 200
max_new_tokens: 100
seed: 1234
bf16: True
save_path: paper_writing/runs/ablations_20251112_132813/1a_dit_2step_64tok/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 3
dataset: gsm8k
info_nce_weight: 0.05
eval_batch_size: 16
no_compile: True
bridge: dit
dit_dim: 512
dit_depth: 6
dit_heads: 8
dit_steps_train: 2
dit_steps_eval: 4
dit_dropout: 0.1
dit_cfg: 0.0
dit_drop_cond: 0.1
dit_cfg_dropout: None
dit_pool: mean
dit_cond_dim: 512
dit_loss_weight: 0.1
dit_loss_warmup: 0
dit_teacher: answer

==== Reproducibility Settings ====
Base seed: 1234
Effective seed (with rank offset): 1234
CUDA deterministic: True
CUDA benchmark: False
Deterministic algorithms: True
CUBLAS workspace: :16:8
World size: 4 (each rank samples different data)
Loading source model/tokenizer...
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 2/4] Starting DDP initialization...
[Rank 2] Checking torch.cuda.is_available()...
[Rank 2]   Result: True
[Rank 2] Checking torch.cuda.device_count()...
[Rank 2]   Result: 4 devices
[Rank 2] Getting device name for GPU 2...
[Rank 2] GPU 2 available: NVIDIA H100 80GB HBM3
[Rank 2] Calling init_process_group (timeout=60s)...
[W1112 13:28:23.845854874 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 2] DDP initialized successfully on GPU 2
[Rank 2] Setting up reproducibility (base_seed=1234)...
[Rank 3] DDP initialized successfully on GPU 3[Rank 2] Reproducibility setup complete (effective_seed=1236)

[Rank 3] Setting up reproducibility (base_seed=1234)...
[Rank 3] Reproducibility setup complete (effective_seed=1237)
[Rank 1] DDP initialized successfully on GPU 1
[Rank 1] Setting up reproducibility (base_seed=1234)...
[Rank 1] Reproducibility setup complete (effective_seed=1235)
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 659.45it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 4163.77it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1503.87it/s]
Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1848.53it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:36, 18.45s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:36, 18.30s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:37, 18.93s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:18<00:36, 18.38s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:35<00:17, 17.52s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:35<00:17, 17.50s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:35<00:17, 17.75s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:35<00:17, 17.63s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 16.79s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 16.81s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.06s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.12s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 16.95s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 16.83s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.29s/it]
Loading checkpoint shards: 100%|██████████| 3/3 [00:51<00:00, 17.11s/it]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1955.61it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9548.79it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9010.32it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1019.15it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.38s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.42s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:20<01:01, 20.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.58s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.64s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:37<00:37, 18.71s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:20, 20.08s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:20, 20.09s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:20, 20.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:59<00:20, 20.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 13.87s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 13.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 13.88s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 15.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 13.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 15.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 15.99s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 16.00s/it]
torch.compile() disabled via --no_compile flag (prevents CUDA Graph OOM)
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
NOTE: RMS scale matching is applied during batch collation (before concat)
Bridge: dit (DiTBridgeTranslator)
  Using DiTBridge: dim=512, depth=6, heads=8, steps(train/eval)=2/4, cfg=0.0, pool=mean, cond_dim=512
Translator parameters: 44.1M
Loading GSM8K...
Data sampler initialized with seed 1234 (base=1234, rank=0)
Optimizer groups: 37 decay, 36 no_decay, 0 gates (LR ×3)

============================================================
INITIAL EVALUATION (Step 0 - Before Training)
============================================================
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================

--- Example 1 ---
Question: ...
Gold answer: 18
Target-alone: 12
Bridged: [invalid]
Bridged generation start: _REFUSE) to the rescue. The rescue is a rescue, but the rescue is a rescue. The rescue is a rescue, but the rescue is a rescue. The rescue is a rescue, but the rescue is a rescue. The rescue is a rescue, but the rescue is a rescue. The rescue is a rescue, but the rescue is a rescue. The rescue is a ...

--- Example 2 ---
Question: ...
Gold answer: 3
Target-alone: 12
Bridged: [invalid]
Bridged generation start: _REFUSE

## Step 1: Identify the main topic of the text
The main topic of the text is the comparison between the two types of texts: a formal text and an informal text.

## Step 2: Identify the key features of the formal text
The key features of the formal text are:
- It is written in a professional...

--- Example 3 ---
Question: ...
Gold answer: 70000
Target-alone: 12
Bridged: [invalid]
Bridged generation start: _REF_1_._|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|_|...
============================================================

[Eval] Step 0 | Target-alone acc: 0.010 | Bridged acc: 0.000
============================================================


============================================================
LABEL ALIGNMENT DIAGNOSTIC (Step 0)
============================================================
  Input embeddings shape: torch.Size([10, 277, 4096])
  Labels shape: torch.Size([10, 277])
  Attention mask shape: torch.Size([10, 277])
  Soft token count (K): 64
  Total tokens: 2770
  Supervised tokens: 1035
  Masked tokens: 1735
  Sample 0 first 10 labels: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
  Sample 0 last 10 labels: [717, 2511, 717, 2033, 12556, 3153, 627, 827, 220, 717]
  All soft token labels == -100? True
  Supervised tokens per sample: [87, 133, 62, 125, 96, 55, 143, 213, 51, 70]
============================================================

/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:655.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:655.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:655.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:655.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: Flash Attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at ../aten/src/ATen/native/transformers/cuda/attention_backward.cu:102.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Step 20/2000 | Loss (avg over last 20): 2.0639
Step 40/2000 | Loss (avg over last 20): 1.8536
Step 60/2000 | Loss (avg over last 20): 1.3995
Step 80/2000 | Loss (avg over last 20): 1.1658
Step 100/2000 | Loss (avg over last 20): 1.1127
Step 120/2000 | Loss (avg over last 20): 1.1965
Step 140/2000 | Loss (avg over last 20): 1.1815
Step 160/2000 | Loss (avg over last 20): 1.1752
Step 180/2000 | Loss (avg over last 20): 1.1536
Step 200/2000 | Loss (avg over last 20): 1.1577
Step 220/2000 | Loss (avg over last 20): 3.4519
Step 240/2000 | Loss (avg over last 20): 2.2501
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================

--- Example 1 ---
Question: ...
Gold answer: 18
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The number of people who have not yet received a gift is 100 - 50 = <<100-50=50>>50.
The number of people who have received a gift is 50 - 20 = <<50-20=30>>30.
The number of people who have not yet received a gift and have a birthday in the next 2 days is 50 - 30 = <<50-30=20>>20.
The number of peop...

--- Example 2 ---
Question: ...
Gold answer: 3
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The number of people who have a birthday in the month of May is 1/12 of the total number of people, so 1/12 * 120 = 10 people. The number of people who have a birthday in the month of June is 1/12 of the total number of people, so 1/12 * 120 = 10 people. The number of people who have a birthday in t...

--- Example 3 ---
Question: ...
Gold answer: 70000
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The number of people who have a birthday in the month of May is 1/12 of the total number of people, which is 1/12 * 100 = 8.33. The number of people who have a birthday in the month of June is 1/12 of the total number of people, which is 1/12 * 100 = 8.33. The number of people who have a birthday in...
============================================================

[Eval] Step 250 | Target-alone acc: 0.010 | Bridged acc: 0.000
[Early Stop] No improvement, patience: 1/3
Step 260/2000 | Loss (avg over last 20): 2.1076
Step 280/2000 | Loss (avg over last 20): 2.0639
Step 300/2000 | Loss (avg over last 20): 2.1140
Step 320/2000 | Loss (avg over last 20): 2.0109
Step 340/2000 | Loss (avg over last 20): 2.1712
Step 360/2000 | Loss (avg over last 20): 1.9709
Step 380/2000 | Loss (avg over last 20): 1.9346
Step 400/2000 | Loss (avg over last 20): 2.0652
Step 420/2000 | Loss (avg over last 20): 1.9886
Step 440/2000 | Loss (avg over last 20): 1.9287
Step 460/2000 | Loss (avg over last 20): 2.0262
Step 480/2000 | Loss (avg over last 20): 2.0434
Step 500/2000 | Loss (avg over last 20): 1.9308
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================

--- Example 1 ---
Question: ...
Gold answer: 18
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The total number of people who have been to the museum is 1000 - 200 = <<1000-200=800>>800
The number of people who have been to the museum and have seen the exhibit is 800 - 300 = <<800-300=500>>500
The number of people who have been to the museum and have seen the exhibit and have also seen the mo...

--- Example 2 ---
Question: ...
Gold answer: 3
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The total number of people who have been to the museum is 1000 - 200 = <<1000-200=800>>800
The number of people who have been to the museum and have seen the exhibit is 800 - 300 = <<800-300=500>>500
The number of people who have been to the museum and have seen the exhibit and have also seen the mo...

--- Example 3 ---
Question: ...
Gold answer: 70000
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The total number of people who have been to the museum is 1000 - 200 = <<1000-200=800>>800
The number of people who have been to the museum and have seen the exhibit is 800 - 300 = <<800-300=500>>500
The number of people who have been to the museum and have seen the exhibit and have also seen the mo...
============================================================

[Eval] Step 500 | Target-alone acc: 0.010 | Bridged acc: 0.000
[Early Stop] No improvement, patience: 2/3
Step 520/2000 | Loss (avg over last 20): 1.9547
Step 540/2000 | Loss (avg over last 20): 2.0037
Step 560/2000 | Loss (avg over last 20): 1.9152
Step 580/2000 | Loss (avg over last 20): 1.9184
Step 600/2000 | Loss (avg over last 20): 1.9252
Step 620/2000 | Loss (avg over last 20): 1.9408
Step 640/2000 | Loss (avg over last 20): 1.9467
Step 660/2000 | Loss (avg over last 20): 1.9299
Step 680/2000 | Loss (avg over last 20): 1.8780
Step 700/2000 | Loss (avg over last 20): 1.9440
Step 720/2000 | Loss (avg over last 20): 1.9687
Step 740/2000 | Loss (avg over last 20): 1.9462
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================

--- Example 1 ---
Question: ...
Gold answer: 18
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The total cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equi...

--- Example 2 ---
Question: ...
Gold answer: 3
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The total cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equi...

--- Example 3 ---
Question: ...
Gold answer: 70000
Target-alone: 12
Bridged: [invalid]
Bridged generation start: The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment is $1,000,000. The cost of the new equipment ...
============================================================

[Eval] Step 750 | Target-alone acc: 0.010 | Bridged acc: 0.000
[Early Stop] No improvement, patience: 3/3
[Early Stop] Stopping early at step 750. Best bridged acc: 0.000
Saved translator to paper_writing/runs/ablations_20251112_132813/1a_dit_2step_64tok/checkpoint.pt
[Distributed Eval] Sharding 200 samples across 4 ranks:
  Rank 0: samples 0 to 50 (50 samples)
  Rank 1: samples 50 to 100 (50 samples)
  Rank 2: samples 100 to 150 (50 samples)
  Rank 3: samples 150 to 200 (50 samples)
[rank1]:[E1112 13:55:32.609942018 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60017 milliseconds before timing out.
[rank1]:[E1112 13:55:32.610046790 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank2]:[E1112 13:55:32.614580089 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60014 milliseconds before timing out.
[rank2]:[E1112 13:55:32.614641020 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank3]:[E1112 13:55:32.620023763 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60035 milliseconds before timing out.
[rank3]:[E1112 13:55:32.620083654 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank3]:[E1112 13:55:32.837425275 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 3] Timeout at NCCL work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank3]:[E1112 13:55:32.837466626 ProcessGroupNCCL.cpp:630] [Rank 3] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank3]:[E1112 13:55:32.837474215 ProcessGroupNCCL.cpp:636] [Rank 3] To avoid data inconsistency, we are taking the entire process down.
[rank2]:[E1112 13:55:32.838976173 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 2] Timeout at NCCL work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank2]:[E1112 13:55:32.838991468 ProcessGroupNCCL.cpp:630] [Rank 2] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank2]:[E1112 13:55:32.839018854 ProcessGroupNCCL.cpp:636] [Rank 2] To avoid data inconsistency, we are taking the entire process down.
[rank3]:[E1112 13:55:32.847855868 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60035 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
[rank2]:[E1112 13:55:32.849001543 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60014 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 2] Process group watchdog thread terminated with exception: [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60014 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe7eb1b (0x155506c4db1b in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

  what():  [PG ID 0 PG GUID 0(default_pg) Rank 3] Process group watchdog thread terminated with exception: [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60035 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe7eb1b (0x155506c4db1b in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

[rank1]:[E1112 13:55:33.888081307 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 1] Timeout at NCCL work: 5281, last enqueued NCCL work: 5281, last completed NCCL work: 5280.
[rank1]:[E1112 13:55:33.888115553 ProcessGroupNCCL.cpp:630] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E1112 13:55:33.888123151 ProcessGroupNCCL.cpp:636] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E1112 13:55:33.889158176 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60017 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG ID 0 PG GUID 0(default_pg) Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=5281, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=60000) ran for 60017 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x155506fcfa92 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x155506fd6ed3 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x155506fd893d in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1601 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1555516b9446 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe7eb1b (0x155506c4db1b in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0x145c0 (0x155551b345c0 in /users/sujinesh/.local/lib/python3.10/site-packages/torch/lib/libtorch.so)
frame #3: <unknown function> + 0x94ac3 (0x15555526fac3 in /lib/x86_64-linux-gnu/libc.so.6)
frame #4: <unknown function> + 0x126850 (0x155555301850 in /lib/x86_64-linux-gnu/libc.so.6)

W1112 13:55:34.495000 2425835 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2425842 closing signal SIGTERM
W1112 13:55:34.498000 2425835 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2425843 closing signal SIGTERM
E1112 13:55:35.411000 2425835 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 2 (pid: 2425844) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
========================================================
paper_writing/cross_attention.py FAILED
--------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
--------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-12_13:55:34
  host      : n28.cm.cluster
  rank      : 2 (local_rank: 2)
  exitcode  : -6 (pid: 2425844)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2425844
========================================================
