W1122 17:33:37.676000 2514677 torch/distributed/run.py:793] 
W1122 17:33:37.676000 2514677 torch/distributed/run.py:793] *****************************************
W1122 17:33:37.676000 2514677 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1122 17:33:37.676000 2514677 torch/distributed/run.py:793] *****************************************
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================================================================================================================================================================================
EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...

EARLY CUDA DIAGNOSTICS (before DDP setup)
============================================================
torch.cuda.is_available() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: True
torch.cuda.device_count() check...
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 1/4] Starting DDP initialization...
[Rank 1] Checking torch.cuda.is_available()...
[Rank 1]   Result: True
[Rank 1] Checking torch.cuda.device_count()...
[Rank 1]   Result: 4 devices
[Rank 1] Getting device name for GPU 1...
[Rank 1] GPU 1 available: NVIDIA H100 80GB HBM3
[Rank 1] Calling init_process_group (timeout=60s)...
[W1122 17:33:47.127721411 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 3/4] Starting DDP initialization...
[Rank 3] Checking torch.cuda.is_available()...
[Rank 3]   Result: True
[Rank 3] Checking torch.cuda.device_count()...
[Rank 3]   Result: 4 devices
[Rank 3] Getting device name for GPU 3...
[Rank 3] GPU 3 available: NVIDIA H100 80GB HBM3
[Rank 3] Calling init_process_group (timeout=60s)...
[W1122 17:33:47.163529699 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 2/4] Starting DDP initialization...
[Rank 2] Checking torch.cuda.is_available()...
[Rank 2]   Result: True
[Rank 2] Checking torch.cuda.device_count()...
[Rank 2]   Result: 4 devices
[Rank 2] Getting device name for GPU 2...
[Rank 2] GPU 2 available: NVIDIA H100 80GB HBM3
[Rank 2] Calling init_process_group (timeout=60s)...
[W1122 17:33:48.600235434 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Result: 4 GPUs
  GPU 0: NVIDIA H100 80GB HBM3
  GPU 1: NVIDIA H100 80GB HBM3
  GPU 2: NVIDIA H100 80GB HBM3
  GPU 3: NVIDIA H100 80GB HBM3
============================================================

[Rank 0/4] Starting DDP initialization...
[Rank 0] Checking torch.cuda.is_available()...
[Rank 0]   Result: True
[Rank 0] Checking torch.cuda.device_count()...
[Rank 0]   Result: 4 devices
[Rank 0] Getting device name for GPU 0...
[Rank 0] GPU 0 available: NVIDIA H100 80GB HBM3
[Rank 0] Calling init_process_group (timeout=60s)...
[W1122 17:33:48.658150503 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[Rank 0] DDP initialized successfully on GPU 0
[Rank 0] Setting up reproducibility (base_seed=1234)...
[Rank 0] Reproducibility setup complete (effective_seed=1234)
Reproducibility enabled: base_seed=1234, effective_seed=1234
==== Config ====
source_model: meta-llama/Meta-Llama-3.1-8B-Instruct
target_model: meta-llama/Meta-Llama-3.1-8B-Instruct
bottleneck_dim: 1024
soft_tokens: 128
eval_prompt_mode: soft_only
source_layer: 0
target_layer: 0
passthrough_soft: True
soft_injection: prepend
adapter_scale: 1.0
skip_source_baseline: True
depth: 2
heads: 8
lr: 0.0001
weight_decay: 0.01
train_steps: 0
warmup_steps: 200
per_device_batch: 2
eval_every: 250
eval_samples: 100
max_new_tokens: 256
log_dir: paper_writing/runs/layer_s0_t0_passthrough_20251122_173336
seed: 1234
bf16: True
save_path: paper_writing/runs/layer_s0_t0_passthrough_20251122_173336/checkpoint.pt
show_eval_samples: 1
early_stop_patience: 0
dataset: gsm8k
info_nce_weight: 0.05
eval_batch_size: 36
decode_loss_weight: 0.0
decode_interval: 50
decode_samples: 1
decode_max_length: 4096
kl_max_length: 512
kl_tokens: 20
prompt_alignment_weight: 0.001
soft_only_curriculum_steps: 0
prompt_contrast_weight: 0.0
aux_probe_weight: 0.0
format_loss_weight: 0.1
token_alignment_weight: 0.0
no_compile: True
bridge: dit
dit_dim: 512
dit_depth: 6
dit_heads: 8
dit_steps_train: 4
dit_steps_eval: 8
dit_dropout: 0.1
dit_cfg: 0.0
dit_drop_cond: 0.1
dit_cfg_dropout: None
dit_pool: mean
dit_cond_dim: 512
dit_loss_weight: 0.1
dit_loss_warmup: 0
dit_teacher: answer

==== Reproducibility Settings ====
Base seed: 1234
Effective seed (with rank offset): 1234
CUDA deterministic: True
CUDA benchmark: False
Deterministic algorithms: True
CUBLAS workspace: :16:8
World size: 4 (each rank samples different data)
Loading source model/tokenizer...
[Rank 1] DDP initialized successfully on GPU 1
[Rank 1] Setting up reproducibility (base_seed=1234)...
[Rank 1] Reproducibility setup complete (effective_seed=1235)
[Rank 2] DDP initialized successfully on GPU 2[Rank 3] DDP initialized successfully on GPU 3
[Rank 2] Setting up reproducibility (base_seed=1234)...

[Rank 3] Setting up reproducibility (base_seed=1234)...
[Rank 2] Reproducibility setup complete (effective_seed=1236)[Rank 3] Reproducibility setup complete (effective_seed=1237)

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 493.67it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8905.10it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1817.88it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8947.85it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:42, 14.03s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.31s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.18s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:40, 13.55s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.92s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.29s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.08s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:26<00:26, 13.00s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.70s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:39<00:12, 12.89s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  9.03s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.07s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.52s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00,  9.01s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.43s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.65s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.47s/it]
Loading target model/tokenizer...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3488.71it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 393.23it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1878.12it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9279.43it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.99s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.02s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:13<00:39, 13.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.75s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:25<00:25, 12.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.90s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:38<00:12, 12.94s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.12s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.51s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.51s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00,  9.15s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.51s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:42<00:00, 10.51s/it]

torch.compile() disabled via --no_compile flag (prevents CUDA Graph OOM)
Source hidden dim: 4096 | Target hidden dim: 4096
Target embedding RMS (median): 0.0107
NOTE: RMS scale matching is applied during batch collation (before concat)
Bridge: dit (DiTBridgeTranslator)
  Using DiTBridge: dim=512, depth=6, heads=8, steps(train/eval)=4/8, cfg=0.0, pool=mean, cond_dim=512
Translator parameters: 60.9M
Loading GSM8K...
Data sampler initialized with seed 1234 (base=1234, rank=0)
Optimizer groups: 38 decay, 36 no_decay, 0 gates (LR ×3)

============================================================
INITIAL EVALUATION (Step 0 - Before Training)
============================================================
[Distributed Eval] Sharding 100 samples across 4 ranks:
  Rank 0: samples 0 to 25 (25 samples)
  Rank 1: samples 25 to 50 (25 samples)
  Rank 2: samples 50 to 75 (25 samples)
  Rank 3: samples 75 to 100 (25 samples)
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(

============================================================
SAMPLE OUTPUTS (first 3 examples):
============================================================
============================================================

[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 2029, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1743, in main
[rank0]:     acc_source, acc_base, acc_bridged = evaluate_numeric_accuracy(
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/paper_writing/cross_attention.py", line 1342, in evaluate_numeric_accuracy
[rank0]:     log(f"[Eval] Saved sample outputs to {jsonl_path}")
[rank0]: UnboundLocalError: local variable 'jsonl_path' referenced before assignment
[rank0]:[W1122 17:37:46.903262646 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W1122 17:37:49.120000 2514677 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2514820 closing signal SIGTERM
W1122 17:37:49.120000 2514677 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2514821 closing signal SIGTERM
W1122 17:37:49.121000 2514677 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2514822 closing signal SIGTERM
E1122 17:37:50.736000 2514677 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2514819) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
paper_writing/cross_attention.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-22_17:37:49
  host      : n02.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2514819)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
