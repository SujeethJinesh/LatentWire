
=========================================
Starting pipeline at Mon Sep 15 10:46:28 PDT 2025
=========================================


=========================================
PHASE 1: TRAINING WITH EPOCH EVALUATIONS
=========================================

Training for 24 epochs with evaluation after each
Checkpoint will be saved to: runs/8B_clean_answer_ftce/ckpt


=========================================
EPOCH 1/24
=========================================

Training epoch 1...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1914.55it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.65s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:14<00:14,  7.43s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:22<00:07,  7.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.90s/it]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1899.38it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:18,  6.01s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.30s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.24s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  5.94s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.04s/it]
Llama hidden size: 4096, Qwen hidden size: 3584
⚠️  No valid checkpoint found to resume; starting fresh.
Epoch 1/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1369/1369 | loss_L=8.4579 | loss_Q=8.6925 | firstCE_L=8.5072 | firstCE_Q=7.7056 | scale_pen(L)=1.5755e-05 | scale_pen(Q)=1.0177e-06 | grad_norm=0.75 | sec/step~0.80 | rms_raw(L)~0.6323 rms_raw(Q)~0.6185 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01058 embed_rms(Q)~0.01363
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 1369
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer_ftce/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.6322672562149861, 'rms_mean_cal': 0.010571094556358542, 'embed_rms': 0.01057521253824234, 'count': 1369}, 'qwen': {'rms_mean_raw': 0.6184672953614011, 'rms_mean_cal': 0.013640873189240019, 'embed_rms': 0.013628026470541954, 'count': 1369}}

Evaluating epoch 1 checkpoint...
Evaluating: runs/8B_clean_answer_ftce/epoch1 -> runs/8B_clean_answer_ftce/eval_epoch1
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer_ftce/epoch1/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer_ftce/eval_epoch1/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3080.65it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.35s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.45s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:04<00:01,  1.33s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.07it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw'] | nlls={'raw': 8.16628575919707} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer_ftce/eval_epoch1/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.67143 -> target=0.01057
[debug:llama] adapter.scale=0.9960 | Z.std=0.6060 Z.mean||=9.6938 | prefix.std=0.0106 prefix.mean||=0.6767 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the of the of the of the of the of'
  1: 'the of the of the of the of the of the of'
  2: 'the of the of the of the of the of the of'
  3: 'the of the of the of the of the of the of'
  4: 'the of the of the of the of the of the of'
Saved Llama results to runs/8B_clean_answer_ftce/eval_epoch1/llama_results.json

Evaluating Qwen...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3320.25it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.01it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw'] | nlls={'raw': 7.774132496307766} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer_ftce/eval_epoch1/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.65671 -> target=0.01364
[debug:qwen] adapter.scale=0.9990 | Z.std=0.6060 Z.mean||=9.6938 | prefix.std=0.0136 prefix.mean||=0.8166 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'the of the of the of the of the of the of'
  1: '1919 1919 19'
  2: '1919 1919 19'
  3: '1919 1919 19'
  4: '100000000000'
Saved Qwen results to runs/8B_clean_answer_ftce/eval_epoch1/qwen_results.json

Joint rescoring...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3014.77it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.09s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3124.83it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.16it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.19it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.33it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 32768 bytes (fp32), and 16384 bytes (fp16); latent/text bytes (one-copy, fp16): 13.01x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.26s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.021  |  NLL/token (gold): 8.168305497591188
       First-token acc: top1=0.030  top5=0.040
Qwen   EM: 0.000   F1: 0.003  |  NLL/token (gold): 7.782977277639682
       First-token acc: top1=0.060  top5=0.125
Wall clock: 13.42s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 11.51s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.003
Inter-model agreement (normalized): 0.250
Oracle upper bound:  EM 0.000  F1 0.021

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 32768,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 32768,
      "fp16": 16384
    },
    "wire_compression": {
      "vs_onecopy_fp16": 13.013502779984114,
      "vs_onecopy_fp32": 26.027005559968227
    },
    "wire_ratio": {
      "latent_over_onecopy_fp16": 13.013502779984114,
      "latent_over_onecopy_fp32": 26.027005559968227,
      "onecopy_over_latent_fp16": 0.07684326171875,
      "onecopy_over_latent_fp32": 0.038421630859375
    }
  },
  "text": {
    "wall_clock_sec": 16.257566928863525,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.417166233062744,
    "llama": {
      "em": 0.0,
      "f1": 0.02050281662781663,
      "nll_token": 8.168305497591188,
      "first_token_top1": 0.029999999329447746,
      "first_token_top5": 0.03999999910593033
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.003478535353535353,
      "nll_token": 7.782977277639682,
      "first_token_top1": 0.05999999865889549,
      "first_token_top5": 0.125
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04086507936507937
    },
    "wall_clock_sec": 11.505087614059448,
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711954711955
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.003478535353535353,
    "agreement": 0.25,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 0.9960306882858276,
      "Z_std": 0.606008768081665,
      "Z_mean_norm": 9.693779945373535,
      "prefix_std": 0.010570434853434563,
      "prefix_mean_norm": 0.6766930818557739,
      "embed_rms": 0.010569815523922443,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 0.9989911913871765,
      "Z_std": 0.606008768081665,
      "Z_mean_norm": 9.693779945373535,
      "prefix_std": 0.013638078235089779,
      "prefix_mean_norm": 0.8165891766548157,
      "embed_rms": 0.01364860124886036,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "yes",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.02050281662781663
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer_ftce/eval_epoch1/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer_ftce/eval_epoch1/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.021 | Qwen 0.003
  FirstTok@1:  Llama 0.030 | Qwen 0.060
  FirstTok@5:  Llama 0.040 | Qwen 0.125


=========================================
EPOCH 2/24
=========================================

Training epoch 2...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3228.25it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:01,  1.01it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.11s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3153.02it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.05it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.24it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.33it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.26it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.25it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer_ftce/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=1, global_step=1369
Epoch 2/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  1369/1369 | loss_L=8.0641 | loss_Q=8.4941 | firstCE_L=8.4719 | firstCE_Q=7.7357 | scale_pen(L)=3.7488e-05 | scale_pen(Q)=5.0242e-07 | grad_norm=0.57 | sec/step~0.86 | rms_raw(L)~0.6964 rms_raw(Q)~0.6920 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01058 embed_rms(Q)~0.01365
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
  ✅ Saved (and pruned to) latest at step 2738
[checkpoint] Freed 0.0B before save.
[checkpoint] Saved latest: encoder.pt, adapter_llama.pt, adapter_qwen.pt, state.pt, config.json, training_stats.json
[checkpoint] Freed 0.0B after save (non-canonical).
✅ Saved latest checkpoint to runs/8B_clean_answer_ftce/ckpt
📝 Saved training_stats.json: {'llama': {'rms_mean_raw': 0.696364540048896, 'rms_mean_cal': 0.010571359936352016, 'embed_rms': 0.0105800935998559, 'count': 1369}, 'qwen': {'rms_mean_raw': 0.6919575832550154, 'rms_mean_cal': 0.013640723401412553, 'embed_rms': 0.013648351654410362, 'count': 1369}}

Evaluating epoch 2 checkpoint...
Evaluating: runs/8B_clean_answer_ftce/epoch2 -> runs/8B_clean_answer_ftce/eval_epoch2
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Auto-detected device: cuda
Loaded training_stats.json: runs/8B_clean_answer_ftce/epoch2/training_stats.json
Building encoder and computing Z...
Saved Z to runs/8B_clean_answer_ftce/eval_epoch2/Z.pt

[Sequential Evaluation Mode - one model at a time]

Evaluating Llama...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2598.70it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.16s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.04s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.13s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.28it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:llama] encoder_text_mode candidates=['raw'] | nlls={'raw': 7.866149151676637} | picked=raw
Saved Z[llama_raw] to runs/8B_clean_answer_ftce/eval_epoch2/Z_llama_raw.pt
[calib:llama] mode=embed_rms prefix_rms=0.71644 -> target=0.01057
[debug:llama] adapter.scale=0.9939 | Z.std=0.6617 Z.mean||=10.5673 | prefix.std=0.0106 prefix.mean||=0.6767 | embed.RMS=0.0106

[DEBUG] First generations (Llama, latent):
  0: 'the of the and the of the are in the of the'
  1: 'the of the and the of the are to the of the'
  2: 'the of the and the of the and the of the and'
  3: 'the of the and the of the and the of the and'
  4: 'the of the and the of the and the of the and'
Saved Llama results to runs/8B_clean_answer_ftce/eval_epoch2/llama_results.json

Evaluating Qwen...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2603.94it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.27it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.35it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.14it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
[align:qwen] encoder_text_mode candidates=['raw'] | nlls={'raw': 7.457290293836089} | picked=raw
Saved Z[qwen_raw] to runs/8B_clean_answer_ftce/eval_epoch2/Z_qwen_raw.pt
[calib:qwen]  mode=embed_rms prefix_rms=0.72400 -> target=0.01364
[debug:qwen] adapter.scale=0.9993 | Z.std=0.6617 Z.mean||=10.5673 | prefix.std=0.0136 prefix.mean||=0.8165 | embed.RMS=0.0136

[DEBUG] First generations (Qwen, latent):
  0: 'the of the of the of the of the of the of'
  1: 'the of the of the of the of the of the of'
  2: '1919 1919 19'
  3: 'the 1911 of the 111'
  4: 'the of the of the of the of the of the of'
Saved Qwen results to runs/8B_clean_answer_ftce/eval_epoch2/qwen_results.json

Joint rescoring...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3025.10it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.10it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.13s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.36it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 5949.37it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.29it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.19it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.16it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.31it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s]

==== LatentWire Evaluation ====
Dataset: squad
Samples: 200  |  Max new tokens: 12
Device: cuda  |  Dtype: bfloat16
Avg prompt tokens (Llama): 245.1 | (Qwen): 231.7 | Latent length M: 32
Compression ratio (Llama): 7.7x | (Qwen): 7.2x
Approx interlingua payload per example: 32768 bytes (fp32), and 16384 bytes (fp16); latent/text bytes (one-copy, fp16): 13.01x

— Baseline: Text prompting
Llama  EM: 0.580  F1: 0.799  |  NLL/token (gold): 12.72173482274305
Qwen   EM: 0.680   F1: 0.853   |  NLL/token (gold): 25.81069942126198
Wall clock: 16.56s

— Latent prompting (shared interlingua)
Llama  EM: 0.000  F1: 0.031  |  NLL/token (gold): 7.863609602121539
       First-token acc: top1=0.025  top5=0.075
Qwen   EM: 0.000   F1: 0.009  |  NLL/token (gold): 7.462445713540234
       First-token acc: top1=0.055  top5=0.140
Wall clock: 13.93s

— Token-budget baseline (mode: content_only)
Llama  EM: 0.005  F1: 0.041
Qwen   EM: 0.040   F1: 0.075
Wall clock: 11.98s

— 2-LLM joint (rescored pick on latent runs)
Joint  EM: 0.000  F1: 0.014
Inter-model agreement (normalized): 0.000
Oracle upper bound:  EM 0.000  F1 0.032

==== METRICS_JSON ====
{
  "samples": 200,
  "max_new_tokens": 12,
  "latent_len": 32,
  "device": "cuda",
  "dtype": "torch.bfloat16",
  "avg_prompt_tokens": {
    "llama": 245.065,
    "qwen": 231.69
  },
  "compression": {
    "llama": 7.65828125,
    "qwen": 7.2403125
  },
  "payload_bytes": 32768,
  "wire": {
    "text_bytes_onecopy": {
      "llama_avg": 1259,
      "qwen_avg": 1107,
      "max_avg": 1259
    },
    "text_bytes_twocopies": {
      "sum_avg": 2366
    },
    "latent_bytes": {
      "fp32": 32768,
      "fp16": 16384
    },
    "wire_compression": {
      "vs_onecopy_fp16": 13.013502779984114,
      "vs_onecopy_fp32": 26.027005559968227
    },
    "wire_ratio": {
      "latent_over_onecopy_fp16": 13.013502779984114,
      "latent_over_onecopy_fp32": 26.027005559968227,
      "onecopy_over_latent_fp16": 0.07684326171875,
      "onecopy_over_latent_fp32": 0.038421630859375
    }
  },
  "text": {
    "wall_clock_sec": 16.559028148651123,
    "llama": {
      "em": 0.58,
      "f1": 0.7994106570072514,
      "nll_token": 12.72173482274305
    },
    "qwen": {
      "em": 0.68,
      "f1": 0.8528460272957177,
      "nll_token": 25.81069942126198
    }
  },
  "latent": {
    "wall_clock_sec": 13.931443691253662,
    "llama": {
      "em": 0.0,
      "f1": 0.031218947718947718,
      "nll_token": 7.863609602121539,
      "first_token_top1": 0.02499999850988388,
      "first_token_top5": 0.07499999552965164
    },
    "qwen": {
      "em": 0.0,
      "f1": 0.009462662337662337,
      "nll_token": 7.462445713540234,
      "first_token_top1": 0.054999999701976776,
      "first_token_top5": 0.14000000059604645
    }
  },
  "token_budget": {
    "mode": "content_only",
    "k": 32,
    "llama": {
      "em": 0.005,
      "f1": 0.04086507936507937
    },
    "wall_clock_sec": 11.981855154037476,
    "qwen": {
      "em": 0.04,
      "f1": 0.07479711954711955
    }
  },
  "joint": {
    "em": 0.0,
    "f1": 0.0144739565989566,
    "agreement": 0.0,
    "oracle": {
      "em": null,
      "f1": null
    }
  },
  "debug": {
    "llama": {
      "adapter_scale": 0.9938772916793823,
      "Z_std": 0.6617497801780701,
      "Z_mean_norm": 10.567349433898926,
      "prefix_std": 0.010571831837296486,
      "prefix_mean_norm": 0.6766675710678101,
      "embed_rms": 0.010569815523922443,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct"
    },
    "qwen": {
      "adapter_scale": 0.999291181564331,
      "Z_std": 0.6617497801780701,
      "Z_mean_norm": 10.567349433898926,
      "prefix_std": 0.013636586256325245,
      "prefix_mean_norm": 0.8164951205253601,
      "embed_rms": 0.01364860124886036,
      "encoder_text_mode": "raw",
      "calibration_mode": "embed_rms",
      "append_bos_after_prefix": "yes",
      "latent_anchor_mode": "text",
      "latent_anchor_text": "Answer: ",
      "model_id": "Qwen/Qwen2.5-7B-Instruct"
    },
    "latent_anchor_mode": "text",
    "latent_anchor_text": "Answer: ",
    "prefix_gain": 1.0,
    "calibration_mode": "embed_rms",
    "append_bos_after_prefix": "yes",
    "decode": {
      "min_new_tokens": 3,
      "eos_ban_steps": 6,
      "first_token_top_p": 1.0,
      "first_token_temperature": 0.0
    }
  },
  "oracle": {
    "em": 0.0,
    "f1": 0.03164751914751915
  },
  "dataset": "squad"
}
Wrote per-example predictions to runs/8B_clean_answer_ftce/eval_epoch2/predictions.jsonl

✓ Metrics from: runs/8B_clean_answer_ftce/eval_epoch2/metrics.json
  Text F1:     Llama 0.799 | Qwen 0.853
  Latent F1:   Llama 0.031 | Qwen 0.009
  FirstTok@1:  Llama 0.025 | Qwen 0.055
  FirstTok@5:  Llama 0.075 | Qwen 0.140


=========================================
EPOCH 3/24
=========================================

Training epoch 3...
/projects/m000066/sujinesh/LatentWire/.venv/lib/python3.9/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading dataset subset...
Loading SQuAD subset...

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 7763.64it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.12s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.00s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.10s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.29it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.13it/s]

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]
Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 4273.36it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:02,  1.28it/s]
Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.02it/s]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:02<00:00,  1.13it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.19it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
Llama hidden size: 4096, Qwen hidden size: 3584
⏪ Resuming from: runs/8B_clean_answer_ftce/ckpt/state.pt
   -> loaded encoder/adapters FROM state.pt
   -> restored optimizer state
   -> restored RNG state
   -> start_epoch=2, global_step=2738
Epoch 3/1
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  step  10/1369 | loss_L=7.9606 | loss_Q=7.3015 | firstCE_L=8.0833 | firstCE_Q=6.8817 | scale_pen(L)=3.8666e-05 | scale_pen(Q)=4.6341e-07 | grad_norm=0.59 | sec/step~1.08 | rms_raw(L)~0.7171 rms_raw(Q)~0.7245 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  20/1369 | loss_L=7.2444 | loss_Q=6.6048 | firstCE_L=7.9579 | firstCE_Q=7.0597 | scale_pen(L)=3.9046e-05 | scale_pen(Q)=4.4327e-07 | grad_norm=0.64 | sec/step~1.06 | rms_raw(L)~0.7169 rms_raw(Q)~0.7247 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  30/1369 | loss_L=7.9508 | loss_Q=7.7728 | firstCE_L=7.7219 | firstCE_Q=6.7889 | scale_pen(L)=3.9211e-05 | scale_pen(Q)=4.3521e-07 | grad_norm=0.58 | sec/step~1.05 | rms_raw(L)~0.7171 rms_raw(Q)~0.7251 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  40/1369 | loss_L=7.5851 | loss_Q=7.4301 | firstCE_L=7.3881 | firstCE_Q=6.6065 | scale_pen(L)=3.9310e-05 | scale_pen(Q)=5.4715e-07 | grad_norm=0.62 | sec/step~1.08 | rms_raw(L)~0.7171 rms_raw(Q)~0.7253 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  50/1369 | loss_L=7.7258 | loss_Q=7.3900 | firstCE_L=7.7892 | firstCE_Q=6.6680 | scale_pen(L)=3.8527e-05 | scale_pen(Q)=6.2071e-07 | grad_norm=0.55 | sec/step~1.16 | rms_raw(L)~0.7171 rms_raw(Q)~0.7256 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  60/1369 | loss_L=7.8649 | loss_Q=7.3424 | firstCE_L=7.9366 | firstCE_Q=7.2688 | scale_pen(L)=3.8252e-05 | scale_pen(Q)=6.3005e-07 | grad_norm=0.62 | sec/step~0.99 | rms_raw(L)~0.7172 rms_raw(Q)~0.7258 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  70/1369 | loss_L=8.0074 | loss_Q=7.7856 | firstCE_L=8.8386 | firstCE_Q=7.6543 | scale_pen(L)=3.9080e-05 | scale_pen(Q)=5.9654e-07 | grad_norm=0.60 | sec/step~0.97 | rms_raw(L)~0.7173 rms_raw(Q)~0.7260 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  80/1369 | loss_L=7.6309 | loss_Q=7.7239 | firstCE_L=8.5677 | firstCE_Q=7.8646 | scale_pen(L)=3.7526e-05 | scale_pen(Q)=5.3889e-07 | grad_norm=0.62 | sec/step~1.01 | rms_raw(L)~0.7173 rms_raw(Q)~0.7260 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  90/1369 | loss_L=7.7464 | loss_Q=7.2193 | firstCE_L=8.3112 | firstCE_Q=7.1046 | scale_pen(L)=3.7154e-05 | scale_pen(Q)=4.4303e-07 | grad_norm=0.60 | sec/step~1.08 | rms_raw(L)~0.7174 rms_raw(Q)~0.7262 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  100/1369 | loss_L=7.5992 | loss_Q=7.3673 | firstCE_L=7.7049 | firstCE_Q=7.1827 | scale_pen(L)=3.7126e-05 | scale_pen(Q)=4.4010e-07 | grad_norm=0.58 | sec/step~1.01 | rms_raw(L)~0.7175 rms_raw(Q)~0.7263 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
  step  110/1369 | loss_L=7.8426 | loss_Q=7.8429 | firstCE_L=8.0526 | firstCE_Q=7.7046 | scale_pen(L)=3.8040e-05 | scale_pen(Q)=3.9348e-07 | grad_norm=0.63 | sec/step~1.03 | rms_raw(L)~0.7177 rms_raw(Q)~0.7265 | rms_cal(L)~0.0106 rms_cal(Q)~0.0136 | embed_rms(L)~0.01057 embed_rms(Q)~0.01365
