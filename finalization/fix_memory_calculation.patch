--- MAIN_EXPERIMENT.py.original	2024-01-01 00:00:00.000000000 +0000
+++ MAIN_EXPERIMENT.py.fixed	2024-01-01 00:00:00.000000000 +0000
@@ -73,12 +73,21 @@
 # GPU Detection and DDP Management
 # ============================================================================

 class ElasticGPUConfig:
     """Elastic GPU configuration that adapts to available hardware.

     Automatically detects GPU count and configures optimal settings for:
     - Batch size (per-GPU and effective)
     - Gradient accumulation steps
     - DDP (Distributed Data Parallel) setup
     - Memory allocation strategy
     """
+
+    # Optimizer memory multipliers (in addition to model + gradients)
+    OPTIMIZER_MULTIPLIERS = {
+        'sgd': 0.0,      # No additional optimizer state
+        'adam': 2.0,     # Momentum + variance (2x model params)
+        'adamw': 2.0,    # Same as Adam
+        'lamb': 2.0,     # Similar to Adam
+        'adafactor': 0.5,  # More memory efficient
+    }

-    def __init__(self, base_batch_size=64, model_size_gb=14.0, target_util=0.75):
+    def __init__(self, base_batch_size=64, model_size_gb=14.0, target_util=0.75, optimizer='adamw'):
         """Initialize elastic GPU configuration.

         Args:
             base_batch_size: Desired effective batch size
             model_size_gb: Estimated model size in GB
             target_util: Target GPU memory utilization (0-1)
+            optimizer: Optimizer type (adam, adamw, sgd, etc.)
         """
         self.base_batch_size = base_batch_size
         self.model_size_gb = model_size_gb
         self.target_util = target_util
+        self.optimizer = optimizer.lower()
         self.config = self._detect_and_configure()

@@ -115,10 +125,23 @@
         gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
         available_memory_gb = gpu_memory_gb * self.target_util

-        # Estimate batch size per GPU based on memory
-        # Rule of thumb: model takes ~2x its size during training
-        memory_for_batch = available_memory_gb - (self.model_size_gb * 2)
-        # Assume ~0.5GB per batch item (very rough estimate)
-        max_batch_per_gpu = max(1, int(memory_for_batch / 0.5))
+        # FIXED: Properly account for optimizer states
+        # Get optimizer memory multiplier
+        optimizer_multiplier = self.OPTIMIZER_MULTIPLIERS.get(self.optimizer, 2.0)
+
+        # Calculate fixed memory requirements:
+        model_params_gb = self.model_size_gb
+        gradients_gb = self.model_size_gb
+        optimizer_state_gb = self.model_size_gb * optimizer_multiplier
+        cuda_overhead_gb = 2.0
+        safety_margin_gb = 2.0
+
+        total_fixed_memory_gb = (model_params_gb + gradients_gb + optimizer_state_gb +
+                                 cuda_overhead_gb + safety_margin_gb)
+
+        # Memory available for batch processing
+        memory_for_batch = available_memory_gb - total_fixed_memory_gb
+
+        # Assume ~1GB per batch item (conservative estimate)
+        max_batch_per_gpu = max(1, int(memory_for_batch / 1.0)) if memory_for_batch > 0 else 1

         # Configure based on GPU count
@@ -543,7 +566,8 @@
         # Initialize elastic GPU configuration
         self.gpu_config = ElasticGPUConfig(
             base_batch_size=config.batch_size,
-            model_size_gb=14.0  # Approximate for 7B models
+            model_size_gb=14.0,  # Approximate for 7-8B models
+            optimizer='adamw'  # Default optimizer
         )