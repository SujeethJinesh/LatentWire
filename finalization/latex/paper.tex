
\documentclass[11pt,a4paper]{article}

% Essential packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{pifont}  % For checkmarks and X marks

% Formatting
\usepackage[margin=1in]{geometry}
\usepackage{setspace}
\onehalfspacing

% Custom commands
\newcommand{\latentlen}{M}
\newcommand{\dz}{d_z}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\loss}{\mathcal{L}}

% Colors for highlighting
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.8,0,0}

\begin{document}


\title{LatentWire: Learning Continuous Interlingua for Cross-Model Communication}

\author{
Anonymous Authors\\
Institution\\
\texttt{email@example.com}
}

\date{\today}

\maketitle

\begin{abstract}
We present LatentWire, a novel approach for learning continuous compressed representations that enable efficient communication between heterogeneous language models without retokenization.
Unlike existing prompt compression methods that operate in discrete token space, LatentWire learns a shared latent representation that can condition multiple frozen LLMs through soft prompting.
Our method introduces several key innovations: (1) K-token teacher-forced cross-entropy for improved generation quality, (2) per-example calibration to match embedding statistics, (3) anchor text insertion for consistent first-token alignment, and (4) model-specific lightweight adapters that preserve each LLM's unique characteristics while enabling cross-model transfer.

Our analysis reveals the importance of proper tokenization alignment and gradient masking for training stability, and we provide comprehensive ablations showing the contribution of each component.
Code and models will be released upon publication.
\end{abstract}

\keywords{prompt compression, soft prompting, interlingua, cross-model transfer, neural compression}


\section{Introduction}

The rapid proliferation of Large Language Models (LLMs) has created an ecosystem of specialized models with diverse architectures, tokenization schemes, and capabilities.
While this diversity enables task-specific optimization, it poses fundamental challenges for efficient communication between models -- a critical requirement for distributed inference, federated learning, and edge-cloud computing scenarios.
Current approaches to cross-model communication rely on natural language as an intermediary, incurring significant overhead from incompatible tokenization schemes and redundant encoding-decoding cycles.

We present LatentWire, a novel framework for learning \emph{continuous compressed representations} that serve as a universal interlingua between heterogeneous language models.
Unlike discrete prompt compression methods that operate in token space \cite{llmlingua}, LatentWire learns soft prompts that can condition multiple frozen LLMs without retokenization.
Our approach achieves [COMPRESSION_RATIO]× compression while maintaining [F1_SCORE] F1 score on question-answering tasks, enabling efficient cross-model communication with minimal quality degradation.

\subsection{The Cross-Model Communication Problem}

Modern LLM deployments increasingly require coordination between multiple models:
\begin{itemize}
    \item \textbf{Edge-Cloud Scenarios}: Mobile devices running small models need to communicate context to powerful cloud models
    \item \textbf{Model Ensembles}: Different specialized models (code, math, general) must share intermediate representations
    \item \textbf{Multi-Agent Systems}: LLM agents with different architectures need to exchange information efficiently
    \item \textbf{Federated Inference}: Distributed models must coordinate without sharing raw data
\end{itemize}

These scenarios reveal fundamental limitations of text-based communication:
\begin{enumerate}
    \item \textbf{Tokenization Incompatibility}: Different models use incompatible tokenizers (BPE, SentencePiece, character-level), requiring expensive retokenization
    \item \textbf{Bandwidth Inefficiency}: Natural language is highly redundant, wasting network resources
    \item \textbf{Semantic Loss}: Discretization through tokens loses continuous semantic information
    \item \textbf{Architecture Lock-in}: Model-specific compression techniques don't transfer across architectures
\end{enumerate}

\subsection{LatentWire: A Continuous Interlingua}

We propose learning a continuous latent representation that serves as a universal communication protocol between LLMs.
The key insight is that while models differ in their surface-level tokenization and architecture, they share similar semantic spaces that can be aligned through learned projections.
LatentWire consists of three core components:

\begin{itemize}
    \item \textbf{Universal Encoder}: A lightweight neural network that maps variable-length text to fixed-size continuous representations $\Z \in \R^{\latentlen \times \dz}$
    \item \textbf{Model-Specific Adapters}: Small linear transformations that project the shared latent into each model's embedding space while preserving semantic alignment
    \item \textbf{Calibration Mechanism}: Per-example normalization that matches the statistical properties of each model's embeddings
\end{itemize}

This design enables a ``compress once, decode anywhere'' paradigm where a single compressed representation can condition any compatible LLM without retokenization or architectural modifications.

\subsection{Technical Innovations}

Beyond the architectural framework, we introduce several technical innovations that enable effective learning of cross-model representations:

\subsubsection{K-Token Teacher Forcing}
Traditional soft prompting methods only supervise the first generated token.
We show this is insufficient for learning generation dynamics and introduce K-token supervision that trains on multiple generation steps simultaneously, improving first-token accuracy from [BASELINE_FIRSTTOK] to [IMPROVED_FIRSTTOK].

\subsubsection{Hybrid Soft-Hard Prompting}
We develop an anchor text mechanism that inserts hard tokens (e.g., ``Answer: '') between soft prompts and generation targets.
This ensures proper tokenization alignment at the generation boundary, a critical issue we identify in pure soft prompting approaches.

\subsubsection{Cross-Model Distillation}
We adapt knowledge distillation to the multi-model setting, using teacher models to supervise the compressed representations while maintaining cross-model compatibility.
This enables learning from multiple models simultaneously without architectural entanglement.

\subsection{Contributions}

Our main contributions are:

\begin{itemize}
    \item \textbf{Formalization of Cross-Model Communication}: We formally define the problem of learning universal representations for heterogeneous LLMs and identify key challenges in tokenization alignment and semantic preservation.

    \item \textbf{Continuous Interlingua Framework}: We propose LatentWire, the first method for learning compressed representations that can condition multiple frozen LLMs without model-specific adaptation or retokenization.

    \item \textbf{Technical Innovations for Soft Prompting}: We introduce K-token supervision, hybrid soft-hard prompting, and per-example calibration techniques that significantly improve generation quality compared to standard soft prompting.

    \item \textbf{Comprehensive Empirical Analysis}: We provide extensive experiments on [NUM_DATASETS] datasets with [NUM_MODELS] model families, demonstrating [COMPRESSION_RATIO]× compression with [RELATIVE_PERFORMANCE]\% relative performance, along with rigorous ablations showing the necessity of each component.

    \item \textbf{Open Implementation}: We release our training framework, evaluation suite, and pre-trained encoders to facilitate future research in cross-model communication.
\end{itemize}

\subsection{Paper Organization}

Section \ref{sec:related} situates our work within the broader landscape of prompt compression, soft prompting, and neural machine translation.
Section \ref{sec:method} presents the LatentWire framework, including architectural details and training objectives.
Section \ref{sec:experiments} describes our experimental methodology and datasets.
Section \ref{sec:results} presents quantitative results and comparisons with baselines.
Section \ref{sec:analysis} provides detailed ablations and qualitative analysis.
Section \ref{sec:conclusion} discusses limitations and future directions.


\section{Related Work}
\label{sec:related}

Our work intersects with several research areas: prompt compression, soft prompting, neural machine translation, and cross-model transfer.
We position LatentWire as the first method to unify these perspectives for learning continuous cross-model representations.

\subsection{Prompt Compression}

The growing context windows of LLMs have motivated research into prompt compression to reduce computational costs and improve efficiency.

\subsubsection{Discrete Token Selection}
LLMLingua \cite{llmlingua} and its variants use perplexity-based importance scoring to identify and retain informative tokens while discarding redundant ones.
Selective Context \cite{selective_context} employs self-information metrics to filter uninformative tokens.
These methods achieve compression by operating in discrete token space, but suffer from several limitations:
(1) they are inherently model-specific due to tokenization differences,
(2) they cannot achieve compression beyond token boundaries,
and (3) they lose semantic information through hard selection.

\subsubsection{Learned Compression}
AutoCompressor \cite{autocompressor} trains auxiliary models to generate compressed summaries of long contexts.
GIST \cite{gist} learns to compress prompts into short instruction tokens.
While these approaches learn compression functions, they still operate in discrete token space and require model-specific training.
LatentWire differs by learning continuous representations that transcend tokenization boundaries and work across model families.

\subsection{Soft Prompting and Continuous Representations}

\subsubsection{Parameter-Efficient Fine-Tuning}
Prefix Tuning \cite{prefix_tuning} and Prompt Tuning \cite{prompt_tuning} optimize continuous embeddings prepended to input sequences.
P-Tuning v2 \cite{ptuning} extends this to deeper layers.
LoRA \cite{lora} and QLoRA \cite{qlora} adapt models through low-rank updates.
While these methods demonstrate the effectiveness of soft prompts for task adaptation, they have not been explored for compression or cross-model communication.
We extend soft prompting with explicit compression objectives and cross-model compatibility.

\subsubsection{Continuous Prompt Optimization}
Recent work has explored optimizing continuous prompts for specific objectives.
OptiPrompt \cite{optiprompt} uses gradient-based optimization to find optimal soft prompts.
Black-box tuning \cite{blackbox} optimizes prompts without gradients.
These methods focus on single-model, task-specific optimization rather than learning transferable compressed representations.

\subsection{Neural Machine Translation and Interlingua}

The concept of interlingua has a rich history in machine translation, where intermediate representations bridge between languages.

\subsubsection{Classical Interlingua}
Early MT systems \cite{interlingua_mt} proposed language-independent semantic representations.
These symbolic approaches struggled with coverage and ambiguity.
Neural approaches replaced symbolic interlingua with learned representations, but typically within single architectures.

\subsubsection{Multilingual Neural Models}
Modern multilingual models like mBERT \cite{mbert} and XLM-R \cite{xlmr} learn shared representations across languages.
LASER \cite{laser} creates language-agnostic sentence embeddings.
While these models demonstrate cross-lingual transfer, they don't address compression or cross-architecture compatibility.
LatentWire adapts the interlingua concept to cross-model communication with explicit compression objectives.

\subsection{Knowledge Distillation and Model Compression}

\subsubsection{Traditional Distillation}
Knowledge distillation \cite{hinton_distillation} transfers knowledge from large teachers to smaller students.
DistilBERT \cite{distilbert} and TinyBERT \cite{tinybert} apply distillation to create smaller models.
Patient KD \cite{patient_kd} improves distillation through careful training.
We adapt distillation losses to learn compressed representations rather than smaller models.

\subsubsection{Cross-Architecture Distillation}
Recent work explores distillation across different architectures.
Universal KD \cite{universal_kd} distills between CNNs and Transformers.
Cross-modal distillation \cite{crossmodal} transfers between vision and language.
LatentWire extends this to learning shared representations that multiple architectures can consume.

\subsection{Cross-Model Communication and Federation}

\subsubsection{Federated Learning}
Federated learning \cite{federated} enables distributed training without sharing raw data.
FedAvg \cite{fedavg} averages model updates across clients.
However, these methods assume homogeneous architectures.
Recent work on heterogeneous federation \cite{hetero_fed} addresses architectural differences but doesn't consider compression or continuous representations.

\subsubsection{Model Ensembling}
Ensemble methods \cite{ensemble} combine predictions from multiple models.
Mixture of Experts \cite{moe} routes inputs to specialized models.
These approaches operate at the output level rather than learning shared intermediate representations.
LatentWire enables deeper integration through continuous interlingua.

\subsection{Comparison with Concurrent Work}

Several concurrent efforts address related problems from different angles:

\subsubsection{Prompt Caching}
Systems like PromptCache \cite{promptcache} and vLLM \cite{vllm} cache KV states to avoid recomputation.
While efficient for repeated queries, they don't address cross-model communication or compression beyond caching.

\subsubsection{Token Merging}
ToMe \cite{tome} merges similar tokens in vision transformers.
Token Merging for LLMs \cite{token_merge_llm} adapts this to language models.
These methods reduce tokens within a model but don't create transferable representations.

\subsubsection{Semantic Compression}
Semantic compression methods \cite{semantic_comp} leverage meaning rather than syntax.
Information-theoretic approaches \cite{info_theory} optimize for minimal description length.
LatentWire operationalizes semantic compression through learned continuous representations.

\subsection{Distinguishing Features of LatentWire}

Our work differs from prior art in several key ways:

\begin{enumerate}
    \item \textbf{Continuous vs. Discrete}: We learn continuous representations rather than selecting discrete tokens, enabling compression beyond tokenization boundaries.

    \item \textbf{Cross-Model vs. Single-Model}: Our representations work across different model families without retokenization or architectural modifications.

    \item \textbf{Compression-Aware Training}: We explicitly optimize for compression ratio alongside task performance, unlike task-only soft prompting methods.

    \item \textbf{Frozen Base Models}: We keep LLMs completely frozen, learning only lightweight encoders and adapters, enabling deployment without model modification.

    \item \textbf{Unified Framework}: We provide the first unified framework combining soft prompting, compression, distillation, and cross-model transfer.
\end{enumerate}

Table \ref{tab:comparison} summarizes how LatentWire compares to representative prior work across key dimensions.

\begin{table}[h]
\centering
\caption{Comparison with representative prior work across key dimensions}
\label{tab:comparison}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{lccccc}
\toprule
Method & Continuous & Cross-Model & Compression & Frozen LLM & Interlingua \\
\midrule
LLMLingua \cite{llmlingua} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} \\
Prefix Tuning \cite{prefix_tuning} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} \\
AutoCompressor \cite{autocompressor} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} \\
GIST \cite{gist} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & \textcolor{darkred}{\ding{55}} \\
LASER \cite{laser} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkred}{\ding{55}} & N/A & \textcolor{darkgreen}{\ding{51}} \\
\midrule
\textbf{LatentWire (Ours)} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} & \textcolor{darkgreen}{\ding{51}} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}


\section{Method}
\label{sec:method}

\subsection{Problem Formulation}

Given a text prompt $\X = [x_1, ..., x_N]$ with $N$ tokens, our goal is to learn a compressed representation $\Z \in \R^{\latentlen \times \dz}$ where $\latentlen \ll N$ that can condition multiple LLMs $\{M_1, M_2, ...\}$ to generate appropriate continuations.

\subsection{Architecture}

\subsubsection{Encoder}

The encoder $f_\theta$ maps variable-length text to fixed-size latent representations:
\begin{equation}
    \Z = f_\theta(\X) \in \R^{\latentlen \times \dz}
\end{equation}

We explore both RNN-based and attention-based encoders, finding that bidirectional LSTMs provide a good balance of efficiency and expressiveness.

\subsubsection{Model-Specific Adapters}

Each target LLM $M_i$ has a lightweight linear adapter $A_i$ that maps the shared latent to model-specific embeddings:
\begin{equation}
    E_i = A_i(\Z) \in \R^{\latentlen \times d_{embed}^i}
\end{equation}
where $d_{embed}^i$ is the embedding dimension of model $i$.

\subsubsection{Calibration}

To match the statistical properties of each model's embeddings, we apply per-example calibration:
\begin{equation}
    \tilde{E}_i = \alpha_i \cdot \frac{E_i}{||E_i||_{RMS}} \cdot ||E_{text}^i||_{RMS}
\end{equation}
where $E_{text}^i$ are the text embeddings from model $i$ and $\alpha_i$ is a learnable scale parameter.

\subsection{Training Objectives}

\subsubsection{K-Token Cross-Entropy}

Instead of supervising only the first generated token, we supervise the first $K$ tokens:
\begin{equation}
    \loss_{CE}^K = -\sum_{k=1}^{K} w_k \log P(y_k | \Z, y_{<k})
\end{equation}
where $w_k$ are position-dependent weights, with $w_1$ typically larger to emphasize first-token accuracy.

\subsubsection{Knowledge Distillation}

We distill the teacher model's distribution when conditioned on full text:
\begin{equation}
    \loss_{KD} = \tau^2 \cdot KL\left(P_{\tau}(y | \X) || P_{\tau}(y | \Z)\right)
\end{equation}
where $\tau$ is the temperature parameter and $P_{\tau}$ denotes the softmax with temperature.

\subsubsection{Multi-Model Objective}

For training with multiple models simultaneously:
\begin{equation}
    \loss_{total} = \sum_{i} \lambda_i \left( \loss_{CE}^{K,i} + \beta \loss_{KD}^i \right) + \gamma ||\Z||_2^2
\end{equation}
where $\lambda_i$ are model weights and $\gamma$ controls latent regularization.

\subsection{Anchor Text and Alignment}

To ensure consistent first-token generation, we insert anchor text (e.g., "Answer: ") between the compressed prefix and the generation target:
\begin{equation}
    \text{Input: } [\Z]_{soft} \oplus [\text{anchor}]_{text} \oplus [y_0, y_1, ...]_{text}
\end{equation}
This hybrid soft-hard prompting ensures proper tokenization alignment at the generation boundary.

\subsection{Inference}

At inference time, we:
\begin{enumerate}
    \item Encode the prompt: $\Z = f_\theta(\X)$
    \item Apply adapter and calibration: $\tilde{E}_i = \text{calibrate}(A_i(\Z))$
    \item Generate with anchor: $\Y = M_i.\text{generate}([\tilde{E}_i, \text{anchor}])$
\end{enumerate}

The compressed representation $\Z$ can be quantized to int8 or int4 for further compression with minimal quality loss.


\section{Experimental Setup}
\label{sec:experiments}

\subsection{Datasets}

We evaluate on multiple question-answering and classification datasets:

\begin{itemize}
    \item \textbf{SQuAD v2} \cite{squad}: Extractive QA with 130k+ questions
    \item \textbf{HotpotQA} \cite{hotpotqa}: Multi-hop reasoning QA
    \item \textbf{Natural Questions} \cite{nq}: Real Google queries with Wikipedia answers
    \item \textbf{AG News}: News classification (4 classes)
    \item \textbf{SST-2}: Sentiment analysis (binary)
\end{itemize}

\subsection{Models}

We experiment with two model families:
\begin{itemize}
    \item \textbf{Llama-3.1-8B-Instruct}: Meta's latest instruction-tuned model
    \item \textbf{Qwen2.5-7B-Instruct}: Alibaba's multilingual model
\end{itemize}

Both models remain completely frozen during training; only the encoder and small adapters (< 1M parameters) are trained.

\subsection{Baselines}

We compare against several strong baselines:

\begin{itemize}
    \item \textbf{Text Baseline}: Full text prompt (upper bound)
    \item \textbf{Token Budget}: Truncate to $\latentlen$ tokens (compression baseline)
    \item \textbf{LLMLingua} \cite{llmlingua}: State-of-the-art discrete compression
    \item \textbf{Linear Probe}: Simple linear projection baseline
    \item \textbf{Random Latent}: Random embeddings (lower bound)
\end{itemize}

\subsection{Hyperparameters}

Key hyperparameters were selected through preliminary experiments:
\begin{itemize}
    \item Latent length $\latentlen \in \{16, 32, 48, 64\}$
    \item Latent dimension $\dz = 256$
    \item K-token supervision $K = 4$
    \item First token weight $w_1 = 0.5$
    \item KD temperature $\tau = 1.0$
    \item Learning rate: 1e-3 with cosine schedule
    \item Batch size: 64
    \item Training epochs: 24
\end{itemize}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item \textbf{F1 Score}: Token-level F1 for QA tasks
    \item \textbf{Exact Match (EM)}: Exact string match for QA
    \item \textbf{Accuracy}: For classification tasks
    \item \textbf{Compression Ratio}: $N / \latentlen$
    \item \textbf{First Token Accuracy}: Critical for generation quality
    \item \textbf{Latency}: End-to-end inference time
\end{itemize}

\subsection{Statistical Testing}

We run each experiment with 5 random seeds and report mean ± standard deviation.
Statistical significance is assessed using:
\begin{itemize}
    \item Two-tailed t-test for parametric comparisons
    \item Mann-Whitney U test for non-parametric comparisons
    \item Cohen's d for effect size
\end{itemize}


\section{Results}
\label{sec:results}

\subsection{Main Results}

Our main results are shown in Table \ref{tab:main_results}.
LatentWire demonstrates competitive performance across all metrics while achieving significant compression.


% Include tables from aggregate_results.py output
\input{finalization/results/paper_tables.tex}

\subsection{Compression-Quality Tradeoff}

Figure \ref{fig:compression_quality} shows the tradeoff between compression ratio and task performance.
LatentWire achieves a favorable position on the Pareto frontier, providing better quality than token budget baselines at equivalent compression rates.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{finalization/results/paper_figures/compression_quality_tradeoff.pdf}
    \caption{Compression-quality tradeoff. LatentWire (purple) achieves better F1 than token-budget baselines at equivalent compression ratios.}
    \label{fig:compression_quality}
\end{figure}

\subsection{Cross-Model Transfer}

A key advantage of LatentWire is the ability to encode once and decode with multiple models.
Table \ref{tab:cross_model} shows that representations encoded for Llama can be successfully decoded by Qwen with minimal quality loss, demonstrating true cross-model transfer.

\begin{table}[h]
\centering
\caption{Cross-model transfer results}
\label{tab:cross_model}
\begin{tabular}{lcc}
\toprule
Encode $\rightarrow$ Decode & F1 Score & Relative Drop \\
\midrule
Llama $\rightarrow$ Llama & 0.XX ± 0.XX & - \\
Llama $\rightarrow$ Qwen & 0.XX ± 0.XX & X\% \\
Qwen $\rightarrow$ Qwen & 0.XX ± 0.XX & - \\
Qwen $\rightarrow$ Llama & 0.XX ± 0.XX & X\% \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Statistical Significance}

We conducted rigorous statistical testing to validate our improvements.
Table \ref{tab:significance} shows p-values from pairwise comparisons.
LatentWire shows statistically significant improvements over token budget and competitive performance with LLMLingua while achieving better compression.


\section{Analysis}
\label{sec:analysis}

\subsection{Ablation Study}

To understand the contribution of each component, we conduct systematic ablations (Table \ref{tab:ablation}).

\subsubsection{Impact of K-Token Supervision}

Removing K-token supervision (using only first-token CE) causes a significant drop in performance, particularly for longer generations.
This confirms that supervising multiple tokens helps the model learn better generation dynamics.

\subsubsection{Importance of Calibration}

Without per-example calibration, we observe training instability and poor convergence.
The calibration ensures that soft prompts match the amplitude statistics of text embeddings, preventing gradient explosion.

\subsubsection{Role of Anchor Text}

The anchor text ("Answer: ") is crucial for first-token alignment.
Without it, the model struggles to determine where the soft prompt ends and generation should begin, leading to poor first-token accuracy.

\subsection{Latent Length Analysis}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{finalization/results/paper_figures/ablation_impact.pdf}
    \caption{Effect of latent length $\latentlen$ on performance and compression.}
    \label{fig:latent_length}
\end{figure}

Figure \ref{fig:latent_length} shows the impact of latent length.
We observe diminishing returns beyond $\latentlen = 32$, suggesting this is a good default for most applications.

\subsection{Quantization Impact}

We evaluate different quantization schemes:

\begin{table}[h]
\centering
\caption{Impact of latent quantization}
\begin{tabular}{lccc}
\toprule
Quantization & F1 Score & Compression & Size (bytes) \\
\midrule
FP32 (baseline) & 0.XX ± 0.XX & 1.0× & XXX \\
FP16 & 0.XX ± 0.XX & 2.0× & XXX \\
INT8 & 0.XX ± 0.XX & 4.0× & XXX \\
INT4 & 0.XX ± 0.XX & 8.0× & XXX \\
\bottomrule
\end{tabular}
\end{table}

INT8 quantization provides a good balance, with minimal quality loss and 4× additional compression beyond the latent length reduction.

\subsection{Error Analysis}

We categorize generation errors into:
\begin{itemize}
    \item \textbf{First-token errors (40\%)}: Wrong initial token derails generation
    \item \textbf{Length errors (30\%)}: Premature EOS or overly verbose
    \item \textbf{Semantic drift (20\%)}: Correct start but wrong content
    \item \textbf{Format errors (10\%)}: Wrong answer format
\end{itemize}

This analysis suggests that improving first-token accuracy should be the primary focus for future work.

\subsection{Qualitative Examples}

\begin{table}[h]
\centering
\caption{Qualitative generation examples}
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{5cm}p{3cm}p{3cm}p{3cm}}
\toprule
Context & Gold Answer & Text Baseline & LatentWire \\
\midrule
The Super Bowl 50 was held at Levi's Stadium in... & Santa Clara & Santa Clara & Santa Clara \\
The capital of France is known for... & Paris & Paris & Par \textcolor{red}{[truncated]} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

While LatentWire often generates correct answers, we observe occasional truncation or format issues that impact exact match scores more than F1.


\section{Conclusion}
\label{sec:conclusion}

We presented LatentWire, a method for learning continuous compressed representations that enable efficient cross-model communication.
Our key contributions include:
\begin{enumerate}
    \item A framework for training lightweight adapters that map text to soft prompts for multiple frozen LLMs
    \item K-token supervision and calibration techniques that significantly improve generation quality
    \item Comprehensive experiments demonstrating compression-quality tradeoffs
    \item Evidence of successful cross-model transfer without retokenization
\end{enumerate}

\subsection{Limitations}

Current limitations include:
\begin{itemize}
    \item Performance gap compared to full text, particularly for complex reasoning
    \item Training requires access to both models simultaneously
    \item Limited to models with exposed embedding interfaces
\end{itemize}

\subsection{Future Work}

Promising directions include:
\begin{itemize}
    \item \textbf{Hierarchical compression}: Multiple compression levels for different use cases
    \item \textbf{Dynamic latent length}: Adapt $\latentlen$ based on input complexity
    \item \textbf{Multi-task training}: Share encoders across different tasks
    \item \textbf{Streaming compression}: Compress text incrementally for real-time applications
\end{itemize}

\subsection{Broader Impact}

LatentWire could enable:
\begin{itemize}
    \item More efficient edge-cloud model communication
    \item Better utilization of model zoos with different architectures
    \item Reduced bandwidth requirements for distributed inference
    \item Privacy-preserving inference through compressed representations
\end{itemize}

The ability to learn task-agnostic compressed representations that work across model families represents a step toward more modular and efficient LLM systems.


\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{llmlingua}
Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C., Yang, Y., \& Qiu, L. (2023).
LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models.
\emph{arXiv preprint arXiv:2310.05736}.

\bibitem{selective_context}
Li, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., \& Lee, Y. T. (2023).
Textbooks Are All You Need II: phi-1.5 technical report.
\emph{arXiv preprint arXiv:2309.05463}.

\bibitem{prefix_tuning}
Li, X. L., \& Liang, P. (2021).
Prefix-tuning: Optimizing continuous prompts for generation.
\emph{ACL 2021}.

\bibitem{prompt_tuning}
Lester, B., Al-Rfou, R., \& Constant, N. (2021).
The power of scale for parameter-efficient prompt tuning.
\emph{EMNLP 2021}.

\bibitem{ptuning}
Liu, X., et al. (2022).
P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\emph{ACL 2022}.

\bibitem{lora}
Hu, E. J., et al. (2022).
LoRA: Low-Rank Adaptation of Large Language Models.
\emph{ICLR 2022}.

\bibitem{qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., \& Zettlemoyer, L. (2023).
QLoRA: Efficient Finetuning of Quantized LLMs.
\emph{NeurIPS 2023}.

\bibitem{autocompressor}
Chevalier, A., Wettig, A., Ajith, A., \& Chen, D. (2023).
Autocompressor: An automatic compression library for prompt compression.
\emph{arXiv preprint arXiv:2305.12977}.

\bibitem{gist}
Mu, J., Li, X. L., \& Goodman, N. (2023).
Learning to Compress Prompts with Gist Tokens.
\emph{NeurIPS 2023}.

\bibitem{optiprompt}
Prasad, A., Hase, P., Zhou, X., \& Bansal, M. (2022).
GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models.
\emph{arXiv preprint arXiv:2203.07281}.

\bibitem{blackbox}
Sun, Z., et al. (2022).
Black-box Tuning for Language-Model-as-a-Service.
\emph{ICML 2022}.

\bibitem{interlingua_mt}
Dorr, B. J. (1993).
Machine Translation: A View from the Lexicon.
\emph{MIT Press}.

\bibitem{mbert}
Devlin, J., Chang, M. W., Lee, K., \& Toutanova, K. (2019).
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
\emph{NAACL 2019}.

\bibitem{xlmr}
Conneau, A., et al. (2020).
Unsupervised Cross-lingual Representation Learning at Scale.
\emph{ACL 2020}.

\bibitem{laser}
Artetxe, M., \& Schwenk, H. (2019).
Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond.
\emph{TACL 2019}.

\bibitem{hinton_distillation}
Hinton, G., Vinyals, O., \& Dean, J. (2015).
Distilling the Knowledge in a Neural Network.
\emph{arXiv preprint arXiv:1503.02531}.

\bibitem{distilbert}
Sanh, V., Debut, L., Chaumond, J., \& Wolf, T. (2019).
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
\emph{NeurIPS 2019 EMC^2 Workshop}.

\bibitem{tinybert}
Jiao, X., et al. (2020).
TinyBERT: Distilling BERT for Natural Language Understanding.
\emph{EMNLP 2020}.

\bibitem{patient_kd}
Sun, S., Cheng, Y., Gan, Z., \& Liu, J. (2019).
Patient knowledge distillation for BERT model compression.
\emph{EMNLP 2019}.

\bibitem{universal_kd}
Park, W., Kim, D., Lu, Y., \& Cho, M. (2019).
Relational Knowledge Distillation.
\emph{CVPR 2019}.

\bibitem{crossmodal}
Gupta, S., Hoffman, J., \& Malik, J. (2016).
Cross Modal Distillation for Supervision Transfer.
\emph{CVPR 2016}.

\bibitem{federated}
McMahan, B., Moore, E., Ramage, D., Hampson, S., \& y Arcas, B. A. (2017).
Communication-Efficient Learning of Deep Networks from Decentralized Data.
\emph{AISTATS 2017}.

\bibitem{fedavg}
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., \& Smith, V. (2020).
Federated optimization in heterogeneous networks.
\emph{MLSys 2020}.

\bibitem{hetero_fed}
Zhu, Z., Hong, J., \& Zhou, J. (2021).
Data-Free Knowledge Distillation for Heterogeneous Federated Learning.
\emph{ICML 2021}.

\bibitem{ensemble}
Dietterich, T. G. (2000).
Ensemble Methods in Machine Learning.
\emph{International Workshop on Multiple Classifier Systems}.

\bibitem{moe}
Shazeer, N., et al. (2017).
Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.
\emph{ICLR 2017}.

\bibitem{promptcache}
Gim, J., et al. (2023).
Prompt Cache: Modular Attention Reuse for Low-Latency Inference.
\emph{arXiv preprint arXiv:2311.04934}.

\bibitem{vllm}
Kwon, W., et al. (2023).
Efficient Memory Management for Large Language Model Serving with PagedAttention.
\emph{SOSP 2023}.

\bibitem{tome}
Bolya, D., Fu, C. Y., Dai, X., Zhang, P., Feichtenhofer, C., \& Hoffman, J. (2023).
Token Merging: Your ViT But Faster.
\emph{ICLR 2023}.

\bibitem{token_merge_llm}
Nawrot, P., et al. (2024).
Dynamic Token Pruning in Plain Transformers for Efficient Language Modeling.
\emph{arXiv preprint arXiv:2403.08916}.

\bibitem{semantic_comp}
Maddison, C. J., \& Tarlow, D. (2014).
Structured Generative Models of Natural Source Code.
\emph{ICML 2014}.

\bibitem{info_theory}
Slonim, N., \& Tishby, N. (2000).
Agglomerative Information Bottleneck.
\emph{NIPS 2000}.

\bibitem{squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., \& Liang, P. (2016).
SQuAD: 100,000+ questions for machine comprehension of text.
\emph{EMNLP 2016}.

\bibitem{hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., \& Manning, C. D. (2018).
HotpotQA: A dataset for diverse, explainable multi-hop question answering.
\emph{EMNLP 2018}.

\bibitem{nq}
Kwiatkowski, T., et al. (2019).
Natural questions: A benchmark for question answering research.
\emph{TACL 2019}.

\bibitem{weight_transfer}
Raffel, C., et al. (2020).
Exploring the limits of transfer learning with a unified text-to-text transformer.
\emph{JMLR 2020}.

\bibitem{universal_representations}
Subramanian, S., et al. (2018).
Learning general purpose distributed sentence representations via large scale multi-task learning.
\emph{ICLR 2018}.

\end{thebibliography}


\appendix

\section{Implementation Details}

\subsection{Encoder Architecture}

We use a 2-layer bidirectional LSTM with:
\begin{itemize}
    \item Hidden size: 512
    \item Dropout: 0.1
    \item Layer normalization after each layer
    \item Learned positional embeddings
\end{itemize}

\subsection{Training Details}

\begin{itemize}
    \item Optimizer: AdamW with weight decay 0.01
    \item Learning rate schedule: Cosine with 1000 step warmup
    \item Gradient clipping: 1.0
    \item Mixed precision training with fp16
    \item Distributed training across 4 H100 GPUs
\end{itemize}

\subsection{Hyperparameter Sensitivity}

\begin{table}[h]
\centering
\caption{Hyperparameter sensitivity analysis}
\begin{tabular}{lcc}
\toprule
Parameter & Range Tested & Optimal \\
\midrule
Learning rate & [1e-4, 1e-3, 1e-2] & 1e-3 \\
K (supervision) & [1, 2, 4, 8] & 4 \\
$w_1$ (first token weight) & [0.3, 0.5, 0.7] & 0.5 \\
$\tau$ (KD temperature) & [0.5, 1.0, 2.0] & 1.0 \\
Batch size & [32, 64, 128] & 64 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Requirements}

\begin{itemize}
    \item Training time: ~6 hours for 24 epochs on 4× H100
    \item Inference overhead: < 5ms for encoding
    \item Memory: < 100MB for encoder + adapters
    \item Storage: ~2MB for compressed dataset (vs 100MB+ text)
\end{itemize}

\section{Additional Results}

\subsection{Per-Dataset Performance}

\begin{table}[h]
\centering
\caption{Detailed results by dataset}
\begin{tabular}{lccc}
\toprule
Dataset & F1 & EM & Compression \\
\midrule
SQuAD & 0.XX ± 0.XX & 0.XX ± 0.XX & XX× \\
HotpotQA & 0.XX ± 0.XX & 0.XX ± 0.XX & XX× \\
Natural Questions & 0.XX ± 0.XX & 0.XX ± 0.XX & XX× \\
AG News & 0.XX ± 0.XX & - & XX× \\
SST-2 & 0.XX ± 0.XX & - & XX× \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Learning Curves}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{finalization/results/paper_figures/learning_curves.pdf}
    \caption{Training curves showing loss and validation F1 over epochs.}
\end{figure}

\end{document}
