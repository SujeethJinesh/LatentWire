% LaTeX Table Templates for LatentWire Paper
% Use these templates with placeholders for experimental results
% Replace [PLACEHOLDER] values with actual experimental data

% ============================================================================
% TABLE 1: Main Results - Performance Comparison Across Methods
% ============================================================================

\begin{table}[t]
\centering
\caption{Performance comparison across different prompting methods on SQuAD. All methods use the same token budget $M=32$ for fair comparison. Best results in \textbf{bold}, second best \underline{underlined}.}
\label{tab:main_results}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c}{\textbf{Llama-3.1-8B}} & \multicolumn{2}{c}{\textbf{Qwen2.5-7B}} & \multicolumn{2}{c}{\textbf{Average}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& EM & F1 & EM & F1 & EM & F1 \\
\midrule
\multicolumn{7}{l}{\textit{Text-based Methods}} \\
Full Prompt (oracle) & [FULL_LLAMA_EM] & [FULL_LLAMA_F1] & [FULL_QWEN_EM] & [FULL_QWEN_F1] & [FULL_AVG_EM] & [FULL_AVG_F1] \\
Token-Budget (M tokens) & [TB_LLAMA_EM] & [TB_LLAMA_F1] & [TB_QWEN_EM] & [TB_QWEN_F1] & [TB_AVG_EM] & [TB_AVG_F1] \\
\midrule
\multicolumn{7}{l}{\textit{Compression Methods}} \\
LLMLingua & [LINGUA_LLAMA_EM] & [LINGUA_LLAMA_F1] & [LINGUA_QWEN_EM] & [LINGUA_QWEN_F1] & [LINGUA_AVG_EM] & [LINGUA_AVG_F1] \\
AutoCompressor & [AUTO_LLAMA_EM] & [AUTO_LLAMA_F1] & [AUTO_QWEN_EM] & [AUTO_QWEN_F1] & [AUTO_AVG_EM] & [AUTO_AVG_F1] \\
Gisting & [GIST_LLAMA_EM] & [GIST_LLAMA_F1] & [GIST_QWEN_EM] & [GIST_QWEN_F1] & [GIST_AVG_EM] & [GIST_AVG_F1] \\
\midrule
\multicolumn{7}{l}{\textit{Our Method}} \\
\textbf{LatentWire} & \textbf{[LW_LLAMA_EM]} & \textbf{[LW_LLAMA_F1]} & \textbf{[LW_QWEN_EM]} & \textbf{[LW_QWEN_F1]} & \textbf{[LW_AVG_EM]} & \textbf{[LW_AVG_F1]} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 2: Ablation Study - Component Analysis
% ============================================================================

\begin{table}[t]
\centering
\caption{Ablation study showing the contribution of each component in LatentWire. All experiments use $M=32$ latent tokens on SQuAD validation set.}
\label{tab:ablation}
\begin{tabular}{lccccc}
\toprule
\textbf{Configuration} & \textbf{FirstTok@1} & \textbf{EM} & \textbf{F1} & \textbf{NLL/tok} & \textbf{$\Delta$F1} \\
\midrule
Full LatentWire & [FULL_FIRSTTOK] & [FULL_EM] & [FULL_F1] & [FULL_NLL] & -- \\
\midrule
\multicolumn{6}{l}{\textit{Training Objectives}} \\
\quad w/o K-token CE ($k=1$) & [NO_KTOKEN_FIRSTTOK] & [NO_KTOKEN_EM] & [NO_KTOKEN_F1] & [NO_KTOKEN_NLL] & [NO_KTOKEN_DELTA] \\
\quad w/o Prefix KD & [NO_KD_FIRSTTOK] & [NO_KD_EM] & [NO_KD_F1] & [NO_KD_NLL] & [NO_KD_DELTA] \\
\quad w/o First-token weighting & [NO_FTCE_FIRSTTOK] & [NO_FTCE_EM] & [NO_FTCE_F1] & [NO_FTCE_NLL] & [NO_FTCE_DELTA] \\
\midrule
\multicolumn{6}{l}{\textit{Architectural Components}} \\
\quad w/o Adapter calibration & [NO_CALIB_FIRSTTOK] & [NO_CALIB_EM] & [NO_CALIB_F1] & [NO_CALIB_NLL] & [NO_CALIB_DELTA] \\
\quad w/o Anchor text & [NO_ANCHOR_FIRSTTOK] & [NO_ANCHOR_EM] & [NO_ANCHOR_F1] & [NO_ANCHOR_NLL] & [NO_ANCHOR_DELTA] \\
\quad Fixed adapter (no learning) & [FIXED_FIRSTTOK] & [FIXED_EM] & [FIXED_F1] & [FIXED_NLL] & [FIXED_DELTA] \\
\midrule
\multicolumn{6}{l}{\textit{Encoder Architecture}} \\
\quad Byte encoder $\rightarrow$ Token encoder & [TOKEN_ENC_FIRSTTOK] & [TOKEN_ENC_EM] & [TOKEN_ENC_F1] & [TOKEN_ENC_NLL] & [TOKEN_ENC_DELTA] \\
\quad No positional encoding & [NO_POS_FIRSTTOK] & [NO_POS_EM] & [NO_POS_F1] & [NO_POS_NLL] & [NO_POS_DELTA] \\
\quad Smaller latent dim ($d_z=128$) & [SMALL_DZ_FIRSTTOK] & [SMALL_DZ_EM] & [SMALL_DZ_F1] & [SMALL_DZ_NLL] & [SMALL_DZ_DELTA] \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 3: Compression Analysis - Wire Protocol Efficiency
% ============================================================================

\begin{table}[t]
\centering
\caption{Compression analysis comparing wire protocol efficiency. Measurements include quantization overhead and report honest compression ratios.}
\label{tab:compression}
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Repr. Size} & \textbf{Bytes/Example} & \textbf{Compression} & \textbf{Quality (F1)} & \textbf{Efficiency} \\
& & & \textbf{Ratio} & & \textbf{(F1/byte)} \\
\midrule
\multicolumn{6}{l}{\textit{Baseline (No Compression)}} \\
Full Text (UTF-8) & -- & [TEXT_BYTES] & 1.0× & [TEXT_F1] & [TEXT_EFF] \\
\midrule
\multicolumn{6}{l}{\textit{Token-level Compression}} \\
Token Budget ($M=32$) & 32 tokens & [TB32_BYTES] & [TB32_RATIO]× & [TB32_F1] & [TB32_EFF] \\
Token Budget ($M=64$) & 64 tokens & [TB64_BYTES] & [TB64_RATIO]× & [TB64_F1] & [TB64_EFF] \\
LLMLingua (target 4×) & Variable & [LINGUA_BYTES] & [LINGUA_RATIO]× & [LINGUA_F1] & [LINGUA_EFF] \\
\midrule
\multicolumn{6}{l}{\textit{LatentWire (Ours)}} \\
\quad FP16 ($M=32$, $d_z=256$) & 32×256×2B & [LW32_FP16_BYTES] & [LW32_FP16_RATIO]× & [LW32_FP16_F1] & [LW32_FP16_EFF] \\
\quad INT8 ($M=32$, $d_z=256$) & 32×256×1B & [LW32_INT8_BYTES] & [LW32_INT8_RATIO]× & [LW32_INT8_F1] & [LW32_INT8_EFF] \\
\quad INT4 ($M=32$, $d_z=256$) & 32×256×0.5B & [LW32_INT4_BYTES] & [LW32_INT4_RATIO]× & [LW32_INT4_F1] & [LW32_INT4_EFF] \\
\midrule
\quad FP16 ($M=64$, $d_z=256$) & 64×256×2B & [LW64_FP16_BYTES] & [LW64_FP16_RATIO]× & [LW64_FP16_F1] & [LW64_FP16_EFF] \\
\quad INT8 ($M=64$, $d_z=256$) & 64×256×1B & [LW64_INT8_BYTES] & [LW64_INT8_RATIO]× & [LW64_INT8_F1] & [LW64_INT8_EFF] \\
\quad INT4 ($M=64$, $d_z=256$) & 64×256×0.5B & [LW64_INT4_BYTES] & [LW64_INT4_RATIO]× & [LW64_INT4_F1] & [LW64_INT4_EFF] \\
\bottomrule
\end{tabular}
\vspace{2mm}
{\footnotesize Note: Compression ratio calculated as (original text bytes) / (compressed representation bytes). INT4/INT8 measurements include quantization scale factors.}
\end{table}

% ============================================================================
% TABLE 4: Runtime Performance - Wall Clock Time Analysis
% ============================================================================

\begin{table}[t]
\centering
\caption{Runtime performance comparison. All timings measured on 4×H100 GPUs with batch size 64. Times reported in milliseconds per example.}
\label{tab:runtime}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c}{\textbf{Encoding}} & \multicolumn{2}{c}{\textbf{Decoding}} & \textbf{Total} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
& Tokenize & Compress & Transfer & Llama & Qwen & \textbf{Time (ms)} \\
\midrule
Full Text & [FULL_TOK_TIME] & -- & [FULL_TRANS_TIME] & [FULL_LLAMA_TIME] & [FULL_QWEN_TIME] & [FULL_TOTAL_TIME] \\
Token Budget & [TB_TOK_TIME] & -- & [TB_TRANS_TIME] & [TB_LLAMA_TIME] & [TB_QWEN_TIME] & [TB_TOTAL_TIME] \\
LLMLingua & [LINGUA_TOK_TIME] & [LINGUA_COMP_TIME] & [LINGUA_TRANS_TIME] & [LINGUA_LLAMA_TIME] & [LINGUA_QWEN_TIME] & [LINGUA_TOTAL_TIME] \\
\midrule
\textbf{LatentWire} & -- & [LW_ENC_TIME] & [LW_TRANS_TIME] & [LW_LLAMA_TIME] & [LW_QWEN_TIME] & \textbf{[LW_TOTAL_TIME]} \\
\quad Speedup & -- & -- & [LW_TRANS_SPEEDUP]× & [LW_LLAMA_SPEEDUP]× & [LW_QWEN_SPEEDUP]× & \textbf{[LW_TOTAL_SPEEDUP]×} \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 5: Scaling Analysis - Latent Dimension Study
% ============================================================================

\begin{table}[t]
\centering
\caption{Impact of latent dimensions on performance and compression. All experiments on SQuAD validation.}
\label{tab:scaling}
\begin{tabular}{cccccccc}
\toprule
\textbf{Latent} & \textbf{Latent} & \textbf{Total} & \textbf{Compression} & \multicolumn{2}{c}{\textbf{F1 Score}} & \textbf{FirstTok} & \textbf{Training} \\
\textbf{Length} & \textbf{Dim} & \textbf{Params} & \textbf{Ratio} & Llama & Qwen & \textbf{@1} & \textbf{Time} \\
$M$ & $d_z$ & & & & & & (hours) \\
\midrule
16 & 128 & [M16_D128_PARAMS]M & [M16_D128_RATIO]× & [M16_D128_LLAMA_F1] & [M16_D128_QWEN_F1] & [M16_D128_FIRSTTOK]\% & [M16_D128_TIME] \\
16 & 256 & [M16_D256_PARAMS]M & [M16_D256_RATIO]× & [M16_D256_LLAMA_F1] & [M16_D256_QWEN_F1] & [M16_D256_FIRSTTOK]\% & [M16_D256_TIME] \\
16 & 512 & [M16_D512_PARAMS]M & [M16_D512_RATIO]× & [M16_D512_LLAMA_F1] & [M16_D512_QWEN_F1] & [M16_D512_FIRSTTOK]\% & [M16_D512_TIME] \\
\midrule
32 & 128 & [M32_D128_PARAMS]M & [M32_D128_RATIO]× & [M32_D128_LLAMA_F1] & [M32_D128_QWEN_F1] & [M32_D128_FIRSTTOK]\% & [M32_D128_TIME] \\
\textbf{32} & \textbf{256} & \textbf{[M32_D256_PARAMS]M} & \textbf{[M32_D256_RATIO]×} & \textbf{[M32_D256_LLAMA_F1]} & \textbf{[M32_D256_QWEN_F1]} & \textbf{[M32_D256_FIRSTTOK]\%} & \textbf{[M32_D256_TIME]} \\
32 & 512 & [M32_D512_PARAMS]M & [M32_D512_RATIO]× & [M32_D512_LLAMA_F1] & [M32_D512_QWEN_F1] & [M32_D512_FIRSTTOK]\% & [M32_D512_TIME] \\
\midrule
64 & 128 & [M64_D128_PARAMS]M & [M64_D128_RATIO]× & [M64_D128_LLAMA_F1] & [M64_D128_QWEN_F1] & [M64_D128_FIRSTTOK]\% & [M64_D128_TIME] \\
64 & 256 & [M64_D256_PARAMS]M & [M64_D256_RATIO]× & [M64_D256_LLAMA_F1] & [M64_D256_QWEN_F1] & [M64_D256_FIRSTTOK]\% & [M64_D256_TIME] \\
64 & 512 & [M64_D512_PARAMS]M & [M64_D512_RATIO]× & [M64_D512_LLAMA_F1] & [M64_D512_QWEN_F1] & [M64_D512_FIRSTTOK]\% & [M64_D512_TIME] \\
\midrule
128 & 256 & [M128_D256_PARAMS]M & [M128_D256_RATIO]× & [M128_D256_LLAMA_F1] & [M128_D256_QWEN_F1] & [M128_D256_FIRSTTOK]\% & [M128_D256_TIME] \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 6: Cross-Dataset Generalization
% ============================================================================

\begin{table}[t]
\centering
\caption{Cross-dataset generalization. Models trained on SQuAD and evaluated on other datasets without fine-tuning.}
\label{tab:generalization}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Test Dataset}} & \multicolumn{2}{c}{\textbf{Full Text}} & \multicolumn{2}{c}{\textbf{Token Budget}} & \multicolumn{2}{c}{\textbf{LatentWire}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
& EM & F1 & EM & F1 & EM & F1 \\
\midrule
SQuAD (in-domain) & [SQUAD_FULL_EM] & [SQUAD_FULL_F1] & [SQUAD_TB_EM] & [SQUAD_TB_F1] & [SQUAD_LW_EM] & [SQUAD_LW_F1] \\
HotpotQA & [HOTPOT_FULL_EM] & [HOTPOT_FULL_F1] & [HOTPOT_TB_EM] & [HOTPOT_TB_F1] & [HOTPOT_LW_EM] & [HOTPOT_LW_F1] \\
Natural Questions & [NQ_FULL_EM] & [NQ_FULL_F1] & [NQ_TB_EM] & [NQ_TB_F1] & [NQ_LW_EM] & [NQ_LW_F1] \\
TriviaQA & [TRIVIA_FULL_EM] & [TRIVIA_FULL_F1] & [TRIVIA_TB_EM] & [TRIVIA_TB_F1] & [TRIVIA_LW_EM] & [TRIVIA_LW_F1] \\
\midrule
Average (OOD) & [AVG_FULL_EM] & [AVG_FULL_F1] & [AVG_TB_EM] & [AVG_TB_F1] & [AVG_LW_EM] & [AVG_LW_F1] \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 7: First-Token Analysis - Detailed Breakdown
% ============================================================================

\begin{table}[t]
\centering
\caption{First-token prediction analysis. Critical for autoregressive generation quality.}
\label{tab:first_token}
\begin{tabular}{lcccccc}
\toprule
\textbf{Method} & \textbf{Top-1} & \textbf{Top-5} & \textbf{Top-10} & \textbf{Entropy} & \textbf{KL from} & \textbf{Avg Token} \\
& \textbf{Acc (\%)} & \textbf{Acc (\%)} & \textbf{Acc (\%)} & & \textbf{Teacher} & \textbf{Rank} \\
\midrule
Teacher (Full Text) & [TEACHER_TOP1] & [TEACHER_TOP5] & [TEACHER_TOP10] & [TEACHER_ENTROPY] & 0.00 & [TEACHER_RANK] \\
Token Budget & [TB_TOP1] & [TB_TOP5] & [TB_TOP10] & [TB_ENTROPY] & [TB_KL] & [TB_RANK] \\
\midrule
LatentWire ($k=1$) & [LW_K1_TOP1] & [LW_K1_TOP5] & [LW_K1_TOP10] & [LW_K1_ENTROPY] & [LW_K1_KL] & [LW_K1_RANK] \\
LatentWire ($k=4$) & [LW_K4_TOP1] & [LW_K4_TOP5] & [LW_K4_TOP10] & [LW_K4_ENTROPY] & [LW_K4_KL] & [LW_K4_RANK] \\
LatentWire ($k=8$) & [LW_K8_TOP1] & [LW_K8_TOP5] & [LW_K8_TOP10] & [LW_K8_ENTROPY] & [LW_K8_KL] & [LW_K8_RANK] \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% TABLE 8: Training Efficiency and Convergence
% ============================================================================

\begin{table}[t]
\centering
\caption{Training efficiency and convergence metrics across different configurations.}
\label{tab:training_efficiency}
\begin{tabular}{lccccc}
\toprule
\textbf{Configuration} & \textbf{Epochs to} & \textbf{Wall Clock} & \textbf{GPU} & \textbf{Peak} & \textbf{Samples} \\
& \textbf{10\% F1} & \textbf{Time (h)} & \textbf{Hours} & \textbf{Memory} & \textbf{per Second} \\
\midrule
Baseline ($k=1$) & [BASE_EPOCHS] & [BASE_WALL_TIME] & [BASE_GPU_HOURS] & [BASE_MEMORY]GB & [BASE_THROUGHPUT] \\
K-token ($k=4$) & [K4_EPOCHS] & [K4_WALL_TIME] & [K4_GPU_HOURS] & [K4_MEMORY]GB & [K4_THROUGHPUT] \\
K-token ($k=8$) & [K8_EPOCHS] & [K8_WALL_TIME] & [K8_GPU_HOURS] & [K8_MEMORY]GB & [K8_THROUGHPUT] \\
\midrule
w/ Gradient Accum (2×) & [GA2_EPOCHS] & [GA2_WALL_TIME] & [GA2_GPU_HOURS] & [GA2_MEMORY]GB & [GA2_THROUGHPUT] \\
w/ Gradient Accum (4×) & [GA4_EPOCHS] & [GA4_WALL_TIME] & [GA4_GPU_HOURS] & [GA4_MEMORY]GB & [GA4_THROUGHPUT] \\
\midrule
Single GPU & [1GPU_EPOCHS] & [1GPU_WALL_TIME] & [1GPU_GPU_HOURS] & [1GPU_MEMORY]GB & [1GPU_THROUGHPUT] \\
4× H100 (DDP) & [4GPU_EPOCHS] & [4GPU_WALL_TIME] & [4GPU_GPU_HOURS] & [4GPU_MEMORY]GB & [4GPU_THROUGHPUT] \\
\bottomrule
\end{tabular}
\end{table}

% ============================================================================
% Instructions for filling placeholders:
% ============================================================================
%
% 1. Search for all [PLACEHOLDER] patterns in this file
% 2. Replace with actual experimental values
% 3. Format numbers consistently:
%    - Percentages: XX.X\% (one decimal)
%    - F1/EM scores: 0.XXX (three decimals)
%    - Times: XX.X (one decimal for ms/hours)
%    - Compression ratios: X.X× (one decimal)
%    - Memory: Integer GB
%
% 4. Use \textbf{} for best results, \underline{} for second best
% 5. Ensure all values are from the same experimental setup for consistency
%
% Example replacements:
% [FULL_LLAMA_EM] -> 0.823
% [LW_TOTAL_SPEEDUP] -> 3.2
% [M32_D256_FIRSTTOK] -> 18.5
%
% ============================================================================