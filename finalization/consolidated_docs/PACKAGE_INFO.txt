LATENTWIRE PACKAGE STRUCTURE
=============================
Core implementation of a continuous interlingua system for cross-model prompt compression.

KEY FILES (10 most important):
-------------------------------

latentwire/train.py
  Main training loop implementing k-token teacher forcing and knowledge distillation for learning compressed representations

latentwire/eval.py
  Evaluation pipeline with text/latent/token-budget baselines for SQuAD/HotpotQA question answering

latentwire/models.py
  Core architecture: ByteEncoder, Adapter, and LMWrapper classes for cross-model latent conditioning

latentwire/losses.py
  Loss functions including k_token_ce_from_prefix and prefix knowledge distillation objectives

latentwire/prefix_utils.py
  Critical utilities for calibration, BOS handling, and anchor text management between models

latentwire/data.py
  Dataset loaders for SQuAD, HotpotQA, GSM8K with proper formatting and tokenization alignment

latentwire/metrics.py
  Evaluation metrics including exact match, F1 scores, and first-token accuracy measurements

latentwire/config.py
  Hyperparameter configuration and experiment settings for reproducibility

scripts/run_pipeline.sh
  Main execution script that orchestrates training and evaluation with proper logging

scripts/statistical_testing.py
  Statistical analysis including McNemar tests and bootstrap confidence intervals for results

SYSTEM OVERVIEW:
----------------
LatentWire learns a compressed latent representation (32-64 soft tokens) that can condition
multiple frozen LLMs (Llama-3.1-8B, Qwen2.5-7B) without retokenization. The system achieves
prompt compression while maintaining task performance through careful teacher forcing and
calibration techniques.