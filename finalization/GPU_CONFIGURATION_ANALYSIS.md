# GPU Configuration Analysis for LatentWire Training

## Executive Summary

Based on analysis of the training requirements for LatentWire experiments, **using 1 or 2 GPUs is optimal** for completing training within the 12-hour target while efficiently utilizing resources.

## Current Configuration
- **Models**: Llama-3.1-8B + Qwen-2.5-7B
- **Training**: 5000 samples, 8 epochs, batch size 4
- **Latent**: 32 tokens × 256 dimensions
- **Total samples to process**: 40,000

## GPU Configuration Options

### Option 1: Single GPU (RECOMMENDED) ✅
**File**: `finalization/submit_single_gpu.slurm`

| Metric | Value |
|--------|-------|
| GPUs Used | 1 |
| Memory per GPU | ~34 GB / 80 GB |
| Estimated Time | **5-6 hours** |
| Resource Efficiency | **Excellent** (75% GPUs free) |
| Complexity | Simple (no distributed training) |

**Benefits**:
- Leaves 3 H100 GPUs available for other experiments
- No distributed training overhead
- Easier debugging and monitoring
- Still completes well within 12-hour limit

**Submit with**:
```bash
sbatch finalization/submit_single_gpu.slurm
```

### Option 2: Two GPUs (Balanced) ✅
**File**: `finalization/submit_optimized.slurm`

| Metric | Value |
|--------|-------|
| GPUs Used | 2 |
| Memory per GPU | ~17 GB / 80 GB |
| Estimated Time | **3-4 hours** |
| Resource Efficiency | **Good** (50% GPUs free) |
| Complexity | Moderate (minimal distribution) |

**Benefits**:
- Faster completion while still resource-efficient
- Leaves 2 GPUs for other work
- Good balance of speed vs resources

**Submit with**:
```bash
sbatch finalization/submit_optimized.slurm
```

### Option 3: Four GPUs (Original)
**File**: Generated by `finalization/RUN.sh slurm`

| Metric | Value |
|--------|-------|
| GPUs Used | 4 |
| Memory per GPU | ~8.5 GB / 80 GB |
| Estimated Time | **1.5-2 hours** |
| Resource Efficiency | **Poor** (0% GPUs free) |
| Complexity | High (full distributed training) |

**When to use**:
- Only if you need results urgently (< 2 hours)
- No other jobs need GPU resources
- Testing distributed training setup

## Memory Analysis

### Per-GPU Memory Usage
```
1 GPU:  34.0 GB (42% of H100's 80 GB) ✅
2 GPUs: 17.0 GB (21% of H100's 80 GB) ✅
4 GPUs:  8.5 GB (11% of H100's 80 GB) ✅
```

All configurations have ample memory headroom.

### Memory Breakdown
- **Base Models** (frozen): ~30 GB total
  - Llama-3.1-8B: ~16 GB
  - Qwen-2.5-7B: ~14 GB
- **Trainable Parameters**: < 1 GB
  - Encoder: 768 → 256 dimensions
  - Adapters: 256 → 4096 dimensions × 2
- **Optimizer States**: ~2 GB (Adam momentum + variance)
- **Activations**: ~2 GB (batch size 4)

## Performance Estimates

### Training Throughput
| GPUs | Samples/sec | Time/Epoch | Total Time |
|------|------------|------------|------------|
| 1 | ~2.0 | 42 min | 5.6 hours |
| 2 | ~3.5 | 24 min | 3.2 hours |
| 4 | ~6.0 | 14 min | 1.9 hours |

*Note: Estimates are conservative. Actual times may be 10-20% faster.*

## Recommendations

### For Most Experiments: **Use 1 GPU**
- Completes in 5-6 hours (well within 12-hour limit)
- Maximizes resource availability for team
- Simplest to debug and monitor
- No distributed training complexity

### For Time-Sensitive Results: **Use 2 GPUs**
- Completes in 3-4 hours
- Still leaves 50% resources available
- Good compromise between speed and efficiency

### Avoid 4 GPUs Unless Necessary
- Only 2-3 hours faster than 2 GPUs
- Blocks all GPU resources
- Adds distributed training complexity
- Overkill for current experiment size

## Monitoring Commands

```bash
# Submit job (choose one)
sbatch finalization/submit_single_gpu.slurm    # 1 GPU
sbatch finalization/submit_optimized.slurm     # 2 GPUs

# Check status
squeue -u $USER

# Watch logs (replace JOB_ID)
tail -f runs/single_gpu_JOB_ID.log
tail -f runs/optimized_JOB_ID.log

# Cancel if needed
scancel JOB_ID
```

## Scaling Considerations

If you need to scale up training:

| Change | Impact on Time | Recommendation |
|--------|---------------|----------------|
| Samples: 5K → 10K | 2× longer | Use 2 GPUs |
| Epochs: 8 → 16 | 2× longer | Use 2 GPUs |
| Batch size: 4 → 8 | ~30% faster | Test memory first |
| Latent: 32 → 64 tokens | ~20% slower | May need 2 GPUs |

## Conclusion

**For the current experiment (5000 samples, 8 epochs):**
- **1 GPU is optimal** - completes in 5-6 hours with maximum efficiency
- **2 GPUs are good** if you need results in 3-4 hours
- **4 GPUs are wasteful** - only use if absolutely urgent

The single GPU configuration saves significant resources while still completing well within the 12-hour target.