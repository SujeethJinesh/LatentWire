# =====================================================================================
# MASTER CONFIGURATION FILE FOR LATENTWIRE FINALIZATION
# =====================================================================================
# This file consolidates ALL configuration parameters for the LatentWire project,
# serving as the single source of truth for experiments, training, and evaluation.
#
# Organization:
#   1. Model Configuration - LLM specifications and loading settings
#   2. Architecture Settings - Encoder, adapter, and latent space configuration
#   3. Training Hyperparameters - Optimization, scheduling, and loss settings
#   4. Data Configuration - Datasets, preprocessing, and sampling
#   5. Evaluation Settings - Metrics, baselines, and generation parameters
#   6. Infrastructure - Hardware, distributed training, and checkpointing
#   7. Experiments - Specific experiment configurations and ablations
#   8. Logging & Monitoring - Tensorboard, W&B, diagnostics
#   9. Paths - Directory structure and file locations
#  10. SLURM/HPC - Cluster-specific settings
#
# Last Updated: January 2026
# =====================================================================================

# =====================================================================================
# 1. MODEL CONFIGURATION
# =====================================================================================
models:
  # Primary source model for latent encoding
  primary:
    id: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    type: "llama"
    param_count: 8.03B
    hidden_dim: 4096
    vocab_size: 128256
    num_layers: 32
    num_heads: 32
    intermediate_size: 14336
    max_position_embeddings: 131072
    rope_theta: 500000.0

  # Secondary models for cross-model experiments
  secondary:
    qwen:
      id: "Qwen/Qwen2.5-7B-Instruct"
      type: "qwen"
      param_count: 7.62B
      hidden_dim: 3584
      vocab_size: 152064
      num_layers: 28
      num_heads: 28
      intermediate_size: 18944
      max_position_embeddings: 32768

    mistral:
      id: "mistralai/Mistral-7B-Instruct-v0.3"
      type: "mistral"
      param_count: 7.24B
      hidden_dim: 4096
      vocab_size: 32768
      num_layers: 32
      num_heads: 32
      intermediate_size: 14336
      max_position_embeddings: 32768

    llama_small:
      id: "meta-llama/Llama-3.2-3B-Instruct"
      type: "llama"
      param_count: 3.21B
      hidden_dim: 3072
      vocab_size: 128256
      num_layers: 28
      num_heads: 24

  # Model loading configuration
  loading:
    device_map: "auto"  # auto, balanced, sequential, or specific mapping
    torch_dtype: "bfloat16"  # bfloat16, float16, float32
    load_in_4bit: false
    load_in_8bit: false
    trust_remote_code: true
    use_flash_attention_2: false
    attn_implementation: "eager"  # eager, sdpa, flash_attention_2
    low_cpu_mem_usage: true
    offload_folder: null
    revision: "main"
    require_cuda: true  # Set to false for CPU-only systems

# =====================================================================================
# 2. ARCHITECTURE SETTINGS
# =====================================================================================
architecture:
  # Encoder configuration
  encoder:
    type: "byte"  # byte, simple-st, stq, mlp, transformer
    hidden_sizes: [512, 512]  # For MLP encoders
    num_layers: 2
    dropout: 0.1
    activation: "gelu"  # relu, gelu, silu, tanh
    layer_norm: true
    residual_connections: false
    byte_encoder_d_model: 512  # Dimension for byte encoder

  # Latent space configuration
  latent:
    latent_len: 32  # M: number of soft tokens (compression factor)
    d_z: 256  # Dimension of each latent token
    initialization: "gaussian"  # gaussian, xavier, kaiming, uniform
    init_scale: 0.02

  # Adapter configuration (maps latent to model embeddings)
  adapter:
    type: "linear"  # linear, affine, lora, mlp
    bias: false
    dropout: 0.0
    lora_rank: 8  # For LoRA adapters only
    lora_alpha: 16

  # Advanced architectural features
  advanced:
    use_cross_attention: false
    use_memory_bank: false
    use_gating: false
    use_residual_stream: false
    sequential_models: true  # Process models sequentially to save memory

# =====================================================================================
# 3. TRAINING HYPERPARAMETERS
# =====================================================================================
# Memory Budget Calculations for H100 (80GB):
# - Frozen Models: 31.3GB (Llama-8B: 16.06GB, Qwen-7B: 15.24GB in bf16)
# - Trainable Components: 0.2GB (encoder + adapters ~100M params in bf16)
# - Optimizer States (AdamW): 1.2GB (12 bytes per trainable param: 4B fp32 master + 4B mom1 + 4B mom2)
# - Gradients: 0.2GB (2 bytes per trainable param in bf16)
# - Overhead: 6.4GB (8% of 80GB for CUDA kernels, buffers)
# - Total Fixed Memory: 39.3GB
# - Available for Activations: 40.7GB @ 80% utilization
# - Activation per sample: 0.403GB (with gradient checkpointing)
# - Max batch size @ 80% utilization: 61 (conservative: 54)
# - CONFIG batch_size=64 gives 81.3% utilization (safe with 14.9GB headroom)
# =====================================================================================
training:
  # Batch size configurations for different hardware
  batch_config:
    # H100 (80GB) configurations - OPTIMIZED for LatentWire (frozen LLMs + trainable encoder)
    h100_single:
      batch_size: 64  # Uses 81.3% GPU memory (safe margin from max ~61)
      gradient_accumulation_steps: 1
      effective_batch_size: 64
      elastic_batch_size: true
      elastic_target_util: 0.80  # Target 80% utilization
      notes: 'Single H100: 39.3GB fixed (models+Adam) + 25.8GB activations = 65.1GB total'

    h100_dual:
      batch_size: 64  # Per-GPU batch size in DDP (81.3% per GPU)
      gradient_accumulation_steps: 1
      effective_batch_size: 128  # 64 * 2 GPUs
      elastic_batch_size: true
      elastic_target_util: 0.80
      notes: 'DDP 2x H100: Each GPU uses 65.1GB (39.3GB fixed + 25.8GB activations)'

    h100_triple:
      batch_size: 64  # Per-GPU batch size in DDP (81.3% per GPU)
      gradient_accumulation_steps: 1
      effective_batch_size: 192  # 64 * 3 GPUs
      elastic_batch_size: true
      elastic_target_util: 0.80
      notes: 'DDP 3x H100: Each GPU uses 65.1GB (39.3GB fixed + 25.8GB activations)'

    h100_quad:
      batch_size: 64  # Per-GPU batch size in DDP (81.3% per GPU)
      gradient_accumulation_steps: 1
      effective_batch_size: 256  # 64 * 4 GPUs
      elastic_batch_size: true
      elastic_target_util: 0.80
      notes: 'DDP 4x H100: Each GPU uses 65.1GB (includes 1.2GB Adam optimizer states)'

    # A100 (40GB/80GB) configurations
    a100_40gb:
      batch_size: 1
      gradient_accumulation_steps: 32
      effective_batch_size: 32
      elastic_batch_size: false

    a100_80gb:
      batch_size: 20
      gradient_accumulation_steps: 2
      effective_batch_size: 40
      elastic_batch_size: true

    # Default/fallback configuration
    default:
      batch_size: 1
      gradient_accumulation_steps: 32
      effective_batch_size: 32
      elastic_batch_size: false

  # Optimizer settings
  optimizer:
    type: "adamw"  # adamw, adam, sgd, adagrad, rmsprop
    learning_rate: 1.0e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 1.0e-8
    amsgrad: false

  # Learning rate scheduler
  scheduler:
    type: "cosine"  # constant, linear, cosine, cosine_with_restarts, polynomial
    warmup_steps: 500
    warmup_ratio: 0.1  # Alternative to warmup_steps
    num_cycles: 1  # For cosine_with_restarts
    power: 1.0  # For polynomial decay
    end_learning_rate: 0.0

  # Training duration
  duration:
    epochs: 3
    max_steps: -1  # -1 for no limit
    early_stopping_patience: 5
    early_stopping_threshold: 0.001
    early_stopping_metric: "validation_loss"
    greater_is_better: false

  # Gradient settings
  gradients:
    max_grad_norm: 1.0
    gradient_checkpointing: true
    gradient_clipping_value: null  # Alternative to max_grad_norm

  # Mixed precision training
  precision:
    fp16: false
    bf16: true
    tf32: true  # Enable TF32 on Ampere+ GPUs
    amp_opt_level: "O2"  # O0, O1, O2, O3
    loss_scaling: "dynamic"
    initial_loss_scale: 2048.0

# =====================================================================================
# 4. LOSS FUNCTION SETTINGS
# =====================================================================================
losses:
  # Cross-entropy loss configuration
  cross_entropy:
    enabled: true
    weight: 1.0
    label_smoothing: 0.0
    ignore_index: -100
    reduction: "mean"  # mean, sum, none

  # K-token supervision (from PLAN.md improvements)
  k_token_ce:
    enabled: true
    k: 4  # Number of tokens to supervise
    weight: 1.0
    first_token_weight: 0.5  # Extra weight for first token
    decay: "linear"  # linear, exponential, constant
    from_prefix: true  # Use k_token_ce_from_prefix loss

  # Knowledge distillation
  distillation:
    enabled: true
    weight: 0.5
    temperature: 1.0
    teacher_forcing: true
    distill_attention: false
    distill_hidden: false
    kd_first_k_prefix_vs_text: true  # Distill first K tokens

  # Auxiliary losses
  auxiliary:
    reconstruction_loss:
      enabled: false
      weight: 0.1
    sparsity_loss:
      enabled: false
      weight: 0.01
      target_sparsity: 0.1
    orthogonality_loss:
      enabled: false
      weight: 0.01

# =====================================================================================
# 5. DATA CONFIGURATION
# =====================================================================================
data:
  # Dataset specifications
  datasets:
    # Question Answering
    squad:
      name: "squad"
      version: "v1"  # v1 or v2
      type: "qa"
      max_context_length: 1024
      max_answer_length: 128
      train_samples: 87599
      eval_samples: 10570

    squad_v2:
      name: "squad_v2"
      type: "qa"
      max_context_length: 1024
      max_answer_length: 128
      train_samples: 130319
      eval_samples: 11873

    hotpot:
      name: "hotpot_qa"
      config: "fullwiki"  # fullwiki or distractor
      type: "qa"
      max_context_length: 2048
      max_answer_length: 128
      train_samples: 90447
      eval_samples: 7405

    # Classification
    sst2:
      name: "sst2"
      type: "classification"
      num_classes: 2
      max_length: 512
      train_samples: 67349
      eval_samples: 872
      test_samples: 1821

    agnews:
      name: "ag_news"
      type: "classification"
      num_classes: 4
      max_length: 512
      train_samples: 120000
      eval_samples: 7600

    trec:
      name: "trec"
      type: "classification"
      num_classes: 6
      max_length: 512
      train_samples: 5452
      eval_samples: 500

    # Generation/Summarization
    xsum:
      name: "xsum"
      type: "summarization"
      max_source_length: 1024
      max_target_length: 128
      train_samples: 204045
      eval_samples: 11332
      test_samples: 11334

    # Math reasoning
    gsm8k:
      name: "gsm8k"
      config: "main"
      type: "reasoning"
      max_length: 2048
      train_samples: 7473
      eval_samples: 1319
      chain_of_thought: true

  # Data loading settings
  loading:
    cache_dir: "~/.cache/huggingface/datasets"
    num_workers: 4
    prefetch_factor: 2
    pin_memory: true
    persistent_workers: true
    drop_last: false
    shuffle: true
    seed: 42

  # Preprocessing
  preprocessing:
    tokenization:
      max_length: 1024
      truncation: true
      padding: "max_length"  # max_length, longest, do_not_pad
      return_tensors: "pt"
      add_special_tokens: true

    augmentation:
      enabled: false
      noise_probability: 0.0
      mask_probability: 0.0
      swap_probability: 0.0

  # Sampling configurations for experiments
  sampling:
    quick_test: 100
    small: 1000
    medium: 5000
    large: 10000
    xlarge: 50000
    full: -1  # Use all available

# =====================================================================================
# 6. EVALUATION SETTINGS
# =====================================================================================
evaluation:
  # Generation settings
  generation:
    max_new_tokens: 128
    min_new_tokens: 1
    temperature: 0.0  # 0.0 for deterministic
    top_p: 1.0
    top_k: 50
    do_sample: false
    num_beams: 1
    early_stopping: false
    repetition_penalty: 1.0
    length_penalty: 1.0
    no_repeat_ngram_size: 0
    fresh_eval: true  # Fresh evaluation from scratch

  # First-token specific settings (critical for quality)
  first_token:
    temperature: 0.7
    top_p: 0.95
    top_k: 50
    ce_weight: 0.5  # Weight for first-token CE loss

  # Calibration settings
  calibration:
    method: "embed_rms"  # embed_rms, batch_norm, layer_norm, none
    per_example: true
    momentum: 0.9

  # Anchor text configuration
  anchoring:
    use_warm_anchor: true
    warm_anchor_text: "Answer: "
    append_bos_after_prefix: true
    latent_anchor_mode: "text"  # text, learned, none

  # Decode hardening
  decode_hardening:
    eos_ban_steps: 4  # Prevent early EOS
    force_words: []
    bad_words: []

  # Metrics to compute
  metrics:
    classification: ["accuracy", "f1", "precision", "recall", "confusion_matrix"]
    qa: ["exact_match", "f1", "has_answer_exact_match", "has_answer_f1"]
    generation: ["bleu", "rouge", "meteor", "bertscore", "perplexity"]
    compression: ["compression_ratio", "bits_per_byte", "reconstruction_loss"]
    first_token: ["first_token_accuracy", "first_token_top5", "first_token_nll"]

  # Baseline comparisons
  baselines:
    text_prompt:
      enabled: true
      description: "Full text prompt (upper bound)"

    token_budget:
      enabled: true
      description: "Text truncated to M tokens"

    llmlingua:
      enabled: true
      rate: 0.5
      target_token: 128
      use_sentence_level: true

    linear_probe:
      enabled: true
      layer: 16
      pool_strategy: "mean"

    selective_context:
      enabled: false
      compression_rate: 0.5

# =====================================================================================
# 7. INFRASTRUCTURE SETTINGS
# =====================================================================================
infrastructure:
  # Checkpointing
  checkpointing:
    save_strategy: "steps"  # steps, epoch, best
    save_steps: 500
    save_interval_seconds: 300  # For preemptible training
    save_total_limit: 3
    save_on_each_node: false
    load_best_model_at_end: false
    metric_for_best_model: "eval_loss"
    greater_is_better: false

    # Preemption support
    preemption:
      enabled: true
      checkpoint_interval_seconds: 300  # 5 minutes
      auto_resume: true
      signal_handler: true

  # Distributed training
  distributed:
    strategy: "ddp"  # ddp, fsdp, deepspeed
    backend: "nccl"  # nccl, gloo, mpi
    find_unused_parameters: false
    gradient_as_bucket_view: true
    static_graph: false

    # FSDP settings
    fsdp:
      sharding_strategy: "full_shard"  # full_shard, shard_grad_op, no_shard
      cpu_offload: false
      auto_wrap_policy: "transformer_based"
      transformer_layer_cls_to_wrap: null
      min_num_params: 1e6

    # DeepSpeed settings
    deepspeed:
      config_file: null
      zero_stage: 2  # 0, 1, 2, 3
      offload_optimizer: false
      offload_param: false

  # Memory optimization
  memory:
    gradient_checkpointing: true
    cpu_offload: false
    empty_cache_steps: 100
    max_memory: {}  # Per-device memory limits

  # Performance optimization
  performance:
    compile_model: false  # PyTorch 2.0 torch.compile
    compile_mode: "default"  # default, reduce-overhead, max-autotune
    use_torch_compile: false
    use_apex: false
    use_native_amp: true
    dataloader_num_workers: 4
    dataloader_prefetch_factor: 2

# =====================================================================================
# 8. EXPERIMENT CONFIGURATIONS
# =====================================================================================
experiments:
  # Tags for experiment tracking
  tag_prefix: "ablation"
  tag: "eval"

  # Ablation studies
  ablations:
    latent_dimensions:
      sweep_latent_len: [8, 16, 32, 48, 64, 128]
      sweep_d_z: [128, 256, 512, 768]

    encoder_types:
      sweep: ["byte", "simple-st", "stq", "mlp", "transformer"]

    loss_weights:
      sweep_first_token_weight: [0.1, 0.3, 0.5, 0.7, 1.0]
      sweep_kd_weight: [0.0, 0.25, 0.5, 0.75, 1.0]

    k_token_supervision:
      sweep_k: [1, 2, 4, 8, 16]

  # Cross-model experiments
  cross_model:
    source_models: ["llama"]
    target_models: ["qwen", "mistral"]
    alignment_strategies: ["adapter", "projection", "fine-tune"]
    sequential_eval: true  # Evaluate models sequentially

  # Compression experiments
  compression:
    quantization:
      methods: ["int8", "int6", "int4", "fp16"]
      group_size: [32, 64, 128, 256]

    target_ratios: [2, 4, 8, 16, 32]

  # Scaling experiments
  scaling:
    model_sizes: ["3B", "7B", "8B", "13B"]
    latent_scales: [0.5, 1.0, 2.0, 4.0]

# =====================================================================================
# 9. LOGGING AND MONITORING
# =====================================================================================
logging:
  # Console logging
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # File logging
  file:
    enabled: true
    path: "logs/training.log"
    max_bytes: 10485760  # 10MB
    backup_count: 5

  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "runs/tensorboard"
    flush_secs: 30
    write_graph: false
    write_images: false
    update_freq: "batch"

  # Weights & Biases
  wandb:
    enabled: false
    entity: null
    project: "latentwire-finalization"
    group: null
    job_type: "train"
    tags: ["finalization"]
    notes: null

  # Metrics tracking
  metrics:
    log_interval: 10
    eval_interval: 100
    save_metrics: true
    metrics_file: "metrics.json"

  # Diagnostics
  diagnostics:
    profile_memory: false
    profile_time: true
    trace_model: false
    log_gradients: false
    log_parameters: false
    log_activations: false
    save_diagnostics: true
    diagnostics_file: "diagnostics.jsonl"

# =====================================================================================
# 10. PATHS AND DIRECTORIES
# =====================================================================================
paths:
  # Base paths
  project_root: "."
  output_dir: "runs/finalization"

  # Model paths
  checkpoint_dir: "checkpoints"
  pretrained_cache: "~/.cache/huggingface/hub"

  # Data paths
  data_dir: "data"
  processed_dir: "data/processed"
  cache_dir: "~/.cache"

  # Results paths
  results_dir: "results"
  figures_dir: "figures"
  tables_dir: "tables"
  logs_dir: "logs"

  # Temporary paths
  tmp_dir: "/tmp/latentwire"
  scratch_dir: null

# =====================================================================================
# 11. SLURM/HPC CONFIGURATION
# =====================================================================================
slurm:
  # Account settings
  account: "marlowe-m000066"
  partition: "preempt"
  qos: "normal"

  # Resource allocation
  nodes: 1
  tasks_per_node: 1
  cpus_per_task: 32
  gpus_per_node: 4
  gpu_type: "h100"  # h100, a100, v100

  # Time and memory
  time_limit: "12:00:00"
  memory: "256GB"
  memory_per_cpu: null

  # Job settings
  job_name: "latentwire-train"
  output: "/projects/m000066/sujinesh/LatentWire/runs/%x_%j.log"
  error: "/projects/m000066/sujinesh/LatentWire/runs/%x_%j.err"

  # Paths
  working_dir: "/projects/m000066/sujinesh/LatentWire"

  # Preemption handling
  signal: "TERM@120"  # 120 seconds grace period
  requeue: true

  # Environment
  modules: []
  conda_env: null

# =====================================================================================
# 12. REPRODUCIBILITY
# =====================================================================================
reproducibility:
  # Random seeds
  seed: 42
  numpy_seed: 42
  torch_seed: 42
  cuda_seed: 42

  # Deterministic behavior
  deterministic: true
  torch_deterministic: true
  torch_benchmark: false  # Disable for reproducibility
  cublas_deterministic: true

  # Environment capture
  capture_env: true
  capture_git: true
  capture_pip: true
  save_config: true

# =====================================================================================
# 13. DEFAULTS AND OVERRIDES
# =====================================================================================
defaults:
  # Default experiment configuration
  experiment_type: "baseline"
  use_defaults: true

  # Override precedence (later overrides earlier)
  override_order:
    - "defaults"
    - "experiment_specific"
    - "command_line"
    - "environment_vars"

# =====================================================================================
# END OF CONFIGURATION
# =====================================================================================