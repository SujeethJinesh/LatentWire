/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading models...
Using device: cuda
Loading meta-llama/Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3097.14it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:22,  7.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:13<00:12,  6.50s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.83s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.84s/it]
Loading mistralai/Mistral-7B-v0.1...
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2413.99it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.52s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  6.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.03s/it]
✓ Models loaded successfully!
Llama device: cuda:0
Mistral device: cuda:0
Loading WikiText-2 calibration data (5000 samples)...
✓ Loaded 5000 calibration texts

============================================================
BASELINE EXPERIMENTS
============================================================

=== Llama 3.1 8B Alone ===
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The future of artificial intelligence is here, and it’s already changing the way we live and work. From self-driving cars to virtual assistants, AI is becoming more and more integrated into our daily lives. But what does this mean for the future of work? Will AI replace humans in

=== Mistral 7B Alone ===
The future of artificial intelligence is a hot topic in the tech world. With the rise of machine learning and deep learning, AI is becoming more and more advanced. But what does the future hold for AI?

There are a few different ways to look at the future of AI

============================================================
SANITY CHECKS (Same Model Transfer)
============================================================

=== Llama 3.1 8B → Llama 3.1 8B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence is bright,

=== Mistral 7B → Mistral 7B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence isbright bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan

============================================================
CROSS-MODEL TRANSFER EXPERIMENTS
============================================================

============================================================
CALIBRATING ALIGNMENT METHODS
============================================================

No Alignment:

Procrustes:
  Computing Procrustes alignment (SVD-based rotation)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Procrustes matrix: torch.Size([4096, 4096])
  ✓ Checkpoint saved: procrustes_Llama-3.1-8B_to_Mistral-7B-v0.1_5000.pt

Centered Procrustes:
  Computing Centered Procrustes alignment (centers data before SVD)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Centered Procrustes matrix: torch.Size([4096, 4096])

Scaled Procrustes:
  Computing Scaled Procrustes alignment (rotation + scale)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Scaled Procrustes matrix: torch.Size([4096, 4096]), scale: 2.5879

L-Cross OLS:
  Computing L-Cross OLS alignment (least squares)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  Warning: lstsq failed (CUDA out of memory. Tried to allocate 11.09 GiB. GPU 0 has a total capacity of 79.19 GiB of which 3.74 GiB is free. Including non-PyTorch memory, this process has 75.44 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 388.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)), using pseudo-inverse
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 356, in calibrate
    solution = torch.linalg.lstsq(source_f32, target_f32)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.09 GiB. GPU 0 has a total capacity of 79.19 GiB of which 3.74 GiB is free. Including non-PyTorch memory, this process has 75.44 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 388.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 673, in <module>
    cross_model_experiment()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 620, in cross_model_experiment
    method.calibrate(llama_model, llama_tokenizer, mistral_model, mistral_tokenizer, calibration_texts, device)
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 361, in calibrate
    self.T = (target_f32.T @ torch.pinverse(source_f32.T)).to(source_states.dtype)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.09 GiB. GPU 0 has a total capacity of 79.19 GiB of which 3.68 GiB is free. Including non-PyTorch memory, this process has 75.50 GiB memory in use. Of the allocated memory 74.21 GiB is allocated by PyTorch, and 388.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
