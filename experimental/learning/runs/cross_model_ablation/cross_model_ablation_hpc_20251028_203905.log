/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading models...
Using device: cuda
Loading meta-llama/Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2059.56it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:17, 25.91s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:21<01:27, 43.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:10<00:45, 45.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:20<00:00, 31.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:20<00:00, 35.08s/it]
Loading mistralai/Mistral-7B-v0.1...
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 1697.07it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:24<01:24, 84.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:53<00:00, 51.53s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:53<00:00, 56.53s/it]
✓ Models loaded successfully!
Llama device: cuda:0
Mistral device: cuda:0
Loading WikiText-2 calibration data (10000 samples)...
Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 260485.33 examples/s]
Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 912301.06 examples/s]
Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 682561.48 examples/s]
✓ Loaded 10000 calibration texts

============================================================
BASELINE EXPERIMENTS
============================================================

=== Llama 3.1 8B Alone ===
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The future of artificial intelligence is here, and it’s already changing the way we live and work. From self-driving cars to virtual assistants, AI is becoming more and more integrated into our daily lives. But what does this mean for the future of work? Will AI replace humans in

=== Mistral 7B Alone ===
The future of artificial intelligence is a hot topic in the tech world. With the rise of machine learning and deep learning, AI is becoming more and more advanced. But what does the future hold for AI?

There are a few different ways to look at the future of AI

============================================================
SANITY CHECKS (Same Model Transfer)
============================================================

=== Llama 3.1 8B → Llama 3.1 8B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence is bright,

=== Mistral 7B → Mistral 7B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence isbright bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan

============================================================
CROSS-MODEL TRANSFER EXPERIMENTS
============================================================

============================================================
CALIBRATING ALIGNMENT METHODS
============================================================

No Alignment:

Procrustes:
  Computing Procrustes alignment (SVD-based rotation)...
    Processing 0/10000...
    Processing 100/10000...
    Processing 200/10000...
    Processing 300/10000...
    Processing 400/10000...
    Processing 500/10000...
    Processing 600/10000...
    Processing 700/10000...
    Processing 800/10000...
    Processing 900/10000...
    Processing 1000/10000...
    Processing 1100/10000...
    Processing 1200/10000...
    Processing 1300/10000...
    Processing 1400/10000...
    Processing 1500/10000...
    Processing 1600/10000...
    Processing 1700/10000...
    Processing 1800/10000...
    Processing 1900/10000...
    Processing 2000/10000...
    Processing 2100/10000...
    Processing 2200/10000...
    Processing 2300/10000...
    Processing 2400/10000...
    Processing 2500/10000...
    Processing 2600/10000...
    Processing 2700/10000...
    Processing 2800/10000...
    Processing 2900/10000...
    Processing 3000/10000...
    Processing 3100/10000...
    Processing 3200/10000...
    Processing 3300/10000...
    Processing 3400/10000...
    Processing 3500/10000...
    Processing 3600/10000...
    Processing 3700/10000...
    Processing 3800/10000...
    Processing 3900/10000...
    Processing 4000/10000...
    Processing 4100/10000...
    Processing 4200/10000...
    Processing 4300/10000...
    Processing 4400/10000...
    Processing 4500/10000...
    Processing 4600/10000...
    Processing 4700/10000...
    Processing 4800/10000...
    Processing 4900/10000...
    Processing 5000/10000...
    Processing 5100/10000...
    Processing 5200/10000...
    Processing 5300/10000...
    Processing 5400/10000...
    Processing 5500/10000...
    Processing 5600/10000...
    Processing 5700/10000...
    Processing 5800/10000...
    Processing 5900/10000...
    Processing 6000/10000...
    Processing 6100/10000...
    Processing 6200/10000...
    Processing 6300/10000...
    Processing 6400/10000...
    Processing 6500/10000...
    Processing 6600/10000...
    Processing 6700/10000...
    Processing 6800/10000...
    Processing 6900/10000...
    Processing 7000/10000...
    Processing 7100/10000...
    Processing 7200/10000...
    Processing 7300/10000...
    Processing 7400/10000...
    Processing 7500/10000...
    Processing 7600/10000...
    Processing 7700/10000...
    Processing 7800/10000...
    Processing 7900/10000...
    Processing 8000/10000...
    Processing 8100/10000...
    Processing 8200/10000...
    Processing 8300/10000...
    Processing 8400/10000...
    Processing 8500/10000...
    Processing 8600/10000...
    Processing 8700/10000...
    Processing 8800/10000...
    Processing 8900/10000...
    Processing 9000/10000...
    Processing 9100/10000...
    Processing 9200/10000...
    Processing 9300/10000...
    Processing 9400/10000...
    Processing 9500/10000...
    Processing 9600/10000...
    Processing 9700/10000...
    Processing 9800/10000...
    Processing 9900/10000...
  ✓ Procrustes matrix: torch.Size([4096, 4096])

Centered Procrustes:
  Computing Centered Procrustes alignment (centers data before SVD)...
    Processing 0/10000...
    Processing 100/10000...
    Processing 200/10000...
    Processing 300/10000...
    Processing 400/10000...
    Processing 500/10000...
    Processing 600/10000...
    Processing 700/10000...
    Processing 800/10000...
    Processing 900/10000...
    Processing 1000/10000...
    Processing 1100/10000...
    Processing 1200/10000...
    Processing 1300/10000...
    Processing 1400/10000...
    Processing 1500/10000...
    Processing 1600/10000...
    Processing 1700/10000...
    Processing 1800/10000...
    Processing 1900/10000...
    Processing 2000/10000...
    Processing 2100/10000...
    Processing 2200/10000...
    Processing 2300/10000...
    Processing 2400/10000...
    Processing 2500/10000...
    Processing 2600/10000...
    Processing 2700/10000...
    Processing 2800/10000...
    Processing 2900/10000...
    Processing 3000/10000...
    Processing 3100/10000...
    Processing 3200/10000...
    Processing 3300/10000...
    Processing 3400/10000...
    Processing 3500/10000...
    Processing 3600/10000...
    Processing 3700/10000...
    Processing 3800/10000...
    Processing 3900/10000...
    Processing 4000/10000...
    Processing 4100/10000...
    Processing 4200/10000...
    Processing 4300/10000...
    Processing 4400/10000...
    Processing 4500/10000...
    Processing 4600/10000...
    Processing 4700/10000...
    Processing 4800/10000...
    Processing 4900/10000...
    Processing 5000/10000...
    Processing 5100/10000...
    Processing 5200/10000...
    Processing 5300/10000...
    Processing 5400/10000...
    Processing 5500/10000...
    Processing 5600/10000...
    Processing 5700/10000...
    Processing 5800/10000...
    Processing 5900/10000...
    Processing 6000/10000...
    Processing 6100/10000...
    Processing 6200/10000...
    Processing 6300/10000...
    Processing 6400/10000...
    Processing 6500/10000...
    Processing 6600/10000...
    Processing 6700/10000...
    Processing 6800/10000...
    Processing 6900/10000...
    Processing 7000/10000...
    Processing 7100/10000...
    Processing 7200/10000...
    Processing 7300/10000...
    Processing 7400/10000...
    Processing 7500/10000...
    Processing 7600/10000...
    Processing 7700/10000...
    Processing 7800/10000...
    Processing 7900/10000...
    Processing 8000/10000...
    Processing 8100/10000...
    Processing 8200/10000...
    Processing 8300/10000...
    Processing 8400/10000...
    Processing 8500/10000...
    Processing 8600/10000...
    Processing 8700/10000...
    Processing 8800/10000...
    Processing 8900/10000...
    Processing 9000/10000...
    Processing 9100/10000...
    Processing 9200/10000...
    Processing 9300/10000...
    Processing 9400/10000...
    Processing 9500/10000...
    Processing 9600/10000...
    Processing 9700/10000...
    Processing 9800/10000...
    Processing 9900/10000...
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 648, in <module>
    cross_model_experiment()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 595, in cross_model_experiment
    method.calibrate(llama_model, llama_tokenizer, mistral_model, mistral_tokenizer, calibration_texts, device)
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 197, in calibrate
    source_centered = source_states - self.source_mean
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 11.26 GiB. GPU 0 has a total capacity of 79.19 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 77.39 GiB memory in use. Of the allocated memory 75.84 GiB is allocated by PyTorch, and 649.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
