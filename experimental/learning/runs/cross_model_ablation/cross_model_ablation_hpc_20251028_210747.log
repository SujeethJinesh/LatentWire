/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading models...
Using device: cuda
Loading meta-llama/Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 3423.92it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:30<01:30, 30.13s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:49<01:58, 59.28s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [03:14<01:10, 70.70s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:34<00:00, 51.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:34<00:00, 53.73s/it]
Loading mistralai/Mistral-7B-v0.1...
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 507.57it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:37<00:37, 37.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 17.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:41<00:00, 20.85s/it]
✓ Models loaded successfully!
Llama device: cuda:0
Mistral device: cuda:0
Loading WikiText-2 calibration data (5000 samples)...
✓ Loaded 5000 calibration texts

============================================================
BASELINE EXPERIMENTS
============================================================

=== Llama 3.1 8B Alone ===
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The future of artificial intelligence is here, and it’s already changing the way we live and work. From self-driving cars to virtual assistants, AI is becoming more and more integrated into our daily lives. But what does this mean for the future of work? Will AI replace humans in

=== Mistral 7B Alone ===
The future of artificial intelligence is a hot topic in the tech world. With the rise of machine learning and deep learning, AI is becoming more and more advanced. But what does the future hold for AI?

There are a few different ways to look at the future of AI

============================================================
SANITY CHECKS (Same Model Transfer)
============================================================

=== Llama 3.1 8B → Llama 3.1 8B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence is bright,

=== Mistral 7B → Mistral 7B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence isbright bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan

============================================================
CROSS-MODEL TRANSFER EXPERIMENTS
============================================================

============================================================
CALIBRATING ALIGNMENT METHODS
============================================================

No Alignment:

Procrustes:
  Computing Procrustes alignment (SVD-based rotation)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Procrustes matrix: torch.Size([4096, 4096])
  ✓ Checkpoint saved: procrustes_Llama-3.1-8B_to_Mistral-7B-v0.1_5000.pt

Centered Procrustes:
  Computing Centered Procrustes alignment (centers data before SVD)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Centered Procrustes matrix: torch.Size([4096, 4096])

Scaled Procrustes:
  Computing Scaled Procrustes alignment (rotation + scale)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  ✓ Scaled Procrustes matrix: torch.Size([4096, 4096]), scale: 2.5879

L-Cross OLS:
  Computing L-Cross OLS alignment (least squares)...
    Processing 0/5000...
    Processing 100/5000...
    Processing 200/5000...
    Processing 300/5000...
    Processing 400/5000...
    Processing 500/5000...
    Processing 600/5000...
    Processing 700/5000...
    Processing 800/5000...
    Processing 900/5000...
    Processing 1000/5000...
    Processing 1100/5000...
    Processing 1200/5000...
    Processing 1300/5000...
    Processing 1400/5000...
    Processing 1500/5000...
    Processing 1600/5000...
    Processing 1700/5000...
    Processing 1800/5000...
    Processing 1900/5000...
    Processing 2000/5000...
    Processing 2100/5000...
    Processing 2200/5000...
    Processing 2300/5000...
    Processing 2400/5000...
    Processing 2500/5000...
    Processing 2600/5000...
    Processing 2700/5000...
    Processing 2800/5000...
    Processing 2900/5000...
    Processing 3000/5000...
    Processing 3100/5000...
    Processing 3200/5000...
    Processing 3300/5000...
    Processing 3400/5000...
    Processing 3500/5000...
    Processing 3600/5000...
    Processing 3700/5000...
    Processing 3800/5000...
    Processing 3900/5000...
    Processing 4000/5000...
    Processing 4100/5000...
    Processing 4200/5000...
    Processing 4300/5000...
    Processing 4400/5000...
    Processing 4500/5000...
    Processing 4600/5000...
    Processing 4700/5000...
    Processing 4800/5000...
    Processing 4900/5000...
  Warning: lstsq failed, using pseudo-inverse
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 352, in calibrate
    solution = torch.linalg.lstsq(source_states, target_states)
RuntimeError: This function doesn't handle types other than float and double

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 669, in <module>
    cross_model_experiment()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 616, in cross_model_experiment
    method.calibrate(llama_model, llama_tokenizer, mistral_model, mistral_tokenizer, calibration_texts, device)
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py", line 357, in calibrate
    self.T = (target_states.T @ torch.pinverse(source_states.T)).to(source_states.dtype)
RuntimeError: linalg.pinv(Half{[4096, 726543]}): expected a tensor with 2 or more dimensions of float, double, cfloat or cdouble types
