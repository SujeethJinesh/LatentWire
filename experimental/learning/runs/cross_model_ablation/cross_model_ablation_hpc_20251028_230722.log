/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Loading models...
Using device: cuda
Loading meta-llama/Llama-3.1-8B...
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 460.66it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:36<01:50, 36.77s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:04<01:03, 31.72s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:32<00:29, 29.85s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:41<00:00, 21.68s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:41<00:00, 25.44s/it]
Loading mistralai/Mistral-7B-v0.1...
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 2404.99it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:10<01:10, 70.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:51<00:00, 53.23s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:51<00:00, 55.82s/it]
✓ Models loaded successfully!
Llama device: cuda:0
Mistral device: cuda:0
Loading WikiText-2 calibration data (5000 samples)...
✓ Loaded 5000 calibration texts

============================================================
BASELINE EXPERIMENTS
============================================================

=== Llama 3.1 8B Alone ===
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The future of artificial intelligence is here, and it’s already changing the way we live and work. From self-driving cars to virtual assistants, AI is becoming more and more integrated into our daily lives. But what does this mean for the future of work? Will AI replace humans in

=== Mistral 7B Alone ===
The future of artificial intelligence is a hot topic in the tech world. With the rise of machine learning and deep learning, AI is becoming more and more advanced. But what does the future hold for AI?

There are a few different ways to look at the future of AI

============================================================
SANITY CHECKS (Same Model Transfer)
============================================================

=== Llama 3.1 8B → Llama 3.1 8B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence is bright,

=== Mistral 7B → Mistral 7B ===
Model A hidden states: torch.Size([1, 7, 4096])
Model A dim: 4096, Model B dim: 4096
We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)
The future of artificial intelligence isbright bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan

============================================================
CROSS-MODEL TRANSFER EXPERIMENTS
============================================================

============================================================
CALIBRATING ALIGNMENT METHODS
============================================================

No Alignment:

Procrustes:
  Loading Procrustes alignment from checkpoint: procrustes_Llama-3.1-8B_to_Mistral-7B-v0.1_5000.pt
/projects/m000066/sujinesh/LatentWire/experimental/learning/cross_model_ablation.py:111: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(checkpoint_path)
  ✓ Procrustes matrix loaded: torch.Size([4096, 4096])

Centered Procrustes:
  Computing Centered Procrustes alignment (centers data before SVD)...
    Processing batch 0/625 (0/5000 texts)...
    Processing batch 10/625 (80/5000 texts)...
    Processing batch 20/625 (160/5000 texts)...
    Processing batch 30/625 (240/5000 texts)...
    Processing batch 40/625 (320/5000 texts)...
    Processing batch 50/625 (400/5000 texts)...
    Processing batch 60/625 (480/5000 texts)...
    Processing batch 70/625 (560/5000 texts)...
    Processing batch 80/625 (640/5000 texts)...
    Processing batch 90/625 (720/5000 texts)...
    Processing batch 100/625 (800/5000 texts)...
    Processing batch 110/625 (880/5000 texts)...
    Processing batch 120/625 (960/5000 texts)...
    Processing batch 130/625 (1040/5000 texts)...
    Processing batch 140/625 (1120/5000 texts)...
    Processing batch 150/625 (1200/5000 texts)...
    Processing batch 160/625 (1280/5000 texts)...
    Processing batch 170/625 (1360/5000 texts)...
    Processing batch 180/625 (1440/5000 texts)...
    Processing batch 190/625 (1520/5000 texts)...
    Processing batch 200/625 (1600/5000 texts)...
    Processing batch 210/625 (1680/5000 texts)...
    Processing batch 220/625 (1760/5000 texts)...
    Processing batch 230/625 (1840/5000 texts)...
    Processing batch 240/625 (1920/5000 texts)...
    Processing batch 250/625 (2000/5000 texts)...
    Processing batch 260/625 (2080/5000 texts)...
    Processing batch 270/625 (2160/5000 texts)...
    Processing batch 280/625 (2240/5000 texts)...
    Processing batch 290/625 (2320/5000 texts)...
    Processing batch 300/625 (2400/5000 texts)...
    Processing batch 310/625 (2480/5000 texts)...
    Processing batch 320/625 (2560/5000 texts)...
    Processing batch 330/625 (2640/5000 texts)...
    Processing batch 340/625 (2720/5000 texts)...
    Processing batch 350/625 (2800/5000 texts)...
    Processing batch 360/625 (2880/5000 texts)...
    Processing batch 370/625 (2960/5000 texts)...
    Processing batch 380/625 (3040/5000 texts)...
    Processing batch 390/625 (3120/5000 texts)...
    Processing batch 400/625 (3200/5000 texts)...
    Processing batch 410/625 (3280/5000 texts)...
    Processing batch 420/625 (3360/5000 texts)...
    Processing batch 430/625 (3440/5000 texts)...
    Processing batch 440/625 (3520/5000 texts)...
    Processing batch 450/625 (3600/5000 texts)...
    Processing batch 460/625 (3680/5000 texts)...
    Processing batch 470/625 (3760/5000 texts)...
    Processing batch 480/625 (3840/5000 texts)...
    Processing batch 490/625 (3920/5000 texts)...
    Processing batch 500/625 (4000/5000 texts)...
    Processing batch 510/625 (4080/5000 texts)...
    Processing batch 520/625 (4160/5000 texts)...
    Processing batch 530/625 (4240/5000 texts)...
    Processing batch 540/625 (4320/5000 texts)...
    Processing batch 550/625 (4400/5000 texts)...
    Processing batch 560/625 (4480/5000 texts)...
    Processing batch 570/625 (4560/5000 texts)...
    Processing batch 580/625 (4640/5000 texts)...
    Processing batch 590/625 (4720/5000 texts)...
    Processing batch 600/625 (4800/5000 texts)...
    Processing batch 610/625 (4880/5000 texts)...
    Processing batch 620/625 (4960/5000 texts)...
  ✓ Centered Procrustes matrix: torch.Size([4096, 4096])

Scaled Procrustes:
  Computing Scaled Procrustes alignment (rotation + scale)...
    Processing batch 0/625 (0/5000 texts)...
    Processing batch 10/625 (80/5000 texts)...
    Processing batch 20/625 (160/5000 texts)...
    Processing batch 30/625 (240/5000 texts)...
    Processing batch 40/625 (320/5000 texts)...
    Processing batch 50/625 (400/5000 texts)...
    Processing batch 60/625 (480/5000 texts)...
    Processing batch 70/625 (560/5000 texts)...
    Processing batch 80/625 (640/5000 texts)...
    Processing batch 90/625 (720/5000 texts)...
    Processing batch 100/625 (800/5000 texts)...
    Processing batch 110/625 (880/5000 texts)...
    Processing batch 120/625 (960/5000 texts)...
    Processing batch 130/625 (1040/5000 texts)...
    Processing batch 140/625 (1120/5000 texts)...
    Processing batch 150/625 (1200/5000 texts)...
    Processing batch 160/625 (1280/5000 texts)...
    Processing batch 170/625 (1360/5000 texts)...
    Processing batch 180/625 (1440/5000 texts)...
    Processing batch 190/625 (1520/5000 texts)...
    Processing batch 200/625 (1600/5000 texts)...
    Processing batch 210/625 (1680/5000 texts)...
    Processing batch 220/625 (1760/5000 texts)...
    Processing batch 230/625 (1840/5000 texts)...
    Processing batch 240/625 (1920/5000 texts)...
    Processing batch 250/625 (2000/5000 texts)...
    Processing batch 260/625 (2080/5000 texts)...
    Processing batch 270/625 (2160/5000 texts)...
    Processing batch 280/625 (2240/5000 texts)...
    Processing batch 290/625 (2320/5000 texts)...
    Processing batch 300/625 (2400/5000 texts)...
    Processing batch 310/625 (2480/5000 texts)...
    Processing batch 320/625 (2560/5000 texts)...
    Processing batch 330/625 (2640/5000 texts)...
    Processing batch 340/625 (2720/5000 texts)...
    Processing batch 350/625 (2800/5000 texts)...
    Processing batch 360/625 (2880/5000 texts)...
    Processing batch 370/625 (2960/5000 texts)...
    Processing batch 380/625 (3040/5000 texts)...
    Processing batch 390/625 (3120/5000 texts)...
    Processing batch 400/625 (3200/5000 texts)...
    Processing batch 410/625 (3280/5000 texts)...
    Processing batch 420/625 (3360/5000 texts)...
    Processing batch 430/625 (3440/5000 texts)...
    Processing batch 440/625 (3520/5000 texts)...
    Processing batch 450/625 (3600/5000 texts)...
    Processing batch 460/625 (3680/5000 texts)...
    Processing batch 470/625 (3760/5000 texts)...
    Processing batch 480/625 (3840/5000 texts)...
    Processing batch 490/625 (3920/5000 texts)...
    Processing batch 500/625 (4000/5000 texts)...
    Processing batch 510/625 (4080/5000 texts)...
    Processing batch 520/625 (4160/5000 texts)...
    Processing batch 530/625 (4240/5000 texts)...
    Processing batch 540/625 (4320/5000 texts)...
    Processing batch 550/625 (4400/5000 texts)...
    Processing batch 560/625 (4480/5000 texts)...
    Processing batch 570/625 (4560/5000 texts)...
    Processing batch 580/625 (4640/5000 texts)...
    Processing batch 590/625 (4720/5000 texts)...
    Processing batch 600/625 (4800/5000 texts)...
    Processing batch 610/625 (4880/5000 texts)...
    Processing batch 620/625 (4960/5000 texts)...
  ✓ Scaled Procrustes matrix: torch.Size([4096, 4096]), scale: 2.5879

L-Cross OLS:
  Computing L-Cross OLS alignment (least squares)...
    Processing batch 0/625 (0/5000 texts)...
    Processing batch 10/625 (80/5000 texts)...
    Processing batch 20/625 (160/5000 texts)...
    Processing batch 30/625 (240/5000 texts)...
    Processing batch 40/625 (320/5000 texts)...
    Processing batch 50/625 (400/5000 texts)...
    Processing batch 60/625 (480/5000 texts)...
    Processing batch 70/625 (560/5000 texts)...
    Processing batch 80/625 (640/5000 texts)...
    Processing batch 90/625 (720/5000 texts)...
    Processing batch 100/625 (800/5000 texts)...
    Processing batch 110/625 (880/5000 texts)...
    Processing batch 120/625 (960/5000 texts)...
    Processing batch 130/625 (1040/5000 texts)...
    Processing batch 140/625 (1120/5000 texts)...
    Processing batch 150/625 (1200/5000 texts)...
    Processing batch 160/625 (1280/5000 texts)...
    Processing batch 170/625 (1360/5000 texts)...
    Processing batch 180/625 (1440/5000 texts)...
    Processing batch 190/625 (1520/5000 texts)...
    Processing batch 200/625 (1600/5000 texts)...
    Processing batch 210/625 (1680/5000 texts)...
    Processing batch 220/625 (1760/5000 texts)...
    Processing batch 230/625 (1840/5000 texts)...
    Processing batch 240/625 (1920/5000 texts)...
    Processing batch 250/625 (2000/5000 texts)...
    Processing batch 260/625 (2080/5000 texts)...
    Processing batch 270/625 (2160/5000 texts)...
    Processing batch 280/625 (2240/5000 texts)...
    Processing batch 290/625 (2320/5000 texts)...
    Processing batch 300/625 (2400/5000 texts)...
    Processing batch 310/625 (2480/5000 texts)...
    Processing batch 320/625 (2560/5000 texts)...
    Processing batch 330/625 (2640/5000 texts)...
    Processing batch 340/625 (2720/5000 texts)...
    Processing batch 350/625 (2800/5000 texts)...
    Processing batch 360/625 (2880/5000 texts)...
    Processing batch 370/625 (2960/5000 texts)...
    Processing batch 380/625 (3040/5000 texts)...
    Processing batch 390/625 (3120/5000 texts)...
    Processing batch 400/625 (3200/5000 texts)...
    Processing batch 410/625 (3280/5000 texts)...
    Processing batch 420/625 (3360/5000 texts)...
    Processing batch 430/625 (3440/5000 texts)...
    Processing batch 440/625 (3520/5000 texts)...
    Processing batch 450/625 (3600/5000 texts)...
    Processing batch 460/625 (3680/5000 texts)...
    Processing batch 470/625 (3760/5000 texts)...
    Processing batch 480/625 (3840/5000 texts)...
    Processing batch 490/625 (3920/5000 texts)...
    Processing batch 500/625 (4000/5000 texts)...
    Processing batch 510/625 (4080/5000 texts)...
    Processing batch 520/625 (4160/5000 texts)...
    Processing batch 530/625 (4240/5000 texts)...
    Processing batch 540/625 (4320/5000 texts)...
    Processing batch 550/625 (4400/5000 texts)...
    Processing batch 560/625 (4480/5000 texts)...
    Processing batch 570/625 (4560/5000 texts)...
    Processing batch 580/625 (4640/5000 texts)...
    Processing batch 590/625 (4720/5000 texts)...
    Processing batch 600/625 (4800/5000 texts)...
    Processing batch 610/625 (4880/5000 texts)...
    Processing batch 620/625 (4960/5000 texts)...
  Moving data to CPU for OLS computation (avoiding OOM)...
run_cross_model_ablation_hpc.sh: line 35: 292297 Killed                  python cross_model_ablation.py
