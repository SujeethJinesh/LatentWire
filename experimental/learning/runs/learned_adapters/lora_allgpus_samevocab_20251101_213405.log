================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B
Model B: meta-llama/Llama-3.2-3B
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_samevocab_20251101_213405.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DistributedDataParallel (DDP)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Rank: 0, Device: cuda:0
Batch size per GPU: 10
Global batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 3942.02it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 3942.02it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.02it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.02it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.69it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.69it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.95it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.95it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.68it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.68it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.20it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.20it/s]

Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|##########| 2/2 [00:00<00:00, 2537.39it/s]Downloading shards: 100%|##########| 2/2 [00:00<00:00, 2537.39it/s]

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|#####     | 1/2 [00:00<00:00,  1.84it/s]Loading checkpoint shards:  50%|#####     | 1/2 [00:00<00:00,  1.84it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.92it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.86it/s]Loading checkpoint shards: 100%|##########| 2/2 [00:01<00:00,  1.86it/s]


Model dimensions:
  Model A (meta-llama/Llama-3.1-8B): 4096
  Model B (meta-llama/Llama-3.2-3B): 3072
  Dimension mismatch detected - adapter will project 4096â†’3072

Training LoRAAdapter...
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================


================================================================================
Epoch 1/5
================================================================================

================================================================================
Epoch 1/5
================================================================================
