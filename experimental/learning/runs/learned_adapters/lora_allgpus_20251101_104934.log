================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B
Model B: mistralai/Mistral-7B-v0.3
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_104934.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DistributedDataParallel (DDP)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Rank: 0, Device: cuda:0
Batch size per GPU: 10
Global batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 7788.87it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 7788.87it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.03it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.03it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.75it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.75it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.76it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.76it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.84it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 343.58it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 343.58it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.46it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.46it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.69it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.69it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  3.11it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  3.11it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.81it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.81it/s]


Training LoRAAdapter...
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================


================================================================================
Epoch 1/5
================================================================================

================================================================================
Epoch 1/5
================================================================================
  Step 10/250: Loss = 11.4603 (Gen: 11.2277, Contr: 2.3055, Align: 117.2588, Uniform: -0.9349)
  Step 20/250: Loss = 11.3884 (Gen: 11.1536, Contr: 2.3050, Align: 109.9200, Uniform: -0.9511)
  Step 30/250: Loss = 11.3370 (Gen: 11.0998, Contr: 2.3057, Align: 112.6303, Uniform: -0.9514)
  Step 40/250: Loss = 11.3176 (Gen: 11.0782, Contr: 2.3046, Align: 107.5033, Uniform: -0.9463)
  Step 50/250: Loss = 11.3089 (Gen: 11.0672, Contr: 2.3039, Align: 109.0084, Uniform: -0.9424)
  Step 60/250: Loss = 11.2902 (Gen: 11.0463, Contr: 2.3036, Align: 104.3329, Uniform: -0.9398)
  Step 70/250: Loss = 11.2801 (Gen: 11.0338, Contr: 2.3039, Align: 103.4679, Uniform: -0.9363)
  Step 80/250: Loss = 11.2704 (Gen: 11.0218, Contr: 2.3038, Align: 103.7548, Uniform: -0.9357)
  Step 90/250: Loss = 11.2536 (Gen: 11.0027, Contr: 2.3039, Align: 104.7091, Uniform: -0.9376)
  [ 40.0%] Step  100/250 | Loss: 11.2348 (Gen: 10.9817, Contr: 2.3037, Align: 105.2703, Uniform: -0.9367) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 11.2348 (Gen: 10.9817, Contr: 2.3037, Align: 105.2703, Uniform: -0.9367) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
