================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251031_233053.log

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8839.42it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8839.42it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.90it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.33it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.33it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.55it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.55it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.85it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.85it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.62it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.62it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 9539.74it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 9539.74it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:01,  1.89it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.04it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.04it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.19it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.19it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.13it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.13it/s]


Training LoRAAdapter...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================

  Step 10/250: Loss = 11.2322 (Gen: 10.9999, Contr: 2.3028, Align: 92.8275, Uniform: -0.9316)
  Step 20/250: Loss = 11.2659 (Gen: 11.0313, Contr: 2.3019, Align: 94.3914, Uniform: -0.9259)
  Step 30/250: Loss = 11.3587 (Gen: 11.1217, Contr: 2.3032, Align: 92.7297, Uniform: -0.9294)
  Step 40/250: Loss = 11.3386 (Gen: 11.0992, Contr: 2.3034, Align: 94.8080, Uniform: -0.9415)
  Step 50/250: Loss = 11.2773 (Gen: 11.0357, Contr: 2.3025, Align: 95.0229, Uniform: -0.9510)
  Step 60/250: Loss = 11.2621 (Gen: 11.0182, Contr: 2.3029, Align: 97.6850, Uniform: -0.9498)
  Step 70/250: Loss = 11.2515 (Gen: 11.0053, Contr: 2.3033, Align: 98.5508, Uniform: -0.9511)
  Step 80/250: Loss = 11.2335 (Gen: 10.9850, Contr: 2.3026, Align: 99.3529, Uniform: -0.9501)
  Step 90/250: Loss = 11.2321 (Gen: 10.9814, Contr: 2.3023, Align: 99.8159, Uniform: -0.9496)
  [ 40.0%] Step  100/250 | Loss: 11.2036 (Gen: 10.9506, Contr: 2.3020, Align: 102.1190, Uniform: -0.9502) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 11.2036 (Gen: 10.9506, Contr: 2.3020, Align: 102.1190, Uniform: -0.9502) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  Step  Step 110/250: Loss = 11.2072 (Gen: 10.9517, Contr: 2.3042, Align: 102.9777, Uniform: -0.9456)
  Step 120/250: Loss = 11.1849 (Gen: 10.9270, Contr: 2.3041, Align: 103.1682, Uniform: -0.9458)
  Step 130/250: Loss = 11.1682 (Gen: 10.9081, Contr: 2.3040, Align: 103.9564, Uniform: -0.9420)
  Step 140/250: Loss = 11.1402 (Gen: 10.8778, Contr: 2.3040, Align: 104.4804, Uniform: -0.9455)
  Step 150/250: Loss = 11.1144 (Gen: 10.8497, Contr: 2.3042, Align: 105.2850, Uniform: -0.9460)
  Step 160/250: Loss = 11.0833 (Gen: 10.8163, Contr: 2.3042, Align: 105.2949, Uniform: -0.9470)
  Step 170/250: Loss = 11.0564 (Gen: 10.7871, Contr: 2.3042, Align: 106.2922, Uniform: -0.9465)
  Step 180/250: Loss = 11.0286 (Gen: 10.7570, Contr: 2.3040, Align: 105.9585, Uniform: -0.9474)
  Step 190/250: Loss = 11.0029 (Gen: 10.7290, Contr: 2.3038, Align: 105.1478, Uniform: -0.9481)
  [ 80.0%] Step  200/250 | Loss: 10.9730 (Gen: 10.6968, Contr: 2.3038, Align: 105.1294, Uniform: -0.9485) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 4.875 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.9730 (Gen: 10.6968, Contr: 2.3038, Align: 105.1294, Uniform: -0.9485) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 4.875 | 0.39 steps/s | ETA: 2.2m
