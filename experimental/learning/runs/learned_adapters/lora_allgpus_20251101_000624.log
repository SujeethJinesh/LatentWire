================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_000624.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DistributedDataParallel (DDP)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Rank: 0, Device: cuda:0
Batch size per GPU: 10
Global batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8991.01it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8991.01it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.17it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.17it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.80it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.80it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00,  3.27it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00,  3.27it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.17it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.17it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 8968.58it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 8968.58it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.51it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.51it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.26it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.26it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  4.13it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  4.13it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.69it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.69it/s]


Training LoRAAdapter...
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================


================================================================================
Epoch 1/5
================================================================================

================================================================================
Epoch 1/5
================================================================================
  Step 10/250: Loss = 11.4590 (Gen: 11.2263, Contr: 2.3055, Align: 117.2588, Uniform: -0.9349)
  Step 20/250: Loss = 11.3830 (Gen: 11.1481, Contr: 2.3050, Align: 109.9203, Uniform: -0.9511)
  Step 30/250: Loss = 11.3309 (Gen: 11.0937, Contr: 2.3057, Align: 112.6309, Uniform: -0.9514)
  Step 40/250: Loss = 11.3106 (Gen: 11.0711, Contr: 2.3046, Align: 107.5041, Uniform: -0.9463)
  Step 50/250: Loss = 11.3017 (Gen: 11.0601, Contr: 2.3039, Align: 109.0095, Uniform: -0.9424)
  Step 60/250: Loss = 11.2812 (Gen: 11.0373, Contr: 2.3036, Align: 104.3340, Uniform: -0.9398)
  Step 70/250: Loss = 11.2703 (Gen: 11.0240, Contr: 2.3039, Align: 103.4691, Uniform: -0.9363)
  Step 80/250: Loss = 11.2582 (Gen: 11.0096, Contr: 2.3038, Align: 103.7558, Uniform: -0.9357)
  Step 90/250: Loss = 11.2397 (Gen: 10.9889, Contr: 2.3039, Align: 104.7100, Uniform: -0.9376)
  [ 40.0%] Step  100/250 | Loss: 11.2188 (Gen: 10.9656, Contr: 2.3037, Align: 105.2712, Uniform: -0.9367) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 11.2188 (Gen: 10.9656, Contr: 2.3037, Align: 105.2712, Uniform: -0.9367) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  Step 110/250: Loss = 11.1943 (Gen: 10.9388, Contr: 2.3036, Align: 104.5654, Uniform: -0.9373)
  Step 120/250: Loss = 11.1727 (Gen: 10.9149, Contr: 2.3038, Align: 104.4972, Uniform: -0.9419)
  Step 130/250: Loss = 11.1492 (Gen: 10.8891, Contr: 2.3036, Align: 104.9429, Uniform: -0.9394)
  Step 140/250: Loss = 11.1301 (Gen: 10.8677, Contr: 2.3036, Align: 104.9709, Uniform: -0.9372)
  Step 150/250: Loss = 11.1036 (Gen: 10.8389, Contr: 2.3035, Align: 105.0279, Uniform: -0.9353)
  Step 160/250: Loss = 11.0759 (Gen: 10.8089, Contr: 2.3035, Align: 104.4862, Uniform: -0.9369)
  Step 170/250: Loss = 11.0523 (Gen: 10.7831, Contr: 2.3032, Align: 103.2438, Uniform: -0.9392)
  Step 180/250: Loss = 11.0296 (Gen: 10.7581, Contr: 2.3033, Align: 103.1537, Uniform: -0.9396)
  Step 190/250: Loss = 11.0026 (Gen: 10.7288, Contr: 2.3033, Align: 103.5063, Uniform: -0.9384)
  [ 80.0%] Step  200/250 | Loss: 10.9761 (Gen: 10.7000, Contr: 2.3032, Align: 103.9985, Uniform: -0.9384) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 5.062 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.9761 (Gen: 10.7000, Contr: 2.3032, Align: 103.9985, Uniform: -0.9384) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 5.062 | 0.39 steps/s | ETA: 2.2m
  Step 210/250: Loss = 10.9526 (Gen: 10.6741, Contr: 2.3033, Align: 104.0108, Uniform: -0.9378)
  Step 220/250: Loss = 10.9306 (Gen: 10.6498, Contr: 2.3034, Align: 104.1917, Uniform: -0.9371)
  Step 230/250: Loss = 10.9060 (Gen: 10.6229, Contr: 2.3033, Align: 105.0531, Uniform: -0.9378)
  Step 240/250: Loss = 10.8783 (Gen: 10.5929, Contr: 2.3034, Align: 105.0996, Uniform: -0.9370)
  Step 250/250: Loss = 10.8559 (Gen: 10.5682, Contr: 2.3035, Align: 104.8771, Uniform: -0.9381)

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 1 Evaluation:
    CKA Similarity: 0.9999
    Cosine Similarity: 0.0006

================================================================================
Epoch 1/5 Complete | Time: 11.0m | Total: 11.0m
  Total Loss: 10.8559 (Gen: 10.5682, Contr: 2.3035)
  CKA Score: 0.9999 | LR: 0.000045
  ETA for remaining 4 epochs: 44.1m
================================================================================

================================================================================
Epoch 1/5 Complete | Time: 11.0m | Total: 11.0m
  Total Loss: 10.8559 (Gen: 10.5682, Contr: 2.3035)
  CKA Score: 0.9999 | LR: 0.000045
  ETA for remaining 4 epochs: 44.1m
================================================================================
  Checkpoint saved to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_checkpoint/checkpoint.pt

================================================================================
Epoch 2/5
================================================================================

================================================================================
Epoch 2/5
================================================================================
