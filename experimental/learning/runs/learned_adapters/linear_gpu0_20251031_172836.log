================================================================================
LEARNED ADAPTER EXPERIMENT - LINEAR
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/linear_gpu0_20251031_172836.log
GPU assigned: 0

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9425.40it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9425.40it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.71it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.71it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:01<00:01,  1.76it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:01<00:01,  1.76it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  1.95it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  1.95it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.26it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.26it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.05it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.05it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 3860.97it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 3860.97it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.09it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.09it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.27it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.27it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.47it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.47it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.39it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.39it/s]


Training LinearAdapter...
Found checkpoint at /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/linear_checkpoint/checkpoint.pt, resuming training...
Resuming from epoch 1
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 10
Steps per epoch: 1000
Total training steps: 10000
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [16]
================================================================================


================================================================================
Epoch 2/10
================================================================================

================================================================================
Epoch 2/10
================================================================================
  Step 10/1000: Loss = 7.5046
  Step 20/1000: Loss = 7.5035
  Step 30/1000: Loss = 7.5029
  Step 40/1000: Loss = 7.4964
  Step 50/1000: Loss = 7.4895
  Step 60/1000: Loss = 7.4803
  Step 70/1000: Loss = 7.4820
  Step 80/1000: Loss = 7.4798
  Step 90/1000: Loss = 7.4784
  [ 10.0%] Step  100/1000 | Loss: 7.4803 | 0.85 steps/s | ETA: 17.7m
  [ 10.0%] Step  100/1000 | Loss: 7.4803 | 0.85 steps/s | ETA: 17.7m
  Step 110/1000: Loss = 7.4730
  Step 120/1000: Loss = 7.4771
  Step 130/1000: Loss = 7.4728
  Step 140/1000: Loss = 7.4729
  Step 150/1000: Loss = 7.4730
  Step 160/1000: Loss = 7.4722
  Step 170/1000: Loss = 7.4748
  Step 180/1000: Loss = 7.4749
  Step 190/1000: Loss = 7.4736
  [ 20.0%] Step  200/1000 | Loss: 7.4726 | 0.86 steps/s | ETA: 15.5m
  [ 20.0%] Step  200/1000 | Loss: 7.4726 | 0.86 steps/s | ETA: 15.5m
  Step 210/1000: Loss = 7.4764
  Step 220/1000: Loss = 7.4752
  Step 230/1000: Loss = 7.4759
  Step 240/1000: Loss = 7.4752
  Step 250/1000: Loss = 7.4724
  Step 260/1000: Loss = 7.4720
  Step 270/1000: Loss = 7.4682
  Step 280/1000: Loss = 7.4679
  Step 290/1000: Loss = 7.4678
  [ 30.0%] Step  300/1000 | Loss: 7.4685 | 0.87 steps/s | ETA: 13.5m
  [ 30.0%] Step  300/1000 | Loss: 7.4685 | 0.87 steps/s | ETA: 13.5m
  Step 310/1000: Loss = 7.4653
  Step 320/1000: Loss = 7.4662
  Step 330/1000: Loss = 7.4611
  Step 340/1000: Loss = 7.4560
  Step 350/1000: Loss = 7.4524
  Step 360/1000: Loss = 7.4523
  Step 370/1000: Loss = 7.4527
  Step 380/1000: Loss = 7.4526
  Step 390/1000: Loss = 7.4552
  [ 40.0%] Step  400/1000 | Loss: 7.4531 | 0.87 steps/s | ETA: 11.5m
  [ 40.0%] Step  400/1000 | Loss: 7.4531 | 0.87 steps/s | ETA: 11.5m
  Step 410/1000: Loss = 7.4511
  Step 420/1000: Loss = 7.4472
  Step 430/1000: Loss = 7.4489
  Step 440/1000: Loss = 7.4494
  Step 450/1000: Loss = 7.4475
  Step 460/1000: Loss = 7.4467
  Step 470/1000: Loss = 7.4466
  Step 480/1000: Loss = 7.4484
  Step 490/1000: Loss = 7.4486
  [ 50.0%] Step  500/1000 | Loss: 7.4471 | 0.87 steps/s | ETA: 9.6m
  [ 50.0%] Step  500/1000 | Loss: 7.4471 | 0.87 steps/s | ETA: 9.6m
  Step 510/1000: Loss = 7.4476
  Step 520/1000: Loss = 7.4467
  Step 530/1000: Loss = 7.4452
  Step 540/1000: Loss = 7.4427
  Step 550/1000: Loss = 7.4438
  Step 560/1000: Loss = 7.4431
  Step 570/1000: Loss = 7.4424
  Step 580/1000: Loss = 7.4409
  Step 590/1000: Loss = 7.4418
  [ 60.0%] Step  600/1000 | Loss: 7.4398 | 0.87 steps/s | ETA: 7.7m
  [ 60.0%] Step  600/1000 | Loss: 7.4398 | 0.87 steps/s | ETA: 7.7m
  Step 610/1000: Loss = 7.4401
  Step 620/1000: Loss = 7.4401
  Step 630/1000: Loss = 7.4399
  Step 640/1000: Loss = 7.4391
  Step 650/1000: Loss = 7.4391
  Step 660/1000: Loss = 7.4380
  Step 670/1000: Loss = 7.4375
  Step 680/1000: Loss = 7.4353
  Step 690/1000: Loss = 7.4338
  [ 70.0%] Step  700/1000 | Loss: 7.4322 | 0.86 steps/s | ETA: 5.8m
  [ 70.0%] Step  700/1000 | Loss: 7.4322 | 0.86 steps/s | ETA: 5.8m
  Step 710/1000: Loss = 7.4314
  Step 720/1000: Loss = 7.4298
  Step 730/1000: Loss = 7.4299
  Step 740/1000: Loss = 7.4291
  Step 750/1000: Loss = 7.4280
  Step 760/1000: Loss = 7.4281
  Step 770/1000: Loss = 7.4279
  Step 780/1000: Loss = 7.4265
  Step 790/1000: Loss = 7.4264
  [ 80.0%] Step  800/1000 | Loss: 7.4262 | 0.86 steps/s | ETA: 3.9m
  [ 80.0%] Step  800/1000 | Loss: 7.4262 | 0.86 steps/s | ETA: 3.9m
  Step 810/1000: Loss = 7.4247
  Step 820/1000: Loss = 7.4233
  Step 830/1000: Loss = 7.4224
  Step 840/1000: Loss = 7.4215
  Step 850/1000: Loss = 7.4200
  Step 860/1000: Loss = 7.4197
  Step 870/1000: Loss = 7.4196
  Step 880/1000: Loss = 7.4193
  Step 890/1000: Loss = 7.4181
  [ 90.0%] Step  900/1000 | Loss: 7.4173 | 0.86 steps/s | ETA: 1.9m
  [ 90.0%] Step  900/1000 | Loss: 7.4173 | 0.86 steps/s | ETA: 1.9m
  Step 910/1000: Loss = 7.4176
  Step 920/1000: Loss = 7.4168
  Step 930/1000: Loss = 7.4154
  Step 940/1000: Loss = 7.4146
  Step 950/1000: Loss = 7.4137
  Step 960/1000: Loss = 7.4120
  Step 970/1000: Loss = 7.4102
  Step 980/1000: Loss = 7.4099
  Step 990/1000: Loss = 7.4086
  [100.0%] Step 1000/1000 | Loss: 7.4078 | 0.86 steps/s | ETA: 0.0m
  [100.0%] Step 1000/1000 | Loss: 7.4078 | 0.86 steps/s | ETA: 0.0m

================================================================================
Epoch 2/10 Complete | Time: 19.3m | Total: 19.3m
  Avg Loss: 7.4078 | CKA Score: 0.1606 | LR: 0.000046
  ETA for remaining 8 epochs: 154.7m
================================================================================

================================================================================
Epoch 2/10 Complete | Time: 19.3m | Total: 19.3m
  Avg Loss: 7.4078 | CKA Score: 0.1606 | LR: 0.000046
  ETA for remaining 8 epochs: 154.7m
================================================================================
  Checkpoint saved to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/linear_checkpoint/checkpoint.pt

================================================================================
Epoch 3/10
================================================================================

================================================================================
Epoch 3/10
================================================================================
  Step 10/1000: Loss = 7.2774
  Step 20/1000: Loss = 7.2873
  Step 30/1000: Loss = 7.3088
  Step 40/1000: Loss = 7.3447
  Step 50/1000: Loss = 7.3338
  Step 60/1000: Loss = 7.3367
  Step 70/1000: Loss = 7.3412
  Step 80/1000: Loss = 7.3387
  Step 90/1000: Loss = 7.3311
  [ 10.0%] Step  100/1000 | Loss: 7.3255 | 0.87 steps/s | ETA: 17.3m
  [ 10.0%] Step  100/1000 | Loss: 7.3255 | 0.87 steps/s | ETA: 17.3m
  Step 110/1000: Loss = 7.3107
  Step 120/1000: Loss = 7.3067
  Step 130/1000: Loss = 7.3049
  Step 140/1000: Loss = 7.3084
  Step 150/1000: Loss = 7.3091
  Step 160/1000: Loss = 7.3054
  Step 170/1000: Loss = 7.3044
  Step 180/1000: Loss = 7.3031
  Step 190/1000: Loss = 7.2984
  [ 20.0%] Step  200/1000 | Loss: 7.3010 | 0.86 steps/s | ETA: 15.4m
  [ 20.0%] Step  200/1000 | Loss: 7.3010 | 0.86 steps/s | ETA: 15.4m
  Step 210/1000: Loss = 7.2983
  Step 220/1000: Loss = 7.2984
  Step 230/1000: Loss = 7.3005
  Step 240/1000: Loss = 7.2981
  Step 250/1000: Loss = 7.2977
  Step 260/1000: Loss = 7.2922
  Step 270/1000: Loss = 7.2971
  Step 280/1000: Loss = 7.2991
  Step 290/1000: Loss = 7.2975
  [ 30.0%] Step  300/1000 | Loss: 7.2960 | 0.86 steps/s | ETA: 13.6m
  [ 30.0%] Step  300/1000 | Loss: 7.2960 | 0.86 steps/s | ETA: 13.6m
