================================================================================
LEARNED ADAPTER EXPERIMENT - AFFINE
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/affine_allgpus_20251031_181524.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DataParallel (multi-GPU)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Primary device: cuda:0
Batch size per GPU: 10
Total batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8900.38it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8900.38it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00, 13.54it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00, 13.54it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:00<00:00, 13.84it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:00<00:00, 13.84it/s]

Wrapped Llama model with DataParallel
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 7830.06it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 7830.06it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00, 13.37it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00, 13.37it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00, 14.28it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00, 14.28it/s]

Wrapped Mistral model with DataParallel

Training AffineAdapter...
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 10
Steps per epoch: 250
Total training steps: 2500
Batch size: 40
Gradient accumulation: 8 (effective batch: 320)
Learning rate: 5e-05
Alignment layers: [16]
================================================================================


================================================================================
Epoch 1/10
================================================================================

================================================================================
Epoch 1/10
================================================================================

================================================================================
AFFINE ADAPTER EXPERIMENT FAILED
================================================================================
Error: grad can be implicitly created only for scalar outputs
Traceback (most recent call last):
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1689, in run_adapter_experiment
    adapter, metrics = train_adapter(
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1689, in run_adapter_experiment
    adapter, metrics = train_adapter(
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1397, in train_adapter
    total_loss.backward()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1397, in train_adapter
    total_loss.backward()
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 340, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 340, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 198, in _make_grads
    raise RuntimeError(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 198, in _make_grads
    raise RuntimeError(
RuntimeError: grad can be implicitly created only for scalar outputs
RuntimeError: grad can be implicitly created only for scalar outputs
