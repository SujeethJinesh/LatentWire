================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251031_233049.log

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9172.89it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9172.89it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.83it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.83it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.65it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.65it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  3.12it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  3.12it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.03it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 6096.37it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 6096.37it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.26it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.26it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.77it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.77it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  3.03it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.86it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.86it/s]


Training LoRAAdapter...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================

  Step 10/250: Loss = 11.2717 (Gen: 11.0393, Contr: 2.3040, Align: 114.0773, Uniform: -0.9567)
  Step 20/250: Loss = 11.2157 (Gen: 10.9811, Contr: 2.3028, Align: 109.3769, Uniform: -0.9679)
  Step 30/250: Loss = 11.1801 (Gen: 10.9431, Contr: 2.3034, Align: 109.4745, Uniform: -0.9808)
  Step 40/250: Loss = 11.1850 (Gen: 10.9456, Contr: 2.3042, Align: 110.0077, Uniform: -0.9792)
  Step 50/250: Loss = 11.2136 (Gen: 10.9719, Contr: 2.3040, Align: 110.1380, Uniform: -0.9733)
  Step 60/250: Loss = 11.2265 (Gen: 10.9826, Contr: 2.3034, Align: 112.2131, Uniform: -0.9605)
  Step 70/250: Loss = 11.2093 (Gen: 10.9631, Contr: 2.3039, Align: 111.3985, Uniform: -0.9627)
  Step 80/250: Loss = 11.2016 (Gen: 10.9530, Contr: 2.3041, Align: 110.5421, Uniform: -0.9631)
  Step 90/250: Loss = 11.1855 (Gen: 10.9346, Contr: 2.3039, Align: 108.5109, Uniform: -0.9629)
  [ 40.0%] Step  100/250 | Loss: 11.1569 (Gen: 10.9037, Contr: 2.3038, Align: 106.6591, Uniform: -0.9640) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 11.1569 (Gen: 10.9037, Contr: 2.3038, Align: 106.6591, Uniform: -0.9640) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  Step 110/250: Loss = 11.1434 (Gen: 10.8879, Contr: 2.3038, Align: 108.4203, Uniform: -0.9622)
  Step 120/250: Loss = 11.1235 (Gen: 10.8657, Contr: 2.3042, Align: 111.3202, Uniform: -0.9619)
  Step 130/250: Loss = 11.1087 (Gen: 10.8486, Contr: 2.3041, Align: 109.5607, Uniform: -0.9590)
  Step 140/250: Loss = 11.0764 (Gen: 10.8140, Contr: 2.3041, Align: 108.5274, Uniform: -0.9597)
  Step 150/250: Loss = 11.0558 (Gen: 10.7911, Contr: 2.3041, Align: 110.0219, Uniform: -0.9602)
  Step 160/250: Loss = 11.0222 (Gen: 10.7552, Contr: 2.3041, Align: 108.8975, Uniform: -0.9614)
  Step 170/250: Loss = 10.9923 (Gen: 10.7229, Contr: 2.3041, Align: 109.2583, Uniform: -0.9600)
  Step 180/250: Loss = 10.9690 (Gen: 10.6974, Contr: 2.3040, Align: 108.0628, Uniform: -0.9593)
  Step 190/250: Loss = 10.9440 (Gen: 10.6700, Contr: 2.3040, Align: 107.9361, Uniform: -0.9590)
  [ 80.0%] Step  200/250 | Loss: 10.9189 (Gen: 10.6426, Contr: 2.3042, Align: 107.2572, Uniform: -0.9575) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 4.875 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.9189 (Gen: 10.6426, Contr: 2.3042, Align: 107.2572, Uniform: -0.9575) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 4.875 | 0.39 steps/s | ETA: 2.2m
