================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_000621.log

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9642.08it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9642.08it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.78it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.78it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  3.13it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  3.13it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.63it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.63it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.19it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.19it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 6364.65it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 6364.65it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  3.15it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  3.15it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.77it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.77it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.96it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.96it/s]


Training LoRAAdapter...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 5
Steps per epoch: 250
Total training steps: 1250
Batch size: 10
Gradient accumulation: 8 (effective batch: 80)
Learning rate: 5e-05
Alignment layers: [8, 16, 24]
================================================================================

  Step 10/250: Loss = 11.3310 (Gen: 11.0984, Contr: 2.3055, Align: 116.1653, Uniform: -0.9305)
  Step 20/250: Loss = 11.3300 (Gen: 11.0951, Contr: 2.3056, Align: 110.0815, Uniform: -0.9392)
  Step 30/250: Loss = 11.3800 (Gen: 11.1428, Contr: 2.3049, Align: 104.2688, Uniform: -0.9357)
  Step 40/250: Loss = 11.3489 (Gen: 11.1094, Contr: 2.3051, Align: 98.6260, Uniform: -0.9388)
  Step 50/250: Loss = 11.3170 (Gen: 11.0753, Contr: 2.3046, Align: 99.1274, Uniform: -0.9421)
  Step 60/250: Loss = 11.3003 (Gen: 11.0561, Contr: 2.3053, Align: 98.9706, Uniform: -0.9477)
  Step 70/250: Loss = 11.2923 (Gen: 11.0459, Contr: 2.3050, Align: 103.6357, Uniform: -0.9440)
  Step 80/250: Loss = 11.2734 (Gen: 11.0248, Contr: 2.3046, Align: 105.5535, Uniform: -0.9452)
  Step 90/250: Loss = 11.2521 (Gen: 11.0011, Contr: 2.3047, Align: 104.9734, Uniform: -0.9466)
  [ 40.0%] Step  100/250 | Loss: 11.2212 (Gen: 10.9679, Contr: 2.3043, Align: 103.4977, Uniform: -0.9477) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 11.2212 (Gen: 10.9679, Contr: 2.3043, Align: 103.4977, Uniform: -0.9477) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  Step 110/250: Loss = 11.2049 (Gen: 10.9493, Contr: 2.3042, Align: 102.9804, Uniform: -0.9456)
  Step 120/250: Loss = 11.1844 (Gen: 10.9266, Contr: 2.3041, Align: 103.1709, Uniform: -0.9458)
  Step 130/250: Loss = 11.1681 (Gen: 10.9079, Contr: 2.3040, Align: 103.9593, Uniform: -0.9421)
  Step 140/250: Loss = 11.1402 (Gen: 10.8777, Contr: 2.3040, Align: 104.4834, Uniform: -0.9456)
  Step 150/250: Loss = 11.1156 (Gen: 10.8509, Contr: 2.3042, Align: 105.2883, Uniform: -0.9461)
  Step 160/250: Loss = 11.0869 (Gen: 10.8199, Contr: 2.3042, Align: 105.2984, Uniform: -0.9471)
  Step 170/250: Loss = 11.0604 (Gen: 10.7911, Contr: 2.3042, Align: 106.2959, Uniform: -0.9466)
  Step 180/250: Loss = 11.0346 (Gen: 10.7629, Contr: 2.3040, Align: 105.9624, Uniform: -0.9475)
  Step 190/250: Loss = 11.0106 (Gen: 10.7366, Contr: 2.3038, Align: 105.1517, Uniform: -0.9482)
  [ 80.0%] Step  200/250 | Loss: 10.9806 (Gen: 10.7043, Contr: 2.3038, Align: 105.1334, Uniform: -0.9486) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 5.062 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.9806 (Gen: 10.7043, Contr: 2.3038, Align: 105.1334, Uniform: -0.9486) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 5.062 | 0.39 steps/s | ETA: 2.2m
  Step 210/250: Loss = 10.9547 (Gen: 10.6761, Contr: 2.3040, Align: 105.2185, Uniform: -0.9491)
  Step 220/250: Loss = 10.9266 (Gen: 10.6457, Contr: 2.3042, Align: 105.5596, Uniform: -0.9491)
  Step 230/250: Loss = 10.8979 (Gen: 10.6147, Contr: 2.3043, Align: 105.9722, Uniform: -0.9504)
  Step 240/250: Loss = 10.8685 (Gen: 10.5830, Contr: 2.3042, Align: 106.8492, Uniform: -0.9500)
  Step 250/250: Loss = 10.8413 (Gen: 10.5535, Contr: 2.3042, Align: 106.9645, Uniform: -0.9507)
  Step 10/250: Loss = 10.2548 (Gen: 9.9071, Contr: 2.3046, Align: 106.4452, Uniform: -0.9580)
  Step 20/250: Loss = 10.1642 (Gen: 9.8141, Contr: 2.3046, Align: 107.5269, Uniform: -0.9583)
  Step 30/250: Loss = 10.1619 (Gen: 9.8094, Contr: 2.3051, Align: 102.2845, Uniform: -0.9553)
  Step 40/250: Loss = 10.1433 (Gen: 9.7885, Contr: 2.3056, Align: 101.1034, Uniform: -0.9594)
  Step 50/250: Loss = 10.1313 (Gen: 9.7744, Contr: 2.3045, Align: 101.1868, Uniform: -0.9641)
  Step 60/250: Loss = 10.1088 (Gen: 9.7496, Contr: 2.3041, Align: 101.6828, Uniform: -0.9598)
  Step 70/250: Loss = 10.1042 (Gen: 9.7426, Contr: 2.3042, Align: 103.5096, Uniform: -0.9620)
  Step 80/250: Loss = 10.0984 (Gen: 9.7345, Contr: 2.3044, Align: 107.2001, Uniform: -0.9559)
  Step 90/250: Loss = 10.0814 (Gen: 9.7152, Contr: 2.3043, Align: 108.3514, Uniform: -0.9490)
  [ 40.0%] Step  100/250 | Loss: 10.0656 (Gen: 9.6972, Contr: 2.3041, Align: 107.0722, Uniform: -0.9479) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.4m
  [ 40.0%] Step  100/250 | Loss: 10.0656 (Gen: 9.6972, Contr: 2.3041, Align: 107.0722, Uniform: -0.9479) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.4m
