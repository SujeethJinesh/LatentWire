================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_gpu2_20251031_164111.log
GPU assigned: 2

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 772.97it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 772.97it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  3.77it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  3.77it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  3.95it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  3.95it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00,  4.00it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00,  4.00it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.88it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.88it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.90it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.90it/s]

Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 812.06it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 812.06it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.91it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.91it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.33it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  3.33it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.49it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.40it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00,  3.40it/s]


Training LoRAAdapter...
Loading dataset (10000 samples)...

Epoch 1/10
  Step 10/1250: Loss = 11.7358
  Step 20/1250: Loss = 11.5810
  Step 30/1250: Loss = 11.5437
  Step 40/1250: Loss = 11.5445
  Step 50/1250: Loss = 11.5331
  Step 60/1250: Loss = 11.5094
  Step 70/1250: Loss = 11.4723
  Step 80/1250: Loss = 11.4252
  Step 90/1250: Loss = 11.3801
  Step 100/1250: Loss = 11.3346
  Step 110/1250: Loss = 11.3037
  Step 120/1250: Loss = 11.2573
  Step 130/1250: Loss = 11.2155
  Step 140/1250: Loss = 11.1557
  Step 150/1250: Loss = 11.1068
  Step 160/1250: Loss = 11.0544
  Step 170/1250: Loss = 11.0146
  Step 180/1250: Loss = 10.9730
  Step 190/1250: Loss = 10.9328
  Step 200/1250: Loss = 10.8919
  Step 210/1250: Loss = 10.8535
  Step 220/1250: Loss = 10.8167
  Step 230/1250: Loss = 10.7854
  Step 240/1250: Loss = 10.7501
  Step 250/1250: Loss = 10.7198
  Step 260/1250: Loss = 10.6872
  Step 270/1250: Loss = 10.6555
  Step 280/1250: Loss = 10.6286
  Step 290/1250: Loss = 10.6022
  Step 300/1250: Loss = 10.5746
  Step 310/1250: Loss = 10.5514
  Step 320/1250: Loss = 10.5262
  Step 330/1250: Loss = 10.5016
  Step 340/1250: Loss = 10.4796
  Step 350/1250: Loss = 10.4564
  Step 360/1250: Loss = 10.4340
  Step 370/1250: Loss = 10.4088
  Step 380/1250: Loss = 10.3874
  Step 390/1250: Loss = 10.3651
  Step 400/1250: Loss = 10.3446
  Step 410/1250: Loss = 10.3281
  Step 420/1250: Loss = 10.3090
  Step 430/1250: Loss = 10.2909
  Step 440/1250: Loss = 10.2691
  Step 450/1250: Loss = 10.2488
  Step 460/1250: Loss = 10.2285
  Step 470/1250: Loss = 10.2110
  Step 480/1250: Loss = 10.1921
  Step 490/1250: Loss = 10.1748
  Step 500/1250: Loss = 10.1542
  Step 510/1250: Loss = 10.1356
  Step 520/1250: Loss = 10.1184
  Step 530/1250: Loss = 10.0994
  Step 540/1250: Loss = 10.0808
  Step 550/1250: Loss = 10.0617
  Step 560/1250: Loss = 10.0446
  Step 570/1250: Loss = 10.0278
  Step 580/1250: Loss = 10.0118
  Step 590/1250: Loss = 9.9950
  Step 600/1250: Loss = 9.9777
  Step 610/1250: Loss = 9.9624
  Step 620/1250: Loss = 9.9446
  Step 630/1250: Loss = 9.9282
  Step 640/1250: Loss = 9.9119
  Step 650/1250: Loss = 9.8960
  Step 660/1250: Loss = 9.8810
  Step 670/1250: Loss = 9.8655
  Step 680/1250: Loss = 9.8499
  Step 690/1250: Loss = 9.8335
  Step 700/1250: Loss = 9.8179
  Step 710/1250: Loss = 9.8040
  Step 720/1250: Loss = 9.7900
  Step 730/1250: Loss = 9.7757
  Step 740/1250: Loss = 9.7620
  Step 750/1250: Loss = 9.7474
  Step 760/1250: Loss = 9.7340
  Step 770/1250: Loss = 9.7209
  Step 780/1250: Loss = 9.7082
  Step 790/1250: Loss = 9.6958
  Step 800/1250: Loss = 9.6830
  Step 810/1250: Loss = 9.6694
  Step 820/1250: Loss = 9.6568
  Step 830/1250: Loss = 9.6441
  Step 840/1250: Loss = 9.6309
  Step 850/1250: Loss = 9.6182
  Step 860/1250: Loss = 9.6065
  Step 870/1250: Loss = 9.5942
  Step 880/1250: Loss = 9.5813
  Step 890/1250: Loss = 9.5692
  Step 900/1250: Loss = 9.5573
  Step 910/1250: Loss = 9.5453
  Step 920/1250: Loss = 9.5336
  Step 930/1250: Loss = 9.5228
  Step 940/1250: Loss = 9.5123
  Step 950/1250: Loss = 9.5008
  Step 960/1250: Loss = 9.4894
  Step 970/1250: Loss = 9.4787
  Step 980/1250: Loss = 9.4681
  Step 990/1250: Loss = 9.4573
  Step 1000/1250: Loss = 9.4468
  Step 1010/1250: Loss = 9.4362
  Step 1020/1250: Loss = 9.4264
  Step 1030/1250: Loss = 9.4160
  Step 1040/1250: Loss = 9.4057
  Step 1050/1250: Loss = 9.3963
  Step 1060/1250: Loss = 9.3870
  Step 1070/1250: Loss = 9.3770
  Step 1080/1250: Loss = 9.3671
  Step 1090/1250: Loss = 9.3576
  Step 1100/1250: Loss = 9.3478
  Step 1110/1250: Loss = 9.3386
  Step 1120/1250: Loss = 9.3295
  Step 1130/1250: Loss = 9.3204
  Step 1140/1250: Loss = 9.3119
  Step 1150/1250: Loss = 9.3028
  Step 1160/1250: Loss = 9.2936
  Step 1170/1250: Loss = 9.2838
  Step 1180/1250: Loss = 9.2749
  Step 1190/1250: Loss = 9.2662
  Step 1200/1250: Loss = 9.2578
  Step 1210/1250: Loss = 9.2492
  Step 1220/1250: Loss = 9.2409
  Step 1230/1250: Loss = 9.2327
  Step 1240/1250: Loss = 9.2251
  Step 1250/1250: Loss = 9.2174
  Epoch 1 avg loss: 9.2174, CKA: 0.0000
  Checkpoint saved to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_checkpoint/checkpoint.pt

Epoch 2/10
  Step 10/1250: Loss = 8.2666
  Step 20/1250: Loss = 8.2555
  Step 30/1250: Loss = 8.2249
  Step 40/1250: Loss = 8.2131
  Step 50/1250: Loss = 8.2176
  Step 60/1250: Loss = 8.2100
  Step 70/1250: Loss = 8.2087
  Step 80/1250: Loss = 8.2201
  Step 90/1250: Loss = 8.2138
  Step 100/1250: Loss = 8.2091
  Step 110/1250: Loss = 8.2076
  Step 120/1250: Loss = 8.2014
  Step 130/1250: Loss = 8.1935
  Step 140/1250: Loss = 8.1994
  Step 150/1250: Loss = 8.1940
  Step 160/1250: Loss = 8.1918
  Step 170/1250: Loss = 8.1917
  Step 180/1250: Loss = 8.1891
  Step 190/1250: Loss = 8.1870
  Step 200/1250: Loss = 8.1851
  Step 210/1250: Loss = 8.1786
  Step 220/1250: Loss = 8.1766
  Step 230/1250: Loss = 8.1732
  Step 240/1250: Loss = 8.1699
  Step 250/1250: Loss = 8.1686
  Step 260/1250: Loss = 8.1668
  Step 270/1250: Loss = 8.1637
  Step 280/1250: Loss = 8.1600
  Step 290/1250: Loss = 8.1594
  Step 300/1250: Loss = 8.1596
  Step 310/1250: Loss = 8.1580
  Step 320/1250: Loss = 8.1562
  Step 330/1250: Loss = 8.1558
  Step 340/1250: Loss = 8.1560
  Step 350/1250: Loss = 8.1538
  Step 360/1250: Loss = 8.1516
  Step 370/1250: Loss = 8.1511
  Step 380/1250: Loss = 8.1482
  Step 390/1250: Loss = 8.1473
  Step 400/1250: Loss = 8.1454
  Step 410/1250: Loss = 8.1452
  Step 420/1250: Loss = 8.1424
  Step 430/1250: Loss = 8.1414
  Step 440/1250: Loss = 8.1399
  Step 450/1250: Loss = 8.1390
  Step 460/1250: Loss = 8.1362
  Step 470/1250: Loss = 8.1352
  Step 480/1250: Loss = 8.1324
  Step 490/1250: Loss = 8.1313
  Step 500/1250: Loss = 8.1288
  Step 510/1250: Loss = 8.1256
  Step 520/1250: Loss = 8.1261
  Step 530/1250: Loss = 8.1237
  Step 540/1250: Loss = 8.1235
  Step 550/1250: Loss = 8.1218
  Step 560/1250: Loss = 8.1198
  Step 570/1250: Loss = 8.1185
  Step 580/1250: Loss = 8.1159
  Step 590/1250: Loss = 8.1158
  Step 600/1250: Loss = 8.1141
  Step 610/1250: Loss = 8.1141
  Step 620/1250: Loss = 8.1128
  Step 630/1250: Loss = 8.1124
  Step 640/1250: Loss = 8.1124
  Step 650/1250: Loss = 8.1124
  Step 660/1250: Loss = 8.1107
  Step 670/1250: Loss = 8.1103
