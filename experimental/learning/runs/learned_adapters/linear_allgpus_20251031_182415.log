================================================================================
LEARNED ADAPTER EXPERIMENT - LINEAR
================================================================================
Platform: hpc
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/linear_allgpus_20251031_182415.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DataParallel (multi-GPU)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Primary device: cuda:0
Batch size per GPU: 10
Total batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 4110.05it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 4110.05it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00, 13.55it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:00<00:00, 13.55it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:00<00:00, 13.83it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:00<00:00, 13.83it/s]

Wrapped Llama model with DataParallel
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 3981.93it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 3981.93it/s]

Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00, 13.45it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00, 13.45it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00, 14.36it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:00<00:00, 14.36it/s]

Wrapped Mistral model with DataParallel

Training LinearAdapter...
Loading dataset (10000 samples)...

================================================================================
TRAINING CONFIGURATION
================================================================================
Total epochs: 10
Steps per epoch: 250
Total training steps: 2500
Batch size: 40
Gradient accumulation: 8 (effective batch: 320)
Learning rate: 5e-05
Alignment layers: [16]
================================================================================


================================================================================
Epoch 1/10
================================================================================

================================================================================
Epoch 1/10
================================================================================
/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(

================================================================================
LINEAR ADAPTER EXPERIMENT FAILED
================================================================================
Error: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.00 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 75.69 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1703, in run_adapter_experiment
    adapter, metrics = train_adapter(
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1703, in run_adapter_experiment
    adapter, metrics = train_adapter(
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1411, in train_adapter
    total_loss.backward()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1411, in train_adapter
    total_loss.backward()
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 33, in backward
    return (None,) + ReduceAddCoalesced.apply(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 33, in backward
    return (None,) + ReduceAddCoalesced.apply(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 46, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 46, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 159, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 159, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 107, in reduce_add
    result = torch.empty_like(inputs[root_index])
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 107, in reduce_add
    result = torch.empty_like(inputs[root_index])
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.00 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 75.69 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 77.00 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 75.69 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
