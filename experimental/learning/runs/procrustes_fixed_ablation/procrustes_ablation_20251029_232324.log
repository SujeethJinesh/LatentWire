/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
================================================================================
PROCRUSTES FIXED ABLATION - Layer Selection Study
================================================================================
Layers to test: [0, 8, 16, 24, 32]
Test prompts: 5
Device: cuda:0

Loading models...
  Llama: meta-llama/Llama-3.1-8B
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1690.74it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:14<00:43, 14.38s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:27<00:27, 13.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:41<00:13, 13.52s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00,  9.65s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:44<00:00, 11.19s/it]
  ✓ Llama loaded
  Mistral: mistralai/Mistral-7B-v0.3
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [00:23<00:46, 23.11s/it]Downloading shards:  67%|██████▋   | 2/3 [00:46<00:23, 23.02s/it]Downloading shards: 100%|██████████| 3/3 [01:06<00:00, 21.78s/it]Downloading shards: 100%|██████████| 3/3 [01:06<00:00, 22.12s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:16<00:32, 16.39s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:32<00:16, 16.16s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:46<00:00, 15.41s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:46<00:00, 15.64s/it]
Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/procrustes_fixed_ablation.py", line 486, in <module>
    run_layer_ablation()
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/procrustes_fixed_ablation.py", line 344, in run_layer_ablation
    mistral_tokenizer = AutoTokenizer.from_pretrained(MISTRAL_MODEL)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 920, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2213, in from_pretrained
    return cls._from_pretrained(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2447, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py", line 107, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.
