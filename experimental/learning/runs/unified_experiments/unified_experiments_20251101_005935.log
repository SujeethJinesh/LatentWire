Using DDP (DistributedDataParallel) with torchrun for 4 GPUs

W1101 00:59:36.834000 30668 torch/distributed/run.py:793] 
W1101 00:59:36.834000 30668 torch/distributed/run.py:793] *****************************************
W1101 00:59:36.834000 30668 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1101 00:59:36.834000 30668 torch/distributed/run.py:793] *****************************************
==> Using CUDA on HPC (4 GPUs available)
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
================================================================================
UNIFIED CROSS-MODEL ALIGNMENT EXPERIMENTS
Timestamp: 2025-11-01T00:59:47.012477
Platform: hpc
Device: cuda
Available CUDA GPUs: 4
================================================================================

1. Starting Procrustes experiment on cuda...

================================================================================
PROCRUSTES ALIGNMENT EXPERIMENT (GPU-ACCELERATED)
================================================================================
Device: cuda (Procrustes on hpc)

Loading models on hpc...
Using bfloat16 for H100
Loading models on separate GPUs to avoid OOM...
Using Flash Attention 2
==> Using CUDA on HPC (4 GPUs available)
==> Using CUDA on HPC (4 GPUs available)
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
================================================================================
UNIFIED CROSS-MODEL ALIGNMENT EXPERIMENTS
Timestamp: 2025-11-01T00:59:47.063207
Platform: hpc
Device: cuda
Available CUDA GPUs: 4
================================================================================

1. Starting Procrustes experiment on cuda...

================================================================================
PROCRUSTES ALIGNMENT EXPERIMENT (GPU-ACCELERATED)
================================================================================
Device: cuda (Procrustes on hpc)

Loading models on hpc...
Using bfloat16 for H100
Loading models on separate GPUs to avoid OOM...
Using Flash Attention 2
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
================================================================================
UNIFIED CROSS-MODEL ALIGNMENT EXPERIMENTS
Timestamp: 2025-11-01T00:59:47.075111
Platform: hpc
Device: cuda
Available CUDA GPUs: 4
================================================================================
==> Using CUDA on HPC (4 GPUs available)

1. Starting Procrustes experiment on cuda...

================================================================================
PROCRUSTES ALIGNMENT EXPERIMENT (GPU-ACCELERATED)
================================================================================
Device: cuda (Procrustes on hpc)

Loading models on hpc...
Using bfloat16 for H100
Loading models on separate GPUs to avoid OOM...
Using Flash Attention 2
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
================================================================================
UNIFIED CROSS-MODEL ALIGNMENT EXPERIMENTS
Timestamp: 2025-11-01T00:59:47.091117
Platform: hpc
Device: cuda
Available CUDA GPUs: 4
================================================================================

1. Starting Procrustes experiment on cuda...

================================================================================
PROCRUSTES ALIGNMENT EXPERIMENT (GPU-ACCELERATED)
================================================================================
Device: cuda (Procrustes on hpc)

Loading models on hpc...
Using bfloat16 for H100
Loading models on separate GPUs to avoid OOM...
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 1189.79it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9162.87it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8285.04it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 9182.93it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.16it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.05it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.07it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  2.07it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.79it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.79it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.78it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:01<00:01,  1.73it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.68it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.63it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.99it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.89it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.97it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.87it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.93it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]
Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.92it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.86it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.10s/it]Downloading shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.11s/it]Downloading shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.11s/it]Downloading shards:  67%|██████▋   | 2/3 [00:20<00:10, 10.13s/it]Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 13.27s/it]Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 12.64s/it]
Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 13.27s/it]Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 12.64s/it]
Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 13.28s/it]Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 12.65s/it]
Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 13.28s/it]Downloading shards: 100%|██████████| 3/3 [00:37<00:00, 12.65s/it]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.09s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.10s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.08s/it]Loading checkpoint shards:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.12it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.06it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.05it/s]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:01<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.02it/s]
Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.01s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.02s/it]
Llama model moved to cuda:0Llama model moved to cuda:0Llama model moved to cuda:0
Mistral model moved to cuda:1

Mistral model moved to cuda:1

Mistral model moved to cuda:1
Llama model moved to cuda:0
Mistral model moved to cuda:1

Loading calibration dataset (50 samples)...
Loading calibration dataset (50 samples)...
Loading calibration dataset (50 samples)...



Loading calibration dataset (50 samples)...

============================================================
Testing Layer 0
============================================================

============================================================
Testing Layer 0
============================================================

============================================================
Testing Layer 0
============================================================

============================================================
Testing Layer 0
============================================================
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  WARNING: Orthogonality error = 0.091007
  WARNING: Orthogonality error = 0.091007  WARNING: Orthogonality error = 0.091007  WARNING: Orthogonality error = 0.091007


  WARNING: Orthogonality error = 0.088094
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.5536
    After alignment:  0.6487
    Improvement:      +0.0951
  CKA Llama→Mistral after alignment: 0.5172
  WARNING: Orthogonality error = 0.088094
  Computing CKA similarity scores...
  WARNING: Orthogonality error = 0.088094
  Computing CKA similarity scores...
  WARNING: Orthogonality error = 0.088094
  Computing CKA similarity scores...
  CKA Mistral→Llama:  CKA Mistral→Llama:
    Before alignment: 0.5536
    After alignment:  0.6487
    Improvement:      +0.0951
  CKA Llama→Mistral after alignment: 0.5172

    Before alignment: 0.5536
    After alignment:  0.6487
    Improvement:      +0.0951
  CKA Llama→Mistral after alignment: 0.5172
  CKA Mistral→Llama:
    Before alignment: 0.5536
    After alignment:  0.6487
    Improvement:      +0.0951
  CKA Llama→Mistral after alignment: 0.5172
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_0.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_0.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_0.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_0.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec      Generated: The capital of France is Paris, but the capital of the French language is Quebec      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...

    Testing Llama→Llama (baseline)...
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...

    Testing Llama→Llama (baseline)...
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/users/sujinesh/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1.2656      Transformation norm: 1.2656
      Transformation norm: 1.2656      Transformation norm: 1.2656


The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Generated: 

## Answer

The given expression is a quadratic equation in the form $ax^      Generated: 

## Answer

The given expression is a quadratic equation in the form $ax^
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

## Answer

The given expression is a quadratic equation in the form $ax^      Generated: 

## Answer

The given expression is a quadratic equation in the form $ax^
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.4062
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4062
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4062
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4062
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The 2019–20 UEFA Champions League was the 65th season of Europe's premier      Generated: 
The 2019–20 UEFA Champions League was the 65th season of Europe's premier
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The 2019–20 UEFA Champions League was the 65th season of Europe's premier
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The 2019–20 UEFA Champions League was the 65th season of Europe's premier
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1.5781
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.5781
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.5781      Transformation norm: 1.5781
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 

\begin{verbatim}
\documentclass{article}
\usepackage
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

\begin{verbatim}
\documentclass{article}
\usepackage
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

\begin{verbatim}
\documentclass{article}
\usepackage
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

\begin{verbatim}
\documentclass{article}
\usepackage
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.7031
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.7031
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.7031
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.7031
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Basilica of St. John the Baptist, also known as the Basilica of San Juan
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Basilica of St. John the Baptist, also known as the Basilica of San Juan      Generated: 
The Basilica of St. John the Baptist, also known as the Basilica of San Juan
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Basilica of St. John the Baptist, also known as the Basilica of San Juan
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1.4609      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4609
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

    Testing Llama→Mistral (cross-model via Procrustes)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4609
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4609
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 

Comment: I'm not sure what you're asking. If you're asking      Generated: 

Comment: I'm not sure what you're asking. If you're asking
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

Comment: I'm not sure what you're asking. If you're asking
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

Comment: I'm not sure what you're asking. If you're asking
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.6094      Transformation norm: 1.6094
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.6094
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.6094
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The first thing that comes to mind when thinking of the word "sacred" is a
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The first thing that comes to mind when thinking of the word "sacred" is a
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The first thing that comes to mind when thinking of the word "sacred" is a
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The first thing that comes to mind when thinking of the word "sacred" is a
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Transformation norm: 1.4141
    Testing Llama→Mistral (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4141
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Transformation norm: 1.4141

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
    Testing Llama→Mistral (cross-model via Procrustes)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.4141
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: {

    public static void main(String[] args) {
        int[] arr =      Generated: {

    public static void main(String[] args) {
        int[] arr =
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: {

    public static void main(String[] args) {
        int[] arr =
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.5156      Transformation norm: 1.5156
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: {

    public static void main(String[] args) {
        int[] arr =
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.5156
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.5156
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The following is a list of the most common types of meditation practices:
1. Mi      Generated: 
The following is a list of the most common types of meditation practices:
1. Mi
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The following is a list of the most common types of meditation practices:
1. Mi
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The following is a list of the most common types of meditation practices:
1. Mi
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1.6953      Transformation norm: 1.6953
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.6953      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.6953
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 

\begin{verbatim}
\begin{itemize}
\item \textbf
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

\begin{verbatim}
\begin{itemize}
\item \textbf
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 

\begin{verbatim}
\begin{itemize}
\item \textbf      Transformation norm: 1.8438
    Testing Mistral→Llama (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 1.8438Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 

\begin{verbatim}
\begin{itemize}
\item \textbf
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1.8438
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1.8438
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The phrase "I'd rather be a failure with dignity than a success without it" is       Generated: 
The phrase "I'd rather be a failure with dignity than a success without it" is 

============================================================
Testing Layer 8
============================================================


============================================================
Testing Layer 8
============================================================
      Generated: 
The phrase "I'd rather be a failure with dignity than a success without it" is 

============================================================
Testing Layer 8
============================================================
      Generated: 
The phrase "I'd rather be a failure with dignity than a success without it" is 

============================================================
Testing Layer 8
============================================================
  Moving 570 samples to GPU 0 for fast SVD  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...


Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  WARNING: Orthogonality error = 0.033122
  WARNING: Orthogonality error = 0.033122
  WARNING: Orthogonality error = 0.033122
  WARNING: Orthogonality error = 0.033122
  WARNING: Orthogonality error = 0.033111  WARNING: Orthogonality error = 0.033111
  Computing CKA similarity scores...
  CKA Mistral→Llama:
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9999
    After alignment:  0.9999
    Improvement:      +0.0000
  CKA Llama→Mistral after alignment: 0.9999

    Before alignment: 0.9999
    After alignment:  0.9999
    Improvement:      +0.0000
  CKA Llama→Mistral after alignment: 0.9999
  WARNING: Orthogonality error = 0.033111
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9999
    After alignment:  0.9999
    Improvement:      +0.0000
  CKA Llama→Mistral after alignment: 0.9999
  WARNING: Orthogonality error = 0.033111
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9999
    After alignment:  0.9999
    Improvement:      +0.0000
  CKA Llama→Mistral after alignment: 0.9999
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_8.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_8.pt

Testing generation...  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_8.pt  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_8.pt
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.


Testing generation...

Testing generation...

  Prompt 1/5: The capital of France is...  Prompt 1/5: The capital of France is...

    Testing Mistral→Mistral (baseline)...    Testing Mistral→Mistral (baseline)...

Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 's work is a reflection of the world around us. His buildings are not just struc
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 's work is a reflection of the world around us. His buildings are not just struc      Generated: 's work is a reflection of the world around us. His buildings are not just struc
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 's work is a reflection of the world around us. His buildings are not just struc
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Church
The Arch
The
The
The
The
The
The
The


  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Church
The Arch
The
The
The
The
The
The
The

      Generated:  Church
The Arch
The
The
The
The
The
The
The


  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Church
The Arch
The
The
The
The
The
The
The


  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Transformation norm: 572.0000
    Testing Llama→Mistral (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: , Qub 1 The architectural style of the Church of the Holy Sepulchre in      Generated: , Qub 1 The architectural style of the Church of the Holy Sepulchre in
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: , Qub 1 The architectural style of the Church of the Holy Sepulchre in
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: , Qub 1 The architectural style of the Church of the Holy Sepulchre in
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The church of the Churchof theChurchassistantassistantassistant
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The church of the Churchof theChurchassistantassistantassistant
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The church of the Churchof theChurchassistantassistantassistant
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The church of the Churchof theChurchassistantassistantassistant
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Architecture of the Church of St. John the Baptist in the village of Kras
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 
The Architecture of the Church of St. John the Baptist in the village of Kras      Generated: 
The Architecture of the Church of St. John the Baptist in the village of Kras
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Architecture of the Church of St. John the Baptist in the village of Kras      Transformation norm: 572.0000
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

      Generated: In the year 2050, the world is a vastly different place. Climate change has ravaThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: , the 19th-century neo-Gothic church in the village of H      Generated: , the 19th-century neo-Gothic church in the village of H
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: , the 19th-century neo-Gothic church in the village of H
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: , the 19th-century neo-Gothic church in the village of H
    Testing Mistral→Llama (cross-model via Procrustes)...

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the Architectural
##Arch
##Arch
##Arch
##Arch
##Arch
      Generated:  the Architectural
##Arch
##Arch
##Arch
##Arch
##Arch

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the Architectural
##Arch
##Arch
##Arch
##Arch
##Arch

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the Architectural
##Arch
##Arch
##Arch
##Arch
##Arch

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
The Architectural School of the University of Navarra is a private institution       Generated: 
The Architectural School of the University of Navarra is a private institution 
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: 
The Architectural School of the University of Navarra is a private institution 
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Generated: 
The Architectural School of the University of Navarra is a private institution 
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  of the Holy Trinity Church of the Holy Trinity Church of theHoly TrinityChurch 

============================================================      Generated:  of the Holy Trinity Church of the Holy Trinity Church of theHoly TrinityChurch 
Testing Layer 16
============================================================


============================================================
Testing Layer 16
============================================================
      Generated:  of the Holy Trinity Church of the Holy Trinity Church of theHoly TrinityChurch 

============================================================
Testing Layer 16
============================================================
      Generated:  of the Holy Trinity Church of the Holy Trinity Church of theHoly TrinityChurch 

============================================================
Testing Layer 16
============================================================
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  WARNING: Orthogonality error = 0.033381
  WARNING: Orthogonality error = 0.033381
  WARNING: Orthogonality error = 0.033381
  WARNING: Orthogonality error = 0.033381
  WARNING: Orthogonality error = 0.033482
  Computing CKA similarity scores...
  CKA Mistral→Llama:  WARNING: Orthogonality error = 0.033482
    Before alignment: 0.9970
    After alignment:  0.9994
    Improvement:      +0.0024
  CKA Llama→Mistral after alignment: 0.9994
  WARNING: Orthogonality error = 0.033482
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9970
    After alignment:  0.9994
    Improvement:      +0.0024
  CKA Llama→Mistral after alignment: 0.9994

  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9970
    After alignment:  0.9994
    Improvement:      +0.0024
  CKA Llama→Mistral after alignment: 0.9994
  WARNING: Orthogonality error = 0.033482
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9970
    After alignment:  0.9994
    Improvement:      +0.0024
  CKA Llama→Mistral after alignment: 0.9994
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_16.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_16.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_16.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_16.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 576.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the Church of St. Mary the Virgin, Oxford, England, is a masterpiece of the
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: of the Church of St. Mary the Virgin, Oxford, England, is a masterpiece of the      Generated: of the Church of St. Mary the Virgin, Oxford, England, is a masterpiece of the
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the Church of St. Mary the Virgin, Oxford, England, is a masterpiece of the
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  School
Archassistantassistantassistant
Arch
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  School
Archassistantassistantassistant
Arch      Generated:  School
Archassistantassistantassistant
Arch
  Prompt 2/5: To solve this problem, we need to...
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...

    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  School
Archassistantassistantassistant
Arch
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the church are of the same style as the church of St. Mary Magdalene, at
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: of the church are of the same style as the church of St. Mary Magdalene, at      Generated: of the church are of the same style as the church of St. Mary Magdalene, at
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the church are of the same style as the church of St. Mary Magdalene, at
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  are designed để نمای
The architectural The architectural
The architectural
The

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  are designed để نمای
The architectural The architectural
The architectural
The
      Generated:  are designed để نمای
The architectural The architectural
The architectural
The

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  are designed để نمای
The architectural The architectural
The architectural
The

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 576.0000      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the church is a masterpiece of the Gothic style. The interior is equally impr
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: of the church is a masterpiece of the Gothic style. The interior is equally impr
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: of the church is a masterpiece of the Gothic style. The interior is equally impr
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the church is a masterpiece of the Gothic style. The interior is equally impr
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: 
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
Arch
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the Church of St. Mary the Virgin, in the Parish of St. Mary-le      Generated: of the Church of St. Mary the Virgin, in the Parish of St. Mary-le
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: of the Church of St. Mary the Virgin, in the Parish of St. Mary-le
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 572.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: of the Church of St. Mary the Virgin, in the Parish of St. Mary-le
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  of the Cathedral of the Cathedral of the Cathedral of the Cathedral of the Cath
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  of the Cathedral of the Cathedral of the Cathedral of the Cathedral of the Cath
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  of the Cathedral of the Cathedral of the Cathedral of the Cathedral of the Cath
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  of the Cathedral of the Cathedral of the Cathedral of the Cathedral of the Cath
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 576.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: is a very important part of the building. It is the first thing that people see 
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: is a very important part of the building. It is the first thing that people see 
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: is a very important part of the building. It is the first thing that people see 
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 572.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: is a very important part of the building. It is the first thing that people see 
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 572.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: s
Architectural Assistantassistantassistantassistant      Generated: s
Architectural Assistantassistantassistantassistant

============================================================
Testing Layer 24
============================================================


============================================================
Testing Layer 24
============================================================
      Generated: s
Architectural Assistantassistantassistantassistant

============================================================
Testing Layer 24
============================================================
      Generated: s
Architectural Assistantassistantassistantassistant

============================================================
Testing Layer 24
============================================================
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  WARNING: Orthogonality error = 0.035329
  WARNING: Orthogonality error = 0.035329
  WARNING: Orthogonality error = 0.035329
  WARNING: Orthogonality error = 0.035329
  WARNING: Orthogonality error = 0.035301
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9660
    After alignment:  0.9894
    Improvement:      +0.0234
  CKA Llama→Mistral after alignment: 0.9871
  WARNING: Orthogonality error = 0.035301
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9660
    After alignment:  0.9894
    Improvement:      +0.0234
  CKA Llama→Mistral after alignment: 0.9871
  WARNING: Orthogonality error = 0.035301
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9660
    After alignment:  0.9894
    Improvement:      +0.0234
  CKA Llama→Mistral after alignment: 0.9871
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_24.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_24.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  WARNING: Orthogonality error = 0.035301
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.9660
    After alignment:  0.9894
    Improvement:      +0.0234
  CKA Llama→Mistral after alignment: 0.9871
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_24.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_24.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: St. Quiberon is a small church in the town of Quiberon, in      Generated: St. Quiberon is a small church in the town of Quiberon, in
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 532.0000      Transformation norm: 532.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: St. Quiberon is a small church in the town of Quiberon, in
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 532.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: St. Quiberon is a small church in the town of Quiberon, in
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 532.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  is a small, unassuming, a small, a small, a small, a small, a      Generated:  is a small, unassuming, a small, a small, a small, a small, a

  Prompt 2/5: To solve this problem, we need to...  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...

    Testing Mistral→Mistral (baseline)...Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  is a small, unassuming, a small, a small, a small, a small, a
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  is a small, unassuming, a small, a small, a small, a small, a
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Francis of Assisi, the church of St. Mary Major, and the      Generated: the church of St. Francis of Assisi, the church of St. Mary Major, and the
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 540.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Francis of Assisi, the church of St. Mary Major, and the
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Francis of Assisi, the church of St. Mary Major, and the
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  building is a, and the building, and the building, and the building, and the bu      Generated:  building is a, and the building, and the building, and the building, and the bu
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  building is a, and the building, and the building, and the building, and the bu
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  building is a, and the building, and the building, and the building, and the bu
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the Church of St. Mary of the Angels, which is the oldest church in the city.
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: the Church of St. Mary of the Angels, which is the oldest church in the city.
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the Church of St. Mary of the Angels, which is the oldest church in the city.
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the Church of St. Mary of the Angels, which is the oldest church in the city.
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  in the 19th century in the 19th century in the 19th century in the
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  in the 19th century in the 19th century in the 19th century in the
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  in the 19th century in the 19th century in the 19th century in the
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  in the 19th century in the 19th century in the 19th century in the
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 592.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Mary Magdalene, which is a Grade I listed building.

      Generated: the church of St. Mary Magdalene, which is a Grade I listed building.


    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 540.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Mary Magdalene, which is a Grade I listed building.


    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the church of St. Mary Magdalene, which is a Grade I listed building.


    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 540.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the building is a building, the building, the building, the building, the build      Generated:  the building is a building, the building, the building, the building, the build
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the building is a building, the building, the building, the building, the build
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  the building is a building, the building, the building, the building, the build
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 592.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the, Question  1: What is the name of the building in which the United States Co
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: the, Question  1: What is the name of the building in which the United States Co
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the, Question  1: What is the name of the building in which the United States Co
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the, Question  1: What is the name of the building in which the United States Co
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 536.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  building's the building's the building's the building's the building's the buil      Generated:  building's the building's the building's the building's the building's the buil

============================================================
Testing Layer 32
============================================================


============================================================
Testing Layer 32
============================================================
      Generated:  building's the building's the building's the building's the building's the buil

============================================================
Testing Layer 32
============================================================
      Generated:  building's the building's the building's the building's the building's the buil

============================================================
Testing Layer 32
============================================================
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  Moving 570 samples to GPU 0 for fast SVD

Fitting Procrustes alignments on cuda...
  WARNING: Orthogonality error = 0.098240
  WARNING: Orthogonality error = 0.098240
  WARNING: Orthogonality error = 0.098240
  WARNING: Orthogonality error = 0.098240
  WARNING: Orthogonality error = 0.096755
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.8480
    After alignment:  0.9275
    Improvement:      +0.0795
  CKA Llama→Mistral after alignment: 0.8867
  WARNING: Orthogonality error = 0.096755
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.8480
    After alignment:  0.9275
    Improvement:      +0.0795
  CKA Llama→Mistral after alignment: 0.8867
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_32.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_32.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  WARNING: Orthogonality error = 0.096755
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.8480
    After alignment:  0.9275
    Improvement:      +0.0795
  CKA Llama→Mistral after alignment: 0.8867
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_32.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  WARNING: Orthogonality error = 0.096755
  Computing CKA similarity scores...
  CKA Mistral→Llama:
    Before alignment: 0.8480
    After alignment:  0.9275
    Improvement:      +0.0795
  CKA Llama→Mistral after alignment: 0.8867
  Saved Procrustes alignment to /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/procrustes_alignments/layer_32.pt

Testing generation...
  Prompt 1/5: The capital of France is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is       Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1020.0000      Transformation norm: 1020.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is Paris, but the capital of the French language is Quebec
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1020.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The capital of France is a city of romance, art, fashion, and cuisine. Paris is 
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1020.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...

    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 924.0000      Transformation norm: 924.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 924.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  University cathedral cad neoutr, cadicipa, cadicipa, cadicipa, cadicip      Generated:  University cathedral cad neoutr, cadicipa, cadicipa, cadicipa, cadicip
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 924.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  University cathedral cad neoutr, cadicipa, cadicipa, cadicipa, cadicip
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  University cathedral cad neoutr, cadicipa, cadicipa, cadicipa, cadicip
  Prompt 2/5: To solve this problem, we need to...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1264.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
      Transformation norm: 1264.0000Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to understand the concept of a "unique" identifie
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1264.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: To solve this problem, we need to find the value of the expression $\frac{1}{1 \
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1264.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: and bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: and bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1184.0000      Transformation norm: 1184.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: and bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1184.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Broad [*], *a**[*. Asa, "a**[s] _[*      Generated:  Broad [*], *a**[*. Asa, "a**[s] _[*
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: and bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1184.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Broad [*], *a**[*. Asa, "a**[s] _[*
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Broad [*], *a**[*. Asa, "a**[s] _[*
  Prompt 3/5: The future of artificial intelligence is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1136.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1136.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is a topic that has been the subject of mu
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1136.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The future of artificial intelligence is bright, but it also raises concerns abo
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1136.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1040.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1040.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1040.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Greg, the first, the most, the most, the most, the most, the most,
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Greg, the first, the most, the most, the most, the most, the most,
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1040.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Greg, the first, the most, the most, the most, the most, the most,
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Greg, the first, the most, the most, the most, the most, the most,
  Prompt 4/5: In the year 2050,...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...

    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1128.0000      Transformation norm: 1128.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a very different place. The United States has bee
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1128.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: In the year 2050, the world is a vastly different place. Climate change has rava
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1128.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1104.0000      Transformation norm: 1104.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1104.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  skys and business, the first, the three, the five, the six, the seven, the
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  skys and business, the first, the three, the five, the six, the seven, the
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: the bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan beka
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1104.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  skys and business, the first, the three, the five, the six, the seven, the
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  skys and business, the first, the three, the five, the six, the seven, the
  Prompt 5/5: The main difference between cats and dogs is...
    Testing Mistral→Mistral (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.

    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1256.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1256.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is that cats are carnivores and dogs a
    Testing Llama→Llama (baseline)...
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1256.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: The main difference between cats and dogs is their behavior, lifestyle, and need
    Testing Llama→Mistral (cross-model via Procrustes)...
      Transformation norm: 1256.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: unique bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan b
    Testing Mistral→Llama (cross-model via Procrustes)...
      Generated: unique bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan b
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1152.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Transformation norm: 1152.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated: unique bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan b
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1152.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Statue, 30,000, a few, a few, a few, a few, a

================================================================================
PROCRUSTES EXPERIMENT SUMMARY
================================================================================

Layer 0:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 8:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 16:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 24:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 32:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed
================================================================================

Cleaning up Procrustes experiment...
      Generated:  Statue, 30,000, a few, a few, a few, a few, a

================================================================================
PROCRUSTES EXPERIMENT SUMMARY
================================================================================

Layer 0:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 8:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 16:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 24:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 32:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed
================================================================================

Cleaning up Procrustes experiment...
      Generated: unique bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan bekan b
    Testing Mistral→Llama (cross-model via Procrustes)...
      Transformation norm: 1152.0000
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
      Generated:  Statue, 30,000, a few, a few, a few, a few, a

================================================================================
PROCRUSTES EXPERIMENT SUMMARY
================================================================================

Layer 0:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 8:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 16:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 24:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 32:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed
================================================================================

Cleaning up Procrustes experiment...
  Models deleted and GPU memory cleared
  Models deleted and GPU memory cleared
      Generated:  Statue, 30,000, a few, a few, a few, a few, a

================================================================================
PROCRUSTES EXPERIMENT SUMMARY
================================================================================

Layer 0:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 8:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 16:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 24:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed

Layer 32:
  Baselines:
    Mistral→Mistral: 5/5 succeeded
    Llama→Llama: 5/5 succeeded
  Cross-model:
    Llama→Mistral: 5/5 succeeded, 0/5 failed
    Mistral→Llama: 5/5 succeeded, 0/5 failed
================================================================================

Cleaning up Procrustes experiment...
Procrustes results saved to: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/unified_experiments/procrustes_results_20251101_005947.json

Cleaning up GPU memory...
Procrustes results saved to: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/unified_experiments/procrustes_results_20251101_005947.json

Cleaning up GPU memory...
  GPU cache cleared

2. Starting all experiments sequentially (PRIORITY ORDER)...
Strategy: Each experiment uses all 4 GPUs for faster completion
Priority: LoRA → Activation → Token Compression → Linear → Affine
Ablation: Each adapter experiment runs for BOTH model pairs
  - Main: Llama 3.1 8B ↔ Mistral 7B (4× vocab mismatch)
  - Ablation: Llama 3.1 8B ↔ Llama 3.2 3B (same vocab)
Benefits: Critical experiments first + full GPU utilization + controlled comparison


================================================================================
EXPERIMENT 2a/9: LORA ADAPTER - MAIN (LLAMA-MISTRAL)
================================================================================
Models: Llama 3.1 8B (128K vocab) ↔ Mistral 7B (32K vocab)
Why First: 260K params vs 16M (98% reduction), transferable across models

  GPU cache cleared

2. Starting all experiments sequentially (PRIORITY ORDER)...
Strategy: Each experiment uses all 4 GPUs for faster completion
Priority: LoRA → Activation → Token Compression → Linear → Affine
Ablation: Each adapter experiment runs for BOTH model pairs
  - Main: Llama 3.1 8B ↔ Mistral 7B (4× vocab mismatch)
  - Ablation: Llama 3.1 8B ↔ Llama 3.2 3B (same vocab)
Benefits: Critical experiments first + full GPU utilization + controlled comparison


================================================================================
EXPERIMENT 2a/9: LORA ADAPTER - MAIN (LLAMA-MISTRAL)
================================================================================
Models: Llama 3.1 8B (128K vocab) ↔ Mistral 7B (32K vocab)
Why First: 260K params vs 16M (98% reduction), transferable across models

================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B-Instruct
Model B: mistralai/Mistral-7B-Instruct-v0.3
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_010723.log
[W1101 01:07:23.837742448 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DistributedDataParallel (DDP)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Rank: 0, Device: cuda:0
Batch size per GPU: 10
Global batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B-Instruct
Model B: mistralai/Mistral-7B-Instruct-v0.3
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_010723.log
[W1101 01:07:23.842632117 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 2804.62it/s]
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8952.62it/s]
  Models deleted and GPU memory cleared
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Procrustes results saved to: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/unified_experiments/procrustes_results_20251101_005947.json

Cleaning up GPU memory...
  GPU cache cleared

2. Starting all experiments sequentially (PRIORITY ORDER)...
Strategy: Each experiment uses all 4 GPUs for faster completion
Priority: LoRA → Activation → Token Compression → Linear → Affine
Ablation: Each adapter experiment runs for BOTH model pairs
  - Main: Llama 3.1 8B ↔ Mistral 7B (4× vocab mismatch)
  - Ablation: Llama 3.1 8B ↔ Llama 3.2 3B (same vocab)
Benefits: Critical experiments first + full GPU utilization + controlled comparison


================================================================================
EXPERIMENT 2a/9: LORA ADAPTER - MAIN (LLAMA-MISTRAL)
================================================================================
Models: Llama 3.1 8B (128K vocab) ↔ Mistral 7B (32K vocab)
Why First: 260K params vs 16M (98% reduction), transferable across models

================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B-Instruct
Model B: mistralai/Mistral-7B-Instruct-v0.3
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_010724.log
[W1101 01:07:24.348354959 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  Models deleted and GPU memory cleared
Procrustes results saved to: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/unified_experiments/procrustes_results_20251101_005947.json

Cleaning up GPU memory...
  GPU cache cleared

2. Starting all experiments sequentially (PRIORITY ORDER)...
Strategy: Each experiment uses all 4 GPUs for faster completion
Priority: LoRA → Activation → Token Compression → Linear → Affine
Ablation: Each adapter experiment runs for BOTH model pairs
  - Main: Llama 3.1 8B ↔ Mistral 7B (4× vocab mismatch)
  - Ablation: Llama 3.1 8B ↔ Llama 3.2 3B (same vocab)
Benefits: Critical experiments first + full GPU utilization + controlled comparison


================================================================================
EXPERIMENT 2a/9: LORA ADAPTER - MAIN (LLAMA-MISTRAL)
================================================================================
Models: Llama 3.1 8B (128K vocab) ↔ Mistral 7B (32K vocab)
Why First: 260K params vs 16M (98% reduction), transferable across models

================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B-Instruct
Model B: mistralai/Mistral-7B-Instruct-v0.3
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_20251101_010724.log
[W1101 01:07:24.676711274 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.03it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.03it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.70it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.65it/s]
Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 8905.10it/s]
Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.64it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.61it/s]
Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 9015.16it/s]
Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.84it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.81it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.68it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.68it/s]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  2.04it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.64it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.20it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:01<00:01,  1.90it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.20it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  2.01it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.27it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  2.22it/s]
Loading checkpoint shards: 100%|##########| 4/4 [00:02<00:00,  2.03it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:02<00:00,  1.97it/s]
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 2640.14it/s]
Downloading shards: 100%|##########| 3/3 [00:00<00:00, 732.67it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.31it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.30it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 8525.01it/s]
Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.63it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.53it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards: 100%|##########| 3/3 [00:00<00:00, 8192.00it/s]
Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.79it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.69it/s]
Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.77it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.67it/s]
Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:00,  2.78it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  67%|######6   | 2/3 [00:00<00:00,  2.86it/s]Loading checkpoint shards:  33%|###3      | 1/3 [00:00<00:01,  1.93it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.68it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  2.70it/s]
Loading checkpoint shards:  67%|######6   | 2/3 [00:01<00:00,  2.00it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  1.94it/s]Loading checkpoint shards: 100%|##########| 3/3 [00:01<00:00,  1.94it/s]
[rank1]:[W1101 01:07:51.905189125 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank0]:[W1101 01:07:51.905405973 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1101 01:07:51.907278921 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1101 01:07:52.912523969 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.

================================================================================
Epoch 1/5
================================================================================
  [ 40.0%] Step  100/250 | Loss: 10.4114 (Gen: 10.1582, Contr: 2.3037, Align: 84.2065, Uniform: -0.7737) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.6m
  [ 40.0%] Step  100/250 | Loss: 10.4239 (Gen: 10.1705, Contr: 2.3062, Align: 85.8392, Uniform: -0.7693) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.6m  [ 40.0%] Step  100/250 | Loss: 10.4292 (Gen: 10.1759, Contr: 2.3054, Align: 85.8530, Uniform: -0.7612) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.6m

  [ 40.0%] Step  100/250 | Loss: 10.3932 (Gen: 10.1398, Contr: 2.3057, Align: 87.2728, Uniform: -0.7868) | ContrW: 0.120 | LR: 4.93e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.6m
  [ 80.0%] Step  200/250 | Loss: 10.2955 (Gen: 10.0193, Contr: 2.3041, Align: 85.3563, Uniform: -0.7701) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 3.484 | 0.38 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.3034 (Gen: 10.0269, Contr: 2.3056, Align: 86.4462, Uniform: -0.7719) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 3.484 | 0.38 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.2856 (Gen: 10.0090, Contr: 2.3062, Align: 87.5465, Uniform: -0.7795) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 3.484 | 0.38 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 10.3021 (Gen: 10.0258, Contr: 2.3048, Align: 85.0437, Uniform: -0.7618) | ContrW: 0.140 | LR: 4.69e-05 | GradNorm: 3.484 | 0.38 steps/s | ETA: 2.2m

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 1 Evaluation:
    CKA Similarity: 0.4019
    Cosine Similarity: 0.0008

================================================================================
Epoch 1/5 Complete | Time: 11.2m | Total: 11.2m
  Total Loss: 10.2396 (Gen: 9.9518, Contr: 2.3050)
  CKA Score: 0.4019 | LR: 0.000045
  ETA for remaining 4 epochs: 44.6m
================================================================================

================================================================================
Epoch 2/5
================================================================================
  [ 40.0%] Step  100/250 | Loss: 9.8010 (Gen: 9.4323, Contr: 2.3056, Align: 86.1928, Uniform: -0.7684) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.7797 (Gen: 9.4114, Contr: 2.3036, Align: 86.7934, Uniform: -0.7708) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.7678 (Gen: 9.3991, Contr: 2.3053, Align: 88.0926, Uniform: -0.7710) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.7935 (Gen: 9.4249, Contr: 2.3050, Align: 82.9721, Uniform: -0.7747) | ContrW: 0.170 | LR: 4.12e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 80.0%] Step  200/250 | Loss: 9.6872 (Gen: 9.2956, Contr: 2.3048, Align: 83.9864, Uniform: -0.7718) | ContrW: 0.190 | LR: 3.57e-05 | GradNorm: 2.906 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.6952 (Gen: 9.3038, Contr: 2.3041, Align: 86.7893, Uniform: -0.7693) | ContrW: 0.190 | LR: 3.57e-05 | GradNorm: 2.906 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.7151 (Gen: 9.3232, Contr: 2.3064, Align: 84.1436, Uniform: -0.7728) | ContrW: 0.190 | LR: 3.57e-05 | GradNorm: 2.906 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.6913 (Gen: 9.2995, Contr: 2.3057, Align: 89.9346, Uniform: -0.7683) | ContrW: 0.190 | LR: 3.57e-05 | GradNorm: 2.906 | 0.39 steps/s | ETA: 2.2m

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 2 Evaluation:
    CKA Similarity: 0.4018
    Cosine Similarity: 0.0009

================================================================================
Epoch 2/5 Complete | Time: 11.1m | Total: 22.3m
  Total Loss: 9.6717 (Gen: 9.2684, Contr: 2.3060)
  CKA Score: 0.4018 | LR: 0.000033
  ETA for remaining 3 epochs: 33.4m
================================================================================

================================================================================
Epoch 3/5
================================================================================
  [ 40.0%] Step  100/250 | Loss: 9.3965 (Gen: 8.9357, Contr: 2.3039, Align: 85.9566, Uniform: -0.7764) | ContrW: 0.200 | LR: 2.70e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.4105 (Gen: 8.9497, Contr: 2.3037, Align: 85.4336, Uniform: -0.7731) | ContrW: 0.200 | LR: 2.70e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.4266 (Gen: 8.9656, Contr: 2.3052, Align: 84.6052, Uniform: -0.7677) | ContrW: 0.200 | LR: 2.70e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.4168 (Gen: 8.9556, Contr: 2.3061, Align: 86.2355, Uniform: -0.7689) | ContrW: 0.200 | LR: 2.70e-05 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 80.0%] Step  200/250 | Loss: 9.3420 (Gen: 8.8808, Contr: 2.3060, Align: 86.9861, Uniform: -0.7624) | ContrW: 0.200 | LR: 2.05e-05 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.3401 (Gen: 8.8791, Contr: 2.3049, Align: 87.6570, Uniform: -0.7735) | ContrW: 0.200 | LR: 2.05e-05 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.3472 (Gen: 8.8863, Contr: 2.3048, Align: 85.8445, Uniform: -0.7631) | ContrW: 0.200 | LR: 2.05e-05 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.3562 (Gen: 8.8952, Contr: 2.3048, Align: 83.8122, Uniform: -0.7715) | ContrW: 0.200 | LR: 2.05e-05 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 3 Evaluation:
    CKA Similarity: 0.4018
    Cosine Similarity: 0.0010

================================================================================
Epoch 3/5 Complete | Time: 11.1m | Total: 33.4m
  Total Loss: 9.3212 (Gen: 8.8603, Contr: 2.3046)
  CKA Score: 0.4018 | LR: 0.000018
  ETA for remaining 2 epochs: 22.3m
================================================================================

================================================================================
Epoch 4/5
================================================================================
  [ 40.0%] Step  100/250 | Loss: 9.1928 (Gen: 8.7318, Contr: 2.3049, Align: 84.2900, Uniform: -0.7585) | ContrW: 0.200 | LR: 1.21e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.1824 (Gen: 8.7213, Contr: 2.3053, Align: 87.0069, Uniform: -0.7692) | ContrW: 0.200 | LR: 1.21e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.1625 (Gen: 8.7015, Contr: 2.3052, Align: 86.0695, Uniform: -0.7631) | ContrW: 0.200 | LR: 1.21e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.1623 (Gen: 8.7014, Contr: 2.3047, Align: 90.7310, Uniform: -0.7742) | ContrW: 0.200 | LR: 1.21e-05 | GradNorm: 0.000 | 0.39 steps/s | ETA: 6.5m
  [ 80.0%] Step  200/250 | Loss: 9.1666 (Gen: 8.7057, Contr: 2.3046, Align: 84.6974, Uniform: -0.7594) | ContrW: 0.200 | LR: 6.97e-06 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1505 (Gen: 8.6896, Contr: 2.3045, Align: 87.8686, Uniform: -0.7699) | ContrW: 0.200 | LR: 6.97e-06 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1679 (Gen: 8.7067, Contr: 2.3060, Align: 90.4822, Uniform: -0.7728) | ContrW: 0.200 | LR: 6.97e-06 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1473 (Gen: 8.6861, Contr: 2.3060, Align: 85.3873, Uniform: -0.7598) | ContrW: 0.200 | LR: 6.97e-06 | GradNorm: 2.859 | 0.39 steps/s | ETA: 2.2m

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 4 Evaluation:
    CKA Similarity: 0.4018
    Cosine Similarity: 0.0010

================================================================================
Epoch 4/5 Complete | Time: 11.1m | Total: 44.5m
  Total Loss: 9.1450 (Gen: 8.6842, Contr: 2.3044)
  CKA Score: 0.4018 | LR: 0.000005
  ETA for remaining 1 epochs: 11.1m
================================================================================

================================================================================
Epoch 5/5
================================================================================
  [ 40.0%] Step  100/250 | Loss: 9.1245 (Gen: 8.6634, Contr: 2.3051, Align: 92.2591, Uniform: -0.7639) | ContrW: 0.200 | LR: 2.00e-06 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.1271 (Gen: 8.6661, Contr: 2.3051, Align: 81.8876, Uniform: -0.7714) | ContrW: 0.200 | LR: 2.00e-06 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m
  [ 40.0%] Step  100/250 | Loss: 9.1239 (Gen: 8.6627, Contr: 2.3059, Align: 88.0257, Uniform: -0.7593) | ContrW: 0.200 | LR: 2.00e-06 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m  [ 40.0%] Step  100/250 | Loss: 9.1208 (Gen: 8.6600, Contr: 2.3040, Align: 84.8396, Uniform: -0.7690) | ContrW: 0.200 | LR: 2.00e-06 | GradNorm: 0.000 | 0.38 steps/s | ETA: 6.5m

  [ 80.0%] Step  200/250 | Loss: 9.1237 (Gen: 8.6626, Contr: 2.3054, Align: 85.5355, Uniform: -0.7640) | ContrW: 0.200 | LR: 2.48e-07 | GradNorm: 2.828 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1108 (Gen: 8.6499, Contr: 2.3049, Align: 88.9063, Uniform: -0.7602) | ContrW: 0.200 | LR: 2.48e-07 | GradNorm: 2.828 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1332 (Gen: 8.6723, Contr: 2.3046, Align: 83.0019, Uniform: -0.7765) | ContrW: 0.200 | LR: 2.48e-07 | GradNorm: 2.828 | 0.39 steps/s | ETA: 2.2m
  [ 80.0%] Step  200/250 | Loss: 9.1220 (Gen: 8.6609, Contr: 2.3056, Align: 89.5051, Uniform: -0.7580) | ContrW: 0.200 | LR: 2.48e-07 | GradNorm: 2.828 | 0.39 steps/s | ETA: 2.2m

  Computing multi-layer CKA similarity...
  Generating quality samples...

  Epoch 5 Evaluation:
    CKA Similarity: 0.4018
    Cosine Similarity: 0.0010

================================================================================
Epoch 5/5 Complete | Time: 11.1m | Total: 55.6m
  Total Loss: 9.1149 (Gen: 8.6539, Contr: 2.3048)
  CKA Score: 0.4018 | LR: 0.000000
================================================================================


================================================================================
TRAINING COMPLETE
================================================================================
Total time: 55.6 minutes (0.93 hours)
Total epochs: 5
Final loss: 9.1149
Final CKA score: 0.4018

Loss progression:
  Epoch  1: Loss 10.2396, CKA 0.4019
  Epoch  2: Loss 9.6717, CKA 0.4018
  Epoch  3: Loss 9.3212, CKA 0.4018
  Epoch  4: Loss 9.1450, CKA 0.4018
  Epoch  5: Loss 9.1149, CKA 0.4018
================================================================================

================================================================================
LORA EXPERIMENT COMPLETE
================================================================================
================================================================================================================================================================
LORA EXPERIMENT COMPLETE
================================================================================

Results saved to: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_results_20251101_010723.json
================================================================================
LORA EXPERIMENT COMPLETE
================================================================================

LORA EXPERIMENT COMPLETE
================================================================================
✓ LoRA (Llama-Mistral) complete, GPU memory cleared

================================================================================
EXPERIMENT 2b/9: LORA ADAPTER - ABLATION (LLAMA 3.1-3.2)
================================================================================
Models: Llama 3.1 8B ↔ Llama 3.2 3B (identical 128,256 token vocab)
Purpose: Control for vocabulary mismatch hypothesis
Expected: Better alignment (CKA 0.6-0.7) and lower generation loss (2.5-3.5)

================================================================================
LEARNED ADAPTER EXPERIMENT - LORA
================================================================================
Platform: hpc
Model A: meta-llama/Llama-3.1-8B-Instruct
Model B: meta-llama/Llama-3.2-3B-Instruct
Log file: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_adapters/lora_allgpus_samevocab_20251101_020432.log

================================================================================
GPU CONFIGURATION
================================================================================
Mode: DistributedDataParallel (DDP)
Number of GPUs: 4
GPU IDs: [0, 1, 2, 3]
Rank: 0, Device: cuda:0
Batch size per GPU: 10
Global batch size: 40
Effective batch (with grad accum): 320
================================================================================


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|##########| 4/4 [00:00<00:00, 3056.52it/s]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|##5       | 1/4 [00:00<00:01,  1.86it/s]Loading checkpoint shards:  50%|#####     | 2/4 [00:00<00:00,  2.58it/s]Loading checkpoint shards:  75%|#######5  | 3/4 [00:01<00:00,  3.19it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.53it/s]Loading checkpoint shards: 100%|##########| 4/4 [00:01<00:00,  3.08it/s]

================================================================================
LORA ADAPTER EXPERIMENT FAILED
================================================================================
Error: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct.
403 Client Error. (Request ID: Root=1-6905cd5b-0b2ffb8513c5f9077589cff3;03256f02-9daf-4a99-b632-27310032480b)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct to ask for access.
Traceback (most recent call last):
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/users/sujinesh/.local/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
    raise head_call_error
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
    r = _request_wrapper(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
    response = _request_wrapper(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
    hf_raise_for_status(response)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-6905cd5b-0b2ffb8513c5f9077589cff3;03256f02-9daf-4a99-b632-27310032480b)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct to ask for access.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 2290, in run_adapter_experiment
    model_b = AutoModelForCausalLM.from_pretrained(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1017, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/configuration_utils.py", line 574, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in _get_config_dict
    resolved_config_file = cached_file(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct.
403 Client Error. (Request ID: Root=1-6905cd5b-0b2ffb8513c5f9077589cff3;03256f02-9daf-4a99-b632-27310032480b)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.
Access to model meta-llama/Llama-3.2-3B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct to ask for access.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
[rank0]:     response.raise_for_status()
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/requests/models.py", line 1026, in raise_for_status
[rank0]:     raise HTTPError(http_error_msg, response=self)
[rank0]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py", line 403, in cached_file
[rank0]:     resolved_file = hf_hub_download(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1007, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1114, in _hf_hub_download_to_cache_dir
[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1655, in _raise_on_head_call_error
[rank0]:     raise head_call_error
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1543, in _get_metadata_or_catch_error
[rank0]:     metadata = get_hf_file_metadata(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 1460, in get_hf_file_metadata
[rank0]:     r = _request_wrapper(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 283, in _request_wrapper
[rank0]:     response = _request_wrapper(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py", line 307, in _request_wrapper
[rank0]:     hf_raise_for_status(response)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/huggingface_hub/utils/_http.py", line 419, in hf_raise_for_status
[rank0]:     raise _format(GatedRepoError, message, response) from e
[rank0]: huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-6905cd5b-0b2ffb8513c5f9077589cff3;03256f02-9daf-4a99-b632-27310032480b)

[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.
[rank0]: Access to model meta-llama/Llama-3.2-3B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct to ask for access.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 3506, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 3332, in main
[rank0]:     run_adapter_experiment("lora", gpu_id=None, model_a_id=LLAMA_31_8B, model_b_id=LLAMA_32_3B)
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 2290, in run_adapter_experiment
[rank0]:     model_b = AutoModelForCausalLM.from_pretrained(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 526, in from_pretrained
[rank0]:     config, kwargs = AutoConfig.from_pretrained(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1017, in from_pretrained
[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/configuration_utils.py", line 574, in get_config_dict
[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in _get_config_dict
[rank0]:     resolved_config_file = cached_file(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/utils/hub.py", line 421, in cached_file
[rank0]:     raise EnvironmentError(
[rank0]: OSError: You are trying to access a gated repo.
[rank0]: Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct.
[rank0]: 403 Client Error. (Request ID: Root=1-6905cd5b-0b2ffb8513c5f9077589cff3;03256f02-9daf-4a99-b632-27310032480b)

[rank0]: Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.
[rank0]: Access to model meta-llama/Llama-3.2-3B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct to ask for access.
W1101 02:05:36.024000 30668 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 30706 closing signal SIGTERM
W1101 02:05:36.025000 30668 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 30707 closing signal SIGTERM
W1101 02:05:36.025000 30668 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 30708 closing signal SIGTERM
/marlowe/apps/Mambaforge/24.3.0-0/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/marlowe/apps/Mambaforge/24.3.0-0/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/marlowe/apps/Mambaforge/24.3.0-0/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E1101 02:05:38.211000 30668 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 30705) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-01_02:05:36
  host      : n29.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 30705)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
