Using DDP (DistributedDataParallel) with torchrun for 4 GPUs

W1101 14:39:30.835000 2657623 torch/distributed/run.py:793] 
W1101 14:39:30.835000 2657623 torch/distributed/run.py:793] *****************************************
W1101 14:39:30.835000 2657623 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1101 14:39:30.835000 2657623 torch/distributed/run.py:793] *****************************************
==> Using CUDA on HPC (4 GPUs available)
==> Using CUDA on HPC (4 GPUs available)
==> Using CUDA on HPC (4 GPUs available)
==> Using CUDA on HPC (4 GPUs available)
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
  - Batch size per GPU: 10[W1101 14:39:42.790263337 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
  - Batch size per GPU: 10[W1101 14:39:42.797940701 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
[W1101 14:39:42.808360734 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
  - Batch size per GPU: 10
  - Global batch size: 40
  - Effective batch (with grad accum): 320
  - Samples: 10000
  - Epochs: 5
  - BF16: True
  - Flash Attention: True
Python: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0]
PyTorch: 2.5.1+cu121
CUDA available: True
CUDA devices: 4
[W1101 14:39:42.816362015 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())

✓ Initialized DDP: 4 processes (rank 0)
  Backend: nccl
  Device for rank 0: cuda:0
================================================================================
UNIFIED CROSS-MODEL ALIGNMENT EXPERIMENTS
Timestamp: 2025-11-01T14:39:42.810595
Platform: hpc
Device: cuda
Available CUDA GPUs: 4
DDP: Running with 4 processes
================================================================================
[rank0]:[W1101 14:39:43.709746150 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank2]:[W1101 14:39:43.724230342 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank1]:[W1101 14:39:43.739554370 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
[rank3]:[W1101 14:39:43.765304349 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.

2. Starting all experiments sequentially (FAST-FIRST ORDER)...
Strategy: Each experiment uses all 4 GPUs for faster completion
NEW PRIORITY: Run FAST experiments first (Procrustes + Activation), then SLOW (trained adapters)
  Phase 1 (FAST): Procrustes + Activation for BOTH model pairs (~30 min)
  Phase 2 (SLOW): LoRA, Linear, Affine on Llama 3.1-3.2 first (~2-3 hours)
  Phase 3 (SLOW): LoRA, Token, Linear, Affine on Llama-Mistral (~2-3 hours)
Benefits: Quick insights + early validation + fail fast on model access


================================================================================
EXPERIMENT 1/11: ACTIVATION COMMUNICATION (LLAMA 3.1-3.2)
================================================================================
Models: Llama 3.1 8B (4096 dim) → Llama 3.2 3B (3072 dim)
Reproducing Ramesh & Li (ICML 2025) - activation injection with learned projection
WHY FIRST: Core feasibility test for cross-model communication
Fixes: Replacement (not addition), last token only, forward hooks, dimension handling


================================================================================
ACTIVATION COMMUNICATION EXPERIMENT (Ramesh & Li 2025 Reproduction)
================================================================================
Model A (source): meta-llama/Llama-3.1-8B
Model B (target): meta-llama/Llama-3.2-3B
Method: Replace target's last-token activation with source's projected activation


Loading models on hpc...
Using bfloat16 for H100
Using Flash Attention 2
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 2017.22it/s]
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:01,  1.94it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  2.18it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:01<00:00,  2.24it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.37it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.28it/s]
Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]Downloading shards: 100%|██████████| 2/2 [00:00<00:00, 3556.00it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  2.39it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]

Model A: 4096 hidden_dim, 32 layers
Model B: 3072 hidden_dim, 28 layers

Dimension mismatch detected (4096 → 3072)
Looking for learned projection at: /projects/m000066/sujinesh/LatentWire/experimental/learning/runs/learned_projection/projection_4096_to_3072.pt
  ✗ No pre-trained projection found
  Training new projection on C4 dataset (per Ramesh & Li 2025)

================================================================================
TRAINING LEARNED PROJECTION (Ramesh & Li 2025)
================================================================================
Loading 3072 sentences from C4 dataset...
Loaded 3069 sentences from C4

Extracting layer-26 final-token activations...
Using batch size 128 for activation extraction
Extracting activations:   0%|          | 0/24 [00:00<?, ?it/s]Extracting activations:   4%|▍         | 1/24 [00:04<01:45,  4.60s/it]Extracting activations:   8%|▊         | 2/24 [00:07<01:18,  3.56s/it]Extracting activations:  12%|█▎        | 3/24 [00:08<00:55,  2.64s/it]Extracting activations:  17%|█▋        | 4/24 [00:12<00:57,  2.86s/it]Extracting activations:  21%|██        | 5/24 [00:14<00:48,  2.58s/it]Extracting activations:  25%|██▌       | 6/24 [00:16<00:44,  2.45s/it]Extracting activations:  29%|██▉       | 7/24 [00:18<00:37,  2.19s/it]Extracting activations:  33%|███▎      | 8/24 [00:20<00:33,  2.11s/it]Extracting activations:  38%|███▊      | 9/24 [00:22<00:33,  2.25s/it]Extracting activations:  42%|████▏     | 10/24 [00:24<00:31,  2.24s/it]Extracting activations:  46%|████▌     | 11/24 [00:26<00:28,  2.16s/it]Extracting activations:  50%|█████     | 12/24 [00:28<00:24,  2.08s/it]Extracting activations:  50%|█████     | 12/24 [00:35<00:35,  2.93s/it]
[rank0]: Traceback (most recent call last):
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 4664, in <module>
[rank0]:     main()
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 4359, in main
[rank0]:     activation_results_ablation = run_activation_communication_experiment(model_a_id=LLAMA_31_8B, model_b_id=LLAMA_32_3B)
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 3521, in run_activation_communication_experiment
[rank0]:     learned_projection = train_learned_projection(
[rank0]:   File "/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py", line 1238, in train_learned_projection
[rank0]:     outputs_b = base_model_b(
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 1210, in forward
[rank0]:     logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.66 GiB. GPU 0 has a total capacity of 79.19 GiB of which 6.20 GiB is free. Including non-PyTorch memory, this process has 72.98 GiB memory in use. Of the allocated memory 71.73 GiB is allocated by PyTorch, and 235.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1101 14:41:24.634000 2657623 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2657633 closing signal SIGTERM
W1101 14:41:24.634000 2657623 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2657634 closing signal SIGTERM
W1101 14:41:24.635000 2657623 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2657635 closing signal SIGTERM
E1101 14:41:25.517000 2657623 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 2657632) of binary: /marlowe/apps/Mambaforge/24.3.0-0/bin/python
Traceback (most recent call last):
  File "/users/sujinesh/.local/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/users/sujinesh/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/projects/m000066/sujinesh/LatentWire/experimental/learning/unified_cross_model_experiments.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-11-01_14:41:24
  host      : n12.cm.cluster
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2657632)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
