Some parameters are on the meta device because they were offloaded to the disk.
Loading HotpotQA subset...
Llama hidden size: 2048, Qwen hidden size: 896
Epoch 1/1
Traceback (most recent call last):
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/train.py", line 173, in <module>
    main()
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/train.py", line 102, in main
    loss_llama = llama.forward_with_prefix_loss(prefix_llama, y_llama)
  File "/Users/sujeethjinesh/Desktop/LatentWire/latentwire/models.py", line 216, in forward_with_prefix_loss
    out = self.model(inputs_embeds=inputs_embeds, attention_mask=attn_mask, labels=labels_full)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1189, in forward
    outputs = self.model(
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1000, in forward
    layer_outputs = decoder_layer(
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 726, in forward
    hidden_states = self.input_layernorm(hidden_states)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 170, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/hooks.py", line 341, in pre_forward
    value = self.weights_map[name]
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/utils/offload.py", line 118, in __getitem__
    return self.dataset[f"{self.prefix}{key}"]
  File "/Users/sujeethjinesh/Desktop/LatentWire/.venv/lib/python3.9/site-packages/accelerate/utils/offload.py", line 178, in __getitem__
    tensor = tensor.to(getattr(torch, weight_info["dtype"]))
TypeError: Trying to convert BFloat16 to the MPS backend but it does not have support for that dtype.
