%%%%%%%% LatentWire: Cross-Model Communication via Soft Tokens %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow} % for multi-row cells in tables
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{mlsys2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{mlsys2025}

% The \mlsystitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\mlsystitlerunning{LatentWire: Cross-Model Communication via Soft Tokens}

\begin{document}

\twocolumn[
\mlsystitle{LatentWire: Cross-Model Communication via Soft Tokens}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the mlsys2025
% package.

\begin{mlsysauthorlist}
\mlsysauthor{Sujeeth Jinesh}{stan}
\mlsysauthor{Thierry Tambe}{stan}
\end{mlsysauthorlist}

\mlsysaffiliation{stan}{Stanford University, Stanford, CA, USA}

\mlsyscorrespondingauthor{Sujeeth Jinesh}{sujinesh@stanford.edu}

\mlsyskeywords{Large Language Models, Soft Prompts, Cross-Model Communication, Latency Optimization}

\vskip 0.3in

\begin{abstract}
We present LatentWire, a method for cross-model communication through learned soft tokens that bypasses autoregressive text generation. A Perceiver Resampler bridge transforms hidden states from a sender LLM (Llama 3.1 8B) into soft tokens conditioning a receiver (Mistral 7B). On classification benchmarks, LatentWire achieves \textbf{90.7\% average accuracy} across SST-2, AG News, and TREC with 32 soft tokens---outperforming zero-shot baselines by +4.1pp, +19.6pp, and +37.3pp respectively---while providing \textbf{4.66$\times$ speedup} over text-relay. Linear probes validate that task-relevant information is extractable from sender representations (95\% on TREC). However, we identify critical limitations: reasoning tasks fail fundamentally (CommonsenseQA below random chance), and extreme compression (8 tokens) causes high variance. These results demonstrate that continuous representations enable faster, more effective cross-model communication than text for classification, while highlighting the need for hybrid architectures to handle reasoning.
\end{abstract}
]

\printAffiliationsAndNotice{}

\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have emerged as powerful tools for natural language understanding and generation \cite{vaswani2017attention, touvron2023llama, jiang2023mistral}. However, the dominant paradigm for combining multiple LLMs involves sequential text generation: one model produces text that another model consumes. This approach incurs substantial latency due to autoregressive decoding and may lose information through the discretization bottleneck of natural language.

We propose \textbf{LatentWire}, a method that enables direct communication between heterogeneous LLMs through learned soft tokens. Rather than having a sender model generate text for a receiver model to process, LatentWire transforms the sender's internal representations into a small set of continuous embeddings (soft tokens) that directly condition the receiver model's inference. On classification benchmarks, this approach:

\begin{enumerate}
    \item \textbf{Reduces latency significantly}: The sender model only performs a single forward pass, achieving 4.66$\times$ speedup compared to text-relay approaches.
    \item \textbf{Preserves task-relevant information}: Soft tokens encode features better than text serialization, achieving +19.6pp over zero-shot on AG News and +37.3pp on TREC.
    \item \textbf{Enables cross-model collaboration}: The bridge achieves 90.7\% average accuracy across three tasks, substantially outperforming individual models.
\end{enumerate}

Our key contributions are:

\begin{itemize}
    \item A bridge architecture based on Perceiver Resampler \cite{jaegle2021perceiver, alayrac2022flamingo} that achieves 90.7\% average accuracy (SST-2: 93.7\%, AG News: 90.7\%, TREC: 87.9\%) while being 4.66$\times$ faster than text-relay.
    \item Comprehensive evaluation showing the bridge substantially outperforms zero-shot baselines (+4.1pp SST-2, +19.6pp AG News, +37.3pp TREC) and dramatically outperforms text-relay (+52--90pp).
    \item Linear probe analysis showing 95\% accuracy on TREC from sender hidden states, validating that task-relevant information is present and extractable.
    \item Identification of critical limitations: reasoning tasks fail fundamentally (CommonsenseQA below random), and extreme compression (8 tokens) causes high variance on complex tasks.
\end{itemize}

\section{Related Work}
\label{sec:related}

\paragraph{Soft Prompts and Prompt Tuning}
Prompt tuning \cite{lester2021power} and prefix tuning \cite{li2021prefix} demonstrated that freezing LLM weights while learning continuous ``soft'' prompt embeddings can match full fine-tuning performance. Our work extends this paradigm from single-model adaptation to cross-model communication, using soft tokens as an interlingua between heterogeneous models.

\paragraph{Perceiver and Vision-Language Bridging}
The Perceiver \cite{jaegle2021perceiver} introduced cross-attention to map arbitrary-length inputs to a fixed-size latent array. BLIP-2 \cite{li2023blip2} and Flamingo \cite{alayrac2022flamingo} applied similar principles to bridge frozen image encoders and LLMs through learned query tokens. Our bridge architecture adapts this design for language-to-language transfer between heterogeneous models.

\paragraph{Model Stitching}
Model stitching \cite{bansal2021revisiting, pan2023stitchable} connects layers from different networks using learned transformations. Unlike these \emph{offline} methods that modify weights before deployment, LatentWire enables \emph{runtime} communication through soft tokens during inference, supporting dynamic, input-dependent information flow.

\paragraph{Prompt Compression}
Methods like LLMLingua \cite{jiang2023llmlingua} compress prompts by removing tokens, while ICAE \cite{ge2024incontext} and 500xCompressor \cite{li2024500x} learn dense embeddings. Our work focuses on cross-model communication rather than single-model compression.

\paragraph{Knowledge Distillation}
Knowledge distillation \cite{hinton2015distilling} transfers knowledge from a teacher to a student model, typically through soft targets or intermediate representations. Recent work extends this to LLMs \cite{xu2024survey, gu2024minillm}, but focuses on training smaller models to mimic larger ones. In contrast, LatentWire enables runtime collaboration between frozen models without modifying either model's weights.

\paragraph{Continuous Latent Reasoning}
Recent work explores reasoning in continuous latent spaces rather than through explicit text tokens. Coconut \cite{hao2024coconut} trains models to reason via continuous ``thought tokens'' without verbalization, achieving improved efficiency on reasoning tasks. Our soft tokens serve a different purpose---cross-model communication rather than internal reasoning---but share the insight that continuous representations can encode information more efficiently than discrete text.

\section{Method}
\label{sec:method}

\subsection{Problem Formulation}

We formalize the cross-model communication problem as follows. Let $\mathcal{S}$ and $\mathcal{R}$ denote a sender and receiver LLM respectively, with potentially different architectures, tokenizers, and training distributions. Given input text $x$, we seek a communication protocol that enables $\mathcal{R}$ to perform a downstream task using information extracted from $\mathcal{S}$'s processing of $x$.

\paragraph{Desiderata} An ideal cross-model communication mechanism should satisfy:
\begin{enumerate}
    \item \textbf{Efficiency}: Communication should be faster than text generation ($O(1)$ vs $O(L)$ autoregressive steps).
    \item \textbf{Fidelity}: Task-relevant information should be preserved through the channel.
    \item \textbf{Modularity}: Both models remain frozen; only the communication channel is learned.
    \item \textbf{Compression}: The transmitted representation should be compact ($M \ll L$ tokens).
\end{enumerate}

\paragraph{Formal Setup} Let $\mathbf{h}_\mathcal{S}^{(\ell)} \in \mathbb{R}^{L \times d_\mathcal{S}}$ denote the hidden states from layer $\ell$ of the sender, where $L$ is the sequence length and $d_\mathcal{S}$ is the hidden dimension. We seek a bridge function $f_\theta: \mathbb{R}^{L \times d_\mathcal{S}} \rightarrow \mathbb{R}^{M \times d_\mathcal{R}}$ that produces soft tokens $\mathbf{z} = f_\theta(\mathbf{h}_\mathcal{S}^{(\ell)})$ satisfying:
\begin{align}
    \mathbf{z}^* = \arg\max_{\mathbf{z}} \; p_\mathcal{R}(y | \mathbf{z}, \mathbf{x}_\text{prompt})
\end{align}
where $y$ is the correct task output and $\mathbf{x}_\text{prompt}$ is an optional task-specific prompt.

\paragraph{Key Challenge: Representation Mismatch} The sender and receiver occupy different representation spaces. Even when hidden dimensions match ($d_\mathcal{S} = d_\mathcal{R} = 4096$ for Llama and Mistral), the geometric structure differs due to:
\begin{itemize}
    \item \textbf{Vocabulary}: Llama (128K tokens) vs. Mistral (32K tokens)
    \item \textbf{Positional encoding}: Different RoPE base frequencies
    \item \textbf{Attention}: Grouped-query (Llama) vs. sliding window (Mistral)
    \item \textbf{Statistics}: Hidden state magnitude differs by $\sim$5$\times$
\end{itemize}

A naive linear projection fails because it assumes isomorphic spaces. The bridge must learn a \emph{semantic translation}, not merely a coordinate transformation. Figure~\ref{fig:architecture} illustrates the overall pipeline.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/architecture.pdf}
\caption{LatentWire architecture. Input text is processed by the frozen sender (Llama), whose hidden states are transformed by the bridge into soft tokens that condition the frozen receiver (Mistral) for classification.}
\label{fig:architecture}
\end{figure}

\subsection{Bridge Architecture}

Our bridge uses a Perceiver Resampler design:

\begin{enumerate}
    \item \textbf{Input Projection}: Linear projection from sender hidden dimension to bridge internal dimension: $\mathbf{h}' = \mathbf{W}_\text{in} \mathbf{h}_\mathcal{S}$, where $\mathbf{W}_\text{in} \in \mathbb{R}^{d_\mathcal{S} \times d}$.

    \item \textbf{Learned Latent Queries}: A set of $M$ learnable query vectors $\mathbf{Q} \in \mathbb{R}^{M \times d}$ that attend to the projected sender states.

    \item \textbf{Cross-Attention Layers}: $N$ transformer blocks where queries attend to keys/values derived from sender states:
    \begin{align}
        \mathbf{z}^{(n+1)} = \text{FFN}(\text{CrossAttn}(\mathbf{z}^{(n)}, \mathbf{h}'))
    \end{align}
    We use $N=2$ layers with $d=512$ internal dimension.

    \item \textbf{Output Projection}: Linear projection to receiver embedding space with RMS normalization:
    \begin{align}
        \mathbf{z} = \alpha \cdot \frac{\mathbf{W}_\text{out} \mathbf{z}^{(N)}}{\text{RMS}(\mathbf{W}_\text{out} \mathbf{z}^{(N)})}
    \end{align}
    where $\alpha$ is calibrated to match the receiver's embedding statistics.
\end{enumerate}

The bridge adds trainable parameters to enable the cross-model mapping, while both base LLMs (8B+7B) remain completely frozen.

\subsection{Design Space: Why Cross-Attention?}
\label{sec:design_space}

We systematically explored several bridge architectures before arriving at the Perceiver-based approach. Our ablations (see Appendix~\ref{app:arch_ablation}) show that cross-attention is essential: the Perceiver achieves 92.0\% on SST-2, while mean pooling completely fails (0\%) by destroying sequential structure. Diffusion Transformers achieve only 85.5\% due to error accumulation across denoising steps and training objective mismatch. Interestingly, simple linear projection achieves 91.5\% on binary SST-2 but degrades on harder multi-class tasks (AG News: 78.3\% vs.\ 90.7\% for Perceiver), motivating our choice of cross-attention for general applicability.

\subsection{Training Objective}

We train the bridge to produce soft tokens that enable $\mathcal{R}$ to perform the target task correctly. For classification tasks, we use cross-entropy loss on the receiver's predictions:
\begin{align}
    \mathcal{L} = -\sum_{c} y_c \log p_\mathcal{R}(c | \mathbf{z}, \mathbf{x}_\text{prompt})
\end{align}
where $y_c$ is the ground-truth label and $p_\mathcal{R}$ is the receiver's predicted probability given soft tokens $\mathbf{z}$ and a task prompt $\mathbf{x}_\text{prompt}$.

We also add a diversity regularization term to prevent mode collapse:
\begin{align}
    \mathcal{L}_\text{div} = -\lambda \cdot H(\bar{\mathbf{z}})
\end{align}
where $H$ is entropy and $\bar{\mathbf{z}}$ is the mean soft token representation across the batch.

\subsection{Inference Pipeline}

At inference time:
\begin{enumerate}
    \item \textbf{Sender Encode}: Pass input through frozen $\mathcal{S}$, extract layer $\ell$ hidden states.
    \item \textbf{Bridge Transform}: Apply $f_\theta$ to obtain $M$ soft tokens.
    \item \textbf{Receiver Decode}: Prepend soft tokens to task prompt, run single forward pass through $\mathcal{R}$.
\end{enumerate}

The bridge adds only 14ms overhead compared to direct Mistral inference (170.6ms vs 156.2ms), achieving 4.66$\times$ speedup over text-relay (795.2ms). See Table~\ref{tab:latency} for detailed measurements.

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\paragraph{Models} We use Llama 3.1 8B Instruct as the sender and Mistral 7B Instruct v0.3 as the receiver. Both models remain frozen throughout training.

\paragraph{Datasets} We evaluate on four text classification benchmarks:
\begin{itemize}
    \item \textbf{SST-2} \cite{socher2013recursive}: Binary sentiment classification of movie reviews.
    \item \textbf{AG News} \cite{zhang2015character}: 4-class topic classification (World, Sports, Business, Sci/Tech).
    \item \textbf{TREC} \cite{li2002learning}: 6-class question type classification.
    \item \textbf{Banking77} \cite{casanueva2020efficient}: 77-class intent classification for banking queries.
\end{itemize}

\paragraph{Baselines} We compare against:
\begin{itemize}
    \item \textbf{Llama/Mistral Direct}: Each model classifies directly from text (zero-shot).
    \item \textbf{5-shot Prompting}: Standard few-shot prompting with 5 balanced examples per class.
    \item \textbf{Text-Relay}: Llama generates a summary, Mistral classifies from summary.
    \item \textbf{CoT-Relay}: Llama generates chain-of-thought reasoning, Mistral classifies from that reasoning.
    \item \textbf{LoRA}: Fine-tuned Mistral with rank-8 LoRA adapter (3.4M params).
    \item \textbf{Prompt-Tuning}: Learnable soft prompts on Mistral only (no Llama). Tests whether the sender actually contributes.
\end{itemize}

\paragraph{Hyperparameters} Default settings: $M=8$ soft tokens, learning rate $10^{-4}$, batch size 8, diversity weight $\lambda=0.1$, 2000 training steps. We extract from layer $\ell=16$ for SST-2 and $\ell=31$ for AG News and TREC. For Banking77 and TREC, we use $M=16$ tokens and 3000 steps.

\subsection{Main Results}

Table \ref{tab:main_results} presents our main accuracy comparison.

\begin{table}[t]
\caption{Classification accuracy (\%) across benchmarks. Bridge (32 tokens) achieves strong performance, outperforming zero-shot baselines and text-relay.$^*$}
\label{tab:main_results}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}lcccc@{}}
\toprule
Method & SST-2 & AG News & TREC & Avg \\
\midrule
Random Chance & 50.0 & 25.0 & 16.7 & 30.6 \\
\midrule
\textbf{Bridge (32 tok)} & \textbf{93.7}$\pm$0.3 & \textbf{90.7}$\pm$1.4 & \textbf{87.9}$\pm$3.3 & \textbf{90.7} \\
Bridge (8 tok) & 93.3$\pm$0.4 & 91.1$\pm$1.2 & 61.5$\pm$23.3$^\dagger$ & 82.0 \\
\midrule
Linear Probe & 92.1 & 86.8 & 95.0 & 91.3 \\
Prompt-Tuning & 50.9 & 25.0 & 15.9 & 30.6 \\
LoRA (rank-8) & 90.1 & 40.8 & 26.4 & 52.4 \\
\midrule
Llama 0-shot & 83.8 & 71.0 & 48.2 & 67.7 \\
Mistral 0-shot & 89.6 & 71.1 & 50.6 & 70.4 \\
Text-Relay & 41.3 & 1.0 & 4.0 & 15.4 \\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.15in
{\footnotesize $^*$Bridge: mean$\pm$std over 5 seeds (42, 123, 456, 789, 1011). Others deterministic.}\\
{\footnotesize $^\dagger$TREC 8-tok bimodal (35--86\%); 32 tokens eliminates instability.}
\vskip -0.1in
\end{table}

\paragraph{Strong Performance Across All Tasks} The bridge (32 tokens) achieves strong performance across all three classification tasks. On SST-2, the bridge (93.7\%) outperforms both Llama (83.8\%) and Mistral (89.6\%) zero-shot baselines by +9.9pp and +4.1pp respectively. On AG News, the bridge (90.7\%) dramatically exceeds Mistral (71.1\%) by +19.6pp. On TREC, the bridge (87.9\%) substantially outperforms Mistral (50.6\%) by +37.3pp. These results demonstrate that cross-model communication via hidden states can effectively leverage information from heterogeneous architectures for classification tasks.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/sender_essential.pdf}
\caption{Bridge vs.\ Prompt-Tuning: The sender model is essential. Prompt-tuning on Mistral alone achieves only 50.9\% on SST-2 (random chance), while the bridge achieves 93.7\%, demonstrating that cross-model communication provides critical task information.}
\label{fig:sender_essential}
\end{figure}

\paragraph{Linear Probe Validation} To verify that task-relevant information exists in the sender representations, we trained linear probes on Llama layer-16 hidden states. The linear probe achieves 95.0\% on TREC, 92.1\% on SST-2, and 86.8\% on AG News. The TREC result (95.0\%) exceeds the bridge (87.9\%) by 7.1pp, indicating that the bridge does not fully extract available information---room remains for architectural improvements. However, this validates our core hypothesis: task-relevant information is linearly accessible in intermediate representations, and the bridge successfully transfers much of this signal cross-model.

\paragraph{Bridge vs. Text-Relay} The bridge dramatically outperforms text-relay across all tasks. Text-relay fails catastrophically: 41.3\% on SST-2 (below random), 1.0\% on AG News, and 4.0\% on TREC. In contrast, the bridge achieves 93.7\%, 90.7\%, and 87.9\% respectively---improvements of +52.4pp, +89.7pp, and +83.9pp. This demonstrates that text summarization destroys classification-relevant information, while continuous representations preserve it effectively.

\paragraph{High Variance Under Extreme Compression} While the bridge with 32 tokens shows stable performance (87.9\% $\pm$ 3.3\% on TREC), using only 8 tokens introduces significant instability. Across 5 seeds on TREC, 8-token accuracy varies from 35.4\% to 85.8\%, with a bimodal distribution: some runs converge to good solutions (~84\%) while others collapse to predicting only 2-3 classes (~38\%). This suggests a minimum capacity threshold exists for reliable multi-class classification---32 tokens provides sufficient capacity to avoid mode collapse.

\paragraph{Why Super-Additive Performance?} The bridge exceeds both individual models on classification tasks. We hypothesize this results from: (1) \emph{complementary representations}---the bridge extracts features from Llama's space that Mistral is well-suited to utilize; (2) \emph{denoising}---the bottleneck filters noise, passing only task-relevant signals; and (3) \emph{implicit ensembling}---combining model capabilities without text discretization loss. The bridge naturally handles architectural differences (vocabulary, positional encoding, attention mechanisms) by operating on hidden state representations rather than tokens.

\subsection{Latency Analysis}

Table \ref{tab:latency} presents latency measurements on an NVIDIA H100 GPU. The primary comparison is Bridge vs.\ Text-Relay, as both represent cross-model communication paradigms.

\begin{table}[t]
\caption{Latency comparison (ms) on H100 GPU. Bridge achieves 4.66$\times$ speedup over text-relay by avoiding autoregressive generation in the sender model.}
\label{tab:latency}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
Method & Latency (ms) & Speedup \\
\midrule
Text-Relay & 795.2 & 1.0$\times$ \\
\textbf{Bridge (ours)} & \textbf{170.6} & \textbf{4.66$\times$} \\
Mistral Direct & 156.2 & 5.09$\times$ \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The bridge is 4.66$\times$ faster than text-relay because it eliminates autoregressive generation in the sender model. Text-relay requires Llama to generate text summaries autoregressively ($\sim$795ms), while the bridge only requires a single forward pass to extract hidden states. The bridge adds only 14ms overhead compared to direct Mistral inference (170.6ms vs 156.2ms), demonstrating efficient cross-model communication.

Bridge latency breakdown:
\begin{itemize}
    \item Llama encode + bridge transform: $\sim$14ms overhead
    \item Mistral forward: $\sim$156ms---soft token conditioning
\end{itemize}

\noindent Figure~\ref{fig:latency} visualizes the latency comparison and breakdown.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/latency_comparison.pdf}
\caption{Latency analysis. \textbf{Left:} Total inference time showing 4.66$\times$ speedup over text-relay by eliminating autoregressive generation. \textbf{Right:} Bridge adds minimal overhead compared to direct model inference.}
\label{fig:latency}
\end{figure}

\subsection{Comparison with Fine-Tuning Baselines}

Table~\ref{tab:stronger_baselines} compares the bridge against fine-tuning baselines. The bridge (93.7\% on SST-2) outperforms LoRA (90.1\%) despite LoRA showing high variance across seeds.

\begin{table}[t]
\caption{Bridge vs.\ fine-tuning baselines. Bridge achieves competitive or superior accuracy while enabling cross-model collaboration.}
\label{tab:stronger_baselines}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Method & SST-2 & AG News & Latency \\
\midrule
Linear Probe & 92.1 & 86.8 & 35ms$^\dagger$ \\
LoRA (rank=8) & 90.1$\pm$6.0 & 40.8$\pm$22.3 & 156ms$^\dagger$ \\
Prompt-Tuning & 50.9 & 25.0 & 155ms$^\dagger$ \\
\midrule
\textbf{Bridge (32 tok)} & \textbf{93.7}$\pm$0.3 & \textbf{90.7}$\pm$1.4 & 171ms \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
{\footnotesize $^\dagger$Single-model inference (Mistral only, or linear probe on Llama). Bridge requires two models.}
\vskip -0.1in
\end{table}

\paragraph{Bridge vs.\ Linear Probe} Linear probes on Llama layer-16 achieve 92.1\% on SST-2 and 86.8\% on AG News. The bridge slightly outperforms on SST-2 (+1.6pp) and substantially outperforms on AG News (+3.9pp), demonstrating that the cross-model transfer adds value beyond what is linearly extractable from the sender alone.

\paragraph{Bridge vs.\ LoRA} LoRA fine-tuning shows high variance, particularly on AG News (40.8\% $\pm$ 22.3\%). The bridge achieves both higher accuracy (90.7\% vs 40.8\% on AG News) and lower variance ($\pm$1.4 vs $\pm$22.3), suggesting more stable optimization.

\paragraph{Bridge vs.\ Prompt-Tuning} Prompt-tuning on Mistral alone (without sender information) achieves only random chance (50.9\% on SST-2, 25.0\% on AG News), as visualized in Figure~\ref{fig:sender_essential}. This demonstrates that the sender model provides essential task information that cannot be recovered through receiver-only soft prompt optimization.

\subsection{Batched Throughput}

Table~\ref{tab:batched} shows throughput scaling with batch size. The bridge maintains its advantage at all batch sizes, achieving over 100 samples/second at batch size 16.

\begin{table}[t]
\caption{Throughput (samples/sec) at various batch sizes. Bridge scales well and maintains significant speedup over text-relay at all batch sizes.}
\label{tab:batched}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Batch & Bridge & Direct & Text-Relay \\
\midrule
1 & 7.4 & 8.8 & 0.9 \\
4 & 28.7 & 31.2 & 1.0 \\
16 & 105.7 & 116.0 & -- \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Bridge throughput scales nearly linearly with batch size (14$\times$ improvement from batch 1 to 16). The slight overhead compared to direct Mistral inference (105.7 vs.\ 116.0 samples/sec at batch 16) reflects the cost of the additional sender model pass, but the bridge provides cross-model benefits that direct inference cannot.

\subsection{Inverse Token Scaling}
\label{sec:token_scaling}

We investigate how the number of soft tokens affects performance on Banking77, a challenging 77-class task.

\begin{table}[t]
\caption{Effect of soft token count on Banking77 accuracy. Fewer tokens yield better performance, suggesting compression acts as regularization.}
\label{tab:token_scaling}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cc}
\toprule
Soft Tokens & Accuracy (\%) \\
\midrule
16 & \textbf{21.5} \\
32 & 13.5 \\
64 & 7.5 \\
128 & 1.0 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table \ref{tab:token_scaling} shows a striking inverse relationship: increasing tokens from 16 to 128 causes accuracy to collapse from 21.5\% to random (1.3\% for 77 classes). This ``inverse scaling'' phenomenon suggests:

\begin{enumerate}
    \item \textbf{Compression as regularization}: Fewer tokens force the bridge to extract only the most task-relevant information.
    \item \textbf{Mode collapse}: More tokens provide more degrees of freedom that can collapse to trivial solutions.
    \item \textbf{Optimization difficulty}: Higher-dimensional soft prompt spaces are harder to optimize.
\end{enumerate}

We observe similar patterns on passkey retrieval tasks, where 16 tokens achieve 23.4\% digit accuracy vs. 9.8\% for 128 tokens. Figure~\ref{fig:token_scaling} visualizes this inverse relationship.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/token_scaling.pdf}
\caption{Inverse token scaling on Banking77. Accuracy decreases monotonically as the number of soft tokens increases, suggesting compression acts as beneficial regularization.}
\label{fig:token_scaling}
\end{figure}

\subsection{Generalization to Reasoning Tasks}
\label{sec:reasoning}

While LatentWire excels on classification, we evaluate whether it generalizes to reasoning tasks. Table~\ref{tab:reasoning} presents results on three standard reasoning benchmarks.

\begin{table}[t]
\caption{Reasoning benchmark results. Unlike classification, the bridge \textbf{underperforms} direct model inference on all reasoning tasks. This reveals a fundamental limitation of soft token compression for tasks requiring multi-step inference.}
\label{tab:reasoning}
\vskip 0.15in
\begin{center}
\begin{scriptsize}
\begin{tabular}{@{}lccccc@{}}
\toprule
\textbf{Task} & \textbf{Rand.} & \textbf{Llama} & \textbf{Mistral} & \textbf{Txt-Relay} & \textbf{Bridge} \\
\midrule
BoolQ & 50.0 & 79.2 & \textbf{83.2} & 80.8 & 72.5 \\
PIQA & 50.0 & \textbf{61.0} & 57.4 & 30.4$^\dagger$ & 60.4 \\
CSQA & 20.0 & \textbf{75.4} & 68.0 & 75.4 & 17.0 \\
\bottomrule
\end{tabular}
\end{scriptsize}
\end{center}
\vskip -0.1in
{\footnotesize $^\dagger$Text-relay fails on PIQA (30.4\%); bridge preserves physical intuition signals. CSQA = CommonsenseQA.}
\end{table}

\paragraph{Classification vs. Reasoning} The contrast with classification is stark. While the bridge achieves super-additive performance on classification (+4.5--26.9pp over individual models), it \emph{underperforms} on reasoning: BoolQ (-10.7pp vs. Mistral), PIQA (-0.6pp vs. Llama), and CommonsenseQA (-58.4pp vs. Llama, falling \emph{below random chance}).

\paragraph{Why Does Reasoning Fail?} We hypothesize that classification and reasoning have fundamentally different information requirements:
\begin{itemize}
    \item \textbf{Classification}: Requires compressing to a simple decision boundary. 8 soft tokens suffice to encode ``positive/negative'' or ``topic A/B/C/D.''
    \item \textbf{Reasoning}: Requires preserving multi-step inference chains and world knowledge. These cannot be compressed into 8 soft tokens without catastrophic information loss.
\end{itemize}

\paragraph{Interesting Exception} On PIQA, text-relay fails catastrophically (30.4\%) while the bridge succeeds (60.4\%). This suggests the bridge preserves implicit ``physical intuition'' signals that explicit text summarization destroys---a promising direction for future work.

\paragraph{Soft Token Interpretability} Analyzing soft tokens by their nearest neighbors in Mistral's vocabulary reveals partially interpretable patterns: for negative sentiment, the literal word ``negative'' appears as the top nearest neighbor for 3 of 8 tokens. The 8 tokens show high pairwise cosine similarity (0.97-0.99), suggesting redundant encoding that may provide robustness.

\section{Limitations and Future Work}
\label{sec:limitations}

\paragraph{Reasoning Tasks Fail Fundamentally} As shown in Section~\ref{sec:reasoning}, the bridge completely fails on reasoning tasks. On CommonsenseQA, performance falls \emph{below random chance} (17.0\% vs 20.0\%), indicating that soft token compression destroys the multi-step inference chains required for reasoning. This is not merely a capacity limitation that more tokens would solve---it reflects a fundamental mismatch between classification (compressing to decision boundaries) and reasoning (preserving inference chains). Extending to reasoning requires fundamentally different approaches, likely \textbf{hybrid architectures} that combine soft token compression for context with text generation for explicit reasoning steps.

\paragraph{High Variance Under Extreme Compression} TREC with 8 tokens exhibits \textbf{bimodal behavior}: across 5 seeds, accuracy clusters into ``successful'' runs (83-86\%) and ``failed'' runs (35-41\%), with coefficient of variation of 38\%. Analysis shows failed runs collapse to predicting primarily 2-3 classes (ENTY, DESC) while ignoring others (NUM, LOC, ABBR, HUM). This instability disappears with 32 tokens (87.9\% $\pm$ 3.3\%), suggesting a minimum capacity threshold exists for reliable multi-class classification. Future work should investigate adaptive token allocation and ensemble methods for stability.

\paragraph{Task-Specific Training} Bridges must be trained per-task. We did not observe meaningful zero-shot transfer between tasks (e.g., SST-2$\to$AG News). Future work could explore universal bridges through meta-learning or larger architectures.

\paragraph{Same-Model vs Cross-Model Transfer} We focused on cross-model transfer (Llama$\to$Mistral); comparing against same-model bridges (Llama$\to$Llama) would quantify whether heterogeneous model pairs provide unique benefits through forced abstraction.

\paragraph{Linear Probe Upper Bound} Linear probes on Llama layer-16 hidden states achieve 95\% on TREC, exceeding the bridge (87.9\%). This 7pp gap suggests the bridge does not fully extract available task information, indicating room for architectural improvements. However, the bridge maintains the key advantage of cross-model transfer, enabling collaboration between heterogeneous architectures.

\paragraph{Computational Cost} While the bridge achieves 4.66$\times$ lower latency than text-relay, it requires inference through two full models (sender + receiver, 15B total parameters) compared to a single model for direct classification. The 14ms overhead compared to direct Mistral inference is modest, but memory requirements remain high.

\paragraph{Compression Ratio Interpretation} With 32 tokens at 4096 dimensions (fp16), soft token size is $\sim$16KB per input. For typical classification inputs (50-300 bytes), this represents \emph{expansion} rather than compression in raw bytes. However, the key insight is that 32 tokens replace the full autoregressive generation required by text-relay, achieving the latency benefits while preserving task-relevant information.

\paragraph{Future Directions: Hybrid Architectures} Our results suggest a promising direction: hybrid systems that use soft token bridges for fast context transfer combined with selective text generation for reasoning steps. This could combine the latency benefits of continuous representations with the interpretability and reasoning capabilities of text.

\section{Conclusion}
\label{sec:conclusion}

We present LatentWire, a method for cross-model communication via learned soft tokens. Our bridge enables a sender LLM to condition a receiver LLM's inference without text generation, achieving:

\begin{itemize}
    \item \textbf{90.7\% average accuracy} across three classification tasks (SST-2: 93.7\%, AG News: 90.7\%, TREC: 87.9\%) with only 32 soft tokens
    \item \textbf{4.66$\times$ speedup} over text-relay (170.6ms vs 795.2ms) with dramatic accuracy improvements (+52.4pp on SST-2, +89.7pp on AG News, +83.9pp on TREC)
    \item \textbf{Substantial gains over zero-shot}: +4.1pp on SST-2, +19.6pp on AG News, +37.3pp on TREC vs best individual model
    \item \textbf{Linear probe validation}: 95\% accuracy on TREC confirms task-relevant information exists in sender representations
\end{itemize}

We also identify critical limitations:

\begin{itemize}
    \item \textbf{Reasoning fundamentally fails}: CommonsenseQA below random chance (17\% vs 20\%), indicating soft token compression cannot preserve multi-step inference
    \item \textbf{High variance under extreme compression}: TREC 8-token shows bimodal distribution (35-86\% across seeds), requiring minimum 32 tokens for stability
\end{itemize}

These results demonstrate that continuous representations enable faster and more effective cross-model communication than text for classification tasks. However, the fundamental failure on reasoning tasks suggests that hybrid architectures---combining soft token bridges for context with text generation for reasoning---represent a promising direction for future work.

\textbf{The key takeaway}: when task-relevant information can be compressed into a decision boundary (classification), soft token bridges dramatically outperform text-based communication in both speed and accuracy. LatentWire thus opens new possibilities for building collaborative multi-model systems, while its failure modes on reasoning tasks provide clear guidance on when continuous representations are---and are not---appropriate.

\section*{Reproducibility Statement}

We are committed to reproducible research. Our implementation will be released as open-source code upon acceptance, including all training scripts, evaluation pipelines, and configuration files.

\paragraph{Random Seeds} All experiments use fixed random seeds for reproducibility. Main results report mean and standard deviation over 5 seeds (42, 123, 456, 789, 1011) for bridge experiments. Other baselines (zero-shot, text-relay) are deterministic and thus reported without variance.

\paragraph{Model Checkpoints} All base models (Llama 3.1 8B Instruct, Mistral 7B Instruct v0.3) are publicly available on the HuggingFace Hub. We use the official model weights without modification; both models remain frozen throughout all experiments.

\paragraph{Hardware} Experiments were conducted on NVIDIA H100 80GB GPUs using Stanford's Marlowe computational cluster~\cite{marlowe2025}. Training a single bridge (2000-3000 steps) requires approximately 3.5-5 minutes. Full reproduction of all experiments requires approximately 1 GPU-hour.

\paragraph{Datasets} All datasets (SST-2, AG News, TREC, Banking77, BoolQ, PIQA, CommonsenseQA) are publicly available through standard NLP benchmarks and will be documented in our release.

\section*{Acknowledgments}

We thank the Stanford Research Computing Center for providing computational resources on the Marlowe cluster~\cite{marlowe2025}. This research was supported in part by Stanford's Data Science Scholars program.

\bibliography{telepathy}
\bibliographystyle{mlsys2025}

\appendix
\section{Architecture Ablation}
\label{app:arch_ablation}

Table~\ref{tab:arch_ablation} presents systematic comparisons of bridge architectures on SST-2.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Architecture} & \textbf{SST-2 Acc.} & \textbf{Verdict} \\
\midrule
Perceiver (ours) & \textbf{92.0\%} & Best \\
MLP Bridge & 91.5\% & Competitive \\
Linear Projection & 91.5\% & Surprisingly good \\
Diffusion Transformer & 85.5\% & Viable but worse \\
Mean Pooling & 0.0\% & Complete failure \\
Identity (no transform) & 0.0\% & Complete failure \\
\bottomrule
\end{tabular}
\caption{Architecture ablation on SST-2 (layer 16, 32 soft tokens). Cross-attention is essential; naive pooling cannot learn the mapping.}
\label{tab:arch_ablation}
\end{table}

\paragraph{Why Pooling Fails} Mean pooling collapses all token representations into a single vector, destroying sequential structure. The resulting representation cannot distinguish ``great movie'' from ``movie great.''

\paragraph{Why Diffusion Underperforms} Diffusion Transformers \cite{peebles2023dit} achieve only 85.5\% vs.\ the Perceiver's 92.0\% due to: (1) error accumulation across multi-step denoising, and (2) training objective mismatch---diffusion optimizes velocity prediction rather than downstream task performance.

\section{Additional Experimental Details}
\label{app:details}

\subsection{Hardware and Training Time}
All experiments were conducted on NVIDIA H100 80GB GPUs. Training times:
\begin{itemize}
    \item SST-2/AG News (2000 steps): 3.5 minutes
    \item TREC (2000 steps): 3.5 minutes
    \item Banking77 (3000 steps): 5.0 minutes
\end{itemize}
Total training time for all bridge variants: approximately 42 minutes.

\subsection{Multi-Seed Results}

All bridge experiments were run with 5 seeds (42, 123, 456, 789, 1011) for reproducibility. Results reported as mean $\pm$ std:
\begin{itemize}
    \item SST-2 Bridge (32 tokens): 93.7\% $\pm$ 0.3\%
    \item AG News Bridge (32 tokens): 90.7\% $\pm$ 1.4\%
    \item TREC Bridge (32 tokens): 87.9\% $\pm$ 3.3\%
    \item TREC Bridge (8 tokens): 61.5\% $\pm$ 23.3\% (bimodal: 35-86\% range)
    \item Linear Probe (TREC): 95.0\%
    \item Prompt-Tuning (SST-2): 50.9\%
    \item Prompt-Tuning (AG News): 25.0\%
\end{itemize}
The bridge shows stable performance with 32 tokens, but 8 tokens introduces high variance on complex tasks like TREC.

\subsection{Hyperparameter Sensitivity}
We found performance relatively robust to hyperparameters within reasonable ranges:
\begin{itemize}
    \item Learning rate: $10^{-5}$ to $10^{-3}$ all work, $10^{-4}$ slightly best
    \item Batch size: 4-16 similar results
    \item Diversity weight: 0.05-0.2 prevents mode collapse
    \item Source layer: We use layer 16 for SST-2 and layer 31 for AG News/TREC. Preliminary ablations suggest deeper layers contain more task-relevant information for classification.
\end{itemize}

\subsection{Layer Selection}

We extracted hidden states from Llama layer 31 (final layer) for the main experiments. Linear probes on layer 16 achieved strong performance (95\% on TREC), suggesting that task-relevant information is present in intermediate layers. The optimal layer may vary by task; deeper layers tend to contain more task-specific information while earlier layers preserve more general features.

\subsection{Comprehensive Ablation Study}

Table~\ref{tab:ablation_full} presents systematic ablations from earlier experiments exploring hyperparameter sensitivity.

\begin{table}[h]
\caption{Ablation study exploring hyperparameter sensitivity. Results from preliminary SST-2 experiments.}
\label{tab:ablation_full}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{llccc}
\toprule
Parameter & Value & Acc.\ (\%) & Params & Loss \\
\midrule
\multirow{3}{*}{Internal Dim} & 256 & 82.0 & 2.6M & 0.351 \\
 & 512 & 85.0 & 6.3M & 0.331 \\
 & 1024 & \textbf{92.0} & 16.8M & 0.304 \\
\midrule
\multirow{3}{*}{Num Heads} & 4 & \textbf{91.0} & 6.3M & 0.380 \\
 & 8 & 84.5 & 6.3M & 0.385 \\
 & 16 & 84.5 & 6.3M & 0.432 \\
\midrule
\multirow{4}{*}{Source Layer} & 16 & 89.5 & 6.3M & 0.403 \\
 & 24 & 92.5 & 6.3M & 0.376 \\
 & 28 & 89.5 & 6.3M & 0.347 \\
 & 31 & \textbf{94.5} & 6.3M & 0.299 \\
\midrule
\multirow{3}{*}{Depth} & 1 & 87.0 & 5.3M & 0.348 \\
 & 2 & \textbf{90.5} & 6.3M & 0.428 \\
 & 4 & 83.0 & 8.4M & 0.329 \\
\midrule
\multirow{4}{*}{Diversity $\lambda$} & 0.0 & \textbf{91.0} & 6.3M & 0.319 \\
 & 0.05 & 86.5 & 6.3M & 0.319 \\
 & 0.1 & 90.0 & 6.3M & 0.404 \\
 & 0.2 & 85.5 & 6.3M & 0.311 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Key findings: (1) Source layer 31 (final layer) achieves best results (94.5\%), confirming that deeper layers contain more task-relevant information. (2) Larger internal dimensions help (256$\to$1024: +10pp) but with diminishing returns and more parameters. (3) Depth 2 is optimal; depth 4 overfits. (4) Fewer attention heads (4) work better than more (16), possibly due to reduced overfitting. (5) Diversity regularization has mixed effects and may not be necessary.

\section{Model Capacity Requirements}
\label{app:capacity}

\subsection{Empirical Discovery}

Our initial experiments with TinyLlama-1.1B and Qwen2-0.5B revealed a fundamental limitation of soft-prompt methods. Despite achieving excellent training metrics:
\begin{itemize}
\item Training loss: 1.39 (Llama), 1.31 (Qwen)
\item Perplexity on gold answers: 7.65 (Llama), 9.22 (Qwen)
\item Compression ratio: 16.8$\times$ achieved
\end{itemize}

The models produced degenerate outputs during generation when miscalibrated---patterns like ``the of the of the of the'' (TinyLlama) or empty outputs and bizarre number sequences (Qwen).

\subsection{The Calibration Fix Reveals Deeper Issues}

After implementing proper calibration (scaling prefix to match embedding RMS), the outputs became even worse: corrupted tokens, system tokens leaking through, and repetitive number patterns---indicating complete failure to decode the soft prompt.

\subsection{Control Experiment: Zero-Gain Prefix}

To isolate the problem, we conducted a control experiment setting the prefix gain to 0.0, effectively zeroing out the latent information while keeping only anchor text. With the latent information removed entirely, both models generate grammatically correct text. This proves:
\begin{itemize}
\item The models function normally with text prompts
\item The latent representation specifically breaks generation
\item The problem is not generation capability but soft-prompt decoding
\end{itemize}

\subsection{Theoretical Analysis}

The failure stems from insufficient model capacity to decompress the latent representation. For successful decompression from $M$ latent vectors of dimension $d_z$, we hypothesize:
\begin{align}
d_{\text{model}} \geq \alpha \cdot M \cdot d_z
\end{align}
where $\alpha \approx 0.5-1.0$ based on empirical observations.

\subsection{Model Size Thresholds}

Our experiments establish clear capacity thresholds:

\begin{table}[h]
\caption{Generation quality vs.\ model size}
\label{tab:size_threshold}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Model Size & $d_{\text{model}}$ & Generation & F1 Score \\
\midrule
0.5B (Qwen2) & 896 & Degenerate & 0.001 \\
1.1B (TinyLlama) & 2048 & Degenerate & 0.001 \\
3B (Llama-3.2) & 3072 & Coherent & 0.28 \\
7B (Qwen2.5) & 3584 & Fluent & 0.42 \\
8B (Llama-3.1) & 4096 & Fluent & 0.45 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The sharp transition between 1B and 3B models suggests a phase change in capability rather than gradual improvement. Models below this threshold cannot perform the continuous-to-discrete mapping required for generation, regardless of training quality or calibration.

\subsection{Implications for System Design}

These findings establish fundamental constraints for soft-prompt systems:
\begin{enumerate}
\item \textbf{Minimum model requirement:} 3B parameters for basic functionality, 7B+ for production quality
\item \textbf{Compression-capacity tradeoff:} Higher compression ($M$ smaller) requires larger models
\item \textbf{Architecture matters:} Models need sufficient attention dimension, not just total parameters
\item \textbf{Calibration is necessary but not sufficient:} Proper amplitude matching cannot overcome capacity limitations
\end{enumerate}

This explains why prior soft-prompt work predominantly uses larger models (GPT-3, T5-XXL) and why attempts to replicate with smaller models often fail silently.

\end{document}
