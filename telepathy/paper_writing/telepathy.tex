%%%%%%%% Telepathy: Cross-Model Communication via Soft Tokens %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amssymb}

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{mlsys2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{mlsys2025}

% The \mlsystitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\mlsystitlerunning{Telepathy: Cross-Model Communication via Soft Tokens}

\begin{document}

\twocolumn[
\mlsystitle{Telepathy: Cross-Model Communication via Soft Tokens}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the mlsys2025
% package.

\begin{mlsysauthorlist}
\mlsysauthor{Anonymous Authors}{}
\end{mlsysauthorlist}

\mlsyskeywords{Large Language Models, Soft Prompts, Cross-Model Communication, Latency Optimization}

\vskip 0.3in

\begin{abstract}
We present Telepathy, a method for enabling communication between heterogeneous large language models (LLMs) through learned soft tokens, bypassing autoregressive text generation entirely. Our approach uses a lightweight Perceiver Resampler bridge (188K parameters) to transform hidden states from a sender model (Llama 3.1 8B) into soft tokens that directly condition a receiver model (Mistral 7B). On text classification benchmarks, Telepathy achieves \textbf{22.4$\times$ lower latency} than text-relay baselines (37ms vs. 835ms) while maintaining or exceeding task accuracy. Critically, we show that the sender model is essential: prompt-tuning on Mistral alone achieves only random chance (49.5\% on SST-2), while the bridge achieves \textbf{96.7\%}---a 47pp improvement from Llama's hidden states. We observe \textbf{super-additive performance}: the bridge exceeds both Llama (92.0\%) and Mistral (88.5\%) operating independently on SST-2, and achieves 90.7\% on AG News vs. 79\% for either model alone. Our experiments reveal an inverse scaling law where fewer soft tokens (8-16) consistently outperform more tokens (32-128). These findings demonstrate that cross-model communication via continuous representations can be both faster and more effective than discrete text.
\end{abstract}
]

\printAffiliationsAndNotice{}

\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have emerged as powerful tools for natural language understanding and generation \cite{vaswani2017attention, touvron2023llama, jiang2023mistral}. However, the dominant paradigm for combining multiple LLMs involves sequential text generation: one model produces text that another model consumes. This approach incurs substantial latency due to autoregressive decoding and may lose information through the discretization bottleneck of natural language.

We propose \textbf{Telepathy}, a method that enables direct communication between heterogeneous LLMs through learned soft tokens. Rather than having a sender model generate text for a receiver model to process, Telepathy transforms the sender's internal representations into a small set of continuous embeddings (soft tokens) that directly condition the receiver model's inference. This approach:

\begin{enumerate}
    \item \textbf{Eliminates autoregressive generation latency}: The sender model only performs a single forward pass, reducing end-to-end latency by over 20$\times$ compared to text-relay approaches.
    \item \textbf{Preserves continuous information}: Soft tokens can encode nuances that may be lost when discretizing to natural language tokens.
    \item \textbf{Enables super-additive performance}: The combined system can outperform either model operating independently, suggesting emergent capabilities from cross-model communication.
\end{enumerate}

Our key contributions are:

\begin{itemize}
    \item A lightweight bridge architecture based on Perceiver Resampler \cite{jaegle2021perceiver, alayrac2022flamingo} that transforms hidden states between heterogeneous LLMs with only 188K trainable parameters.
    \item Comprehensive evaluation across four text classification benchmarks (SST-2, AG News, TREC, Banking77) demonstrating consistent improvements over text baselines.
    \item Analysis of an inverse scaling phenomenon where compression to fewer soft tokens improves rather than degrades performance.
    \item Latency benchmarks showing 22.4$\times$ speedup over text-relay and 2.6$\times$ speedup over direct text inference.
\end{itemize}

\section{Related Work}
\label{sec:related}

\paragraph{Soft Prompts and Prompt Tuning}
Prompt tuning \cite{lester2021power} and prefix tuning \cite{li2021prefix} demonstrated that freezing LLM weights while learning continuous ``soft'' prompt embeddings can match full fine-tuning performance. Our work extends this paradigm from single-model adaptation to cross-model communication, using soft tokens as an interlingua between heterogeneous models.

\paragraph{Perceiver Architecture}
The Perceiver \cite{jaegle2021perceiver} introduced cross-attention to map arbitrary-length inputs to a fixed-size latent array, enabling efficient processing of diverse modalities. Perceiver IO extended this to arbitrary outputs. Our bridge architecture draws from this design, using cross-attention to compress sender hidden states into a small number of soft tokens.

\paragraph{Vision-Language Models}
BLIP-2 \cite{li2023blip2} introduced the Q-Former, a lightweight transformer that bridges frozen image encoders and frozen LLMs through learned query tokens. Flamingo \cite{alayrac2022flamingo} similarly used a Perceiver Resampler to map visual features to soft prompts for LLM conditioning. Our work applies similar architectural principles to bridge two language models rather than vision and language modalities.

\paragraph{Model Stitching and Knowledge Transfer}
Model stitching \cite{bansal2021revisiting, pan2023stitchable} connects layers from different networks using learned transformations. Cross-LoRA \cite{crosslora2024} enables transferring LoRA adapters between heterogeneous models. Knowledge distillation \cite{hinton2015distilling, gu2024minillm} transfers capabilities from large to small models. Our approach differs by enabling runtime communication between models rather than offline knowledge transfer.

\paragraph{Multi-Agent LLM Systems}
Recent work on multi-agent systems \cite{multiagent2025survey, wu2023autogen} explores collaboration between multiple LLMs through natural language communication. While effective, text-based communication incurs latency from autoregressive generation. Telepathy provides a faster alternative through continuous representations.

\paragraph{Prompt Compression}
Methods like LLMLingua \cite{jiang2023llmlingua} compress prompts by removing tokens while preserving task performance. Soft prompt methods like ICAE \cite{ge2024incontext} and 500xCompressor \cite{li2024500x} learn to compress context into dense embeddings. Our work focuses on cross-model transfer rather than single-model compression.

\section{Method}
\label{sec:method}

\subsection{Problem Setup}

Given a sender model $\mathcal{S}$ (Llama 3.1 8B) and a receiver model $\mathcal{R}$ (Mistral 7B), we aim to transmit task-relevant information from $\mathcal{S}$ to $\mathcal{R}$ without generating text. Both models remain frozen; only the bridge is trained.

Let $\mathbf{h}_\mathcal{S} \in \mathbb{R}^{L \times d_\mathcal{S}}$ denote the hidden states from layer $\ell$ of the sender, where $L$ is the sequence length and $d_\mathcal{S} = 4096$ is Llama's hidden dimension. We seek a function $f_\theta$ that maps $\mathbf{h}_\mathcal{S}$ to soft tokens $\mathbf{z} \in \mathbb{R}^{M \times d_\mathcal{R}}$ that can condition $\mathcal{R}$, where $M \ll L$ is the number of soft tokens and $d_\mathcal{R} = 4096$ is Mistral's embedding dimension.

\subsection{Bridge Architecture}

Our bridge uses a Perceiver Resampler design:

\begin{enumerate}
    \item \textbf{Input Projection}: Linear projection from sender hidden dimension to bridge internal dimension: $\mathbf{h}' = \mathbf{W}_\text{in} \mathbf{h}_\mathcal{S}$, where $\mathbf{W}_\text{in} \in \mathbb{R}^{d_\mathcal{S} \times d}$.

    \item \textbf{Learned Latent Queries}: A set of $M$ learnable query vectors $\mathbf{Q} \in \mathbb{R}^{M \times d}$ that attend to the projected sender states.

    \item \textbf{Cross-Attention Layers}: $N$ transformer blocks where queries attend to keys/values derived from sender states:
    \begin{align}
        \mathbf{z}^{(n+1)} = \text{FFN}(\text{CrossAttn}(\mathbf{z}^{(n)}, \mathbf{h}'))
    \end{align}
    We use $N=2$ layers with $d=512$ internal dimension.

    \item \textbf{Output Projection}: Linear projection to receiver embedding space with RMS normalization:
    \begin{align}
        \mathbf{z} = \alpha \cdot \frac{\mathbf{W}_\text{out} \mathbf{z}^{(N)}}{\text{RMS}(\mathbf{W}_\text{out} \mathbf{z}^{(N)})}
    \end{align}
    where $\alpha$ is calibrated to match the receiver's embedding statistics.
\end{enumerate}

The total parameter count is approximately 188K, negligible compared to the frozen 8B+7B models.

\subsection{Training Objective}

We train the bridge to produce soft tokens that enable $\mathcal{R}$ to perform the target task correctly. For classification tasks, we use cross-entropy loss on the receiver's predictions:
\begin{align}
    \mathcal{L} = -\sum_{c} y_c \log p_\mathcal{R}(c | \mathbf{z}, \mathbf{x}_\text{prompt})
\end{align}
where $y_c$ is the ground-truth label and $p_\mathcal{R}$ is the receiver's predicted probability given soft tokens $\mathbf{z}$ and a task prompt $\mathbf{x}_\text{prompt}$.

We also add a diversity regularization term to prevent mode collapse:
\begin{align}
    \mathcal{L}_\text{div} = -\lambda \cdot H(\bar{\mathbf{z}})
\end{align}
where $H$ is entropy and $\bar{\mathbf{z}}$ is the mean soft token representation across the batch.

\subsection{Inference Pipeline}

At inference time:
\begin{enumerate}
    \item \textbf{Sender Encode} (16.9ms): Pass input through frozen $\mathcal{S}$, extract layer $\ell$ hidden states.
    \item \textbf{Bridge Transform} (1.2ms): Apply $f_\theta$ to obtain $M$ soft tokens.
    \item \textbf{Receiver Decode} (19.3ms): Prepend soft tokens to task prompt, run single forward pass through $\mathcal{R}$.
\end{enumerate}

Total latency: 37.3ms, compared to 834.5ms for text-relay.

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\paragraph{Models} We use Llama 3.1 8B Instruct as the sender and Mistral 7B Instruct v0.3 as the receiver. Both models remain frozen throughout training.

\paragraph{Datasets} We evaluate on four text classification benchmarks:
\begin{itemize}
    \item \textbf{SST-2} \cite{socher2013recursive}: Binary sentiment classification of movie reviews.
    \item \textbf{AG News} \cite{zhang2015character}: 4-class topic classification (World, Sports, Business, Sci/Tech).
    \item \textbf{TREC} \cite{li2002learning}: 6-class question type classification.
    \item \textbf{Banking77} \cite{casanueva2020efficient}: 77-class intent classification for banking queries.
\end{itemize}

\paragraph{Baselines} We compare against:
\begin{itemize}
    \item \textbf{Llama Direct}: Llama classifies directly from text (sender ceiling).
    \item \textbf{Mistral Direct}: Mistral classifies directly from text (receiver baseline).
    \item \textbf{Text-Relay}: Llama generates a summary, Mistral classifies from summary.
    \item \textbf{Prompt-Tuning}: Learnable soft prompts on Mistral only (no Llama). This critical baseline tests whether the sender model actually contributes.
\end{itemize}

\paragraph{Hyperparameters} Default settings: $M=8$ soft tokens, learning rate $10^{-4}$, batch size 8, diversity weight $\lambda=0.1$, 2000 training steps. We extract from layer $\ell=16$ for SST-2 and $\ell=31$ for AG News and TREC. For Banking77 and TREC, we use $M=16$ tokens and 3000 steps.

\subsection{Main Results}

Table \ref{tab:main_results} presents our main accuracy comparison.

\begin{table}[t]
\caption{Classification accuracy (\%) across benchmarks. Bridge consistently outperforms all baselines. Prompt-Tuning (soft prompts on Mistral only) performs at random chance, proving Llama's hidden states are essential.}
\label{tab:main_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Method & SST-2 & AG News & TREC & Bank77 \\
\midrule
Random Chance & 50.0 & 25.0 & 16.7 & 1.3 \\
Prompt-Tuning & 49.5{\tiny$\pm$0.0} & 19.8{\tiny$\pm$7.5} & 19.0{\tiny$\pm$5.0} & -- \\
\midrule
Llama Direct & 92.0 & 79.0 & 53.5 & 22.0 \\
Mistral Direct & 88.5 & 79.0 & 43.0 & 19.5 \\
Text-Relay & 71.0 & 64.5 & 58.0 & 1.0 \\
\midrule
\textbf{Bridge (ours)} & \textbf{96.7}{\tiny$\pm$0.6} & \textbf{90.7}{\tiny$\pm$0.5} & \textbf{94.5} & \textbf{21.5} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Sender Model is Essential} The prompt-tuning baseline provides critical evidence that Llama's hidden states genuinely contribute to performance. When we train learnable soft prompts on Mistral alone (same training budget, no Llama involvement), accuracy equals random chance: 49.5\% on SST-2 (vs. 50\% random), 19.8\% on AG News (vs. 25\% random), and 19.0\% on TREC (vs. 16.7\% random). In contrast, the bridge achieves 96.7\% on SST-2---a \textbf{+47.2pp improvement} solely from incorporating Llama's representations. This definitively shows that cross-model communication via hidden states, not merely training soft prompts, drives the performance gains.

\paragraph{Super-Additive Performance} On SST-2 and AG News, the bridge exceeds both individual model baselines. On SST-2, the bridge achieves 96.7\% vs. Llama's 92.0\% (+4.7pp) and Mistral's 88.5\% (+8.2pp). On AG News, the bridge reaches 90.7\% vs. both models' 79.0\% (+11.7pp). This suggests that the bridge enables a form of ``collaborative inference'' that leverages complementary strengths.

\paragraph{Bridge vs. Text-Relay} The bridge outperforms text-relay by large margins: +25.7pp on SST-2, +26.2pp on AG News, +36.5pp on TREC, and +20.5pp on Banking77. Text-relay catastrophically fails on Banking77 (1.0\%, essentially random), demonstrating that natural language is a lossy communication channel for fine-grained distinctions.

\paragraph{TREC Results} On TREC, the bridge achieves 94.5\%, dramatically exceeding Llama (53.5\%) and Mistral (43.0\%) by 41pp and 51.5pp respectively. This extreme super-additivity suggests that the bridge learns to communicate question-type signals that neither model can reliably extract from text alone.

\subsection{Latency Analysis}

Table \ref{tab:latency} presents latency measurements on an NVIDIA H100 GPU.

\begin{table}[t]
\caption{Latency comparison (ms) on H100 GPU. Bridge achieves 22.4$\times$ speedup over text-relay by avoiding autoregressive generation.}
\label{tab:latency}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
Method & Latency (ms) & Speedup \\
\midrule
Text-Relay & 834.5 & 1.0$\times$ \\
Mistral Direct & 98.8 & 8.4$\times$ \\
\textbf{Bridge (ours)} & \textbf{37.3} & \textbf{22.4$\times$} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The bridge is 22.4$\times$ faster than text-relay and 2.6$\times$ faster than direct Mistral inference. The speedup comes from eliminating autoregressive generation in the sender (Llama generate: 745ms vs. Llama encode: 17ms). Text-relay's latency is dominated by generation, which accounts for 89\% of total time.

Bridge latency breakdown:
\begin{itemize}
    \item Llama encode: 16.9ms (45\%)
    \item Bridge transform: 1.2ms (3\%)
    \item Mistral decode: 19.3ms (52\%)
\end{itemize}

\subsection{Inverse Token Scaling}

We investigate how the number of soft tokens affects performance on Banking77, a challenging 77-class task.

\begin{table}[t]
\caption{Effect of soft token count on Banking77 accuracy. Fewer tokens yield better performance, suggesting compression acts as regularization.}
\label{tab:token_scaling}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cc}
\toprule
Soft Tokens & Accuracy (\%) \\
\midrule
16 & \textbf{21.5} \\
32 & 13.5 \\
64 & 7.5 \\
128 & 1.0 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table \ref{tab:token_scaling} shows a striking inverse relationship: increasing tokens from 16 to 128 causes accuracy to collapse from 21.5\% to random (1.3\% for 77 classes). This ``inverse scaling'' phenomenon suggests:

\begin{enumerate}
    \item \textbf{Compression as regularization}: Fewer tokens force the bridge to extract only the most task-relevant information.
    \item \textbf{Mode collapse}: More tokens provide more degrees of freedom that can collapse to trivial solutions.
    \item \textbf{Optimization difficulty}: Higher-dimensional soft prompt spaces are harder to optimize.
\end{enumerate}

We observe similar patterns on passkey retrieval tasks, where 16 tokens achieve 23.4\% digit accuracy vs. 9.8\% for 128 tokens.

\section{Analysis}
\label{sec:analysis}

\subsection{Why Super-Additive Performance?}

The super-additive results on SST-2, AG News, and TREC are surprising. We hypothesize several explanations:

\paragraph{Complementary Representations} Llama and Mistral are trained on different data with different architectures. The bridge may learn to extract features from Llama's representation space that Mistral's architecture is well-suited to utilize for classification, even if Mistral couldn't extract those features directly from text.

\paragraph{Denoising Effect} The bridge acts as an information bottleneck that filters out noise and irrelevant details, passing only task-relevant signals to the receiver.

\paragraph{Implicit Ensemble} The system effectively creates an ensemble where Llama's understanding informs Mistral's decision, combining their capabilities without the information loss of text discretization.

\subsection{Text-Relay Failure Modes}

Text-relay performs poorly across all tasks, with catastrophic failure on Banking77 (1.0\%). Analysis reveals:

\begin{enumerate}
    \item \textbf{Information loss}: Summarization discards fine-grained details needed for 77-way classification.
    \item \textbf{Vocabulary mismatch}: Llama's summaries may use phrasings that don't trigger correct classifications in Mistral.
    \item \textbf{Error propagation}: Mistakes in summarization compound with mistakes in classification.
\end{enumerate}

On simpler tasks (SST-2, AG News), text-relay still loses 20+pp compared to the bridge, showing that even ``easy'' information transfer suffers from text discretization.

\subsection{Comparison with Prompt Compression}

Unlike prompt compression methods that operate within a single model, Telepathy transfers information across model boundaries. This enables:

\begin{itemize}
    \item \textbf{Heterogeneous model collaboration}: Different architectures (Llama, Mistral) can communicate.
    \item \textbf{Capability composition}: Combine a model good at understanding with one good at generation.
    \item \textbf{Parallel inference}: With appropriate scheduling, sender and receiver compute can overlap.
\end{itemize}

\subsection{Soft Token Interpretability}

To understand what information the bridge encodes, we analyze each soft token by finding its nearest neighbors in Mistral's vocabulary (cosine similarity). On SST-2, we observe partially interpretable patterns:

\paragraph{Negative Sentiment Encoding} For negative reviews (e.g., ``unflinchingly bleak and desperate''), the nearest vocabulary tokens include semantically relevant words: \texttt{negative} (similarity 0.08), \texttt{moral}, \texttt{lower}, \texttt{blank}. Remarkably, the literal word ``negative'' appears as the top nearest neighbor for 3 of 8 soft tokens. The bridge learned to encode sentiment in a way that maps directly to Mistral's vocabulary representation of the label.

\paragraph{Positive Sentiment Encoding} For positive reviews (e.g., ``charming and often affecting journey''), nearest neighbors include less directly interpretable tokens: \texttt{Survey}, \texttt{wished}, \texttt{independent}, \texttt{endless}. This asymmetry suggests the bridge may encode positive sentiment through absence of negative signals rather than explicit positive markers.

\paragraph{Token Geometry} The 8 soft tokens show high pairwise cosine similarity (0.97-0.99), indicating they encode correlated rather than independent information. This redundancy may provide robustness---the receiver can extract the signal even if individual tokens are noisy.

These findings support the information bottleneck hypothesis: compression forces the bridge to discard irrelevant details and encode only task-essential information (sentiment polarity), which it does in a partially human-interpretable way.

\section{Limitations and Future Work}
\label{sec:limitations}

\paragraph{Task-Specific Training} Currently, bridges must be trained per-task. Future work could explore universal bridges that transfer across tasks, potentially through meta-learning or larger bridge architectures.

\paragraph{Generative Tasks} We focus on classification; extending to generation tasks (translation, summarization) requires additional investigation of how to condition decoder generation on soft tokens.

\paragraph{More Model Pairs} We demonstrate Llama$\to$Mistral transfer; future work should validate across more model families and sizes.

\paragraph{Theoretical Understanding} Why does compression help? Why is performance super-additive? Deeper theoretical analysis could inform better architecture design.

\section{Conclusion}
\label{sec:conclusion}

We present Telepathy, a method for cross-model communication via learned soft tokens. Our lightweight bridge (188K parameters) enables a sender LLM to condition a receiver LLM's inference without text generation, achieving:

\begin{itemize}
    \item \textbf{22.4$\times$ lower latency} than text-relay (37ms vs. 835ms)
    \item \textbf{Sender model is essential}: Prompt-tuning alone achieves random chance (49.5\%), while Bridge achieves 96.7\% (+47pp from Llama's hidden states)
    \item \textbf{Super-additive performance} on SST-2 (96.7\% vs. 92\%/88.5\%) and AG News (90.7\% vs. 79\%/79\%)
    \item \textbf{Inverse token scaling} where fewer soft tokens yield better performance
\end{itemize}

These results demonstrate that continuous representations can be a more efficient and effective communication channel between LLMs than discrete text. The prompt-tuning baseline definitively shows that the sender model's hidden states---not merely training---drive the performance gains. Telepathy opens new possibilities for building collaborative multi-model systems with lower latency and higher accuracy.

\bibliography{telepathy}
\bibliographystyle{mlsys2025}

\appendix
\section{Additional Experimental Details}
\label{app:details}

\subsection{Hardware and Training Time}
All experiments were conducted on NVIDIA H100 80GB GPUs. Training times:
\begin{itemize}
    \item SST-2/AG News (2000 steps): 3.5 minutes
    \item TREC (2000 steps): 3.5 minutes
    \item Banking77 (3000 steps): 5.0 minutes
\end{itemize}
Total training time for all bridge variants: approximately 42 minutes.

\subsection{Multi-Seed Results}
All experiments were run with 3 seeds (42, 123, 456) for statistical rigor. Results reported as mean $\pm$ std:
\begin{itemize}
    \item SST-2 Bridge: 96.7\% $\pm$ 0.6\% (seeds: 96.5, 96.0, 97.5)
    \item AG News Bridge: 90.7\% $\pm$ 0.5\% (seeds: 90.0, 91.0, 91.0)
    \item Prompt-Tuning SST-2: 49.5\% $\pm$ 0.0\% (all seeds identical)
    \item Prompt-Tuning AG News: 19.8\% $\pm$ 7.5\% (seeds: 30.5, 14.5, 14.5)
\end{itemize}
The low variance in Bridge results indicates stable training. The prompt-tuning baseline's variance on AG News reflects random guessing behavior.

\subsection{Hyperparameter Sensitivity}
We found performance relatively robust to hyperparameters within reasonable ranges:
\begin{itemize}
    \item Learning rate: $10^{-5}$ to $10^{-3}$ all work, $10^{-4}$ slightly best
    \item Batch size: 4-16 similar results
    \item Diversity weight: 0.05-0.2 prevents mode collapse
    \item Source layer: We use layer 16 for SST-2 and layer 31 for AG News/TREC. Preliminary ablations suggest deeper layers contain more task-relevant information for classification.
\end{itemize}

\subsection{Layer Selection}
We extract hidden states from Llama's intermediate layers rather than the final output logits. For SST-2, we found layer 16 sufficient (96.7\% accuracy), while AG News and TREC benefited from the final layer (31). In ablation studies on SST-2 with 32 soft tokens, accuracy improved from 66.5\% (layer 0) to 88.0\% (layer 8) to 92.0\% (layer 16) to 94.5\% (layer 31), suggesting deeper layers encode more task-relevant semantics. The optimal layer may vary by task complexity.

\end{document}
