%%%%%%%% Telepathy: Cross-Model Communication via Soft Tokens %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath}
\usepackage{amssymb}

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{mlsys2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{mlsys2025}

% The \mlsystitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\mlsystitlerunning{Telepathy: Cross-Model Communication via Soft Tokens}

\begin{document}

\twocolumn[
\mlsystitle{Telepathy: Cross-Model Communication via Soft Tokens}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the mlsys2025
% package.

\begin{mlsysauthorlist}
\mlsysauthor{Anonymous Authors}{}
\end{mlsysauthorlist}

\mlsyskeywords{Large Language Models, Soft Prompts, Cross-Model Communication, Latency Optimization}

\vskip 0.3in

\begin{abstract}
We present Telepathy, a method for enabling communication between heterogeneous large language models (LLMs) through learned soft tokens, bypassing autoregressive text generation entirely. Our approach uses a lightweight Perceiver Resampler bridge (188K parameters) to transform hidden states from a sender model (Llama 3.1 8B) into soft tokens that directly condition a receiver model (Mistral 7B). On text classification benchmarks, Telepathy achieves \textbf{22.4$\times$ lower latency} than text-relay baselines while exceeding task accuracy. The bridge outperforms stronger baselines including 5-shot prompting (+2.2--59pp), LoRA fine-tuning (+1.4pp with 18$\times$ fewer parameters), and chain-of-thought relay (+7.7pp while being 85$\times$ faster). Critically, prompt-tuning on Mistral alone achieves only random chance (49.5\% on SST-2), while the bridge achieves \textbf{96.7\%}---proving the sender model's hidden states are essential. We observe \textbf{super-additive performance}: the bridge exceeds both Llama (92.0\%) and Mistral (88.5\%) operating independently on SST-2, and achieves 90.7\% on AG News vs. 79\% for either model alone. These findings demonstrate that cross-model communication via continuous representations can be both faster and more effective than discrete text for classification tasks.
\end{abstract}
]

\printAffiliationsAndNotice{}

\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have emerged as powerful tools for natural language understanding and generation \cite{vaswani2017attention, touvron2023llama, jiang2023mistral}. However, the dominant paradigm for combining multiple LLMs involves sequential text generation: one model produces text that another model consumes. This approach incurs substantial latency due to autoregressive decoding and may lose information through the discretization bottleneck of natural language.

We propose \textbf{Telepathy}, a method that enables direct communication between heterogeneous LLMs through learned soft tokens. Rather than having a sender model generate text for a receiver model to process, Telepathy transforms the sender's internal representations into a small set of continuous embeddings (soft tokens) that directly condition the receiver model's inference. This approach:

\begin{enumerate}
    \item \textbf{Eliminates autoregressive generation latency}: The sender model only performs a single forward pass, reducing end-to-end latency by over 20$\times$ compared to text-relay approaches.
    \item \textbf{Preserves continuous information}: Soft tokens can encode nuances that may be lost when discretizing to natural language tokens.
    \item \textbf{Enables super-additive performance}: The combined system can outperform either model operating independently, suggesting emergent capabilities from cross-model communication.
\end{enumerate}

Our key contributions are:

\begin{itemize}
    \item A lightweight bridge architecture based on Perceiver Resampler \cite{jaegle2021perceiver, alayrac2022flamingo} that transforms hidden states between heterogeneous LLMs with only 188K trainable parameters.
    \item Comprehensive evaluation against strong baselines (5-shot prompting, LoRA, chain-of-thought) showing the bridge outperforms all approaches in accuracy and/or efficiency.
    \item Analysis of an inverse scaling phenomenon where compression to fewer soft tokens improves rather than degrades performance.
    \item Latency and throughput benchmarks showing 22--85$\times$ speedup over text-based communication, scaling to 100+ samples/second.
\end{itemize}

\section{Related Work}
\label{sec:related}

\paragraph{Soft Prompts and Prompt Tuning}
Prompt tuning \cite{lester2021power} and prefix tuning \cite{li2021prefix} demonstrated that freezing LLM weights while learning continuous ``soft'' prompt embeddings can match full fine-tuning performance. Our work extends this paradigm from single-model adaptation to cross-model communication, using soft tokens as an interlingua between heterogeneous models.

\paragraph{Perceiver Architecture}
The Perceiver \cite{jaegle2021perceiver} introduced cross-attention to map arbitrary-length inputs to a fixed-size latent array, enabling efficient processing of diverse modalities. Perceiver IO extended this to arbitrary outputs. Our bridge architecture draws from this design, using cross-attention to compress sender hidden states into a small number of soft tokens.

\paragraph{Vision-Language Models}
BLIP-2 \cite{li2023blip2} introduced the Q-Former, a lightweight transformer that bridges frozen image encoders and frozen LLMs through learned query tokens. Flamingo \cite{alayrac2022flamingo} similarly used a Perceiver Resampler to map visual features to soft prompts for LLM conditioning. Our work applies similar architectural principles to bridge two language models rather than vision and language modalities.

\paragraph{Model Stitching and Knowledge Transfer}
Model stitching \cite{bansal2021revisiting, pan2023stitchable} connects layers from different networks using learned transformations. Cross-LoRA \cite{crosslora2024} enables transferring LoRA adapters between heterogeneous models. Knowledge distillation \cite{hinton2015distilling, gu2024minillm} transfers capabilities from large to small models. Our approach differs by enabling runtime communication between models rather than offline knowledge transfer.

\paragraph{Multi-Agent LLM Systems}
Recent work on multi-agent systems \cite{multiagent2025survey, wu2023autogen} explores collaboration between multiple LLMs through natural language communication. While effective, text-based communication incurs latency from autoregressive generation. Telepathy provides a faster alternative through continuous representations.

\paragraph{Prompt Compression}
Methods like LLMLingua \cite{jiang2023llmlingua} compress prompts by removing tokens while preserving task performance. Soft prompt methods like ICAE \cite{ge2024incontext} and 500xCompressor \cite{li2024500x} learn to compress context into dense embeddings. Our work focuses on cross-model transfer rather than single-model compression.

\section{Method}
\label{sec:method}

\subsection{Problem Setup}

Given a sender model $\mathcal{S}$ (Llama 3.1 8B) and a receiver model $\mathcal{R}$ (Mistral 7B), we aim to transmit task-relevant information from $\mathcal{S}$ to $\mathcal{R}$ without generating text. Both models remain frozen; only the bridge is trained.

Let $\mathbf{h}_\mathcal{S} \in \mathbb{R}^{L \times d_\mathcal{S}}$ denote the hidden states from layer $\ell$ of the sender, where $L$ is the sequence length and $d_\mathcal{S} = 4096$ is Llama's hidden dimension. We seek a function $f_\theta$ that maps $\mathbf{h}_\mathcal{S}$ to soft tokens $\mathbf{z} \in \mathbb{R}^{M \times d_\mathcal{R}}$ that can condition $\mathcal{R}$, where $M \ll L$ is the number of soft tokens and $d_\mathcal{R} = 4096$ is Mistral's embedding dimension. Figure~\ref{fig:architecture} illustrates the overall pipeline.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/architecture.pdf}
\caption{Telepathy architecture. Input text is processed by the frozen sender (Llama), whose hidden states are transformed by the lightweight bridge (188K params) into soft tokens that condition the frozen receiver (Mistral) for classification.}
\label{fig:architecture}
\end{figure}

\subsection{Bridge Architecture}

Our bridge uses a Perceiver Resampler design:

\begin{enumerate}
    \item \textbf{Input Projection}: Linear projection from sender hidden dimension to bridge internal dimension: $\mathbf{h}' = \mathbf{W}_\text{in} \mathbf{h}_\mathcal{S}$, where $\mathbf{W}_\text{in} \in \mathbb{R}^{d_\mathcal{S} \times d}$.

    \item \textbf{Learned Latent Queries}: A set of $M$ learnable query vectors $\mathbf{Q} \in \mathbb{R}^{M \times d}$ that attend to the projected sender states.

    \item \textbf{Cross-Attention Layers}: $N$ transformer blocks where queries attend to keys/values derived from sender states:
    \begin{align}
        \mathbf{z}^{(n+1)} = \text{FFN}(\text{CrossAttn}(\mathbf{z}^{(n)}, \mathbf{h}'))
    \end{align}
    We use $N=2$ layers with $d=512$ internal dimension.

    \item \textbf{Output Projection}: Linear projection to receiver embedding space with RMS normalization:
    \begin{align}
        \mathbf{z} = \alpha \cdot \frac{\mathbf{W}_\text{out} \mathbf{z}^{(N)}}{\text{RMS}(\mathbf{W}_\text{out} \mathbf{z}^{(N)})}
    \end{align}
    where $\alpha$ is calibrated to match the receiver's embedding statistics.
\end{enumerate}

The total parameter count is approximately 188K, negligible compared to the frozen 8B+7B models.

\subsection{Training Objective}

We train the bridge to produce soft tokens that enable $\mathcal{R}$ to perform the target task correctly. For classification tasks, we use cross-entropy loss on the receiver's predictions:
\begin{align}
    \mathcal{L} = -\sum_{c} y_c \log p_\mathcal{R}(c | \mathbf{z}, \mathbf{x}_\text{prompt})
\end{align}
where $y_c$ is the ground-truth label and $p_\mathcal{R}$ is the receiver's predicted probability given soft tokens $\mathbf{z}$ and a task prompt $\mathbf{x}_\text{prompt}$.

We also add a diversity regularization term to prevent mode collapse:
\begin{align}
    \mathcal{L}_\text{div} = -\lambda \cdot H(\bar{\mathbf{z}})
\end{align}
where $H$ is entropy and $\bar{\mathbf{z}}$ is the mean soft token representation across the batch.

\subsection{Inference Pipeline}

At inference time:
\begin{enumerate}
    \item \textbf{Sender Encode} (16.9ms): Pass input through frozen $\mathcal{S}$, extract layer $\ell$ hidden states.
    \item \textbf{Bridge Transform} (1.2ms): Apply $f_\theta$ to obtain $M$ soft tokens.
    \item \textbf{Receiver Decode} (19.3ms): Prepend soft tokens to task prompt, run single forward pass through $\mathcal{R}$.
\end{enumerate}

Total latency: 37.3ms, compared to 834.5ms for text-relay.

\section{Experiments}
\label{sec:experiments}

\subsection{Setup}

\paragraph{Models} We use Llama 3.1 8B Instruct as the sender and Mistral 7B Instruct v0.3 as the receiver. Both models remain frozen throughout training.

\paragraph{Datasets} We evaluate on four text classification benchmarks:
\begin{itemize}
    \item \textbf{SST-2} \cite{socher2013recursive}: Binary sentiment classification of movie reviews.
    \item \textbf{AG News} \cite{zhang2015character}: 4-class topic classification (World, Sports, Business, Sci/Tech).
    \item \textbf{TREC} \cite{li2002learning}: 6-class question type classification.
    \item \textbf{Banking77} \cite{casanueva2020efficient}: 77-class intent classification for banking queries.
\end{itemize}

\paragraph{Baselines} We compare against:
\begin{itemize}
    \item \textbf{Llama/Mistral Direct}: Each model classifies directly from text (zero-shot).
    \item \textbf{5-shot Prompting}: Standard few-shot prompting with 5 balanced examples per class.
    \item \textbf{Text-Relay}: Llama generates a summary, Mistral classifies from summary.
    \item \textbf{CoT-Relay}: Llama generates chain-of-thought reasoning, Mistral classifies from that reasoning.
    \item \textbf{LoRA}: Fine-tuned Mistral with rank-8 LoRA adapter (3.4M params).
    \item \textbf{Prompt-Tuning}: Learnable soft prompts on Mistral only (no Llama). Tests whether the sender actually contributes.
\end{itemize}

\paragraph{Hyperparameters} Default settings: $M=8$ soft tokens, learning rate $10^{-4}$, batch size 8, diversity weight $\lambda=0.1$, 2000 training steps. We extract from layer $\ell=16$ for SST-2 and $\ell=31$ for AG News and TREC. For Banking77 and TREC, we use $M=16$ tokens and 3000 steps.

\subsection{Main Results}

Table \ref{tab:main_results} presents our main accuracy comparison.

\begin{table}[t]
\caption{Classification accuracy (\%) across benchmarks. Bridge outperforms all baselines including few-shot prompting. Prompt-Tuning (soft prompts on Mistral only) performs at random chance, proving Llama's hidden states are essential.}
\label{tab:main_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccc}
\toprule
Method & SST-2 & AG News & TREC & Bank77 \\
\midrule
Random Chance & 50.0 & 25.0 & 16.7 & 1.3 \\
Prompt-Tuning & 49.5{\tiny$\pm$0.0} & 19.8{\tiny$\pm$7.5} & 19.0{\tiny$\pm$5.0} & -- \\
\midrule
Llama 0-shot & 92.0 & 79.0 & 53.5 & 22.0 \\
Mistral 0-shot & 88.5 & 79.0 & 43.0 & 19.5 \\
Llama 5-shot & 94.3{\tiny$\pm$0.2} & 62.0{\tiny$\pm$3.6} & 32.0{\tiny$\pm$0.0} & -- \\
Mistral 5-shot & 94.5{\tiny$\pm$1.1} & 80.3{\tiny$\pm$1.7} & 36.0{\tiny$\pm$0.0} & -- \\
Text-Relay & 71.0 & 64.5 & 58.0 & 1.0 \\
\midrule
\textbf{Bridge (ours)} & \textbf{96.7}{\tiny$\pm$0.6} & \textbf{90.7}{\tiny$\pm$0.5} & \textbf{95.3}{\tiny$\pm$0.3} & \textbf{21.5} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Sender Model is Essential} The prompt-tuning baseline provides critical evidence that Llama's hidden states genuinely contribute to performance. When we train learnable soft prompts on Mistral alone (same training budget, no Llama involvement), accuracy equals random chance: 49.5\% on SST-2 (vs. 50\% random), 19.8\% on AG News (vs. 25\% random), and 19.0\% on TREC (vs. 16.7\% random). In contrast, the bridge achieves 96.7\% on SST-2---a \textbf{+47.2pp improvement} solely from incorporating Llama's representations. This definitively shows that cross-model communication via hidden states, not merely training soft prompts, drives the performance gains. Figure~\ref{fig:sender_essential} visualizes this critical finding.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/sender_essential.pdf}
\caption{Bridge vs.\ Prompt-Tuning: The sender model is essential. Without Llama's hidden states, prompt-tuning on Mistral alone achieves only random chance (red markers). The bridge's +47pp improvement on SST-2 comes entirely from cross-model communication.}
\label{fig:sender_essential}
\end{figure}

\paragraph{Super-Additive Performance} On SST-2 and AG News, the bridge exceeds both individual model baselines. On SST-2, the bridge achieves 96.7\% vs. Llama's 92.0\% (+4.7pp) and Mistral's 88.5\% (+8.2pp). On AG News, the bridge reaches 90.7\% vs. both models' 79.0\% (+11.7pp). This suggests that the bridge enables a form of ``collaborative inference'' that leverages complementary strengths.

\paragraph{Bridge vs. Few-Shot Prompting} A key question is whether the bridge merely provides implicit few-shot learning through training. Table~\ref{tab:main_results} shows the bridge outperforms 5-shot prompting on all datasets: +2.2pp on SST-2 (96.7\% vs.\ 94.5\%), +10.4pp on AG News (90.7\% vs.\ 80.3\%), and \textbf{+59.3pp on TREC} (95.3\% vs.\ 36.0\%). The TREC result is particularly striking: while few-shot prompting barely improves over zero-shot (32-36\% vs.\ 43-54\%), the bridge achieves 95.3\%---demonstrating that the bridge captures task-relevant signals that few-shot examples cannot provide.

\paragraph{Bridge vs. Text-Relay} The bridge outperforms text-relay by large margins: +25.7pp on SST-2, +26.2pp on AG News, +37.3pp on TREC, and +20.5pp on Banking77. Text-relay catastrophically fails on Banking77 (1.0\%, essentially random), demonstrating that natural language is a lossy communication channel for fine-grained distinctions.

\paragraph{TREC Results} On TREC, the bridge achieves 95.3\% $\pm$ 0.3\%, dramatically exceeding Llama (53.5\%) and Mistral (43.0\%) by 41.8pp and 52.3pp respectively. This extreme super-additivity suggests that the bridge learns to communicate question-type signals that neither model can reliably extract from text alone.

\subsection{Latency Analysis}

Table \ref{tab:latency} presents latency measurements on an NVIDIA H100 GPU.

\begin{table}[t]
\caption{Latency comparison (ms) on H100 GPU. Bridge achieves 22.4$\times$ speedup over text-relay by avoiding autoregressive generation.}
\label{tab:latency}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
Method & Latency (ms) & Speedup \\
\midrule
Text-Relay & 834.5 & 1.0$\times$ \\
Mistral Direct & 98.8 & 8.4$\times$ \\
\textbf{Bridge (ours)} & \textbf{37.3} & \textbf{22.4$\times$} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The bridge is 22.4$\times$ faster than text-relay and 2.6$\times$ faster than direct Mistral inference. The speedup comes from eliminating autoregressive generation in the sender (Llama generate: 745ms vs. Llama encode: 17ms). Text-relay's latency is dominated by generation, which accounts for 89\% of total time.

Bridge latency breakdown:
\begin{itemize}
    \item Llama encode: 16.9ms (45\%)
    \item Bridge transform: 1.2ms (3\%)
    \item Mistral decode: 19.3ms (52\%)
\end{itemize}

\noindent Figure~\ref{fig:latency} visualizes the latency comparison and breakdown.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/latency_comparison.pdf}
\caption{Latency analysis. \textbf{Left:} Total inference time showing 22.4$\times$ speedup over text-relay. \textbf{Right:} Bridge latency breakdown---the bridge transform itself takes only 1.2ms (3\%).}
\label{fig:latency}
\end{figure}

\subsection{Comparison with Fine-Tuning and Chain-of-Thought}

Table~\ref{tab:stronger_baselines} compares the bridge against stronger baselines: LoRA fine-tuning and chain-of-thought (CoT) text relay.

\begin{table}[t]
\caption{Bridge vs.\ stronger baselines on SST-2. Bridge outperforms LoRA with 18$\times$ fewer parameters and CoT with 85$\times$ lower latency while achieving higher accuracy on both.}
\label{tab:stronger_baselines}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Method & Acc.\ (\%) & Params & Latency \\
\midrule
LoRA (rank=8) & 95.3{\tiny$\pm$0.9} & 3.4M & 113ms \\
CoT-Relay & 89.0 & -- & 3,169ms \\
\midrule
\textbf{Bridge (ours)} & \textbf{96.7}{\tiny$\pm$0.6} & \textbf{188K} & \textbf{37ms} \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Bridge vs.\ LoRA} LoRA fine-tuning achieves 95.3\% accuracy on SST-2 with a rank-8 adapter (3.4M trainable parameters). The bridge achieves 96.7\% with only 188K parameters---\textbf{18$\times$ more parameter-efficient} while being 1.4pp more accurate. This suggests the bridge's cross-model signal provides information that single-model fine-tuning cannot capture.

\paragraph{Bridge vs.\ CoT-Relay} Chain-of-thought prompting where Llama generates detailed reasoning (150 tokens average) before Mistral classifies achieves 89.0\% accuracy at 3,169ms latency. The bridge achieves \textbf{+7.7pp higher accuracy} (96.7\% vs.\ 89.0\%) while being \textbf{85$\times$ faster} (37ms vs.\ 3,169ms). Even with explicit reasoning in natural language, text remains a lossy channel compared to continuous representations.

\subsection{Batched Throughput}

Table~\ref{tab:batched} shows throughput scaling with batch size. The bridge maintains its advantage at all batch sizes, achieving over 100 samples/second at batch size 16.

\begin{table}[t]
\caption{Throughput (samples/sec) at various batch sizes. Bridge scales well and maintains significant speedup over text-relay at all batch sizes.}
\label{tab:batched}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lccc}
\toprule
Batch & Bridge & Direct & Text-Relay \\
\midrule
1 & 7.4 & 8.8 & 0.9 \\
4 & 28.7 & 31.2 & 1.0 \\
16 & 105.7 & 116.0 & -- \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Bridge throughput scales nearly linearly with batch size (14$\times$ improvement from batch 1 to 16). The slight overhead compared to direct Mistral inference (105.7 vs.\ 116.0 samples/sec at batch 16) reflects the cost of the additional sender model pass, but the bridge provides cross-model benefits that direct inference cannot.

\subsection{Inverse Token Scaling}

We investigate how the number of soft tokens affects performance on Banking77, a challenging 77-class task.

\begin{table}[t]
\caption{Effect of soft token count on Banking77 accuracy. Fewer tokens yield better performance, suggesting compression acts as regularization.}
\label{tab:token_scaling}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{cc}
\toprule
Soft Tokens & Accuracy (\%) \\
\midrule
16 & \textbf{21.5} \\
32 & 13.5 \\
64 & 7.5 \\
128 & 1.0 \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Table \ref{tab:token_scaling} shows a striking inverse relationship: increasing tokens from 16 to 128 causes accuracy to collapse from 21.5\% to random (1.3\% for 77 classes). This ``inverse scaling'' phenomenon suggests:

\begin{enumerate}
    \item \textbf{Compression as regularization}: Fewer tokens force the bridge to extract only the most task-relevant information.
    \item \textbf{Mode collapse}: More tokens provide more degrees of freedom that can collapse to trivial solutions.
    \item \textbf{Optimization difficulty}: Higher-dimensional soft prompt spaces are harder to optimize.
\end{enumerate}

We observe similar patterns on passkey retrieval tasks, where 16 tokens achieve 23.4\% digit accuracy vs. 9.8\% for 128 tokens. Figure~\ref{fig:token_scaling} visualizes this inverse relationship.

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/token_scaling.pdf}
\caption{Inverse token scaling on Banking77. Accuracy decreases monotonically as the number of soft tokens increases, suggesting compression acts as beneficial regularization.}
\label{fig:token_scaling}
\end{figure}

\section{Analysis}
\label{sec:analysis}

\subsection{Why Super-Additive Performance?}

The super-additive results on SST-2, AG News, and TREC are surprising. We hypothesize several explanations:

\paragraph{Complementary Representations} Llama and Mistral are trained on different data with different architectures. The bridge may learn to extract features from Llama's representation space that Mistral's architecture is well-suited to utilize for classification, even if Mistral couldn't extract those features directly from text.

\paragraph{Denoising Effect} The bridge acts as an information bottleneck that filters out noise and irrelevant details, passing only task-relevant signals to the receiver.

\paragraph{Implicit Ensemble} The system effectively creates an ensemble where Llama's understanding informs Mistral's decision, combining their capabilities without the information loss of text discretization.

\subsection{Text-Relay Failure Modes}

Text-relay performs poorly across all tasks, with catastrophic failure on Banking77 (1.0\%). Analysis reveals:

\begin{enumerate}
    \item \textbf{Information loss}: Summarization discards fine-grained details needed for 77-way classification.
    \item \textbf{Vocabulary mismatch}: Llama's summaries may use phrasings that don't trigger correct classifications in Mistral.
    \item \textbf{Error propagation}: Mistakes in summarization compound with mistakes in classification.
\end{enumerate}

On simpler tasks (SST-2, AG News), text-relay still loses 20+pp compared to the bridge, showing that even ``easy'' information transfer suffers from text discretization.

\subsection{Comparison with Prompt Compression}

Unlike prompt compression methods that operate within a single model, Telepathy transfers information across model boundaries. This enables:

\begin{itemize}
    \item \textbf{Heterogeneous model collaboration}: Different architectures (Llama, Mistral) can communicate.
    \item \textbf{Capability composition}: Combine a model good at understanding with one good at generation.
    \item \textbf{Parallel inference}: With appropriate scheduling, sender and receiver compute can overlap.
\end{itemize}

\subsection{Handling Architectural Differences}

A key advantage of operating on hidden states rather than tokens is that the bridge naturally handles architectural differences between models:

\paragraph{Vocabulary Size} Llama 3.1 uses a 128K vocabulary while Mistral uses 32K tokens. Since we extract hidden states (not token IDs) from the sender and output soft tokens in the receiver's embedding space, vocabulary differences are irrelevant---the bridge learns a direct mapping between representation spaces.

\paragraph{Positional Encoding} Llama and Mistral use different RoPE (Rotary Position Embedding) configurations with different base frequencies and scaling. The bridge bypasses this entirely: we extract hidden states \emph{after} the sender has applied its positional encoding, and the receiver applies its own RoPE to the soft tokens at their positions in the sequence. The bridge need not understand or translate positional information.

\paragraph{Attention Mechanisms} Llama uses grouped-query attention while Mistral uses sliding window attention with different head configurations. These architectural choices affect how models process sequences internally, but the bridge only sees the resulting hidden state representations---a common ``lingua franca'' of high-dimensional vectors that abstracts away attention implementation details.

\paragraph{Hidden Dimensions} Both Llama 3.1 8B and Mistral 7B use 4096-dimensional hidden states, but our bridge architecture includes input and output projection layers that can map between arbitrary dimensions. This enables future extensions to model pairs with different hidden sizes.

This architectural agnosticism is why the same bridge design works for heterogeneous models without modification---we communicate through the universal language of dense representations rather than model-specific tokenization or attention patterns.

\subsection{Bidirectional Transfer}

To verify that communication works in both directions, we train a reverse bridge (Mistral$\to$Llama) on SST-2 using identical hyperparameters. Table~\ref{tab:bidirectional} shows that both directions achieve strong performance:

\begin{table}[h]
\caption{Bidirectional transfer on SST-2. Both directions achieve super-additive performance, with Mistral$\to$Llama slightly outperforming the forward direction.}
\label{tab:bidirectional}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{lcc}
\toprule
Direction & Accuracy (\%) & vs.\ Individual Models \\
\midrule
Llama$\to$Mistral & 96.7 $\pm$ 0.6 & +4.7pp over Llama \\
Mistral$\to$Llama & \textbf{97.2 $\pm$ 0.6} & +5.2pp over Llama \\
\midrule
Llama Direct & 92.0 & --- \\
Mistral Direct & 88.5 & --- \\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Both directions exhibit super-additive performance, exceeding either model operating independently. Interestingly, Mistral$\to$Llama (97.2\%) slightly outperforms Llama$\to$Mistral (96.7\%), suggesting that Llama may be a marginally better decoder for this task. The symmetric success demonstrates that the bridge architecture generalizes across sender-receiver configurations without modification.

\subsection{Soft Token Interpretability}

To understand what information the bridge encodes, we analyze each soft token by finding its nearest neighbors in Mistral's vocabulary (cosine similarity). On SST-2, we observe partially interpretable patterns:

\paragraph{Negative Sentiment Encoding} For negative reviews (e.g., ``unflinchingly bleak and desperate''), the nearest vocabulary tokens include semantically relevant words: \texttt{negative} (similarity 0.08), \texttt{moral}, \texttt{lower}, \texttt{blank}. Remarkably, the literal word ``negative'' appears as the top nearest neighbor for 3 of 8 soft tokens. The bridge learned to encode sentiment in a way that maps directly to Mistral's vocabulary representation of the label.

\paragraph{Positive Sentiment Encoding} For positive reviews (e.g., ``charming and often affecting journey''), nearest neighbors include less directly interpretable tokens: \texttt{Survey}, \texttt{wished}, \texttt{independent}, \texttt{endless}. This asymmetry suggests the bridge may encode positive sentiment through absence of negative signals rather than explicit positive markers.

\paragraph{Token Geometry} The 8 soft tokens show high pairwise cosine similarity (0.97-0.99), indicating they encode correlated rather than independent information. This redundancy may provide robustness---the receiver can extract the signal even if individual tokens are noisy.

These findings support the information bottleneck hypothesis: compression forces the bridge to discard irrelevant details and encode only task-essential information (sentiment polarity), which it does in a partially human-interpretable way.

\section{Limitations and Future Work}
\label{sec:limitations}

\paragraph{Classification Only} Our experiments focus exclusively on classification tasks. We evaluated the bridge on GSM8K math reasoning, where Llama and Mistral achieve 19.5\% and 18.5\% respectively with direct prompting. The bridge achieved only 1.5\%---a complete failure. The bridge generates incoherent solutions unrelated to the input problems, suggesting that reasoning transfer requires fundamentally different approaches than classification signal transfer. Extending to generation and reasoning tasks remains important future work.

\paragraph{Task-Specific Training} Bridges must be trained per-task. We did not observe meaningful zero-shot transfer between tasks (e.g., SST-2$\to$AG News). Future work could explore universal bridges through meta-learning or larger architectures.

\paragraph{More Model Pairs} We demonstrate bidirectional Llama$\leftrightarrow$Mistral transfer; future work should validate across more model families (e.g., Gemma, Qwen) and sizes.

\paragraph{Theoretical Understanding} Why does compression help? Why is performance super-additive? Why does reasoning fail while classification succeeds? Deeper theoretical analysis could inform better architecture design and identify which tasks are amenable to cross-model communication.

\section{Conclusion}
\label{sec:conclusion}

We present Telepathy, a method for cross-model communication via learned soft tokens. Our lightweight bridge (188K parameters) enables a sender LLM to condition a receiver LLM's inference without text generation, achieving:

\begin{itemize}
    \item \textbf{22.4$\times$ lower latency} than text-relay (37ms vs. 835ms)
    \item \textbf{Sender model is essential}: Prompt-tuning alone achieves random chance (49.5\%), while Bridge achieves 96.7\% (+47pp from Llama's hidden states)
    \item \textbf{Super-additive performance} on SST-2 (96.7\% vs. 92\%/88.5\%) and AG News (90.7\% vs. 79\%/79\%)
    \item \textbf{Bidirectional transfer}: Both Llama$\to$Mistral (96.7\%) and Mistral$\to$Llama (97.2\%) achieve strong performance
    \item \textbf{Inverse token scaling} where fewer soft tokens yield better performance
\end{itemize}

These results demonstrate that continuous representations can be a more efficient and effective communication channel between LLMs than discrete text. The prompt-tuning baseline definitively shows that the sender model's hidden states---not merely training---drive the performance gains. Telepathy opens new possibilities for building collaborative multi-model systems with lower latency and higher accuracy.

\bibliography{telepathy}
\bibliographystyle{mlsys2025}

\appendix
\section{Additional Experimental Details}
\label{app:details}

\subsection{Hardware and Training Time}
All experiments were conducted on NVIDIA H100 80GB GPUs. Training times:
\begin{itemize}
    \item SST-2/AG News (2000 steps): 3.5 minutes
    \item TREC (2000 steps): 3.5 minutes
    \item Banking77 (3000 steps): 5.0 minutes
\end{itemize}
Total training time for all bridge variants: approximately 42 minutes.

\subsection{Multi-Seed Results}
All experiments were run with 3 seeds (42, 123, 456) for statistical rigor. Results reported as mean $\pm$ std:
\begin{itemize}
    \item SST-2 Bridge (Llama$\to$Mistral): 96.7\% $\pm$ 0.6\% (seeds: 96.5, 96.0, 97.5)
    \item SST-2 Bridge (Mistral$\to$Llama): 97.2\% $\pm$ 0.6\% (seeds: 97.0, 98.0, 96.5)
    \item AG News Bridge: 90.7\% $\pm$ 0.5\% (seeds: 90.0, 91.0, 91.0)
    \item TREC Bridge: 95.3\% $\pm$ 0.3\% (seeds: 95.0, 95.5, 95.5)
    \item Prompt-Tuning SST-2: 49.5\% $\pm$ 0.0\% (all seeds identical)
    \item Prompt-Tuning AG News: 19.8\% $\pm$ 7.5\% (seeds: 30.5, 14.5, 14.5)
    \item Prompt-Tuning TREC: 19.0\% $\pm$ 5.0\% (seeds: 14.5, 26.0, 16.5)
\end{itemize}
The low variance in Bridge results ($\leq$0.6\%) indicates stable training across all configurations, including bidirectional transfer. The prompt-tuning baseline's high variance on AG News and TREC reflects random guessing behavior.

\subsection{Hyperparameter Sensitivity}
We found performance relatively robust to hyperparameters within reasonable ranges:
\begin{itemize}
    \item Learning rate: $10^{-5}$ to $10^{-3}$ all work, $10^{-4}$ slightly best
    \item Batch size: 4-16 similar results
    \item Diversity weight: 0.05-0.2 prevents mode collapse
    \item Source layer: We use layer 16 for SST-2 and layer 31 for AG News/TREC. Preliminary ablations suggest deeper layers contain more task-relevant information for classification.
\end{itemize}

\subsection{Layer Selection}
We extract hidden states from Llama's intermediate layers rather than the final output logits. For SST-2, we found layer 16 sufficient (96.7\% accuracy), while AG News and TREC benefited from the final layer (31). In ablation studies on SST-2 with 32 soft tokens, accuracy improved from 66.5\% (layer 0) to 88.0\% (layer 8) to 92.0\% (layer 16) to 94.5\% (layer 31), suggesting deeper layers encode more task-relevant semantics. The optimal layer may vary by task complexity.

\end{document}
