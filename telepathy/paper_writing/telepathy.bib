% Transformer and Foundation Papers
@inproceedings{vaswani2017attention,
  title={Attention is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

% LLaMA Papers
@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={{Llama 2}: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{llama3,
  title={The {Llama 3} Herd of Models},
  author={{Llama Team, AI @ Meta}},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

% Mistral
@article{jiang2023mistral,
  title={{Mistral 7B}},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

% Prompt Tuning and Prefix Tuning
@inproceedings{lester2021power,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@inproceedings{li2021prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  pages={4582--4597},
  year={2021}
}

% Perceiver
@inproceedings{jaegle2021perceiver,
  title={Perceiver: General Perception with Iterative Attention},
  author={Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  booktitle={International Conference on Machine Learning},
  pages={4651--4664},
  year={2021}
}

@article{jaegle2021perceiverio,
  title={{Perceiver IO}: A General Architecture for Structured Inputs \& Outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

% Vision-Language Models
@inproceedings{li2023blip2,
  title={{BLIP-2}: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={19730--19742},
  year={2023}
}

@inproceedings{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

% Model Stitching
@article{bansal2021revisiting,
  title={Revisiting Model Stitching to Compare Neural Representations},
  author={Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={225--236},
  year={2021}
}

@article{pan2023stitchable,
  title={Stitchable Neural Networks},
  author={Pan, Zizheng and Cai, Jianfei and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2302.06586},
  year={2023}
}

@article{crosslora2024,
  title={{Cross-LoRA}: A Data-Free {LoRA} Transfer Framework across Heterogeneous {LLMs}},
  author={Anonymous},
  journal={arXiv preprint arXiv:2508.05232},
  year={2024}
}

% Knowledge Distillation
@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{gu2024minillm,
  title={{MiniLLM}: Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2024}
}

@article{xu2024survey,
  title={A Survey on Knowledge Distillation of Large Language Models},
  author={Xu, Xiaohan and Li, Ming and Tao, Chongyang and Shen, Tao and Cheng, Reynold and Li, Jinyang and Xu, Can and Tao, Dacheng and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2402.13116},
  year={2024}
}

% Multi-Agent Systems
@article{multiagent2025survey,
  title={Multi-Agent Collaboration Mechanisms: A Survey of {LLMs}},
  author={Anonymous},
  journal={arXiv preprint arXiv:2501.06322},
  year={2025}
}

@article{wu2023autogen,
  title={{AutoGen}: Enabling Next-Gen {LLM} Applications via Multi-Agent Conversation},
  author={Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and others},
  journal={arXiv preprint arXiv:2308.08155},
  year={2023}
}

% Prompt Compression
@inproceedings{jiang2023llmlingua,
  title={{LLMLingua}: Compressing Prompts for Accelerated Inference of Large Language Models},
  author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={13358--13376},
  year={2023}
}

@article{ge2024incontext,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Ge, Tao and Hu, Jing and Wang, Lei and Wang, Xun and Chen, Si-Qing and Wei, Furu},
  journal={arXiv preprint arXiv:2307.06945},
  year={2024}
}

@article{li2024500x,
  title={500xCompressor: Generalized Prompt Compression for Large Language Models},
  author={Li, Zongqian and Liu, Yixuan and Zhu, Yifan and Liu, Xiaoyu and Xiong, Zhen and Liu, Hui and Chen, Xiaofan and Zhou, Jie},
  journal={arXiv preprint arXiv:2410.11324},
  year={2024}
}

% LoRA
@inproceedings{hu2022lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

% Datasets
@inproceedings{socher2013recursive,
  title={Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{zhang2015character,
  title={Character-level Convolutional Networks for Text Classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@inproceedings{li2002learning,
  title={Learning Question Classifiers},
  author={Li, Xin and Roth, Dan},
  booktitle={COLING 2002: The 19th International Conference on Computational Linguistics},
  year={2002}
}

@inproceedings{casanueva2020efficient,
  title={Efficient Intent Detection with Dual Sentence Encoders},
  author={Casanueva, I{\~n}igo and Tem{\v{c}}inas, Tadas and Gerz, Daniela and Henderson, Matthew and Vuli{\'c}, Ivan},
  booktitle={Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI},
  pages={38--45},
  year={2020}
}

% BERT
@inproceedings{devlin2019bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={4171--4186},
  year={2019}
}

% Additional References
@article{liu2023ptunning,
  title={{P-Tuning v2}: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2023}
}

@inproceedings{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  booktitle={OpenAI Blog},
  year={2019}
}

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% New citations 2024-2025

@article{promptbridge2024,
  title={{PromptBridge}: Cross-Model Prompt Transfer for Large Language Models},
  author={Liu, Zhuohan and others},
  journal={arXiv preprint arXiv:2512.01420},
  year={2024}
}

@article{stitchllm2025,
  title={{StitchLLM}: Serving {LLMs}, One Block at a Time},
  author={Anonymous},
  journal={Proceedings of the Association for Computational Linguistics},
  year={2025}
}

@article{modelstitching2024,
  title={Transferring Linear Features Across Language Models With Model Stitching},
  author={Anonymous},
  journal={arXiv preprint arXiv:2506.06609},
  year={2024}
}

@inproceedings{xu2024soft,
  title={Soft Prompt Recovers Compressed {LLMs}, Transferably},
  author={Xu, Zhaozhuo and others},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{hiddentransfer2024,
  title={Parallel Decoding via Hidden Transfer for Lossless Large Language Model Acceleration},
  author={Anonymous},
  journal={arXiv preprint arXiv:2404.12022},
  year={2024}
}

@article{cobbe2021gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

% Diffusion Transformers
@inproceedings{peebles2023dit,
  title={Scalable Diffusion Models with Transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

% Information Bottleneck
@article{tishby2015deep,
  title={Deep Learning and the Information Bottleneck Principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  journal={IEEE Information Theory Workshop},
  pages={1--5},
  year={2015}
}

% Rectified Flow
@article{liu2022rectified,
  title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}
