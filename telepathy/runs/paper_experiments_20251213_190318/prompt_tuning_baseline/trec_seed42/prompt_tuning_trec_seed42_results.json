{
  "experiment": "prompt_tuning_baseline_trec",
  "timestamp": "2025-12-13T20:37:14.059200",
  "config": {
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "dataset": "trec",
    "soft_tokens": 8,
    "lr": 0.0002,
    "batch_size": 16,
    "grad_accum": 2,
    "steps": 2000,
    "warmup_steps": 100,
    "output_dir": "runs/paper_experiments_20251213_190318/prompt_tuning_baseline/trec_seed42",
    "save_every": 500,
    "eval_every": 200,
    "seed": 42,
    "bf16": true
  },
  "num_classes": 6,
  "final_results": {
    "accuracy": 14.5,
    "correct": 29,
    "total": 200
  },
  "training_log": [
    {
      "step": 200,
      "accuracy": 10.0,
      "correct": 5,
      "total": 50
    },
    {
      "step": 400,
      "accuracy": 16.0,
      "correct": 8,
      "total": 50
    },
    {
      "step": 600,
      "accuracy": 32.0,
      "correct": 16,
      "total": 50
    },
    {
      "step": 800,
      "accuracy": 26.0,
      "correct": 13,
      "total": 50
    },
    {
      "step": 1000,
      "accuracy": 16.0,
      "correct": 8,
      "total": 50
    },
    {
      "step": 1200,
      "accuracy": 10.0,
      "correct": 5,
      "total": 50
    },
    {
      "step": 1400,
      "accuracy": 32.0,
      "correct": 16,
      "total": 50
    },
    {
      "step": 1600,
      "accuracy": 10.0,
      "correct": 5,
      "total": 50
    },
    {
      "step": 1800,
      "accuracy": 10.0,
      "correct": 5,
      "total": 50
    },
    {
      "step": 2000,
      "accuracy": 10.0,
      "correct": 5,
      "total": 50
    }
  ],
  "baseline_type": "prompt_tuning_no_sender"
}