{
  "experiment": "prompt_tuning_baseline_agnews",
  "timestamp": "2025-12-13T20:30:52.605244",
  "config": {
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "dataset": "agnews",
    "soft_tokens": 8,
    "lr": 0.0002,
    "batch_size": 16,
    "grad_accum": 2,
    "steps": 2000,
    "warmup_steps": 100,
    "output_dir": "runs/paper_experiments_20251213_190318/prompt_tuning_baseline/agnews_seed456",
    "save_every": 500,
    "eval_every": 200,
    "seed": 456,
    "bf16": true
  },
  "num_classes": 4,
  "final_results": {
    "accuracy": 14.5,
    "correct": 29,
    "total": 200
  },
  "training_log": [
    {
      "step": 200,
      "accuracy": 20.0,
      "correct": 10,
      "total": 50
    },
    {
      "step": 400,
      "accuracy": 4.0,
      "correct": 2,
      "total": 50
    },
    {
      "step": 600,
      "accuracy": 22.0,
      "correct": 11,
      "total": 50
    },
    {
      "step": 800,
      "accuracy": 20.0,
      "correct": 10,
      "total": 50
    },
    {
      "step": 1000,
      "accuracy": 4.0,
      "correct": 2,
      "total": 50
    },
    {
      "step": 1200,
      "accuracy": 22.0,
      "correct": 11,
      "total": 50
    },
    {
      "step": 1400,
      "accuracy": 4.0,
      "correct": 2,
      "total": 50
    },
    {
      "step": 1600,
      "accuracy": 22.0,
      "correct": 11,
      "total": 50
    },
    {
      "step": 1800,
      "accuracy": 54.0,
      "correct": 27,
      "total": 50
    },
    {
      "step": 2000,
      "accuracy": 4.0,
      "correct": 2,
      "total": 50
    }
  ],
  "baseline_type": "prompt_tuning_no_sender"
}